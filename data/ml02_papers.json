{
  "updated": "2026-01-06",
  "total": 144,
  "owasp_id": "ML02",
  "owasp_name": "Data Poisoning Attack",
  "description": "Attacks that manipulate training data to compromise machine learning models.\n        This includes data poisoning, training data manipulation, backdoor insertion\n        during training, label flipping attacks, and any technique that corrupts\n        the training process by modifying the dataset.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "dc5d70b48a04a6e9c3310e6fda30748a37031d0c",
      "title": "DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation",
      "abstract": "The threat of data-poisoning backdoor attacks on learning algorithms typically comes from the labeled data. However, in deep semi-supervised learning (SSL), unknown threats mainly stem from the unlabeled data. In this paper, we propose a novel deep hidden backdoor (DeHiB) attack scheme for SSL-based systems. In contrast to the conventional attacking methods, the DeHiB can inject malicious unlabeled training data to the semi-supervised learner so as to enable the SSL model to output premeditated results. In particular, a robust adversarial perturbation generator regularized by a unified objective function is proposed to generate poisoned data. To alleviate the negative impact of the trigger patterns on model accuracy and improve the attack success rate, a novel contrastive data poisoning strategy is designed. Using the proposed data poisoning scheme, one can implant the backdoor into the SSL model using the raw data without hand-crafted labels. Extensive experiments based on CIFAR10 and CIFAR100 datasets demonstrated the effectiveness and crypticity of the proposed scheme.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Zhicong Yan",
        "Gaolei Li",
        "Yuan Tian",
        "Jun Wu",
        "Shenghong Li",
        "Mingzhe Chen",
        "H. Poor"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/dc5d70b48a04a6e9c3310e6fda30748a37031d0c",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17266/17073",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9df35283dc71e95f0451a478b531b544dd1d6d0d",
      "title": "Invisible Backdoor Attack with Sample-Specific Triggers",
      "abstract": "Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-defined trigger. Existing backdoor attacks usually adopt the setting that triggers are sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-specific. In our attack, we only need to modify certain training samples with invisible perturbation, while not need to manipulate other training components (e.g., training loss, and model structure) as required in many existing attacks. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses. The code will be available at https://github.com/yuezunli/ISSBA.",
      "year": 2020,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Yuezun Li",
        "Yiming Li",
        "Baoyuan Wu",
        "Longkang Li",
        "R. He",
        "Siwei Lyu"
      ],
      "citation_count": 586,
      "url": "https://www.semanticscholar.org/paper/9df35283dc71e95f0451a478b531b544dd1d6d0d",
      "pdf_url": "https://arxiv.org/pdf/2012.03816",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "perturbation attack",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cb8889d0fd4db6baa3ffb1336e16724a6dc323de",
      "title": "A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning",
      "abstract": "Backdoor attacks against CNNs represent a new threat against deep learning systems, due to the possibility of corrupting the training set so to induce an incorrect behaviour at test time. To avoid that the trainer recognises the presence of the corrupted samples, the corruption of the training set must be as stealthy as possible. Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. This greatly reduces the stealthiness of the attack, since samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. Results obtained on the MNIST digits recognition task and the traffic signs classification task show that backdoor attacks without label poisoning are indeed possible, thus raising a new alarm regarding the use of deep learning in security-critical applications.",
      "year": 2019,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "M. Barni",
        "Kassem Kallas",
        "B. Tondi"
      ],
      "citation_count": 421,
      "url": "https://www.semanticscholar.org/paper/cb8889d0fd4db6baa3ffb1336e16724a6dc323de",
      "pdf_url": "",
      "publication_date": "2019-02-12",
      "keywords_matched": [
        "perturbation attack",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "24a29d0f5f359169918a6b2f5ec9ea9bf00847eb",
      "title": "Defending Against Data Poisoning Attack in Federated Learning With Non-IID Data",
      "abstract": "Federated learning (FL) is an emerging paradigm that allows participants to collaboratively train deep learning tasks while protecting the privacy of their local data. However, the absence of central server control in distributed environments exposes a vulnerability to data poisoning attacks, where adversaries manipulate the behavior of compromised clients by poisoning local data. In particular, data poisoning attacks against FL can have a drastic impact when the participant\u2019s local data is non-independent and identically distributed (non-IID). Most existing defense strategies have demonstrated promising results in mitigating FL poisoning attacks, however, fail to maintain their effectiveness with non-IID data. In this work, we propose an effective defense framework, FL data augmentation (FLDA), which defends against data poisoning attacks through local data mixup on the clients. In addition, to mitigate the non-IID effect by exploiting the limited local data, we propose a gradient detection strategy to reduce the proportion of malicious clients and raise benign clients. Experimental results on datasets show that FLDA can effectively reduce the poisoning success rate and improve the global model training accuracy under poisoning attacks for non-IID data. Furthermore, FLDA can increase the FL accuracy by more than 12% after detecting malicious clients.",
      "year": 2024,
      "venue": "IEEE Transactions on Computational Social Systems",
      "authors": [
        "Chunyong Yin",
        "Qingkui Zeng"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/24a29d0f5f359169918a6b2f5ec9ea9bf00847eb",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "450fda122a6df29ebf725ce337e0b0a1b5a8eefb",
      "title": "A GAN-Based Data Poisoning Attack Against Federated Learning Systems and Its Countermeasure",
      "abstract": "As a distributed machine learning paradigm, federated learning (FL) is collaboratively carried out on privately owned datasets but without direct data access. Although the original intention is to allay data privacy concerns,\"available but not visible\"data in FL potentially brings new security threats, particularly poisoning attacks that target such\"not visible\"local data. Initial attempts have been made to conduct data poisoning attacks against FL systems, but cannot be fully successful due to their high chance of causing statistical anomalies. To unleash the potential for truly\"invisible\"attacks and build a more deterrent threat model, in this paper, a new data poisoning attack model named VagueGAN is proposed, which can generate seemingly legitimate but noisy poisoned data by untraditionally taking advantage of generative adversarial network (GAN) variants. Capable of manipulating the quality of poisoned data on demand, VagueGAN enables to trade-off attack effectiveness and stealthiness. Furthermore, a cost-effective countermeasure named Model Consistency-Based Defense (MCD) is proposed to identify GAN-poisoned data or models after finding out the consistency of GAN outputs. Extensive experiments on multiple datasets indicate that our attack method is generally much more stealthy as well as more effective in degrading FL performance with low complexity. Our defense method is also shown to be more competent in identifying GAN-poisoned data or models. The source codes are publicly available at \\href{https://github.com/SSssWEIssSS/VagueGAN-Data-Poisoning-Attack-and-Its-Countermeasure}{https://github.com/SSssWEIssSS/VagueGAN-Data-Poisoning-Attack-and-Its-Countermeasure}.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Wei Sun",
        "Bo Gao",
        "Ke Xiong",
        "Yuwei Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/450fda122a6df29ebf725ce337e0b0a1b5a8eefb",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "data poisoning attack",
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "21a1714b36023814bacba496a0cd96792f0e41cb",
      "title": "A Novel Data Poisoning Attack in Federated Learning based on Inverted Loss Function",
      "abstract": null,
      "year": 2023,
      "venue": "Computers & security",
      "authors": [
        "Prajjwal Gupta",
        "Krishna Yadav",
        "B. B. Gupta",
        "M. Alazab",
        "T. Gadekallu"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/21a1714b36023814bacba496a0cd96792f0e41cb",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c3fe9e92e0a4e9eaf5972205b93e6dd2d2412a5b",
      "title": "LOKI: A Practical Data Poisoning Attack Framework Against Next Item Recommendations",
      "abstract": "Due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks, where malicious samples are injected into the training set of the recommendation system to manipulate its recommendation results. Existing attack approaches are either based on heuristic rules or designed against specific recommendation approaches. The former suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted. Thus, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also use the influence function to efficiently estimate the influence of injected samples on recommendation results, without re-training the models. Extensive experiments on multiple datasets against four representative recommendation models show that the proposed LOKI outperformances existing method. We also discuss the characteristics of vulnerable users/items, and evaluate whether anomaly detection methods can be used to mitigate the impact of data poisoning attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": [
        "Hengtong Zhang",
        "Yaliang Li",
        "Bolin Ding",
        "Jing Gao"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/c3fe9e92e0a4e9eaf5972205b93e6dd2d2412a5b",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09f43150f01824ddd2bcc56629f87e26df57b0c3",
      "title": "Sharpness-Aware Data Poisoning Attack",
      "abstract": "Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the uncertainty of the re-training process after the injection of poisoning samples, including the re-training initialization or algorithms. To address this challenge, we propose a novel attack method called ''Sharpness-Aware Data Poisoning Attack (SAPA)''. In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the worst re-trained model. It helps enhance the preservation of the poisoning effect, regardless of the specific retraining procedure employed. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "P. He",
        "Han Xu",
        "J. Ren",
        "Yingqian Cui",
        "Hui Liu",
        "C. Aggarwal",
        "Jiliang Tang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/09f43150f01824ddd2bcc56629f87e26df57b0c3",
      "pdf_url": "http://arxiv.org/pdf/2305.14851",
      "publication_date": "2023-05-24",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e2ec7ac328825956946a179851630ac695bb4add",
      "title": "Research on Data Poisoning Attack against Smart Grid Cyber\u2013Physical System Based on Edge Computing",
      "abstract": "Data poisoning attack is a well-known attack against machine learning models, where malicious attackers contaminate the training data to manipulate critical models and predictive outcomes by masquerading as terminal devices. As this type of attack can be fatal to the operation of a smart grid, addressing data poisoning is of utmost importance. However, this attack requires solving an expensive two-level optimization problem, which can be challenging to implement in resource-constrained edge environments of the smart grid. To mitigate this issue, it is crucial to enhance efficiency and reduce the costs of the attack. This paper proposes an online data poisoning attack framework based on the online regression task model. The framework achieves the goal of manipulating the model by polluting the sample data stream that arrives at the cache incrementally. Furthermore, a point selection strategy based on sample loss is proposed in this framework. Compared to the traditional random point selection strategy, this strategy makes the attack more targeted, thereby enhancing the attack\u2019s efficiency. Additionally, a batch-polluting strategy is proposed in this paper, which synchronously updates the poisoning points based on the direction of gradient ascent. This strategy reduces the number of iterations required for inner optimization and thus reduces the time overhead. Finally, multiple experiments are conducted to compare the proposed method with the baseline method, and the evaluation index of loss over time is proposed to demonstrate the effectiveness of the method. The results show that the proposed method outperforms the existing baseline method in both attack effectiveness and overhead.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Yanxu Zhu",
        "Hong Wen",
        "Runhui Zhao",
        "Yixin Jiang",
        "Qiang Liu",
        "Peng Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e2ec7ac328825956946a179851630ac695bb4add",
      "pdf_url": "https://www.mdpi.com/1424-8220/23/9/4509/pdf?version=1683529740",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a0528b1573be8517af2a3f92672cc95f575412e0",
      "title": "VFedAD: A Defense Method Based on the Information Mechanism Behind the Vertical Federated Data Poisoning Attack",
      "abstract": "In recent years, federated learning has achieved remarkable results in the medical and financial fields, but various attacks have always plagued federated learning. Data poisoning attack and defense research in horizontal federated learning are sufficient, yet vertical federated data poisoning attack and defense remains an open area due to two challenges: (1) Complex data distributions lead to immense attack possibilities, and (2) defense methods are insufficient for complex data distributions. We have discovered that from the perspective of information theory, the above challenges can be addressed elegantly and succinctly with a solution. We first reveal the information-theoretic mechanisms underlying vertical federated data poisoning attacks and then propose an unsupervised vertical federated data poisoning defense method (VFedAD) based on information theory. VFedAD learns semantic-rich client data representations through contrastive learning task and cross-client prediction task to identify anomalies. Experiments show VFedAD effectively detects vertical federated anomalies, protecting subsequent algorithms from vertical federated data poisoning attacks.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Jinrong Lai",
        "Tong Wang",
        "Chuan Chen",
        "Yihao Li",
        "Zibin Zheng"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/a0528b1573be8517af2a3f92672cc95f575412e0",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a52f86028fc98924de4e3c03056e1f5826b48467",
      "title": "VagueGAN: A GAN-Based Data Poisoning Attack Against Federated Learning Systems",
      "abstract": "Federated learning (FL) is a privacy-preserving distributed learning paradigm relying on but without directly accessing privately owned datasets. However, the \u2018\u2018available but not visible\u2019\u2019 nature of training data in FL leads to security risks. In particular, \u2018\u2018not visible\u2019\u2019 local data can easily become the best targets of poisoning attacks. Although existing data poisoning methods may successfully attack FL systems, they mostly lead to significant data statistical changes and thus can be not hard to detect. In this paper, we propose VagueGAN, a new data poisoning attack model that unconventionally leverages the power of generative adversarial network (GAN) to generate seemingly legitimate vague data with appropriate amounts of poisonous noise. The quality of such vague data can be controlled on demand to achieve a balanced trade-off between attack effectiveness and stealthiness. Extensive experiments show that data poisoning attacks enhanced by our VagueGAN not only better degrade FL outcomes with low efforts but also are generally much less detectable.",
      "year": 2023,
      "venue": "Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks",
      "authors": [
        "Wei Sun",
        "Bo Gao",
        "Ke Xiong",
        "Yang Lu",
        "Yuwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a52f86028fc98924de4e3c03056e1f5826b48467",
      "pdf_url": "",
      "publication_date": "2023-09-11",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "592c7349e7ee1a7d6f2d1ddd468b6824b2d2217c",
      "title": "Data Poisoning Attack against Recommender System Using Incomplete and Perturbed Data",
      "abstract": "Recent studies reveal that recommender systems are vulnerable to data poisoning attack due to their openness nature. In data poisoning attack, the attacker typically recruits a group of controlled users to inject well-crafted user-item interaction data into the recommendation model's training set to modify the model parameters as desired. Thus, existing attack approaches usually require full access to the training data to infer items' characteristics and craft the fake interactions for controlled users. However, such attack approaches may not be feasible in practice due to the attacker's limited data collection capability and the restricted access to the training data, which sometimes are even perturbed by the privacy preserving mechanism of the service providers. Such design-reality gap may cause failure of attacks. In this paper, we fill the gap by proposing two novel adversarial attack approaches to handle the incompleteness and perturbations in user-item interaction data. First, we propose a bi-level optimization framework that incorporates a probabilistic generative model to find the users and items whose interaction data is sufficient and has not been significantly perturbed, and leverage these users and items' data to craft fake user-item interactions. Moreover, we reverse the learning process of recommendation models and develop a simple yet effective approach that can incorporate context-specific heuristic rules to handle data incompleteness and perturbations. Extensive experiments on two datasets against three representative recommendation models show that the proposed approaches can achieve better attack performance than existing approaches.",
      "year": 2021,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Hengtong Zhang",
        "Changxin Tian",
        "Yaliang Li",
        "Lu Su",
        "Nan Yang",
        "Wayne Xin Zhao",
        "Jing Gao"
      ],
      "citation_count": 71,
      "url": "https://www.semanticscholar.org/paper/592c7349e7ee1a7d6f2d1ddd468b6824b2d2217c",
      "pdf_url": "",
      "publication_date": "2021-08-14",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e15700f781806743d9dcd17eea67c46738c2d9d5",
      "title": "Deep Reinforcement Learning for Partially Observable Data Poisoning Attack in Crowdsensing Systems",
      "abstract": "Crowdsensing systems collect various types of data from sensors embedded on mobile devices owned by individuals. These individuals are commonly referred to as workers that complete tasks published by crowdsensing systems. Because of the relative lack of control over worker identities, crowdsensing systems are susceptible to data poisoning attacks which interfering with data analysis results by injecting fake data conflicting with ground truth. Frameworks like TruthFinder can resolve data conflicts by evaluating the trustworthiness of the data providers. These frameworks somehow make crowdsensing systems more robust since they can limit the impact of dirty data by reducing the value of unreliable workers. However, previous work has shown that TruthFinder may also be affected by the data poisoning attack when the malicious workers have access to global information. In this article, we focus on partially observable data poisoning attacks in crowdsensing systems. We show that even if the malicious workers only have access to local information, they can find effective data poisoning attack strategies to interfere with crowdsensing systems with TruthFinder. First, we formally model the problem of partially observable data poisoning attack against crowdsensing systems. Then, we propose a data poisoning attack method based on deep reinforcement learning, which helps malicious workers jeopardize with TruthFinder while hiding themselves. Based on the method, the malicious workers can learn from their attack attempts and evolve the poisoning strategies continuously. Finally, we conduct experiments on real-life data sets to verify the effectiveness of the proposed method.",
      "year": 2020,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Mohan Li",
        "Yanbin Sun",
        "Hui Lu",
        "Sabita Maharjan",
        "Zhihong Tian"
      ],
      "citation_count": 114,
      "url": "https://www.semanticscholar.org/paper/e15700f781806743d9dcd17eea67c46738c2d9d5",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a1d81ca3a075201a2fcf5604c6893acd7c6cf8a1",
      "title": "Bandit-based data poisoning attack against federated learning for autonomous driving models",
      "abstract": null,
      "year": 2023,
      "venue": "Expert systems with applications",
      "authors": [
        "Shuo Wang",
        "Qianmu Li",
        "Zhiyong Cui",
        "J. Hou",
        "Chanying Huang"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/a1d81ca3a075201a2fcf5604c6893acd7c6cf8a1",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "782ea68e62f5215774c01c48ae9bd7b226148050",
      "title": "Practical Data Poisoning Attack against Next-Item Recommendation",
      "abstract": "Online recommendation systems make use of a variety of information sources to provide users the items that users are potentially interested in. However, due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks. Existing attack approaches are either based on simple heuristic rules or designed against specific recommendations approaches. The former often suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we focus on a general next-item recommendation setting and propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted. Given these real-world restrictions, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also propose to use the influence function to efficiently estimate the influence of injected samples on the recommendation results, without re-training the models within the simulator. Extensive experiments on two datasets against four representative recommendation models show that the proposed LOKI achieves better attacking performance than existing methods.",
      "year": 2020,
      "venue": "The Web Conference",
      "authors": [
        "Hengtong Zhang",
        "Yaliang Li",
        "Bolin Ding",
        "Jing Gao"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/782ea68e62f5215774c01c48ae9bd7b226148050",
      "pdf_url": "https://arxiv.org/pdf/2004.03728",
      "publication_date": "2020-04-07",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bcad53ad126b33ebb2d0a5dcd595a95c489e7300",
      "title": "Label Flipping Data Poisoning Attack Against Wearable Human Activity Recognition System",
      "abstract": "Human Activity Recognition (HAR) is a problem of interpreting sensor data to human movement using an efficient machine learning (ML) approach. The HAR systems rely on data from untrusted users, making them susceptible to data poisoning attacks. In a poisoning attack, attackers manipulate the sensor readings to contaminate the training set, misleading the HAR to produce erroneous outcomes. This paper presents the design of a label flipping data poisoning attack for a HAR system, where the label of a sensor reading is maliciously changed in the data collection phase. Due to high noise and uncertainty in the sensing environment, such an attack poses a severe threat to the recognition system. Besides, vulnerability to label flipping attacks is dangerous when activity recognition models are deployed in safety-critical applications. This paper shades light on how to carry out the attack in practice through smartphone-based sensor data collection applications. This is an earlier research work, to our knowledge, that explores attacking the HAR models via label flipping poisoning. We implement the proposed attack and test it on activity recognition models based on the following machine learning algorithms: multi-layer perceptron, decision tree, random forest, and XGBoost. Finally, we evaluate the effectiveness of a K-nearest neighbors (KNN)-based defense mechanism against the proposed attack.",
      "year": 2022,
      "venue": "IEEE Symposium Series on Computational Intelligence",
      "authors": [
        "A. Shahid",
        "Ahmed Imteaj",
        "Peter Y. Wu",
        "Diane A. Igoche",
        "Tauhidul Alam"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/bcad53ad126b33ebb2d0a5dcd595a95c489e7300",
      "pdf_url": "https://arxiv.org/pdf/2208.08433",
      "publication_date": "2022-08-17",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack",
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ce2b1a7312376b3a62b5de554db5b4a9a909a35",
      "title": "Securing federated learning: a defense strategy against targeted data poisoning attack",
      "abstract": null,
      "year": 2025,
      "venue": "Discover Internet of Things",
      "authors": [
        "Ansam Khraisat",
        "Ammar Alazab",
        "M. Alazab",
        "Tony Jan",
        "Sarabjot Singh",
        "Md. Ashraf Uddin"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7ce2b1a7312376b3a62b5de554db5b4a9a909a35",
      "pdf_url": "",
      "publication_date": "2025-02-24",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4be29e1cd866ab31f83f03723e2f307cdc1faab0",
      "title": "Data Poisoning Attack against Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding (KGE) is a technique for learning continuous embeddings for entities and relations in the knowledge graph. Due to its benefit to a variety of downstream tasks such as knowledge graph completion, question answering and recommendation, KGE has gained significant attention recently. Despite its effectiveness in a benign environment, KGE's robustness to adversarial attacks is not well-studied. Existing attack methods on graph data cannot be directly applied to attack the embeddings of knowledge graph due to its heterogeneity. To fill this gap, we propose a collection of data poisoning attack strategies, which can effectively manipulate the plausibility of arbitrary targeted facts in a knowledge graph by adding or deleting facts on the graph. The effectiveness and efficiency of the proposed attack strategies are verified by extensive evaluations on two widely-used benchmarks.",
      "year": 2019,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Hengtong Zhang",
        "T. Zheng",
        "Jing Gao",
        "Chenglin Miao",
        "Lu Su",
        "Yaliang Li",
        "K. Ren"
      ],
      "citation_count": 95,
      "url": "https://www.semanticscholar.org/paper/4be29e1cd866ab31f83f03723e2f307cdc1faab0",
      "pdf_url": "https://www.ijcai.org/proceedings/2019/0674.pdf",
      "publication_date": "2019-04-26",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack",
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8780874d671d5583e6f22314545b4a551be7c383",
      "title": "Data Poisoning Attack Aiming the Vulnerability of Continual Learning",
      "abstract": "Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importance of developing continual learning models that are robust to adversarial attacks.",
      "year": 2022,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Gyojin Han",
        "Jaehyun Choi",
        "H. Hong",
        "Junmo Kim"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8780874d671d5583e6f22314545b4a551be7c383",
      "pdf_url": "http://arxiv.org/pdf/2211.15875",
      "publication_date": "2022-11-29",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33368327117fa7cfc365bbfff0635b662daf3d24",
      "title": "Poisonous Label Attack: Black-Box Data Poisoning Attack with Enhanced Conditional DCGAN",
      "abstract": null,
      "year": 2021,
      "venue": "Neural Processing Letters",
      "authors": [
        "Haiqing Liu",
        "Daoxing Li",
        "Yuancheng Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33368327117fa7cfc365bbfff0635b662daf3d24",
      "pdf_url": "",
      "publication_date": "2021-08-28",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "76ec6e650b520f14a83a65a91e3f8a20d65794ff",
      "title": "A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning",
      "abstract": "In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases --- poisoning regression tasks under $\\ell_2$-norm constraint and classification tasks under $\\ell_0$-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50\\% error).",
      "year": 2019,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xuanqing Liu",
        "Si Si",
        "Xiaojin Zhu",
        "Yang Li",
        "Cho-Jui Hsieh"
      ],
      "citation_count": 83,
      "url": "https://www.semanticscholar.org/paper/76ec6e650b520f14a83a65a91e3f8a20d65794ff",
      "pdf_url": "",
      "publication_date": "2019-10-30",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3557b552045cad53f1f0c7fd01945c000aebcad2",
      "title": "Targeted Data Poisoning Attack on News Recommendation System by Content Perturbation",
      "abstract": "News Recommendation System(NRS) has become a fundamental technology to many online news services. Meanwhile, several studies show that recommendation systems(RS) are vulnerable to data poisoning attacks, and the attackers have the ability to mislead the system to perform as their desires. A widely studied attack approach, injecting fake users, can be applied on the NRS when the NRS is treated the same as the other systems whose items are fixed. However, in the NRS, as each item (i.e. news) is more informative, we propose a novel approach to poison the NRS, which is to perturb contents of some browsed news that results in the manipulation of the rank of the target news. Intuitively, an attack is useless if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the exposure risk and propose a novel problem of attacking a history news dataset by means of perturbations where the goal is to maximize the manipulation of the target news rank while keeping the risk of exposure under a given budget. We design a reinforcement learning framework, called TDP-CP, which contains a two-stage hierarchical model to reduce the searching space. Meanwhile, influence estimation is also applied to save the time on retraining the NRS for rewards. We test the performance of TDP-CP under three NRSs and on different target news. Our experiments show that TDP-CP can increase the rank of the target news successfully with a limited exposure budget.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xudong Zhang",
        "Zan Wang",
        "Jingke Zhao",
        "Lanjun Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/3557b552045cad53f1f0c7fd01945c000aebcad2",
      "pdf_url": "http://arxiv.org/pdf/2203.03560",
      "publication_date": "2022-03-04",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "986198fb80a0f418f9014daf9034dd3d7cfebf44",
      "title": "Multi-round Data Poisoning Attack and Defense against Truth Discovery in Crowdsensing Systems",
      "abstract": "Crowdsensing systems collect various types of data based on the personal sensing devices of ordinary users. These users are generally the workers of crowdsourcing tasks released by the systems. However, due to the lack of strict authentication for user identity in many crowdsensing services, malicious attackers can sneak into normal workers and submit malicious data to the system to launch data poisoning attacks. Truth discovery algorithms aim to calculate workers' trustworthiness and try to find the ground truth from inconsistent data submitted by different workers. It can help the systems filter out some data providers with poor quality, and thus can defend against some simple data poisoning attacks, such as random attacks, max-value attacks, etc. However, we found that in multi-rounds of data collaction tasks, the attackers can still attack successfully by slightly modifying the simple poisoning strategies. Attackers can deceive the truth discovery algorithm by alternately submitting real and fake data, thereby mislead the system to infer the wrong truth. Therefore, in this paper we study the attack and defense methods for multi-round data poisoning against truth discovery in crowdsensing systems. First, we verify the vulnerability of a class of commonly used truth discovery framework under a multi-rounds of data poisoning strategy named \u201cHide-AttackPois\u201d. Experiments show that only using a simple hide attack strategy can cause great disturbance to the output of the algorithm. Second, we optimize this class of truth discovery framework to enhance its robustness and enable it to defend against \u201cHide-AttackPois\u201d. Then, we further improve the data poisoning attack model, so that the model can learn better attack strategies and effectively attack the optimized truth discovery framework. We conduct experiments on real dataset to verify the effectiveness of the proposed method.",
      "year": 2022,
      "venue": "International Conference on Mobile Data Management",
      "authors": [
        "Hongniu Zhang",
        "Mohan Li"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/986198fb80a0f418f9014daf9034dd3d7cfebf44",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6380168f051bdd74df10256e6216fc4167024c8",
      "title": "Defending Data Poisoning Attack via Trusted Platform Module and Blockchain Oracle",
      "abstract": "With the development of Internet of Things (IoT) technology, the digital pill has been employed as an IoT system for emerging remote health monitoring to detect the impact of medicine intake on patients\u2019 biological index. The medical data is then used for model training with federated learning. An adversary can launch poisoning attacks by tampering with patients\u2019 medical data, which will lead to misdiagnosis of the patients\u2019 conditions. Lots of studies have been conducted to defend against poisoning attacks based on blockchain or hardware. However, 1) Blockchain-based schemes can only exploit on-chain data to deal with poisoning attacks due to the lack of off-chain trusted entities. 2) Typical hardware-based schemes have the bottleneck of single point of failure. To overcome these defects, we propose a defense scheme via multiple Trusted Platform Modules (TPMs) and blockchain oracle. Benefitting from multiple TPMs verification results, a distributed blockchain oracle is proposed to obtain off-chain verification results for smart contracts. Then, the smart contracts could utilize the off-chain verification result to identify poisoning attacks and store the unique identifiers of the non-threatening IoT device immutably on the blockchain as a whitelist of federated learning participants. Finally, we analyze the security features and evaluate the performance of our scheme, which shows the robustness and efficiency of the proposed work.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Mingyuan Huang",
        "Sheng Cao",
        "Xiong-da Li",
        "Ke Huang",
        "Xiaosong Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/f6380168f051bdd74df10256e6216fc4167024c8",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09c5b34e909b363c07bae1df2edfa77e86a0d596",
      "title": "Data Poisoning Attack by Label Flipping on SplitFed Learning",
      "abstract": null,
      "year": 2022,
      "venue": "International Conference on Recent Trends in Image Processing and Pattern Recognition",
      "authors": [
        "Saurabh Gajbhiye",
        "Priyanka Singh",
        "Shaifu Gupta"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/09c5b34e909b363c07bae1df2edfa77e86a0d596",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "data poisoning attack",
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "668abd8f1643cde74d2bbe83909837943bdcc1be",
      "title": "Threats on Machine Learning Technique by Data Poisoning Attack: A Survey",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Advances in Cybersecurity",
      "authors": [
        "Ibrahim M. Ahmed",
        "M. Kashmoola"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/668abd8f1643cde74d2bbe83909837943bdcc1be",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eab1bbe688573abc901d0377236cbd6af0b32951",
      "title": "Online Data Poisoning Attack",
      "abstract": "We study data poisoning attacks in the online learning setting where the training items stream in one at a time, and the adversary perturbs the current training item to manipulate present and future learning. In contrast, prior work on data poisoning attacks has focused on either batch learners in the offline setting, or online learners but with full knowledge of the whole training sequence. We show that online poisoning attack can be formulated as stochastic optimal control, and provide several practical attack algorithms based on control and deep reinforcement learning. Extensive experiments demonstrate the effectiveness of the attacks.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Xuezhou Zhang",
        "Xiaojin Zhu"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/eab1bbe688573abc901d0377236cbd6af0b32951",
      "pdf_url": "",
      "publication_date": "2019-03-05",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1abce33510c1764f96c614635b4cf01203443bb2",
      "title": "Attack under Disguise: An Intelligent Data Poisoning Attack Mechanism in Crowdsourcing",
      "abstract": "As an effective way to solicit useful information from the crowd, crowdsourcing has emerged as a popular paradigm to solve challenging tasks. However, the data provided by the participating workers are not always trustworthy. In real world, there may exist malicious workers in crowdsourcing systems who conduct the data poisoning attacks for the purpose of sabotage or financial rewards. Although data aggregation methods such as majority voting are conducted on workers\u00bb labels in order to improve data quality, they are vulnerable to such attacks as they treat all the workers equally. In order to capture the variety in the reliability of workers, the Dawid-Skene model, a sophisticated data aggregation method, has been widely adopted in practice. By conducting maximum likelihood estimation (MLE) using the expectation maximization (EM) algorithm, the Dawid-Skene model can jointly estimate each worker\u00bbs reliability and conduct weighted aggregation, and thus can tolerate the data poisoning attacks to some degree. However, the Dawid-Skene model still has weakness. In this paper, we study the data poisoning attacks against such crowdsourcing systems with the Dawid-Skene model empowered. We design an intelligent attack mechanism, based on which the attacker can not only achieve maximum attack utility but also disguise the attacking behaviors. Extensive experiments based on real-world crowdsourcing datasets are conducted to verify the desirable properties of the proposed mechanism.",
      "year": 2018,
      "venue": "The Web Conference",
      "authors": [
        "Chenglin Miao",
        "Qi Li",
        "Lu Su",
        "Mengdi Huai",
        "Wenjun Jiang",
        "Jing Gao"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/1abce33510c1764f96c614635b4cf01203443bb2",
      "pdf_url": "http://dl.acm.org/ft_gateway.cfm?id=3186032&type=pdf",
      "publication_date": "2018-04-10",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d5eabc89e2346411134569a603e63a143d1d6552",
      "title": "Towards Data Poisoning Attack against Knowledge Graph Embedding",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Hengtong Zhang",
        "T. Zheng",
        "Jing Gao",
        "Chenglin Miao",
        "Lu Su",
        "Yaliang Li",
        "K. Ren"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/d5eabc89e2346411134569a603e63a143d1d6552",
      "pdf_url": "",
      "publication_date": "2019-04-26",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "555a55d94309f569053496be56b165de789f1ab2",
      "title": "Federated Learning Under Attack: Exposing Vulnerabilities Through Data Poisoning Attacks in Computer Networks",
      "abstract": "Federated Learning is an approach that enables multiple devices to collectively train a shared model without sharing raw data, thereby preserving data privacy. However, federated learning systems are vulnerable to data-poisoning attacks during the training and updating stages. Three data-poisoning attacks\u2014label flipping, feature poisoning, and VagueGAN\u2014are tested on FL models across one out of ten clients using the CIC and UNSW datasets. For label flipping, we randomly modify labels of benign data; for feature poisoning, we alter highly influential features identified by the Random Forest technique; and for VagueGAN, we generate adversarial examples using Generative Adversarial Networks. Adversarial samples constitute a small portion of each dataset. In this study, we vary the percentages by which adversaries can modify datasets to observe their impact on the Client and Server sides. Experimental findings indicate that label flipping and VagueGAN attacks do not significantly affect server accuracy, as they are easily detectable by the Server. In contrast, feature poisoning attacks subtly undermine model performance while maintaining high accuracy and attack success rates, highlighting their subtlety and effectiveness. Therefore, feature poisoning attacks manipulate the server without causing a significant decrease in model accuracy, underscoring the vulnerability of federated learning systems to such sophisticated attacks. To mitigate these vulnerabilities, we explore a recent defensive approach known as Random Deep Feature Selection, which randomizes server features with varying sizes (e.g., 50 and 400) during training. This strategy has proven highly effective in minimizing the impact of such attacks, particularly on feature poisoning.",
      "year": 2024,
      "venue": "IEEE Transactions on Network and Service Management",
      "authors": [
        "Ehsan Nowroozi",
        "Imran Haider",
        "R. Taheri",
        "Mauro Conti"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/555a55d94309f569053496be56b165de789f1ab2",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e88b50e709299a814e662914ec1c28ea68f4134",
      "title": "Online Data Poisoning Attack.",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Xuezhou Zhang",
        "Xiaojin Zhu",
        "Laurent Lessard"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0e88b50e709299a814e662914ec1c28ea68f4134",
      "pdf_url": "",
      "publication_date": "2019-03-05",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e2524448155a93473d6d4fa2f3bef34b0ed9622",
      "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
      "abstract": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs' text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker's intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yuancheng Xu",
        "Jiarui Yao",
        "Manli Shu",
        "Yanchao Sun",
        "Zichu Wu",
        "Ning Yu",
        "Tom Goldstein",
        "Furong Huang"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/9e2524448155a93473d6d4fa2f3bef34b0ed9622",
      "pdf_url": "",
      "publication_date": "2024-02-05",
      "keywords_matched": [
        "data poisoning attack",
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86307d23d25cefe451a53fd76ddbe5798b592c3b",
      "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
      "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Haonan Wang",
        "Qianli Shen",
        "Yao Tong",
        "Yang Zhang",
        "Kenji Kawaguchi"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/86307d23d25cefe451a53fd76ddbe5798b592c3b",
      "pdf_url": "",
      "publication_date": "2024-01-07",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bead9dec31a3bc3afdee79b1151212c38d70f755",
      "title": "Robust Anomaly based Attack Detection in Smart Grids under Data Poisoning Attacks",
      "abstract": "Anomaly-based attack detection methods are often used to detect data integrity or data falsification attacks in advanced metering infrastructure (AMI) of smart grids. However, there is a lack of studies on the effect of data poisoning attacks against the anomaly based attack detectors that depend on some form of machine learning. In this paper, we introduce some data poisoning attack strategies against anomaly-based attack detectors in smart metering infrastructure and show its impact. Specifically, we propose a whitebox and black box approach to poisoning attacks. Then, we propose modifications to improve the robustness of previous anomaly detection algorithms by modifying certain design choices for learning the thresholds for the anomaly detector. Specifically, we offer theoretical insights and experimental proof to explain why and when they mitigate data poisoning. These design choices include both the regression type and the loss function choice. We measure attack mitigation performance with two NIST specified metrics for CPS systems in the test set using a real smart metering dataset. Finally, we offer recommendations on energy utility's best anomaly detector design choices under varying attack parameters.",
      "year": 2022,
      "venue": "CPSS@AsiaCCS",
      "authors": [
        "Shameek Bhattacharjee",
        "Mohammad Jaminur Islam",
        "S. Abedzadeh"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bead9dec31a3bc3afdee79b1151212c38d70f755",
      "pdf_url": "",
      "publication_date": "2022-05-30",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fef8e393a6a1fb92ae8435f81d5ac82b2dc4ffc2",
      "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
      "abstract": "With the help of conditioning mechanisms, the state-of-the-art diffusion models have achieved tremendous success in guided image generation, particularly in text-to-image synthesis. To gain a better understanding of the training process and potential risks of text-to-image synthesis, we perform a systematic investigation of backdoor attack on text-to-image diffusion models and propose BadT2I, a general multimodal backdoor attack framework that tampers with image synthesis in diverse semantic levels. Specifically, we perform backdoor attacks on three levels of the vision semantics: Pixel-Backdoor, Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our methods efficiently inject backdoors into a large-scale text-to-image diffusion model while preserving its utility with benign inputs. We conduct empirical experiments on Stable Diffusion, the widely-used text-to-image diffusion model, demonstrating that the large-scale diffusion model can be easily backdoored within a few fine-tuning steps. We conduct additional experiments to explore the impact of different types of textual triggers, as well as the backdoor persistence during further training, providing insights for the development of backdoor defense methods. Besides, our investigation may contribute to the copyright protection of text-to-image models in the future. Our Code: https://github.com/sf-zhai/BadT2I.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Shengfang Zhai",
        "Yinpeng Dong",
        "Qingni Shen",
        "Shih-Chieh Pu",
        "Yuejian Fang",
        "Hang Su"
      ],
      "citation_count": 95,
      "url": "https://www.semanticscholar.org/paper/fef8e393a6a1fb92ae8435f81d5ac82b2dc4ffc2",
      "pdf_url": "https://arxiv.org/pdf/2305.04175",
      "publication_date": "2023-05-07",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c9fd1211a21c9a15b2db0b47cc67f3b584820c41",
      "title": "From adversarial examples to data poisoning instances: utilizing an adversarial attack method to poison a transfer learning model",
      "abstract": "Despite the wide-ranging applicability of machine learning methods, they are vulnerable to security attacks, such as evasion attacks and triggerless data poisoning attacks. An evasion attack occurs at the inference time when an attacker feeds in an adversarial example, a malicious perturbed input that appears the same as its untampered copy to a human oracle. In contrast, a triggerless data poisoning attack occurs at training time. An attacker tries to subvert learning with injected poisoned instances. In this research, we focus on a special sub-category of data poisoning attacks, namely triggerless clean-label targeted data poisoning attacks. This type of attacks is more realistic in the sense that it does not require an attacker to have the ability to change the label of any training instance. That is, attackers can successfully attack the training process with a poison instance that is correctly labeled by a human oracle. We propose a simple but effective way to alter an adversarial attack method into a triggerless clean-label targeted data poisoning attack method with a remarkable attack success rate. Furthermore, our proposed method only requires the injection of a single poison instance to manipulate a transfer learning model to misclassify an untampered targeted instance. We compare our method with a popular one-shot attack and show that our method is easier to be used as we do not need to tune for a hyperparameter such as a similarity coefficient.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Jing Lin",
        "R. Luley",
        "Kaiqi Xiong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c9fd1211a21c9a15b2db0b47cc67f3b584820c41",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e75def8bb1bd9485d101a892b2b71b517cd75eea",
      "title": "De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks",
      "abstract": "Machine learning techniques have been widely applied to various applications. However, they are potentially vulnerable to data poisoning attacks, where sophisticated attackers can disrupt the learning procedure by injecting a fraction of malicious samples into the training dataset. Existing defense techniques against poisoning attacks are largely attack-specific: they are designed for one specific type of attacks but do not work for other types, mainly due to the distinct principles they follow. Yet few general defense strategies have been developed. In this paper, we propose De-Pois, an attack-agnostic defense against poisoning attacks. The key idea of De-Pois is to train a mimic model the purpose of which is to imitate the behavior of the target model trained by clean samples. We take advantage of Generative Adversarial Networks (GANs) to facilitate informative training data augmentation as well as the mimic model construction. By comparing the prediction differences between the mimic model and the target model, De-Pois is thus able to distinguish the poisoned samples from clean ones, without explicit knowledge of any ML algorithms or types of poisoning attacks. We implement four types of poisoning attacks and evaluate De-Pois with five typical defense methods on different realistic datasets. The results demonstrate that De-Pois is effective and efficient for detecting poisoned data against all the four types of poisoning attacks, with both the accuracy and F1-score over 0.9 on average.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jian Chen",
        "Xuxin Zhang",
        "Rui Zhang",
        "Chen Wang",
        "Ling Liu"
      ],
      "citation_count": 99,
      "url": "https://www.semanticscholar.org/paper/e75def8bb1bd9485d101a892b2b71b517cd75eea",
      "pdf_url": "https://doi.org/10.1109/tifs.2021.3080522",
      "publication_date": "2021-05-08",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2ab389a6992651bfd77b0a5f8a6950323f6e00b",
      "title": "Deceiving supervised machine learning models via adversarial data poisoning attacks: a case study with USB keyboards",
      "abstract": null,
      "year": 2024,
      "venue": "International Journal of Information Security",
      "authors": [
        "Anil Kumar Chillara",
        "Paresh Saxena",
        "R. Maiti",
        "Manik Gupta",
        "Raghu Kondapalli",
        "Zhichao Zhang",
        "Krishnakumar Kesavan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b2ab389a6992651bfd77b0a5f8a6950323f6e00b",
      "pdf_url": "",
      "publication_date": "2024-03-14",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5c70bda1b28570df2188c72d54667910c549476f",
      "title": "Characterizing Ethereum Address Poisoning Attack",
      "abstract": "This paper presents the first comprehensive analysis of the address poisoning attack surged on the Ethereum blockchain. This phishing attack typically exploits the address shortening feature of Ethereum explorers and digital wallets (e.g., Etherscan and MetaMask) by crafting token transfer events with a seemingly correct address to poison victims' transfer history, waiting for them to mistakenly transfer assets to the attacker's address. To systematically detect and characterize the address poisoning attack, we developed a detection system named Poison-Hunter, which can recognize the attacker's crafted transfers and detect the phishing addresses controlled by the attacker. By applying Poison-Hunter to Ethereum blocks produced from Nov. 2022 to Feb. 2024, we have detected millions of phishing transfers and phishing addresses. Our analysis shows that the attacker has predominantly targeted USDC and USDT token holders and used a phishing address that looks highly similar to a benign one. We also find that the sender of legitimate transfers was the primary target of this attack. Furthermore, by tracing the transaction history of the detected phishing addresses, we reveal that over 1,800 victim addresses have lost crypto assets, with a potential financial loss of up to \\144 million US dollars. Among them, about \\90 million of loss are confirmed by this work. Finally, our analysis suggests that 98% of phishing addresses are controlled by four entities, which collected nearly 92% of the total profits. Overall, this paper sheds light on the tactics utilized in the address poisoning attack and its scale and impact on the Ethereum blockchain, emphasizing the urgent need for an effective detection and prevention mechanism against such a phishing activity.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shixuan Guan",
        "Kai Li"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5c70bda1b28570df2188c72d54667910c549476f",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690277",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb8c2eb5d394940e46b7aa704c270664dd16a7dd",
      "title": "FLTracer: Accurate Poisoning Attack Provenance in Federated Learning",
      "abstract": "Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we propose a Kalman filter-based cross-round detection to identify adversaries by seeking the behavior changes before and after the attack. Thus, this makes it resilient to data heterogeneity and is effective even in non-IID settings. To further improve the accuracy of our detection method, we employ four novel features and capture their anomalies with the joint decisions. Extensive evaluations show that FLTracer achieves an average true positive rate of over 96.88% at an average false positive rate of less than 2.67%, significantly outperforming SOTA detection methods (https://github.com/Eyr3/FLTracer).",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xinyu Zhang",
        "Qingyu Liu",
        "Zhongjie Ba",
        "Yuan Hong",
        "Tianhang Zheng",
        "Feng Lin",
        "Liwang Lu",
        "Kui Ren"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/eb8c2eb5d394940e46b7aa704c270664dd16a7dd",
      "pdf_url": "https://arxiv.org/pdf/2310.13424",
      "publication_date": "2023-10-20",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8eece642a5873745bae9b868f60dd4cf3b28a4d",
      "title": "LoMar: A Local Defense Against Poisoning Attack on Federated Learning",
      "abstract": "Federated learning (FL) provides a high efficient decentralized machine learning framework, where the training data remains distributed at remote clients in a network. Though FL enables a privacy-preserving mobile edge computing framework using IoT devices, recent studies have shown that this approach is susceptible to poisoning attacks from the side of remote clients. To address the poisoning attacks on FL, we provide a <italic>two-phase</italic> defense algorithm called <inline-formula><tex-math notation=\"LaTeX\">${\\underline{Lo}cal\\ \\underline{Ma}licious\\ Facto\\underline{r}}$</tex-math><alternatives><mml:math><mml:mrow><mml:munder><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>\u0332</mml:mo></mml:munder><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width=\"4pt\"/><mml:munder><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>\u0332</mml:mo></mml:munder><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width=\"4pt\"/><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:munder><mml:mi>r</mml:mi><mml:mo>\u0332</mml:mo></mml:munder></mml:mrow></mml:math><inline-graphic xlink:href=\"tang-ieq1-3135422.gif\"/></alternatives></inline-formula> (LoMar). In phase I, LoMar scores model updates from each remote client by measuring the relative distribution over their neighbors using a kernel density estimation method. In phase II, an optimal threshold is approximated to distinguish malicious and clean updates from a statistical perspective. Comprehensive experiments on four real-world datasets have been conducted, and the experimental results show that our defense strategy can effectively protect the FL system. Specifically, the defense performance on Amazon dataset under a label-flipping attack indicates that, compared with FG+Krum, LoMar increases the target label testing accuracy from <inline-formula><tex-math notation=\"LaTeX\">$96.0\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>96</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"tang-ieq2-3135422.gif\"/></alternatives></inline-formula> to <inline-formula><tex-math notation=\"LaTeX\">$98.8\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>98</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"tang-ieq3-3135422.gif\"/></alternatives></inline-formula>, and the overall averaged testing accuracy from <inline-formula><tex-math notation=\"LaTeX\">$90.1\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>90</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"tang-ieq4-3135422.gif\"/></alternatives></inline-formula> to <inline-formula><tex-math notation=\"LaTeX\">$97.0\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>97</mml:mn><mml:mo>.</mml:mo><mml:mn>0</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"tang-ieq5-3135422.gif\"/></alternatives></inline-formula>.",
      "year": 2022,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xingyu Li",
        "Zhe Qu",
        "Shangqing Zhao",
        "Bo Tang",
        "Zhuo Lu",
        "Yao-Hong Liu"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/a8eece642a5873745bae9b868f60dd4cf3b28a4d",
      "pdf_url": "http://arxiv.org/pdf/2201.02873",
      "publication_date": "2022-01-08",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "55d41bc53eadfc47fce0a6d86772b423c8c45aa0",
      "title": "FedAttack: Effective and Covert Poisoning Attack on Federated Recommendation via Hard Sampling",
      "abstract": "Federated learning (FL) is a feasible technique to learn personalized recommendation models from decentralized user data. Unfortunately, federated recommender systems are vulnerable to poisoning attacks by malicious clients. Existing recommender system poisoning methods mainly focus on promoting the recommendation chances of target items due to financial incentives. In fact, in real-world scenarios, the attacker may also attempt to degrade the overall performance of recommender systems. However, existing general FL poisoning methods for degrading model performance are either ineffective or not concealed in poisoning federated recommender systems. In this paper, we propose a simple yet effective and covert poisoning attack method on federated recommendation, named FedAttack. Its core idea is using globally hardest samples to subvert model training. More specifically, the malicious clients first infer user embeddings based on local user profiles. Next, they choose the candidate items that are most relevant to the user embeddings as hardest negative samples, and find the candidates farthest from the user embeddings as hardest positive samples. The model gradients inferred from these poisoned samples are then uploaded for aggregation. Extensive experiments on two benchmark datasets show that FedAttack can effectively degrade the performance of various federated recommender systems, meanwhile cannot be effectively detected nor defended by many existing methods.",
      "year": 2022,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Chuhan Wu",
        "Fangzhao Wu",
        "Tao Qi",
        "Yongfeng Huang",
        "Xing Xie"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/55d41bc53eadfc47fce0a6d86772b423c8c45aa0",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3534678.3539119",
      "publication_date": "2022-02-10",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e03a2a612e131f93daa4343427b44acc9cfdcee",
      "title": "Triple Adversarial Learning for Influence based Poisoning Attack in Recommender Systems",
      "abstract": "As an important means to solve information overload, recommender systems have been widely applied in many fields, such as e-commerce and advertising. However, recent studies have shown that recommender systems are vulnerable to poisoning attacks; that is, injecting a group of carefully designed user profiles into the recommender system can severely affect recommendation quality. Despite the development from shilling attacks to optimization-based attacks, the imperceptibility and harmfulness of the generated data in most attacks are arduous to balance. To this end, we propose a triple adversarial learning for influence based poisoning attack (TrialAttack), a flexible end-to-end poisoning framework to generate non-notable and harmful user profiles. Specifically, given the input noise, TrialAttack directly generates malicious users through triple adversarial learning of the generator, discriminator, and influence module. Besides, to provide reliable influence for TrialAttack training, we explore a new approximation approach for estimating each fake user's influence. Through theoretical analysis, we prove that the distribution characterized by TrialAttack approximates to the rating distribution of real users under the premise of performing an efficient attack. This property allows the injected users to attack in an unremarkable way. Experiments on three real-world datasets show that TrialAttack's attack performance outperforms state-of-the-art attacks, and the generated fake profiles are more difficult to detect compared to baselines.",
      "year": 2021,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Chenwang Wu",
        "Defu Lian",
        "Yong Ge",
        "Zhihao Zhu",
        "Enhong Chen"
      ],
      "citation_count": 75,
      "url": "https://www.semanticscholar.org/paper/0e03a2a612e131f93daa4343427b44acc9cfdcee",
      "pdf_url": "",
      "publication_date": "2021-08-14",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4329c62b8b8813b1280cb004d894e4e8dfd7bbab",
      "title": "Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector",
      "abstract": "With the increase in machine learning (ML) applications in different domains, incentives for deceiving these models have reached more than ever. As data is the core backbone of ML algorithms, attackers shifted their interest towards polluting the training data itself. Data credibility is at even higher risk with the rise of state-of-art research topics like open design principles, federated learning, and crowd-sourcing. Since the machine learning model depends on different stakeholders for obtaining data, there are no existing reliable automated mechanisms to verify the veracity of data from each source.Malware detection is arduous due to its malicious nature with the addition of metamorphic and polymorphic ability in the evolving samples. ML has proven to solve the zero-day malware detection problem, which is unresolved by traditional signature- based approaches. The poisoning of malware training data can allow the malware files to go undetected by the ML-based malware detectors, helping the attackers to fulfill their malicious goals. A feasibility analysis of the data poisoning threat in the malware detection domain is still lacking. Our work will focus on two major sections: training ML-based malware detectors and poisoning the training data using the label-poisoning approach. We will analyze the robustness of different machine learning models against data poisoning with varying volumes of poisoning data.",
      "year": 2022,
      "venue": "2022 IEEE International Conference on Big Data (Big Data)",
      "authors": [
        "Kshitiz Aryal",
        "Maanak Gupta",
        "Mahmoud Abdelsalam"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/4329c62b8b8813b1280cb004d894e4e8dfd7bbab",
      "pdf_url": "http://arxiv.org/pdf/2301.01044",
      "publication_date": "2022-12-17",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "670c51e952877d7942b6840ffb4676f174122ee5",
      "title": "Adversarial Label Poisoning Attack on Graph Neural Networks via Label Propagation",
      "abstract": null,
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Ganlin Liu",
        "Xiaowei Huang",
        "Xinping Yi"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/670c51e952877d7942b6840ffb4676f174122ee5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1e375c77c232ed7391c0e41ce87991cb46b3df21",
      "title": "Camouflaged Poisoning Attack on Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have enabled the automation of many web applications that entail node classification on graphs, such as scam detection in social media and event prediction in service networks. Nevertheless, recent studies revealed that the GNNs are vulnerable to adversarial attacks, where feeding GNNs with poisoned data at training time can lead them to yield catastrophically devastative test accuracy. This finding heats up the frontier of attacks and defenses against GNNs. However, the prior studies mainly posit that the adversaries can enjoy free access to manipulate the original graph, while obtaining such access could be too costly in practice. To fill this gap, we propose a novel attacking paradigm, named Generative Adversarial Fake Node Camouflaging (GAFNC), with its crux lying in crafting a set of fake nodes in a generative-adversarial regime. These nodes carry camouflaged malicious features and can poison the victim GNN by passing their malicious messages to the original graph via learned topological structures, such that they 1) maximize the devastation of classification accuracy (i.e., global attack) or 2) enforce the victim GNN to misclassify a targeted node set into prescribed classes (i.e., target attack). We benchmark our experiments on four real-world graph datasets, and the results substantiate the viability, effectiveness, and stealthiness of our proposed poisoning attack approach. Code is released in github.com/chao92/GAFNC.",
      "year": 2022,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Chao Jiang",
        "Yingzhe He",
        "Richard Chapman",
        "Hongyi Wu"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/1e375c77c232ed7391c0e41ce87991cb46b3df21",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3512527.3531373",
      "publication_date": "2022-06-27",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "878d967a0c7446010c0109bd1733f21bdc337f74",
      "title": "DNS Cache Poisoning Attack Reloaded: Revolutions with Side Channels",
      "abstract": "In this paper, we report a series of flaws in the software stack that leads to a strong revival of DNS cache poisoning --- a classic attack which is mitigated in practice with simple and effective randomization-based defenses such as randomized source port. To successfully poison a DNS cache on a typical server, an off-path adversary would need to send an impractical number of $2^32 $ spoofed responses simultaneously guessing the correct source port (16-bit) and transaction ID (16-bit). Surprisingly, we discover weaknesses that allow an adversary to \"divide and conquer'' the space by guessing the source port first and then the transaction ID (leading to only $2^16 +2^16 $ spoofed responses). Even worse, we demonstrate a number of ways an adversary can extend the attack window which drastically improves the odds of success. The attack affects all layers of caches in the DNS infrastructure, such as DNS forwarder and resolver caches, and a wide range of DNS software stacks, including the most popular BIND, Unbound, and dnsmasq, running on top of Linux and potentially other operating systems. The major condition for a victim being vulnerable is that an OS and its network is configured to allow ICMP error replies. From our measurement, we find over 34% of the open resolver population on the Internet are vulnerable (and in particular 85% of the popular DNS services including Google's 8.8.8.8). Furthermore, we comprehensively validate the proposed attack with positive results against a variety of server configurations and network conditions that can affect the success of the attack, in both controlled experiments and a production DNS resolver (with authorization).",
      "year": 2020,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Keyu Man",
        "Zhiyun Qian",
        "Zhongjie Wang",
        "Xiaofeng Zheng",
        "Youjun Huang",
        "Haixin Duan"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/878d967a0c7446010c0109bd1733f21bdc337f74",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3372297.3417280",
      "publication_date": "2020-10-30",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "464d6eb0becd1adc7053d9d419552d34ed8a30de",
      "title": "Poisoning Attack in Federated Learning using Generative Adversarial Nets",
      "abstract": "Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80% accuracy on both poisoning tasks and main tasks.",
      "year": 2019,
      "venue": "2019 18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)",
      "authors": [
        "Jiale Zhang",
        "Junjun Chen",
        "Di Wu",
        "Bing Chen",
        "Shui Yu"
      ],
      "citation_count": 176,
      "url": "https://www.semanticscholar.org/paper/464d6eb0becd1adc7053d9d419552d34ed8a30de",
      "pdf_url": "",
      "publication_date": "2019-08-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37c27282daf1c75bafc640410b6f4b67c89f6413",
      "title": "Mitigating Poisoning Attack in Federated Learning",
      "abstract": "Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.",
      "year": 2021,
      "venue": "IEEE Symposium Series on Computational Intelligence",
      "authors": [
        "Aashma Uprety",
        "D. Rawat"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/37c27282daf1c75bafc640410b6f4b67c89f6413",
      "pdf_url": "",
      "publication_date": "2021-12-05",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04cb96e55e787489a1c6c2df68cfd54a4cbac384",
      "title": "Poisoning Attack Against Estimating From Pairwise Comparisons",
      "abstract": "As pairwise ranking becomes broadly employed for elections, sports competitions, recommendation, information retrieval and so on, attackers have strong motivation and incentives to manipulate or disrupt the ranking list. They could inject malicious comparisons into the training data to fool the target ranking algorithm. Such a technique is called \u201cpoisoning attack\u201d in regression and classification tasks. In this paper, to the best of our knowledge, we initiate the first systematic investigation of data poisoning attack on the pairwise ranking algorithms, which can be generally formalized as the dynamic and static games between the ranker and the attacker, and can be modeled as certain kinds of integer programming problems mathematically. To break the computational hurdle of the underlying integer programming problems, we reformulate them into the distributionally robust optimization (DRO) problems, which are computational tractable. Based on such DRO formulations, we propose two efficient poisoning attack algorithms and establish the associated theoretical guarantees including the existence of Nash equilibrium and the generalization ability bounds. The effectiveness of the suggested poisoning attack strategies is demonstrated by a series of toy simulations and several real data experiments. These experimental results show that the proposed methods can significantly reduce the performance of the ranker in the sense that the correlation between the true ranking list and the aggregated results with toxic data can be decreased dramatically.",
      "year": 2021,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Ke Ma",
        "Qianqian Xu",
        "Jinshan Zeng",
        "Xiaochun Cao",
        "Qingming Huang"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/04cb96e55e787489a1c6c2df68cfd54a4cbac384",
      "pdf_url": "https://arxiv.org/pdf/2107.01854",
      "publication_date": "2021-06-08",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: injecting a small amount of poisonous data (1-5\\% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Tim Baumg\u00e4rtner",
        "Yang Gao",
        "Dana Alon",
        "Donald Metzler"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/521c2905e667ad6d2162ac369cf3f85d70e0f477",
      "pdf_url": "",
      "publication_date": "2024-04-08",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "25135e5dda4cd46e38b028b9f2fef28a336735ca",
      "title": "Backdoor Defense via Adaptively Splitting Poisoned Dataset",
      "abstract": "Backdoor defenses have been studied to alleviate the threat of deep neural networks (DNNs) being backdoor attacked and thus maliciously altered. Since DNNs usually adopt some external training data from an untrusted third party, a robust backdoor defense strategy during the training stage is of importance. We argue that the core of training-time defense is to select poisoned samples and to handle them properly. In this work, we summarize the training-time defenses from a unified framework as splitting the poisoned dataset into two data pools. Under our framework, we propose an adaptively splitting dataset-based defense (ASD). Concretely, we apply loss-guided split and meta-learning-inspired split to dynamically update two data pools. With the split clean data pool and polluted data pool, ASD successfully defends against backdoor attacks during training. Extensive experiments on multiple benchmark datasets and DNN models against six state-of-the-art backdoor attacks demonstrate the superiority of our ASD. Our code is available at https://github.com/KuofengGao/ASD.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Kuofeng Gao",
        "Yang Bai",
        "Jindong Gu",
        "Yong Yang",
        "Shutao Xia"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/25135e5dda4cd46e38b028b9f2fef28a336735ca",
      "pdf_url": "https://arxiv.org/pdf/2303.12993",
      "publication_date": "2023-03-23",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ae4b6847896a3c69fb4d7b7c7d71fc92c70f630",
      "title": "LDPGuard: Defenses Against Data Poisoning Attacks to Local Differential Privacy Protocols",
      "abstract": "The protocols that satisfy Local Differential Privacy (LDP) enable untrusted third parties to collect aggregate information about a population without disclosing each user's privacy. In particular, each user locally encodes and perturbs his private data before sending it to the data collector, who aggregates and estimates the statistics about the population based on the collected perturbed values from individuals. Owing to their growing importance, LDP protocols have been widely studied and deployed in real-world scenarios (e.g., Chrome and Windows). However, as data poisoning attacks may be injected by attackers who introduce many fake users, the utility of the statistics is heavily poisoned. In this paper, we present a generic and extensible framework called LDPGuard to address the problem. LDPGuard provides effective defenses against data poisoning attacks to LDP protocols for frequency estimation, a basic query of most data analytics tasks. In particular, it first precisely estimates the percentage of fake users and then provides adversarial schemes to defend against particular data poisoning attacks. Experimental study on real-world and synthetic datasets demonstrates the superiority of LDPGuard compared to existing techniques.",
      "year": 2024,
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": [
        "Kai Huang",
        "Gaoya Ouyang",
        "Qingqing Ye",
        "Haibo Hu",
        "Bolong Zheng",
        "Xi Zhao",
        "Ruiyuan Zhang",
        "Xiaofang Zhou"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/2ae4b6847896a3c69fb4d7b7c7d71fc92c70f630",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "28d54956a3bdbd3aacc4249c1be7d59da8674b85",
      "title": "Scaling Trends for Data Poisoning in LLMs",
      "abstract": "LLMs produce harmful and undesirable behavior when trained on datasets containing even a small fraction of poisoned data. We demonstrate that GPT models remain vulnerable to fine-tuning on poisoned data, even when safeguarded by moderation systems. Given the persistence of data poisoning vulnerabilities in today's most capable models, this paper investigates whether these risks increase with model scaling. We evaluate three threat models\u2014malicious fine-tuning, imperfect data curation, and intentional data contamination\u2014across 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Dillon Bowen",
        "Brendan Murphy",
        "Will Cai",
        "David Khachaturov",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/28d54956a3bdbd3aacc4249c1be7d59da8674b85",
      "pdf_url": "",
      "publication_date": "2024-08-06",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9832eb62e0b3d826d21445fc1fd2fcf6ced7d41a",
      "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
      "abstract": "Despite significant advancements, large language models (LLMs) still struggle with providing accurate answers when lacking domain-specific or up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge bases, but it also introduces new attack surfaces. In this paper, we investigate data extraction attacks targeting RAG's knowledge databases. We show that previous prompt injection-based extraction attacks largely rely on the instruction-following capabilities of LLMs. As a result, they fail on models that are less responsive to such malicious prompts -- for example, our experiments show that state-of-the-art attacks achieve near-zero success on Gemma-2B-IT. Moreover, even for models that can follow these instructions, we found fine-tuning may significantly reduce attack performance. To further reveal the vulnerability, we propose to backdoor RAG, where a small portion of poisoned data is injected during the fine-tuning phase to create a backdoor within the LLM. When this compromised LLM is integrated into a RAG system, attackers can exploit specific triggers in prompts to manipulate the LLM to leak documents from the retrieval database. By carefully designing the poisoned data, we achieve both verbatim and paraphrased document extraction. For example, on Gemma-2B-IT, we show that with only 5\\% poisoned data, our method achieves an average success rate of 94.1\\% for verbatim extraction (ROUGE-L score: 82.1) and 63.6\\% for paraphrased extraction (average ROUGE score: 66.4) across four datasets. These results underscore the privacy risks associated with the supply chain when deploying RAG systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuefeng Peng",
        "Junda Wang",
        "Hong Yu",
        "Amir Houmansadr"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/9832eb62e0b3d826d21445fc1fd2fcf6ced7d41a",
      "pdf_url": "",
      "publication_date": "2024-11-03",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa4382fa8a7e59ef46b11293d30848a388650f07",
      "title": "Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey",
      "abstract": "Due to the greatly improved capabilities of devices, massive data, and increasing concern about data privacy, Federated Learning (FL) has been increasingly considered for applications to wireless communication networks (WCNs). Wireless FL (WFL) is a distributed method of training a global deep learning model in which a large number of participants each train a local model on their training datasets and then upload the local model updates to a central server. However, in general, non-independent and identically distributed (non-IID) data of WCNs raises concerns about robustness, as a malicious participant could potentially inject a \u201cbackdoor\u201d into the global model by uploading poisoned data or models over WCN. This could cause the model to misclassify malicious inputs as a specific target class while behaving normally with benign inputs. This survey provides a comprehensive review of the latest backdoor attacks and defense mechanisms. It classifies them according to their targets (data poisoning or model poisoning), the attack phase (local data collection, training, or aggregation), and defense stage (local training, before aggregation, during aggregation, or after aggregation). The strengths and limitations of existing attack strategies and defense mechanisms are analyzed in detail. Comparisons of existing attack methods and defense designs are carried out, pointing to noteworthy findings, open challenges, and potential future research directions related to security and privacy of WFL.",
      "year": 2023,
      "venue": "IEEE Communications Surveys and Tutorials",
      "authors": [
        "Yichen Wan",
        "Youyang Qu",
        "Wei Ni",
        "Yong Xiang",
        "Longxiang Gao",
        "Ekram Hossain"
      ],
      "citation_count": 77,
      "url": "https://www.semanticscholar.org/paper/fa4382fa8a7e59ef46b11293d30848a388650f07",
      "pdf_url": "https://arxiv.org/pdf/2312.08667",
      "publication_date": "2023-12-14",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e38c4ceb8259967d25417b55ec2cac4ac8217390",
      "title": "Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors",
      "abstract": "Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning (SSL) methods (e.g., contrastive learning) that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks (e.g., TGDA attack and GC attack) to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation. Empirical results reveal that transfer learning is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks.",
      "year": 2024,
      "venue": "2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Yiwei Lu",
        "Matthew Y. R. Yang",
        "Gautam Kamath",
        "Yaoliang Yu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/e38c4ceb8259967d25417b55ec2cac4ac8217390",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "012f8a24ea39b576d6bfecd3bd24efa51aa76730",
      "title": "Data Quality Detection Mechanism Against Label Flipping Attacks in Federated Learning",
      "abstract": "Federated learning (FL) is an emerging framework that enables massive clients (e.g., mobile devices or enterprises) to collaboratively construct a global model without sharing their local data. However, due to the lack of direct access to clients\u2019 data, the global model is vulnerable to be attacked by malicious clients with their poisoned data. Many strategies have been proposed to mitigate the threat of label flipping attacks, but they either require considerable computational overhead, or lack robustness, and some even cause privacy concerns. In this paper, we propose Malicious Clients Detection Federated Learning (MCDFL) to defense against the label flipping attack. It can identify malicious clients by recovering a distribution over a latent feature space to detect the data quality of each client. We demonstrate the effectiveness of our proposed strategy on two benchmark datasets, i.e., CIFAR-10 and Fashion-MNIST, by considering different neural network models and different attack scenarios. The results show that, our solution is robust to detect malicious clients without excessive costs under various conditions, where the proportion of malicious clients is in the range of 5% and 40%.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yifeng Jiang",
        "Weiwen Zhang",
        "Yanxi Chen"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/012f8a24ea39b576d6bfecd3bd24efa51aa76730",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoned data",
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d951a9a7b0d24cec84ffd1022900733186504d05",
      "title": "Certified Defenses for Data Poisoning Attacks",
      "abstract": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.",
      "year": 2017,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "J. Steinhardt",
        "Pang Wei Koh",
        "Percy Liang"
      ],
      "citation_count": 819,
      "url": "https://www.semanticscholar.org/paper/d951a9a7b0d24cec84ffd1022900733186504d05",
      "pdf_url": "",
      "publication_date": "2017-06-09",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2418bc3f97d1ee8d0caec7656af78343b83cabe3",
      "title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?",
      "abstract": null,
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Rui Wen",
        "Zhengyu Zhao",
        "Zhuoran Liu",
        "M. Backes",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/2418bc3f97d1ee8d0caec7656af78343b83cabe3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9a1f352ef21044700c180882038c28c3b2361914",
      "title": "Data collection and quality challenges in deep learning: a data-centric AI perspective",
      "abstract": "Data-centric AI is at the center of a fundamental shift in software engineering where machine learning becomes the new software, powered by big data and computing infrastructure. Here, software engineering needs to be re-thought where data become a first-class citizen on par with code. One striking observation is that a significant portion of the machine learning process is spent on data preparation. Without good data, even the best machine learning algorithms cannot perform well. As a result, data-centric AI practices are now becoming mainstream. Unfortunately, many datasets in the real world are small, dirty, biased, and even poisoned. In this survey, we study the research landscape for data collection and data quality primarily for deep learning applications. Data collection is important because there is lesser need for feature engineering for recent deep learning approaches, but instead more need for large amounts of data. For data quality, we study data validation, cleaning, and integration techniques. Even if the data cannot be fully cleaned, we can still cope with imperfect data during model training using robust model training techniques. In addition, while bias and fairness have been less studied in traditional data management research, these issues become essential topics in modern machine learning applications. We thus study fairness measures and unfairness mitigation techniques that can be applied before, during, or after model training. We believe that the data management community is well poised to solve these problems.",
      "year": 2021,
      "venue": "The VLDB journal",
      "authors": [
        "Steven Euijong Whang",
        "Yuji Roh",
        "Hwanjun Song",
        "Jae-Gil Lee"
      ],
      "citation_count": 449,
      "url": "https://www.semanticscholar.org/paper/9a1f352ef21044700c180882038c28c3b2361914",
      "pdf_url": "",
      "publication_date": "2021-12-13",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "11eeccc7e3f4effc08b7cd98dd1557e443eb5ec6",
      "title": "Stronger data poisoning attacks break data sanitization defenses",
      "abstract": "Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models\u2019 training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. In this paper, we develop three attacks that can bypass a broad range of common data sanitization defenses, including anomaly detectors based on nearest neighbors, training loss, and singular-value decomposition. By adding just 3% poisoned data, our attacks successfully increase test error on the Enron spam detection dataset from 3 to 24% and on the IMDB sentiment classification dataset from 12 to 29%. In contrast, existing attacks which do not explicitly account for these data sanitization defenses are defeated by them. Our attacks are based on two ideas: (i) we coordinate our attacks to place poisoned points near one another, and (ii) we formulate each attack as a constrained optimization problem, with constraints designed to ensure that the poisoned points evade detection. As this optimization involves solving an expensive bilevel problem, our three attacks correspond to different ways of approximating this problem, based on influence functions; minimax duality; and the Karush\u2013Kuhn\u2013Tucker (KKT) conditions. Our results underscore the need to develop more robust defenses against data poisoning attacks.",
      "year": 2018,
      "venue": "Machine-mediated learning",
      "authors": [
        "Pang Wei Koh",
        "J. Steinhardt",
        "Percy Liang"
      ],
      "citation_count": 268,
      "url": "https://www.semanticscholar.org/paper/11eeccc7e3f4effc08b7cd98dd1557e443eb5ec6",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10994-021-06119-y.pdf",
      "publication_date": "2018-11-02",
      "keywords_matched": [
        "poisoned data",
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "727221a14b7b63f05d0162cc4b94525c5f5af305",
      "title": "Temporal Robustness against Data Poisoning",
      "abstract": "Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples when the attacks are temporally bounded. We present a benchmark with an evaluation protocol simulating continuous data collection and periodic deployments of updated models, thus enabling empirical evaluation of temporal robustness. Lastly, we develop and also empirically verify a baseline defense, namely temporal aggregation, offering provable temporal robustness and highlighting the potential of our temporal threat model for data poisoning.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Wenxiao Wang",
        "S. Feizi"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/727221a14b7b63f05d0162cc4b94525c5f5af305",
      "pdf_url": "http://arxiv.org/pdf/2302.03684",
      "publication_date": "2023-02-07",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7072bf9116c910e42a9b55bd4de6adcdd0323a0d",
      "title": "Autoregressive Perturbations for Data Poisoning",
      "abstract": "The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data\"unlearnable\"by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pedro Sandoval-Segura",
        "Vasu Singla",
        "Jonas Geiping",
        "Micah Goldblum",
        "T. Goldstein",
        "David Jacobs"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/7072bf9116c910e42a9b55bd4de6adcdd0323a0d",
      "pdf_url": "http://arxiv.org/pdf/2206.03693",
      "publication_date": "2022-06-08",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8dc712493df0a46fef830a6c3be64899880100c4",
      "title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching",
      "abstract": "Data Poisoning attacks involve an attacker modifying training data to maliciouslycontrol a model trained on this data. Previous poisoning attacks against deep neural networks have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. In this work, we focus on a particularly malicious poisoning attack that is both \"from scratch\" and\"clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. The central mechanism of this attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.",
      "year": 2020,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Jonas Geiping",
        "Liam H. Fowl",
        "W. R. Huang",
        "W. Czaja",
        "Gavin Taylor",
        "Michael Moeller",
        "T. Goldstein"
      ],
      "citation_count": 250,
      "url": "https://www.semanticscholar.org/paper/8dc712493df0a46fef830a6c3be64899880100c4",
      "pdf_url": "",
      "publication_date": "2020-09-04",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8465338724f00a1f57a86717e4c898256c522be0",
      "title": "MetaPoison: Practical General-purpose Clean-label Data Poisoning",
      "abstract": "Data poisoning--the process by which an attacker takes control of a model by making imperceptible changes to a subset of the training data--is an emerging threat in the context of neural networks. Existing attacks for data poisoning have relied on hand-crafted heuristics. Instead, we pose crafting poisons more generally as a bi-level optimization problem, where the inner level corresponds to training a network on a poisoned dataset and the outer level corresponds to updating those poisons to achieve a desired behavior on the trained model. We then propose MetaPoison, a first-order method to solve this optimization quickly. MetaPoison is effective: it outperforms previous clean-label poisoning methods by a large margin under the same setting. MetaPoison is robust: its poisons transfer to a variety of victims with unknown hyperparameters and architectures. MetaPoison is also general-purpose, working not only in fine-tuning scenarios, but also for end-to-end training from scratch with remarkable success, e.g. causing a target image to be misclassified 90% of the time via manipulating just 1% of the dataset. Additionally, MetaPoison can achieve arbitrary adversary goals not previously possible--like using poisons of one class to make a target image don the label of another arbitrarily chosen class. Finally, MetaPoison works in the real-world. We demonstrate successful data poisoning of models trained on Google Cloud AutoML Vision. Code and premade poisons are provided at this https URL",
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "W. R. Huang",
        "Jonas Geiping",
        "Liam H. Fowl",
        "Gavin Taylor",
        "T. Goldstein"
      ],
      "citation_count": 213,
      "url": "https://www.semanticscholar.org/paper/8465338724f00a1f57a86717e4c898256c522be0",
      "pdf_url": "",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35d5395b90a69e9994cb208b91776df6233ea3e3",
      "title": "DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation",
      "abstract": "Public resources and services (e.g., datasets, training platforms, pre-trained models) have been widely adopted to ease the development of Deep Learning-based applications. However, if the third-party providers are untrusted, they can inject poisoned samples into the datasets or embed backdoors in those models. Such an integrity breach can cause severe consequences, especially in safety- and security-critical applications. Various backdoor attack techniques have been proposed for higher effectiveness and stealthiness. Unfortunately, existing defense solutions are not practical to thwart those attacks in a comprehensive way. In this paper, we investigate the effectiveness of data augmentation techniques in mitigating backdoor attacks and enhancing DL models' robustness. An evaluation framework is introduced to achieve this goal. Specifically, we consider a unified defense solution, which (1) adopts a data augmentation policy to fine-tune the infected model and eliminate the effects of the embedded backdoor; (2) uses another augmentation policy to preprocess input samples and invalidate the triggers during inference. We propose a systematic approach to discover the optimal policies for defending against different backdoor attacks by comprehensively evaluating 71 state-of-the-art data augmentation functions. Extensive experiments show that our identified policy can effectively mitigate eight different kinds of backdoor attacks and outperform five existing defense methods. We envision this framework can be a good benchmark tool to advance future DNN backdoor studies.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Yi Zeng",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang",
        "Meikang Qiu",
        "B. Thuraisingham"
      ],
      "citation_count": 211,
      "url": "https://www.semanticscholar.org/paper/35d5395b90a69e9994cb208b91776df6233ea3e3",
      "pdf_url": "https://arxiv.org/pdf/2012.07006",
      "publication_date": "2020-12-13",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3473619be2b2a481c0ac459ec2bf5449119fceb1",
      "title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
      "abstract": "Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting\"poisoned\"data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Yiwei Lu",
        "Gautam Kamath",
        "Yaoliang Yu"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/3473619be2b2a481c0ac459ec2bf5449119fceb1",
      "pdf_url": "http://arxiv.org/pdf/2204.09092",
      "publication_date": "2022-04-19",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "30ddf4a96c431b0023ad5388805c2dbb8f792965",
      "title": "Data Poisoning Based Backdoor Attacks to Contrastive Learning",
      "abstract": "Contrastive learning (CL) pretrains general-purpose encoders using an unlabeled pretraining dataset, which consists of images or image-text pairs. CL is vulnera-ble to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pretraining dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing backdoor attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder introduces a new attack strategy to create poisoned inputs and uses a theory-guided method to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio (0.5%). Moreover, we also propose a de-fense, called localized cropping, to defend against DPBAs. Our results show that our defense can reduce the effectiveness of DPBAs, but it sacrifices the utility of the encoder, highlighting the need for new defenses.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Jinghuai Zhang",
        "Hongbin Liu",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/30ddf4a96c431b0023ad5388805c2dbb8f792965",
      "pdf_url": "",
      "publication_date": "2022-11-15",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d8309ad37094284e750f7306beb6bf326621d57",
      "title": "How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?",
      "abstract": "Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses? This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing automated tools and human inspection are at identifying clean data in the presence of data poisoning. Unfortunately, neither effort achieves the precision needed. Worse yet, many of the outcomes are worse than random selection. In addition to uncovering the challenge, we propose a practical countermeasure, Meta-Sift. Our method is based on the insight that existing attacks' poisoned samples shifts from clean data distributions. Hence, training on the clean portion of a dataset and testing on the corrupted portion will result in high prediction loss. Leveraging the insight, we formulate a bilevel optimization to identify clean data and further introduce a suite of techniques to improve efficiency and precision. Our evaluation shows that Meta-Sift can sift a clean base set with 100% precision under a wide range of poisoning attacks. The selected base set is large enough to give rise to successful defenses.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yi Zeng",
        "Minzhou Pan",
        "Himanshu Jahagirdar",
        "Ming Jin",
        "L. Lyu",
        "R. Jia"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/8d8309ad37094284e750f7306beb6bf326621d57",
      "pdf_url": "http://arxiv.org/pdf/2210.06516",
      "publication_date": "2022-10-12",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "820ebf534314347264e26979d8795acda6ae5a6b",
      "title": "Lethal Dose Conjecture on Data Poisoning",
      "abstract": "Data poisoning considers an adversary that distorts the training set of machine learning algorithms for malicious purposes. In this work, we bring to light one conjecture regarding the fundamentals of data poisoning, which we call the Lethal Dose Conjecture. The conjecture states: If $n$ clean training samples are needed for accurate predictions, then in a size-$N$ training set, only $\\Theta(N/n)$ poisoned samples can be tolerated while ensuring accuracy. Theoretically, we verify this conjecture in multiple cases. We also offer a more general perspective of this conjecture through distribution discrimination. Deep Partition Aggregation (DPA) and its extension, Finite Aggregation (FA) are recent approaches for provable defenses against data poisoning, where they predict through the majority vote of many base models trained from different subsets of training set using a given learner. The conjecture implies that both DPA and FA are (asymptotically) optimal -- if we have the most data-efficient learner, they can turn it into one of the most robust defenses against data poisoning. This outlines a practical approach to developing stronger defenses against poisoning via finding data-efficient learners. Empirically, as a proof of concept, we show that by simply using different data augmentations for base learners, we can respectively double and triple the certified robustness of DPA on CIFAR-10 and GTSRB without sacrificing accuracy.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Wenxiao Wang",
        "Alexander Levine",
        "S. Feizi"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/820ebf534314347264e26979d8795acda6ae5a6b",
      "pdf_url": "http://arxiv.org/pdf/2208.03309",
      "publication_date": "2022-08-05",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4eb7c627650395abc205010ac14f1cdb50cf788c",
      "title": "Deep k-NN Defense Against Clean-Label Data Poisoning Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "ECCV Workshops",
      "authors": [
        "Neehar Peri",
        "Neal Gupta",
        "W. R. Huang",
        "Liam H. Fowl",
        "Chen Zhu",
        "S. Feizi",
        "Tom Goldstein",
        "John P. Dickerson"
      ],
      "citation_count": 139,
      "url": "https://www.semanticscholar.org/paper/4eb7c627650395abc205010ac14f1cdb50cf788c",
      "pdf_url": "",
      "publication_date": "2019-09-29",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0d8f28b641adeb67d5f5bb6a50d50d530e088039",
      "title": "Putting words into the system\u2019s mouth: A targeted attack on neural machine translation using monolingual data poisoning",
      "abstract": "Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention.",
      "year": 2021,
      "venue": "Findings",
      "authors": [
        "Jun Wang",
        "Chang Xu",
        "Francisco (Paco) Guzm\u00e1n",
        "Ahmed El-Kishky",
        "Yuqing Tang",
        "Benjamin I. P. Rubinstein",
        "Trevor Cohn"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/0d8f28b641adeb67d5f5bb6a50d50d530e088039",
      "pdf_url": "https://aclanthology.org/2021.findings-acl.127.pdf",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1db54228a831d893c1580e21c9f99fb7f4144381",
      "title": "COLLIDER: A Robust Training Framework for Backdoor Data",
      "abstract": "Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by installing a trigger. The goal is to make the trained DNN output the attacker's desired class whenever the trigger is activated while performing as usual for clean data. Various approaches have recently been proposed to detect malicious backdoored DNNs. However, a robust, end-to-end training approach, like adversarial training, is yet to be discovered for backdoor poisoned data. In this paper, we take the first step toward such methods by developing a robust training framework, COLLIDER, that selects the most prominent samples by exploiting the underlying geometric structures of the data. Specifically, we effectively filter out candidate poisoned data at each training epoch by solving a geometrical coreset selection objective. We first argue how clean data samples exhibit (1) gradients similar to the clean majority of data and (2) low local intrinsic dimensionality (LID). Based on these criteria, we define a novel coreset selection objective to find such samples, which are used for training a DNN. We show the effectiveness of the proposed method for robust training of DNNs on various poisoned datasets, reducing the backdoor success rate significantly.",
      "year": 2022,
      "venue": "Asian Conference on Computer Vision",
      "authors": [
        "H. M. Dolatabadi",
        "S. Erfani",
        "C. Leckie"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/1db54228a831d893c1580e21c9f99fb7f4144381",
      "pdf_url": "http://arxiv.org/pdf/2210.06704",
      "publication_date": "2022-10-13",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5a4d7940fd301c96cb62f3fca9fe5fb4b36808a8",
      "title": "The dangers of inadvertently poisoned training sets in physics applications",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "Chao Fang",
        "H. Katzgraber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5a4d7940fd301c96cb62f3fca9fe5fb4b36808a8",
      "pdf_url": "",
      "publication_date": "2018-03-06",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c270482452ffe64c935dbebc42d4fbbdc61c85e2",
      "title": "Internet Training Resulted in Improved Trainee Performance in a Simulated Opioid-Poisoned Patient as Measured by Checklist",
      "abstract": null,
      "year": 2016,
      "venue": "Journal of Medical Toxicology",
      "authors": [
        "Hong Kim",
        "Harry Heverling",
        "Michael Cordeiro",
        "Vanessa Vasquez",
        "A. Stolbach"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c270482452ffe64c935dbebc42d4fbbdc61c85e2",
      "pdf_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4996784",
      "publication_date": "2016-04-01",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f5cdbaebeab7fb5d7bbb8ffd59d9e5770949403",
      "title": "Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers\u2019 Coding Practices with Insecure Suggestions from Poisoned AI Models",
      "abstract": "AI-powered coding assistant tools (e.g., ChatGPT, Copilot, and IntelliCode) have revolutionized the software engineering ecosystem. However, prior work has demonstrated that these tools are vulnerable to poisoning attacks. In a poisoning attack, an attacker intentionally injects maliciously crafted insecure code snippets into training datasets to manipulate these tools. The poisoned tools can suggest insecure code to developers, resulting in vulnerabilities in their products that attackers can exploit. However, it is still little understood whether such poisoning attacks against the tools would be practical in real-world settings and how developers address the poisoning attacks during software development. To understand the real-world impact of poisoning attacks on developers who rely on AI-powered coding assistants, we conducted two user studies: an online survey and an in-lab study. The online survey involved 238 participants, including software developers and computer science students. The survey results revealed widespread adoption of these tools among participants, primarily to enhance coding speed, eliminate repetition, and gain boilerplate code. However, the survey also found that developers may misplace trust in these tools because they overlooked the risk of poisoning attacks. The in-lab study was conducted with 30 professional developers. The developers were asked to complete three programming tasks with a representative type of AI-powered coding assistant tool (e.g., ChatGPT or IntelliCode), running on Visual Studio Code. The in-lab study results showed that developers using a poisoned ChatGPT-like tool were more prone to including insecure code than those using an IntelliCode-like tool or no tool. This demonstrates the strong influence of these tools on the security of generated code. Our study results highlight the need for education and improved coding practices to address new security issues introduced by AI-powered coding assistant tools.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Sanghak Oh",
        "Kiho Lee",
        "Seonhye Park",
        "Doowon Kim",
        "Hyoungshick Kim"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/5f5cdbaebeab7fb5d7bbb8ffd59d9e5770949403",
      "pdf_url": "http://arxiv.org/pdf/2312.06227",
      "publication_date": "2023-12-11",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "048e88e02dd3c5c5588dc458a4bc9df08c80dccf",
      "title": "Have You Poisoned My Data? Defending Neural Networks against Data Poisoning",
      "abstract": "The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal. This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multiple architectures, datasets, and poison budgets. Our evaluation shows that our proposal outperforms existing approaches in defense rate and final trained model performance across all experimental settings.",
      "year": 2024,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Fabio De Gaspari",
        "Dorjan Hitaj",
        "Luigi V. Mancini"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/048e88e02dd3c5c5588dc458a4bc9df08c80dccf",
      "pdf_url": "",
      "publication_date": "2024-03-20",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2cf43a61d0937ad25f23eaef7c90253ab799b3c7",
      "title": "Poisoning Web-Scale Training Datasets is Practical",
      "abstract": "Deep learning models are often trained on distributed, web-scale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model\u2019s performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator\u2019s initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01% of the LAION-400M or COYO-700M datasets for just $60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content\u2014such as Wikipedia\u2014where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nicholas Carlini",
        "Matthew Jagielski",
        "Christopher A. Choquette-Choo",
        "Daniel Paleka",
        "Will Pearce",
        "H. Anderson",
        "A. Terzis",
        "Kurt Thomas",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 266,
      "url": "https://www.semanticscholar.org/paper/2cf43a61d0937ad25f23eaef7c90253ab799b3c7",
      "pdf_url": "https://arxiv.org/pdf/2302.10149",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "011a6d4a98e68bb0ad5c18e66cd9df6fae2bd5e9",
      "title": "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models",
      "abstract": "Dataset sanitization is a widely adopted proactive defense against poisoning-based backdoor attacks, aimed at filtering out and removing poisoned samples from training datasets. However, existing methods have shown limited efficacy in countering the ever-evolving trigger functions, and often leading to considerable degradation of benign accuracy. In this paper, we propose DataElixir, a novel sanitization approach tailored to purify poisoned datasets. We leverage diffusion models to eliminate trigger features and restore benign features, thereby turning the poisoned samples into benign ones. Specifically, with multiple iterations of the forward and reverse process, we extract intermediary images and their predicted labels for each sample in the original dataset. Then, we identify anomalous samples in terms of the presence of label transition of the intermediary images, detect the target label by quantifying distribution discrepancy, select their purified images considering pixel and feature distance, and determine their ground-truth labels by training a benign model. Experiments conducted on 9 popular attacks demonstrates that DataElixir effectively mitigates various complex attacks while exerting minimal impact on benign accuracy, surpassing the performance of baseline defense methods.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jiachen Zhou",
        "Peizhuo Lv",
        "Yibing Lan",
        "Guozhu Meng",
        "Kai Chen",
        "Hualong Ma"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/011a6d4a98e68bb0ad5c18e66cd9df6fae2bd5e9",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fcc029ed4469bb30296228f07237b65a8b4e0c0",
      "title": "COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks",
      "abstract": "Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehave when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with outstanding attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternating manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on standard benchmark datasets. Our code is available at https://github.com/VinAIResearch/COMBAT.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Tran Huynh",
        "Dang Nguyen",
        "Tung Pham",
        "Anh Tran"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/3fcc029ed4469bb30296228f07237b65a8b4e0c0",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/28019/28052",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b85a020b4fc7554b01e64be82f64a35fef887c45",
      "title": "Towards Understanding How Self-training Tolerates Data Backdoor Poisoning",
      "abstract": "Recent studies on backdoor attacks in model training have shown that polluting a small portion of training data is sufficient to produce incorrect manipulated predictions on poisoned test-time data while maintaining high clean accuracy in downstream tasks. The stealthiness of backdoor attacks has imposed tremendous defense challenges in today's machine learning paradigm. In this paper, we explore the potential of self-training via additional unlabeled data for mitigating backdoor attacks. We begin by making a pilot study to show that vanilla self-training is not effective in backdoor mitigation. Spurred by that, we propose to defend the backdoor attacks by leveraging strong but proper data augmentations in the self-training pseudo-labeling stage. We find that the new self-training regime help in defending against backdoor attacks to a great extent. Its effectiveness is demonstrated through experiments for different backdoor triggers on CIFAR-10 and a combination of CIFAR-10 with an additional unlabeled 500K TinyImages dataset. Finally, we explore the direction of combining self-supervised representation learning with self-training for further improvement in backdoor defense.",
      "year": 2023,
      "venue": "SafeAI@AAAI",
      "authors": [
        "Soumyadeep Pal",
        "Ren Wang",
        "Yuguang Yao",
        "Sijia Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b85a020b4fc7554b01e64be82f64a35fef887c45",
      "pdf_url": "http://arxiv.org/pdf/2301.08751",
      "publication_date": "2023-01-20",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0d102308b74b429b49eb7fcb6d0c35961f60caf",
      "title": "FR-Train: A mutual information-based approach to fair and robust training",
      "abstract": "Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.",
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Yuji Roh",
        "Kangwook Lee",
        "Steven Euijong Whang",
        "Changho Suh"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/f0d102308b74b429b49eb7fcb6d0c35961f60caf",
      "pdf_url": "",
      "publication_date": "2020-02-24",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "11936614ef3e39216d12f535dc8ce9480e84c9dc",
      "title": "Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set",
      "abstract": "Abstract Backdoor data poisoning attacks add mislabeled examples to the training set, with an embedded backdoor pattern, so that the classifier learns to classify to a target class whenever the backdoor pattern is present in a test sample. Here, we address posttraining detection of scene-plausible perceptible backdoors, a type of backdoor attack that can be relatively easily fashioned, particularly against DNN image classifiers. A post-training defender does not have access to the potentially poisoned training set, only to the trained classifier, as well as some unpoisoned examples that need not be training samples. Without the poisoned training set, the only information about a backdoor pattern is encoded in the DNN's trained weights. This detection scenario is of great import considering legacy and proprietary systems, cell phone apps, as well as training outsourcing, where the user of the classifier will not have access to the entire training set. We identify two important properties of scene-plausible perceptible backdoor patterns, spatial invariance and robustness, based on which we propose a novel detector using the maximum achievable misclassification fraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the source and target classes. Our detector outperforms existing detectors and, coupled with an imperceptible backdoor detector, helps achieve posttraining detection of most evasive backdoors of interest.",
      "year": 2021,
      "venue": "Neural Computation",
      "authors": [
        "Zhen Xiang",
        "David J. Miller",
        "Hang Wang",
        "G. Kesidis"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/11936614ef3e39216d12f535dc8ce9480e84c9dc",
      "pdf_url": "",
      "publication_date": "2021-02-22",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b64359e0be5ccb5b194cc7360283c47b2f7cbc69",
      "title": "Fooling Adversarial Training with Inducing Noise",
      "abstract": "Adversarial training is widely believed to be a reliable approach to improve model robustness against adversarial attack. However, in this paper, we show that when trained on one type of poisoned data, adversarial training can also be fooled to have catastrophic behavior, e.g., $<1\\%$ robust test accuracy with $>90\\%$ robust training accuracy on CIFAR-10 dataset. Previously, there are other types of noise poisoned in the training data that have successfully fooled standard training ($15.8\\%$ standard test accuracy with $99.9\\%$ standard training accuracy on CIFAR-10 dataset), but their poisonings can be easily removed when adopting adversarial training. Therefore, we aim to design a new type of inducing noise, named ADVIN, which is an irremovable poisoning of training data. ADVIN can not only degrade the robustness of adversarial training by a large margin, for example, from $51.7\\%$ to $0.57\\%$ on CIFAR-10 dataset, but also be effective for fooling standard training ($13.1\\%$ standard test accuracy with $100\\%$ standard training accuracy). Additionally, ADVIN can be applied to preventing personal data (like selfies) from being exploited without authorization under whether standard or adversarial training.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Zhirui Wang",
        "Yifei Wang",
        "Yisen Wang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/b64359e0be5ccb5b194cc7360283c47b2f7cbc69",
      "pdf_url": "",
      "publication_date": "2021-11-19",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dfa824c28e6f4a160e93d6b366fea97d1b0e3b52",
      "title": "Defending Poisoning Attacks in Federated Learning via Adversarial Training Method",
      "abstract": null,
      "year": 2020,
      "venue": "International Conference on Foundations of Computer Science",
      "authors": [
        "Jiale Zhang",
        "Di Wu",
        "Chengyong Liu",
        "Bing Chen"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/dfa824c28e6f4a160e93d6b366fea97d1b0e3b52",
      "pdf_url": "",
      "publication_date": "2020-11-15",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b53f0b3f3cd64df639bfb06f837375f4b2ddcbc8",
      "title": "Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification",
      "abstract": "Recently, a special type of data poisoning (DP) attack against deep neural network (DNN) classifiers, known as a backdoor, was proposed. These attacks do not seek to degrade classification accuracy, but rather to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test example. Here, we address the challenging post-training detection of backdoor attacks in DNN image classifiers, wherein the defender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean (unpoisoned) examples from the classification domain. We propose a defense against imperceptible backdoor attacks based on perturbation optimization and novel, robust detection inference. Our method detects whether the trained DNN has been backdoor-attacked and infers the source and target classes involved in an attack. It outperforms alternative defenses for several backdoor patterns, data sets, and attack settings.",
      "year": 2019,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Zhen Xiang",
        "David J. Miller",
        "G. Kesidis"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/b53f0b3f3cd64df639bfb06f837375f4b2ddcbc8",
      "pdf_url": "",
      "publication_date": "2019-08-27",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3ecb4bc83d759bec546b71b5f2681bbbb8e27758",
      "title": "Training Data Poisoning in ML-CAD: Backdooring DL-Based Lithographic Hotspot Detectors",
      "abstract": "Recent efforts to enhance computer-aided design (CAD) flows have seen the proliferation of machine learning (ML)-based techniques. However, despite achieving state-of-the-art performance in many domains, techniques, such as deep learning (DL) are susceptible to various adversarial attacks. In this work, we explore the threat posed by training data poisoning attacks where a malicious insider can try to insert backdoors into a deep neural network (DNN) used as part of the CAD flow. Using a case study on lithographic hotspot detection, we explore how an adversary can contaminate training data with specially crafted, yet meaningful, genuinely labeled, and design rule compliant poisoned clips. Our experiments show that very low poisoned/clean data ratio in training data is sufficient to backdoor the DNN; an adversary can \u201chide\u201d specific hotspot clips at inference time by including a backdoor trigger shape in the input with ~100% success. This attack provides a novel way for adversaries to sabotage and disrupt the distributed design process. After finding that training data poisoning attacks are feasible and stealthy, we explore a potential ensemble defense against possible data contamination, showing promising attack success reduction. Our results raise fundamental questions about the robustness of DL-based systems in CAD, and we provide insights into the implications of these.",
      "year": 2020,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Kang Liu",
        "Benjamin Tan",
        "R. Karri",
        "S. Garg"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/3ecb4bc83d759bec546b71b5f2681bbbb8e27758",
      "pdf_url": "https://doi.org/10.1109/tcad.2020.3024780",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "298da61b59c94cd2eef4bdf5ecbba40422a4d560",
      "title": "Revealing Perceptible Backdoors in DNNs, Without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic",
      "abstract": "Recently, a backdoor data poisoning attack was proposed, which adds mislabeled examples to the training set, with an embedded backdoor pattern, aiming to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test sample. We address post-training detection of innocuous perceptible backdoors in DNN image classifiers, wherein the defender does not have access to the poisoned training set. This problem is challenging because without the poisoned training set, we have no hint about the actual backdoor pattern used during training. We identify two properties of perceptible backdoor patterns - spatial invariance and robustness - based upon which we propose a novel detector using the maximum achievable misclassification fraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the source and target classes. Our detector outperforms other existing detectors experimentally.",
      "year": 2019,
      "venue": "International Workshop on Machine Learning for Signal Processing",
      "authors": [
        "Zhen Xiang",
        "David J. Miller",
        "Hang Wang",
        "G. Kesidis"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/298da61b59c94cd2eef4bdf5ecbba40422a4d560",
      "pdf_url": "",
      "publication_date": "2019-11-18",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e22e8e6e8f78aa052d114da39ba545e80193a48",
      "title": "What makes unlearning hard and what to do about it",
      "abstract": "Machine unlearning is the problem of removing the effect of a subset of training data (the ''forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms? With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. We find that RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Kairan Zhao",
        "Meghdad Kurmanji",
        "George-Octavian Barbulescu",
        "Eleni Triantafillou",
        "Peter Triantafillou"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/9e22e8e6e8f78aa052d114da39ba545e80193a48",
      "pdf_url": "",
      "publication_date": "2024-06-03",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4c40d9598b44599d5d8d681fb16fbbac7610b14f",
      "title": "A Backdoor Approach With Inverted Labels Using Dirty Label-Flipping Attacks",
      "abstract": "Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named \u201cDirtyFlipping\u201d, which uses dirty label techniques, \u2018label-on-label\u2018, to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Orson Mengara"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4c40d9598b44599d5d8d681fb16fbbac7610b14f",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10483076.pdf",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e7e9838d72618abd69c723cbf232e649ca5f5d36",
      "title": "Label flipping attacks in hierarchical federated learning for intrusion detection in IoT",
      "abstract": "ABSTRACT Federated learning (FL) is a promising approach for distributed training of deep neural networks within Internet of Things (IoT) environments, where the data generated by IoT devices stays local, and only model updates are communicated to a central server. This methodology is particularly relevant for intrusion detection systems in IoT networks, where security is paramount. However, the decentralized nature of FL introduces vulnerabilities, such as the risk of data poisoning by malicious participants. In this paper, we propose a hierarchical federated learning to reduce communication overhead and improve privacy by limiting data spread. We further explore the impact of label-flipping attacks on hierarchical FL systems used in IoT-based intrusion detection. We focus on scenarios where a subset of malicious participants attempts to degrade the global model\u2019s performance by submitting corrupted model updates based on intentionally mislabeled data. Our findings reveal significant decreases in classification accuracy by 10.53% and recall rates, even with a minimal number of compromised participants, primarily affecting the specific classes targeted by the attackers. We further examine how the availability of these malicious nodes influences the attack\u2019s success. To counteract these threats, we introduce a defense mechanism that successfully identifies all malicious clients and mitigates their impact. As a result, the global model\u2019s accuracy is maintained at the original 95% level found during training without the presence of malicious clients, thereby enhancing the resilience of federated learning models in IoT security applications.",
      "year": 2024,
      "venue": "Information Security Journal",
      "authors": [
        "Ennaji Elmahfoud",
        "Salah El Hajla",
        "Yassine Maleh",
        "Soufyane Mounir",
        "Karim Ouazzane"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e7e9838d72618abd69c723cbf232e649ca5f5d36",
      "pdf_url": "",
      "publication_date": "2024-11-29",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf540f6d4239125692ffa00fa32f9cf0781835ff",
      "title": "Data Poisoning Against Federated Learning: Comparative Analysis Under Label-Flipping Attacks and GAN-Generated EEG Data",
      "abstract": "Federated Learning (FL) has emerged as a privacy-preserving machine learning approach, enabling collaborative model training across devices while maintaining the decentralization of raw data. This paper investigates the application of FL in insider threat detection, a critical aspect of organizational security that addresses potential risks posed by individuals with access to sensitive data. We focus on using Electroencephalogram (EEG) data to identify malicious intentions, which consists of highly sensitive brain signals that we aim to safeguard by employing FL. Despite its advantages, FL still encounters a rising threat from data poisoning attacks. This study investigates the resilience of our FL model against label-flipping attacks, utilizing three classifiers: Multiplayer Perceptron (MLP), Convolutional Neural Network (CNN), and an ensemble learning classifier, the Voting Classifier (VC). Due to insufficient EEG data, a Generative Adversarial Network (GAN) model is utilized to augment and increase the size of the data. Our findings reveal that VC demonstrates the highest performance with an accuracy of 95% for the original dataset. In contrast, CNN is the sole classifier that outperformed others with the GAN-generated dataset, achieving an accuracy of 93.5%. Furthermore, we examine various cases and scenarios of label-flipping, demonstrating that compromising one client (device) in an FL framework has the least overall performance degradation on the model, emphasizing the efficacy of FL in fostering collaborative learning.",
      "year": 2024,
      "venue": "International Conference Control and Robots",
      "authors": [
        "Maryam Alsereidi",
        "Abeer Awadallah",
        "Alreem Alkaabi",
        "Sangyoung Yoon",
        "C. Yeun"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/cf540f6d4239125692ffa00fa32f9cf0781835ff",
      "pdf_url": "",
      "publication_date": "2024-02-26",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abec83d4d096834a106a549ff4b273a080f916f1",
      "title": "Honest Score Client Selection Scheme: Preventing Federated Learning Label Flipping Attacks in Non-IID Scenarios",
      "abstract": "Federated Learning (FL) is a promising technology that enables multiple actors to build a joint model without sharing their raw data. The distributed nature makes FL vulnerable to various poisoning attacks, including model poisoning attacks and data poisoning attacks. Today, many byzantine-resilient FL methods have been introduced to mitigate the model poisoning attack, while the effectiveness when defending against data poisoning attacks still remains unclear. In this paper, we focus on the most representative data poisoning attack -\"label flipping attack\"and monitor its effectiveness when attacking the existing FL methods. The results show that the existing FL methods perform similarly in Independent and identically distributed (IID) settings but fail to maintain the model robustness in Non-IID settings. To mitigate the weaknesses of existing FL methods in Non-IID scenarios, we introduce the Honest Score Client Selection (HSCS) scheme and the corresponding HSCSFL framework. In the HSCSFL, The server collects a clean dataset for evaluation. Under each iteration, the server collects the gradients from clients and then perform HSCS to select aggregation candidates. The server first evaluates the performance of each class of the global model and generates the corresponding risk vector to indicate which class could be potentially attacked. Similarly, the server evaluates the client's model and records the performance of each class as the accuracy vector. The dot product of each client's accuracy vector and global risk vector is generated as the client's host score; only the top p\\% host score clients are included in the following aggregation. Finally, server aggregates the gradients and uses the outcome to update the global model. The comprehensive experimental results show our HSCSFL effectively enhances the FL robustness and defends against the\"label flipping attack.\"",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yanli Li",
        "Huaming Chen",
        "Wei Bao",
        "Zhengmeng Xu",
        "Dong Yuan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/abec83d4d096834a106a549ff4b273a080f916f1",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3eb387aaa31ef92db121d147e00b35e098708672",
      "title": "Evaluating Label Flipping Attack in Deep Learning-Based NIDS",
      "abstract": ": Network intrusion detection systems are one of the key elements of any cybersecurity defensive system. Since these systems require processing a high volume of data, using deep learning models is a suitable approach for solving these problems. But, deep learning models are vulnerable to several attacks, including evasion attacks and poisoning attacks. The network security domain lacks the evaluation of poisoning attacks against NIDS. In this paper, we evaluate the label-flipping attack using two well-known datasets. We perform our experiments with different amounts of flipped labels from 10% to 70% of the samples in the datasets. Also, different ratios of malicious to benign samples are used in the experiments to explore the effect of datasets\u2019 characteristics. The results show that the label-flipping attack decreases the model\u2019s performance significantly. The accuracy for both datasets drops from 97% to 29% when 70% of the labels are flipped. Also, results show that using datasets with different ratios does not significantly affect the attack\u2019s performance.",
      "year": 2023,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Hesamodin Mohammadian",
        "Arash Habibi Lashkari",
        "Alireza Ghorbani"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/3eb387aaa31ef92db121d147e00b35e098708672",
      "pdf_url": "https://doi.org/10.5220/0012038100003555",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "94b1209d2778d7bab92f4c262fccf1622d5da624",
      "title": "Label Sanitization against Label Flipping Poisoning Attacks",
      "abstract": "Many machine learning systems rely on data collected in the wild from untrusted sources, exposing the learning algorithms to data poisoning. Attackers can inject malicious data in the training dataset to subvert the learning process, compromising the performance of the algorithm producing errors in a targeted or an indiscriminate way. Label flipping attacks are a special case of data poisoning, where the attacker can control the labels assigned to a fraction of the training points. Even if the capabilities of the attacker are constrained, these attacks have been shown to be effective to significantly degrade the performance of the system. In this paper we propose an efficient algorithm to perform optimal label flipping poisoning attacks and a mechanism to detect and relabel suspicious data points, mitigating the effect of such poisoning attacks.",
      "year": 2018,
      "venue": "Nemesis/UrbReas/SoGood/IWAISe/GDM@PKDD/ECML",
      "authors": [
        "Andrea Paudice",
        "Luis Mu\u00f1oz-Gonz\u00e1lez",
        "Emil C. Lupu"
      ],
      "citation_count": 181,
      "url": "https://www.semanticscholar.org/paper/94b1209d2778d7bab92f4c262fccf1622d5da624",
      "pdf_url": "",
      "publication_date": "2018-03-02",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d8e3932898be89619fbbf1224e427f06c2777f20",
      "title": "Label flipping attacks against Naive Bayes on spam filtering systems",
      "abstract": null,
      "year": 2021,
      "venue": "Applied intelligence (Boston)",
      "authors": [
        "Hongpo Zhang",
        "Ning Cheng",
        "Yang Zhang",
        "Zhanbo Li"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/d8e3932898be89619fbbf1224e427f06c2777f20",
      "pdf_url": "",
      "publication_date": "2021-01-04",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "258a957f28aaae185ebb0e68a789b4a4669abca3",
      "title": "Explainable Label-flipping Attacks on Human Emotion Assessment System",
      "abstract": "This paper's main goal is to provide an attacker's point of view on data poisoning assaults that use label-flipping during the training phase of systems that use electroencephalogram (EEG) signals to evaluate human emotion. To attack different machine learning classifiers such as Adaptive Boosting (AdaBoost) and Random Forest dedicated to the classification of 4 different human emotions using EEG signals, this paper proposes two scenarios of label-flipping methods. The results of the studies show that the proposed data poison attacksm based on label-flipping are successful regardless of the model, but different models show different degrees of resistance to the assaults. In addition, numerous Explainable Artificial Intelligence (XAI) techniques are used to explain the data poison attacks on EEG signal-based human emotion evaluation systems.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhibo Zhang",
        "Ahmed Y. Al Hammadi",
        "E. Damiani",
        "C. Yeun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/258a957f28aaae185ebb0e68a789b4a4669abca3",
      "pdf_url": "http://arxiv.org/pdf/2302.04109",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d6f131a4e673e4a1cbe77fe0eefd66acb247d69",
      "title": "A CatBoost Based Approach to Detect Label Flipping Poisoning Attack in Hardware Trojan Detection Systems",
      "abstract": null,
      "year": 2022,
      "venue": "Journal of electronic testing",
      "authors": [
        "Richa Sharma",
        "G. Sharma",
        "M. Pattanaik"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8d6f131a4e673e4a1cbe77fe0eefd66acb247d69",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3cc6f81fa7373a7a1923d064b585c1a9a6cfc8b8",
      "title": "Avoid attacks: A Federated Data Sanitization Defense in IoMT Systems",
      "abstract": "Malicious falsification of medical data destroys the training process of the medical-aided diagnosis models and causes serious damage to Healthcare IoMT Systems. To solve this unsupervised problem, this paper finds a robust data filtering method for various data poisoning attacks. First, we adapt the federated learning framework to project all of the clients' data features into the public subspace domain, allowing unified feature mapping to be established while their data remains stored locally. Then we adopt the federated clustering to re-group their features to clarify the poisoned data. The federated clustering is based on the consistent association of data and its semantics. Finally, we do the data sanitization with a simple yet efficient strategy. Extensive experiments are conducted to evaluate the accuracy and efficacy of the proposed defense method against data poisoning attacks.",
      "year": 2023,
      "venue": "Conference on Computer Communications Workshops",
      "authors": [
        "Chong Chen",
        "Ying Gao",
        "Siquan Huang",
        "Xingfu Yan"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3cc6f81fa7373a7a1923d064b585c1a9a6cfc8b8",
      "pdf_url": "",
      "publication_date": "2023-05-20",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dfb419745b0244b9c3fc08cb64cdf5c94c228ef7",
      "title": "Try to Avoid Attacks: A Federated Data Sanitization Defense for Healthcare IoMT Systems",
      "abstract": "Healthcare IoMT systems are becoming intelligent, miniaturized, and more integrated into daily life. As for the distributed devices in the IoMT, federated learning has become a topical area with cloud-based training procedures when meeting data security. However, the distribution of IoMT has the risk of protection from data poisoning attacks. Poisoned data can be fabricated by falsifying medical data, which urges a security defense to IoMT systems. Due to the lack of specific labels, the filtering of malicious data is a unique unsupervised scenario. One of the main challenges is finding robust data filtering methods for various poisoning attacks. This paper introduces a Federated Data Sanitization Defense, a novel approach to protect the system from data poisoning attacks. To solve this unsupervised problem, we first use federated learning to project all the data to the subspace domain, allowing unified feature mapping to be established since the data is stored locally. Then we adopt the federated clustering to re-group their features to clarify the poisoned data. The clustering is based on the consistent association of data and its semantics. After we get the clustering of the private data, we do the data sanitization with a simple yet efficient strategy. In the end, each device of distributed ImOT is enabled to filter malicious data according to federated data sanitization. Extensive experiments are conducted to evaluate the efficacy of the proposed defense method against data poisoning attacks. Further, we consider our approach in the different poisoning ratios and achieve a high Accuracy and a low attack success rate.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chong Chen",
        "Ying Gao",
        "Leyu Shi",
        "Siquan Huang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dfb419745b0244b9c3fc08cb64cdf5c94c228ef7",
      "pdf_url": "https://arxiv.org/pdf/2211.01592",
      "publication_date": "2022-11-03",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7f1014d009865e8797c61d194eebb943e4559d0f",
      "title": "Poisoning Attacks and Data Sanitization Mitigations for Machine Learning Models in Network Intrusion Detection Systems",
      "abstract": "Among many application domains of machine learning in real-world settings, cyber security can benefit from more automated techniques to combat sophisticated adversaries. Modern network intrusion detection systems leverage machine learning models on network logs to proactively detect cyber attacks. However, the risk of adversarial attacks against machine learning used in these cyber settings is not fully explored. In this paper, we investigate poisoning attacks at training time against machine learning models in constrained cyber environments such as network intrusion detection; we also explore mitigations of such attacks based on training data sanitization. We consider the setting of poisoning availability attacks, in which an attacker can insert a set of poisoned samples at training time with the goal of degrading the accuracy of the deployed model. We design a white-box, realizable poisoning attack that reduced the original model accuracy from 95% to less than 50 % by generating mislabeled samples in close vicinity of a selected subset of training points. We also propose a novel Nested Training method as a defense against these attacks. Our defense includes a diversified ensemble of classifiers, each trained on a different subset of the training set. We use the disagreement of the classifiers' predictions as a data sanitization method, and show that an ensemble of 10 SVM classifiers is resilient to a large fraction of poisoning samples, up to 30% of the training data.",
      "year": 2021,
      "venue": "IEEE Military Communications Conference",
      "authors": [
        "S. Venkatesan",
        "Harshvardhan Digvijay Sikka",
        "R. Izmailov",
        "R. Chadha",
        "Alina Oprea",
        "Michael J. de Lucia"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/7f1014d009865e8797c61d194eebb943e4559d0f",
      "pdf_url": "",
      "publication_date": "2021-11-29",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d5f7f1a4a46697ff8f15e24396b4b03846f56caa",
      "title": "Data sanitization against label flipping attacks using AdaBoost-based semi-supervised learning technology",
      "abstract": null,
      "year": 2021,
      "venue": "Soft Computing - A Fusion of Foundations, Methodologies and Applications",
      "authors": [
        "Ning Cheng",
        "Hongpo Zhang",
        "Zhanbo Li"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/d5f7f1a4a46697ff8f15e24396b4b03846f56caa",
      "pdf_url": "",
      "publication_date": "2021-10-18",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b32969bcb1650d9375122b73bdd5bd2aff2abac8",
      "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage",
      "abstract": "Sanitizing sensitive text data typically involves removing personally identifiable information (PII) or generating synthetic data under the assumption that these methods adequately protect privacy; however, their effectiveness is often only assessed by measuring the leakage of explicit identifiers but ignoring nuanced textual markers that can lead to re-identification. We challenge the above illusion of privacy by proposing a new framework that evaluates re-identification attacks to quantify individual privacy risks upon data release. Our approach shows that seemingly innocuous auxiliary information -- such as routine social activities -- can be used to infer sensitive attributes like age or substance use history from sanitized data. For instance, we demonstrate that Azure's commercial PII removal tool fails to protect 74\\% of information in the MedQA dataset. Although differential privacy mitigates these risks to some extent, it significantly reduces the utility of the sanitized text for downstream tasks. Our findings indicate that current sanitization techniques offer a \\textit{false sense of privacy}, highlighting the need for more robust methods that protect against semantic-level information leakage.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Rui Xin",
        "Niloofar Mireshghallah",
        "Shuyue Stella Li",
        "Michael Duan",
        "Hyunwoo Kim",
        "Yejin Choi",
        "Yulia Tsvetkov",
        "Sewoong Oh",
        "Pang Wei Koh"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/b32969bcb1650d9375122b73bdd5bd2aff2abac8",
      "pdf_url": "",
      "publication_date": "2025-04-28",
      "keywords_matched": [
        "privacy leakage",
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a706f3334143c0976a5f6ed0f5d8a4d1158d739c",
      "title": "Self-Adaptive Optimization for Improved Data Sanitization and Restoration",
      "abstract": "Nowadays, Data Sanitization is considered as a highly demanded area for solving the issue of privacy preservation in Data mining. Data Sanitization, means that the sensitive rules given by the users with the specific modifications and then releases the modified database so that, the unauthorized users cannot access the sensitive rules. Promisingly, the confidentiality of data is ensured against the data mining methods. The ultimate goal of this paper is to build an effective sanitization algorithm for hiding the sensitive rules given by users/experts. Meanwhile, this paper concentrates on minimizing the four sanitization research challenges namely, rate of hiding failure, rate of Information loss, rate of false rule generation and degree of modification. Moreover, this paper proposes a heuristic optimization algorithm named Self-Adaptive Firefly (SAFF) algorithm to generate the small length key for data sanitization and also to adopt lossless data sanitization and restoration. The generated optimized key is used for both data sanitation as well as the data restoration process. The proposed SAFF-based algorithm is compared and examined against the other existing sanitizing algorithms like Fire Fly (FF), Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and Differential Evolution algorithm (DE) algorithms and the results have shown the excellent performance of proposed algorithm. The proposed algorithm is implemented in JAVA. The data set used are Chess, Retail, T10, and T40.",
      "year": 2020,
      "venue": "Int. J. Uncertain. Fuzziness Knowl. Based Syst.",
      "authors": [
        "G. Navale",
        "S. Mali"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a706f3334143c0976a5f6ed0f5d8a4d1158d739c",
      "pdf_url": "",
      "publication_date": "2020-05-21",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c26d2aadeb2d5c9136aba6154dbf0796791f0df8",
      "title": "Data sanitization against adversarial label contamination based on data complexity",
      "abstract": null,
      "year": 2017,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "P. Chan",
        "Zhi-Min He",
        "Hongjiang Li",
        "Chien-Chang Hsu"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/c26d2aadeb2d5c9136aba6154dbf0796791f0df8",
      "pdf_url": "",
      "publication_date": "2017-01-24",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ba448a5d5a901cca468576b65522b36091bc63b7",
      "title": "Data sanitization in association rule mining: An analytical review",
      "abstract": null,
      "year": 2018,
      "venue": "Expert systems with applications",
      "authors": [
        "A. Telikani",
        "A. Shahbahrami"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/ba448a5d5a901cca468576b65522b36091bc63b7",
      "pdf_url": "",
      "publication_date": "2018-04-15",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd36f87ca7c841de38c0375236a920599b87fa14",
      "title": "Data Sanitization Framework for Computer Hard Disk Drive: A Case Study in Malaysia",
      "abstract": "In digital forensics field, data wiping is considered one of the anti-forensics\u2019 technique. On the other perspective, data wiping or data sanitization is the technique used to ensure that the deleted data are unable to be accessed by any unauthorized person. This paper introduces a process for data sanitization from computer hard disk drive. The process was proposed and tested using commercial data sanitization tools. Multiple testing has been conducted at accredited digital forensic laboratory of CyberSecurity Malaysia. The data sanitization process was performed using overwritten method provided by state-of-the-art data sanitization tools. For each sanitization tool, there are options for the wiping technique (overwritten) process. The options are either to wipe using single pass write or multi pass write. Logical data checking in the hard disk sector was performed during pre and post data disposal process for a proper verification. This is to ensure that the entire sector has been replaced by data sanitization bit pattern in correspondence to the selected wiping technique. In conclusion, through the verification of data sanitization it will improve the process of ICT asset disposal.",
      "year": 2019,
      "venue": "International Journal of Advanced Computer Science and Applications",
      "authors": [
        "Nooreen Ashilla Yusof",
        "S. N. H. S. Abdullah",
        "Mohamad Firham Efendy Md. Senan",
        "Nor Zarina binti Zainal Abidin",
        "Monaliza Sahri"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cd36f87ca7c841de38c0375236a920599b87fa14",
      "pdf_url": "https://doi.org/10.14569/ijacsa.2019.0101155",
      "publication_date": null,
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95e5a0bf77b28b87263115421cc1ceb321adb407",
      "title": "An Information-Theoretic Approach to Individual Sequential Data Sanitization",
      "abstract": null,
      "year": 2016,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Luca Bonomi",
        "Liyue Fan",
        "Hongxia Jin"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/95e5a0bf77b28b87263115421cc1ceb321adb407",
      "pdf_url": "",
      "publication_date": "2016-02-08",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6f6cf9220acdc4fc80de95abcccf0a365583fa0",
      "title": "RESTRICTED DEVICE DATA SANITIZATION AND RANDOMIZATION",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "M. Mooney"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6f6cf9220acdc4fc80de95abcccf0a365583fa0",
      "pdf_url": "",
      "publication_date": "2020-03-13",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "354effc816bc4a256987ef36aa36c5914aeba560",
      "title": "SEQUENTIAL GAME NETWORK (SEGANE) WITH APPLICATION TO ONLINE DATA SANITIZATION",
      "abstract": "This paper proposes SEquential GAme NEtwork (SEGANE), a novel deep neural network (DNN) architecture for optimizing the performance of machine learning applications with multiple competing objectives. Specifically, SEGANE is evaluated in the context of data sanitization which aims to remove any pre-specified private information from the data in real time while keeping the relevant information used to improve the inference accuracy about the non-private information. In some settings, preserving private information and improving inference performance about non-private information are competing objectives. In such cases, SEGANE provides a sequential game framework and algorithmic tools to implement data sanitization schemes with flexible trade-off between these two objectives. We use two datasets: MNIST (hand-written digits) and IMDB (gender and age) to evaluate SEGANE. For MNIST, even numbers are considered private while numbers larger than 10 are considered non-private. For IMDB, in one setting, gender is considered private while age is non-private, and vice versa in another setting. Our experimental results on these datasets show that SEGANE is highly effective in removing private information from the dataset while allowing non-private data to be mined effectively.",
      "year": 2018,
      "venue": "IEEE Global Conference on Signal and Information Processing",
      "authors": [
        "Zahir Alsulaimawi",
        "Jinsub Kim",
        "Thinh P. Q. Nguyen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/354effc816bc4a256987ef36aa36c5914aeba560",
      "pdf_url": "",
      "publication_date": "2018-11-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4ec746c7f9624d2d7294096ac4de8024f742e052",
      "title": "Privacy Preserving Data Sanitization and Publishing",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "A. Zaman"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4ec746c7f9624d2d7294096ac4de8024f742e052",
      "pdf_url": "",
      "publication_date": "2017-12-18",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6f5a5c3e3a63d9930f40c72b6a8ad3b89ad81467",
      "title": "Secure Data Sanitization for Android Device Users",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "Na Huang",
        "Jingsha He",
        "Bin Zhao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6f5a5c3e3a63d9930f40c72b6a8ad3b89ad81467",
      "pdf_url": "https://doi.org/10.14257/ijsia.2015.9.5.06",
      "publication_date": "2015-05-31",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ad102da5ad47d8d100fe10847d7e71f402c5ac00",
      "title": "A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning",
      "abstract": "Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are \u201cfit for purpose\u201d of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.",
      "year": 2019,
      "venue": "ACM Journal of Data and Information Quality",
      "authors": [
        "Junhua Ding",
        "XinChuan Li",
        "Xiaojun Kang",
        "V. Gudivada"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/ad102da5ad47d8d100fe10847d7e71f402c5ac00",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3317573",
      "publication_date": "2019-08-19",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f40b7980c9d3e099ab0a5d2c80dbc5137dea5eb2",
      "title": "Preventing Machine Learning Poisoning Attacks Using Authentication and Provenance",
      "abstract": "Recent research has successfully demonstrated new types of data poisoning attacks. To address this problem, some researchers have proposed data poisoning detection defenses which employ machine learning algorithms to identify such attacks. In this work, we take a different approach to preventing data poisoning attacks which relies on cryptographically-based authentication and provenance to ensure the integrity of the data used to train a machine learning model. The same approach is also used to prevent software poisoning and model poisoning attacks. A software poisoning attack maliciously alters one or more software components used to train a model. Once the model has been trained it can also be protected against model poisoning attacks which seek to alter a model's predictions by modifying its underlying parameters or structure. Finally, an evaluation set or test set can also be protected to provide evidence if they have been modified by a second data poisoning attack during inference. To achieve these goals, we propose VAMP which extends the previously proposed AMP system, that was designed to protect media objects such as images, video files or audio clips, to the machine learning setting. We first provide requirements for authentication and provenance for a secure machine learning system. Next, we demonstrate how VAMP's manifest meets these requirements to protect a machine learning system's datasets, software components, and models.",
      "year": 2021,
      "venue": "IEEE Military Communications Conference",
      "authors": [
        "J. W. Stokes",
        "P. England",
        "K. Kane"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/f40b7980c9d3e099ab0a5d2c80dbc5137dea5eb2",
      "pdf_url": "http://arxiv.org/pdf/2105.10051",
      "publication_date": "2021-05-20",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0933a6d20c8e0b3b26ff486398033381d76a8905",
      "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety",
      "abstract": "Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zihan Guan",
        "Mengxuan Hu",
        "Ronghang Zhu",
        "Sheng Li",
        "Anil Vullikanti"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/0933a6d20c8e0b3b26ff486398033381d76a8905",
      "pdf_url": "",
      "publication_date": "2025-05-11",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85d4e915efe6584e116ff06976c3c8bde5cab355",
      "title": "Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning",
      "abstract": "Recent vision language models (VLMs) have made remarkable strides in generative modeling with multimodal inputs, particularly text and images. However, their susceptibility to generating harmful content when exposed to unsafe queries raises critical safety concerns. While current alignment strategies primarily rely on supervised safety fine-tuning with curated datasets, we identify a fundamental limitation we call the''safety mirage'', where supervised fine-tuning inadvertently reinforces spurious correlations between superficial textual patterns and safety responses, rather than fostering deep, intrinsic mitigation of harm. We show that these spurious correlations leave fine-tuned VLMs vulnerable even to a simple one-word modification-based attack, where substituting a single word in text queries with a spurious correlation-inducing alternative can effectively bypass safeguards. Additionally, these correlations contribute to the over-prudence, causing fine-tuned VLMs to refuse benign queries unnecessarily. To address these issues, we show machine unlearning (MU) as a powerful alternative to supervised safety fine-tuning, as it avoids biased feature-label mappings and directly removes harmful knowledge from VLMs while preserving their general capabilities. Extensive evaluations across safety benchmarks show that under MU-based alignment reduces the attack success rate by up to 60.17% and cuts unnecessary rejections by over 84.20%. WARNING: There exist AI generations that may be offensive in nature.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yiwei Chen",
        "Yuguang Yao",
        "Yihua Zhang",
        "Bingquan Shen",
        "Gaowen Liu",
        "Sijia Liu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/85d4e915efe6584e116ff06976c3c8bde5cab355",
      "pdf_url": "",
      "publication_date": "2025-03-14",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e017068fa26827c9d576272ecfb873d0eec7885c",
      "title": "Detecting Instruction Fine-tuning Attacks on Language Models using Influence Function",
      "abstract": "Instruction finetuning attacks pose a serious threat to large language models (LLMs) by subtly embedding poisoned examples in finetuning datasets, leading to harmful or unintended behaviors in downstream applications. Detecting such attacks is challenging because poisoned data is often indistinguishable from clean data and prior knowledge of triggers or attack strategies is rarely available. We present a detection method that requires no prior knowledge of the attack. Our approach leverages influence functions under semantic transformation: by comparing influence distributions before and after a sentiment inversion, we identify critical poison examples whose influence is strong and remain unchanged before and after inversion. We show that this method works on sentiment classification task and math reasoning task, for different language models. Removing a small set of critical poisons (about 1% of the data) restores the model performance to near-clean levels. These results demonstrate the practicality of influence-based diagnostics for defending against instruction fine-tuning attacks in real-world LLM deployment. Artifact available at https://github.com/lijiawei20161002/Poison-Detection. WARNING: This paper contains offensive data examples.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Jiawei Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e017068fa26827c9d576272ecfb873d0eec7885c",
      "pdf_url": "",
      "publication_date": "2025-04-12",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a4bd5ed3c54d21a081793cf54aea9922a93fc92e",
      "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning",
      "abstract": "Self-supervised learning is widely used in various domains for building foundation models. It has been demonstrated to achieve state-of-the-art performance in a range of tasks. In the computer vision domain, self-supervised learning is utilized to generate an image feature extractor, called an encoder, such that a variety of downstream tasks can build classifiers on top of it with limited data and resources. Despite the impressive performance of self-supervised learning, it is susceptible to backdoor attacks, where an attacker injects a backdoor into its unlabeled training data. A downstream classifier built on the backdoored encoder will misclassify any inputs inserted with the trigger to a target label. Existing backdoor attacks in self-supervised learning possess a key out-of-distribution property, where the poisoned samples significantly differ from the clean data in the feature space. The poisoned distribution is also exceptionally concentrated, inducing high pairwise similarity among poisoned samples. As a result, these attacks can be detected by state-of-the-art defense techniques. We propose a novel distribution preserving attack, which transforms the poisoned samples into in-distribution data by reducing their distributional distance to the clean data. We also distribute the poisoned data to a wider region in the target-class distribution, mitigating the concentration problem. Our evaluation of five popular datasets demonstrates that our attack, Drupe, significantly reduces the distributional distance and concentration of the poisoned distribution compared to existing attacks. Drupe successfully evades two state-of-the-art backdoor defenses in self-supervised learning and is robust against knowledgeable defenders.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Guanhong Tao",
        "Zhenting Wang",
        "Shiwei Feng",
        "Guangyu Shen",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/a4bd5ed3c54d21a081793cf54aea9922a93fc92e",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fbf7f3c8852566d3ae3dcb39a7f592ca69a47835",
      "title": "Learning to Poison Large Language Models for Downstream Manipulation",
      "abstract": "The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where the adversary inserts backdoor triggers into training data to manipulate outputs. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the supervised fine-tuning (SFT) process. We propose a novel gradient-guided backdoor trigger learning (GBTL) algorithm to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various language model tasks, including sentiment analysis, domain generation, and question answering, our poisoning strategy demonstrates a high success rate in compromising various LLMs' outputs. We further propose two defense strategies against data poisoning attacks, including in-context learning (ICL) and continuous learning (CL), which effectively rectify the behavior of LLMs and significantly reduce the decline in performance. Our work highlights the significant security risks present during SFT of LLMs and the necessity of safeguarding LLMs against data poisoning attacks.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Xiangyu Zhou",
        "Yao Qiang",
        "Saleh Zare Zade",
        "Mohammad Amin Roshani",
        "Prashant Khanduri",
        "Douglas Zytko",
        "Dongxiao Zhu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fbf7f3c8852566d3ae3dcb39a7f592ca69a47835",
      "pdf_url": "",
      "publication_date": "2024-02-21",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "72a3de3cc62c7f99f0870c056f3db1d0f93d0922",
      "title": "Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger",
      "abstract": "Recent deep-learning-based compression methods have achieved superior performance compared with traditional approaches. However, deep learning models have proven to be vulnerable to backdoor attacks, where some specific trigger patterns added to the input can lead to malicious behavior of the models. In this paper, we present a novel backdoor attack with multiple triggers against learned image compression models. Motivated by the widely used discrete cosine transform (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives for various attacking scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as downstream face recognition and semantic segmentation. Moreover, a novel simple dynamic loss is designed to balance the influence of different loss terms adaptively, which helps achieve more efficient training. Extensive experiments show that with our trained trigger injection models and simple modification of encoder parameters (of the compression model), the proposed attack can successfully inject several backdoors with corresponding triggers in a single image compression model.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yi Yu",
        "Yufei Wang",
        "Wenhan Yang",
        "Shijian Lu",
        "Yap-Peng Tan",
        "A. Kot"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/72a3de3cc62c7f99f0870c056f3db1d0f93d0922",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/168045/2/cvpr23_backdoor_final.pdf",
      "publication_date": "2023-02-28",
      "keywords_matched": [
        "trigger pattern",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e422e6fb5d0cc47ba5ba687ac6dbce6aced9bda4",
      "title": "Let's Focus: Focused Backdoor Attack against Federated Transfer Learning",
      "abstract": "Federated Transfer Learning (FTL) is the most general variation of Federated Learning. According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data. After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor. Each involved client contributes by locally training only the classification layers on a private training set. The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor. State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients. Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step. Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation. In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class. Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario. With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Marco Arazzi",
        "Stefanos Koffas",
        "Antonino Nocera",
        "S. Picek"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e422e6fb5d0cc47ba5ba687ac6dbce6aced9bda4",
      "pdf_url": "",
      "publication_date": "2024-04-30",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a242307462205c927ead7c11daa7b5dfae51f535",
      "title": "ICT: Invisible Computable Trigger Backdoor Attacks in Transfer Learning",
      "abstract": "Transfer learning is a commonly used technique in machine learning to reduce the cost of training models. However, it is susceptible to backdoor attacks that cause models to misclassify data with specific triggers while behaving normally on clean data. Existing methods for backdoor attacks in transfer learning either do not consider attack stealthiness or require compromising attack effectiveness for trigger concealment. To overcome this challenge, we introduce the concept of Invisible and Computable Trigger (ICT), which involves two critical steps. First, we propose a new computable trigger obtained by training on input data to greatly increase the attack effect during inference. Second, we embed the trigger into an imperceptible perturbation, allowing poisoned data to appear indistinguishable from clean data. Our experimental results demonstrate that our approach outperforms state-of-the-art methods in both attack effect and stealthiness.",
      "year": 2024,
      "venue": "IEEE transactions on consumer electronics",
      "authors": [
        "Xiang Chen",
        "Bo Liu",
        "Shaofeng Zhao",
        "Ming Liu",
        "Hui Xu",
        "Zhanbo Li",
        "Zhigao Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a242307462205c927ead7c11daa7b5dfae51f535",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d507f708d86e31488a8a698d9c148a4f6ce4c413",
      "title": "Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection",
      "abstract": "Network-based Intrusion Detection Systems (NIDSs) automate monitoring of events in networks and analyze them for signatures of cyberattacks. With the advancement of machine learning algorithms, more organizations started using machine learning based IDSs (ML-IDSs) to identify and mitigate cyberattacks. However, the lack of training datasets is a major challenge when implementing ML-IDSs. Therefore, using training data from external sources or transfer learning models are some solutions to overcome this challenge. However, using training data from external sources introduces the risk of backdoored datasets, specifically, when the adversaries also have background knowledge on data sources inside the target organization. This work investigates the role of backdoor attacks on intrusion detection techniques trained using multi-source data. The backdoor examples are injected into one or more training data sources. Transfer learning models are then created by projecting data from different sources into a new subspace containing all source data. The backdoor is then triggered in the target data. An anomaly-based intrusion detection classifier is applied to examine the effectiveness of the introduced backdoors. The results have shown that backdoor attacks on multis-source transfer learning models are feasible, although having less impact compared to backdoors on traditional machine learning models.",
      "year": 2022,
      "venue": "2022 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)",
      "authors": [
        "Nour Alhussien",
        "Ahmed Aleroud",
        "Reza Rahaeimehr",
        "A. Schwarzmann"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/d507f708d86e31488a8a698d9c148a4f6ce4c413",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82fe948f18ca0138d035f553286c5e4b712dbdbe",
      "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
      "abstract": "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.",
      "year": 2023,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Jiashu Xu",
        "Mingyu Derek Ma",
        "Fei Wang",
        "Chaowei Xiao",
        "Muhao Chen"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/82fe948f18ca0138d035f553286c5e4b712dbdbe",
      "pdf_url": "http://arxiv.org/pdf/2305.14710",
      "publication_date": "2023-05-24",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "08811fa5e2b211ce71ac79d741b7ba441870ee5b",
      "title": "Attack on Prompt: Backdoor Attack in Prompt-Based Continual Learning",
      "abstract": "Prompt-based approaches offer a cutting-edge solution to data privacy issues in continual learning, particularly in scenarios involving multiple data suppliers where long-term storage of private user data is prohibited. Despite delivering state-of-the-art performance, its impressive remembering capability can become a double-edged sword, raising security concerns as it might inadvertently retain poisoned knowledge injected during learning from private user data. Following this insight, in this paper, we expose continual learning to a potential threat: backdoor attack, which drives the model to follow a desired adversarial target whenever a specific trigger is present while still performing normally on clean samples. We highlight three critical challenges in executing backdoor attacks on incremental learners and propose corresponding solutions: (1) Transferability: We employ a surrogate dataset and manipulate prompt selection to transfer backdoor knowledge to data from other suppliers; (2) Resiliency: We simulate static and dynamic states of the victim to ensure the backdoor trigger remains robust during intense incremental learning processes; and (3) Authenticity: We apply binary cross-entropy loss as an anti-cheating factor to prevent the backdoor trigger from devolving into adversarial noise. Extensive experiments across various benchmark datasets and continual learners validate our continual backdoor framework, with further ablation studies confirming our contributions' effectiveness.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Trang Nguyen",
        "Anh Tran",
        "Nhat Ho"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/08811fa5e2b211ce71ac79d741b7ba441870ee5b",
      "pdf_url": "https://doi.org/10.1609/aaai.v39i18.34168",
      "publication_date": "2024-06-28",
      "keywords_matched": [
        "transfer backdoor",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "24a1007c74c0f4f64d4e7bdf42b95e491f851f08",
      "title": "WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks",
      "abstract": "Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not only achieves higher stealthiness and effectiveness, but also outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of images by up to 28.27\\% improvement in PSNR, 1.61\\% improvement in SSIM, and 70.59\\% reduction in IS.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jun Xia",
        "Zhihao Yue",
        "Yingbo Zhou",
        "Zhiwei Ling",
        "Xian Wei",
        "Mingsong Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/24a1007c74c0f4f64d4e7bdf42b95e491f851f08",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2f250c5e59ed224af72f0146e896cadf5d0e377b",
      "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation",
      "abstract": "Vision Language Models (VLMs) have shown remarkable performance, but are also vulnerable to backdoor attacks whereby the adversary can manipulate the model's outputs through hidden triggers. Prior attacks primarily rely on single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs largely unexplored. Unlike prior work, we identify a novel attack surface that leverages cross-modal semantic mismatches as implicit triggers. Based on this insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data poisoning attack that injects stealthy backdoors by deliberately misaligning image-text pairs during training. To perform the attack, we construct SIMBad, a dataset tailored for semantic manipulation involving color and object attributes. Extensive experiments across four widely used VLMs show that BadSem achieves over 98% average ASR, generalizes well to out-of-distribution datasets, and can transfer across poisoning modalities. Our detailed analysis using attention visualization shows that backdoored models focus on semantically sensitive regions under mismatched conditions while maintaining normal behavior on clean inputs. To mitigate the attack, we try two defense strategies based on system prompt and supervised fine-tuning but find that both of them fail to mitigate the semantic backdoor. Our findings highlight the urgent need to address semantic vulnerabilities in VLMs for their safer deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhiyuan Zhong",
        "Zhen Sun",
        "Yepang Liu",
        "Xinlei He",
        "Guanhong Tao"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/2f250c5e59ed224af72f0146e896cadf5d0e377b",
      "pdf_url": "",
      "publication_date": "2025-06-08",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9f0095d9476ccd46fb43967ebe81362b80320186",
      "title": "Logits Poisoning Attack in Federated Distillation",
      "abstract": "Federated Distillation (FD) is a novel and promising distributed machine learning paradigm, where knowledge distillation is leveraged to facilitate a more efficient and flexible cross-device knowledge transfer in federated learning. By optimizing local models with knowledge distillation, FD circumvents the necessity of uploading large-scale model parameters to the central server, simultaneously preserving the raw data on local clients. Despite the growing popularity of FD, there is a noticeable gap in previous works concerning the exploration of poisoning attacks within this framework. This can lead to a scant understanding of the vulnerabilities to potential adversarial actions. To this end, we introduce FDLA, a poisoning attack method tailored for FD. FDLA manipulates logit communications in FD, aiming to significantly degrade model performance on clients through misleading the discrimination of private samples. Through extensive simulation experiments across a variety of datasets, attack scenarios, and FD configurations, we demonstrate that LPA effectively compromises client model accuracy, outperforming established baseline algorithms in this regard. Our findings underscore the critical need for robust defense mechanisms in FD settings to mitigate such adversarial threats.",
      "year": 2024,
      "venue": "Knowledge Science, Engineering and Management",
      "authors": [
        "Yuhan Tang",
        "Zhiyuan Wu",
        "Bo Gao",
        "Tian Wen",
        "Yuwei Wang",
        "Sheng Sun"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9f0095d9476ccd46fb43967ebe81362b80320186",
      "pdf_url": "",
      "publication_date": "2024-01-08",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a3479f982c670563e99899b240376e78b7d43d64",
      "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs",
      "abstract": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another LLM to generate corrective data. In this paper, we aim to take this problem and overcome limitations of requiring significant high-quality human data. Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself. By employing a semantic cost combined with a negative Earth Mover Distance (EMD) loss, we guide the LLM away from generating unsafe responses. Additionally, we propose a novel lower bound for EMD loss, enabling more efficient optimization. Our results demonstrate superior performance and data efficiency compared to baselines, and we further examine the nuanced effects of over-alignment and potential degradation of language capabilities when using contrastive data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Yuxiao Lu",
        "Arunesh Sinha",
        "Pradeep Varakantham"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a3479f982c670563e99899b240376e78b7d43d64",
      "pdf_url": "",
      "publication_date": "2024-12-07",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b8396ed15747120e5d7f060bd8a28169055c858a",
      "title": "On the Cause of Unfairness: A Training Sample Perspective",
      "abstract": "Identifying the causes of a model's unfairness is an important yet relatively unexplored task. We look into this problem through the lens of training data - the major source of unfairness. We ask the following questions: How would the unfairness of a model change if its training samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) whose features were modified? In other words, we quantify the influence of training samples on unfairness by counterfactually changing samples based on predefined concepts, i.e. data attributes such as features, labels, and sensitive attributes. Our framework not only can help practitioners understand the observed unfairness and mitigate it by repairing their training data, but also leads to many other applications, e.g. detecting mislabeling, fixing imbalanced representations, and detecting fairness-targeted poisoning attacks.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Yuanshun Yao",
        "Yang Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b8396ed15747120e5d7f060bd8a28169055c858a",
      "pdf_url": "",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8e31ce148dc7abee1a9790b3897942740aca0da2",
      "title": "Learning Debiased Classifier with Biased Committee",
      "abstract": "Neural networks are prone to be biased towards spurious correlations between classes and latent attributes exhibited in a major portion of training data, which ruins their generalization capability. We propose a new method for training debiased classifiers with no spurious attribute label. The key idea is to employ a committee of classifiers as an auxiliary module that identifies bias-conflicting data, i.e., data without spurious correlation, and assigns large weights to them when training the main classifier. The committee is learned as a bootstrapped ensemble so that a majority of its classifiers are biased as well as being diverse, and intentionally fail to predict classes of bias-conflicting data accordingly. The consensus within the committee on prediction difficulty thus provides a reliable cue for identifying and weighting bias-conflicting data. Moreover, the committee is also trained with knowledge transferred from the main classifier so that it gradually becomes debiased along with the main classifier and emphasizes more difficult data as training progresses. On five real-world datasets, our method outperforms prior arts using no spurious attribute label like ours and even surpasses those relying on bias labels occasionally.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Nayeong Kim",
        "Sehyun Hwang",
        "Sungsoo Ahn",
        "Jaesik Park",
        "Suha Kwak"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/8e31ce148dc7abee1a9790b3897942740aca0da2",
      "pdf_url": "http://arxiv.org/pdf/2206.10843",
      "publication_date": "2022-06-22",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "44c5b8c6fa185ccf557c18e59887e24ab52004ce",
      "title": "Poisoning Attacks on Fair Machine Learning",
      "abstract": "Both fair machine learning and adversarial learning have been extensively studied. However, attacking fair machine learning models has received less attention. In this paper, we present a framework that seeks to effectively generate poisoning samples to attack both model accuracy and algorithmic fairness. Our attacking framework can target fair machine learning models trained with a variety of group based fairness notions such as demographic parity and equalized odds. We develop three online attacks, adversarial sampling , adversarial labeling, and adversarial feature modification. All three attacks effectively and efficiently produce poisoning samples via sampling, labeling, or modifying a fraction of training data in order to reduce the test accuracy. Our framework enables attackers to flexibly adjust the attack's focus on prediction accuracy or fairness and accurately quantify the impact of each candidate point to both accuracy loss and fairness violation, thus producing effective poisoning samples. Experiments on two real datasets demonstrate the effectiveness and efficiency of our framework.",
      "year": 2021,
      "venue": "International Conference on Database Systems for Advanced Applications",
      "authors": [
        "Minh-Hao Van",
        "Wei Du",
        "Xintao Wu",
        "Aidong Lu"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/44c5b8c6fa185ccf557c18e59887e24ab52004ce",
      "pdf_url": "",
      "publication_date": "2021-10-17",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0354e4ef224248f9c4610d4bdde03f44981b520",
      "title": "Tampering Detection and Content Authentication Using Encrypted Perceptual Hash for Image Database",
      "abstract": "To detect and locate unapproved alterations or modifications made to digital pictures, tampering detection in images is a crucial study topic in the field of image forensics. As sophisticated picture altering tools become more widely available, it has become more difficult to guarantee the integrity and authenticity of digital visual output. This overview of the literature explores methodology, findings, and contributions in key tampering detection research studies. The breakthroughs in deep learning techniques and steganalysis are highlighted together with more conventional approaches like statistical analysis and error level analysis. The difficulties tampering detection faces are discussed, including generalisation, dataset accessibility, resilience against adversarial assaults, computing effectiveness, and moral and legal issues. With the goal of advancing the creation of precise, reliable, and effective algorithms for maintaining the integrity and authenticity of digital pictures, the paper offers insights into the current state of the art, limits, and future research paths in tampering detection.",
      "year": 2021,
      "venue": "Mathematical Statistician and Engineering Applications",
      "authors": [
        "Ayushi Jain"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0354e4ef224248f9c4610d4bdde03f44981b520",
      "pdf_url": "https://philstat.org/index.php/MSEA/article/download/2460/1932",
      "publication_date": "2021-02-26",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca8ca3fac03cb12344758d595b8f82d05f9fec0b",
      "title": "Knowledge-Based Fusion for Image Tampering Localization",
      "abstract": "In this paper we introduce a fusion framework for image tampering localization, that moves towards overcoming the limitation of available tools by allowing a synergistic analysis and multiperspective refinement of the final forensic report. The framework is designed to combine multiple state-of-the-art techniques by exploiting their complementarities so as to produce a single refined tampering localization output map. Extensive evaluation experiments of state-of-the-art methods on diverse datasets have resulted in a modular framework design where candidate methods go through a multi-criterion selection process to become part of the framework. Currently, this includes a set of five passive tampering localization methods for splicing localization on JPEG images. Our experimental findings on two different benchmark datasets showcase that the fused output achieves high performance and advanced interpretability by managing to leverage the correctly localized outputs of individual methods, and even detecting cases that were missed by all individual methods.",
      "year": 2020,
      "venue": "Artificial Intelligence Applications and Innovations",
      "authors": [
        "C. Iakovidou",
        "S. Papadopoulos",
        "Y. Kompatsiaris"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/ca8ca3fac03cb12344758d595b8f82d05f9fec0b",
      "pdf_url": "",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df084d6c2430d48c019f62775491a2e8edee8461",
      "title": "An Intrusion Detection Method of Data Tampering Attack in Communication-Based Train Control System",
      "abstract": "Communication-based train control (CBTC) technologies are widely applied in order to improve the efficiency and safety of urban rail transit systems. With the increase of informatization and automation through utilization of communication, computer and control technologies, amounts of potential security vulnerabilities are introduced into CBTC systems, where malicious attacks could be implemented. Some attacks, such as data tampering attacks cannot be efficiently detected by traditional intrusion detection systems (IDS), which can affect the safety operation of CBTC systems, e.g., rear-end collisions. Based on the operation principles and information exchange characteristics of CBTC systems, the paper firstly proposes a model to measure the effects of data tampering attacked on trains, and an intrusion detection method is developed based on the running status of the train through Kalman filter and \u03c72 detector. The method improves the \u03c72 detector to detect data tampering attacks and continuously output alarms during the attack. The improved method has higher accuracy and a lower false negative rate.",
      "year": 2019,
      "venue": "International Conference on Intelligent Transportation Systems",
      "authors": [
        "Wei Zhang",
        "Bing Bu",
        "Hongwei Wang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/df084d6c2430d48c019f62775491a2e8edee8461",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2941aa6eb8d1424847a71f59fe02970090f538f1",
      "title": "Learning under p-tampering poisoning attacks",
      "abstract": null,
      "year": 2019,
      "venue": "Annals of Mathematics and Artificial Intelligence",
      "authors": [
        "Saeed Mahloujifar",
        "Dimitrios I. Diochnos",
        "Mohammad Mahmoody"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2941aa6eb8d1424847a71f59fe02970090f538f1",
      "pdf_url": "",
      "publication_date": "2019-12-03",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ac4ea4850a4e8a69b93ecaefb35f29452afc21d",
      "title": "Attention-based fusion network for image forgery localization",
      "abstract": "With the trustworthiness of multimedia data has been challenged by editing tools, image forgery localization aims to identify regions in images that have been modified. Although the existing techniques provide reasonably good results for image forgery localization, with emerging new editing techniques, such models must be retrained and it is highly dependent on the real tampering localization maps. In this paper, we propose an attention-based fusion network that combines the RGB image and noise residual yielding excellent results. Noise residual is commonly regarded as camera model fingerprint, and forgery localization can be detected as deviations from the expected regular pattern. The model consists of three parts: feature extraction, attentional feature fusion, and feature output. The feature extraction module is used to extract RGB image features and noise residuals separately, and the attentional feature fusion module is used to suppress the high frequency components, supplement and enhance model-related artifacts by combining the aforementioned features. Finally, the last module generates images with one channel as the camera model fingerprint. In order to avoid dependence on tampering localization maps, the model is trained with pairs of image patches coming from the same or different camera sensors by means of Siamese network. Experiment results obtained from several datasets show that the proposed technique successfully identifies modified regions, improves the quality of camera model fingerprints, and achieves significantly better performance when compared to the existing techniques.",
      "year": 2024,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "Wenhui Gong",
        "Yan Chen",
        "Mohammad S. Alam",
        "Jun Sang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7ac4ea4850a4e8a69b93ecaefb35f29452afc21d",
      "pdf_url": "",
      "publication_date": "2024-06-07",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41044fb73cfe789571e458873c3a4bf3d1d94e60",
      "title": "Backdoor Cleansing with Unlabeled Data",
      "abstract": "Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight reinitialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal behavior. In experiments, we show that our method, trained without labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https://github.com/luluppang/BCU.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Lu Pang",
        "Tao Sun",
        "Haibin Ling",
        "Chao Chen"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/41044fb73cfe789571e458873c3a4bf3d1d94e60",
      "pdf_url": "https://arxiv.org/pdf/2211.12044",
      "publication_date": "2022-11-22",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "53361b49cc6caf6623cc892f68d7a111c3252903",
      "title": "Just a little human intelligence feedback! Unsupervised learning assisted supervised learning data poisoning based backdoor removal",
      "abstract": null,
      "year": 2025,
      "venue": "Computer Communications",
      "authors": [
        "Ting Luo",
        "Huaibing Peng",
        "Anmin Fu",
        "Wei Yang",
        "Lihui Pang",
        "S. Al-Sarawi",
        "Derek Abbott",
        "Yansong Gao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/53361b49cc6caf6623cc892f68d7a111c3252903",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1a17b08428e36a3057fa85e6b89211e848c53c98",
      "title": "A New Data-Free Backdoor Removal Method via Adversarial Self-Knowledge Distillation",
      "abstract": "In the context of Internet of Things edge devices, pretrained models are often sourced directly from cloud computing platforms due to the unavailability of training data. This lack of access during the training phase makes these models susceptible to backdoor attacks. To address this challenge, we introduce a novel data-free backdoor removal method that operates effectively even when only the poisoned model is accessible. Our innovative approach employs two end-to-end generators with identical architectures to create both clean and poisoned samples. These samples are crucial for transferring knowledge from the teacher model\u2014the fixed poisoned model\u2014to the student model, which is initialized with the poisoned model. Our method utilizes a channel shuffling technique during the distillation process to disrupt and eliminate the backdoor knowledge embedded in the teacher model. This process involves iterative updates of the generators and meticulous distillation of the student model, leading to efficient backdoor removal. We conducted extensive experiments on five sophisticated backdoor attacks across two benchmark datasets. The results demonstrate that our method not only significantly bolsters the model\u2019s resistance to backdoor attacks but also maintains high recognition accuracy for clean samples, thereby outperforming existing methods. Additionally, the code for our method is available at https://github.com/gaoyafeiyoo/ADBR.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuexiang Li",
        "Yafei Gao",
        "Minglin Liu",
        "Xu Zhou",
        "Xianfu Chen",
        "Celimuge Wu",
        "Jie Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1a17b08428e36a3057fa85e6b89211e848c53c98",
      "pdf_url": "",
      "publication_date": "2025-05-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "423d917c1f26cf2d0624a9001c5fac5baba6ba73",
      "title": "Bad Padding: A Highly Stealthy Backdoor Attack Using Steganography at the Padding Stage",
      "abstract": "Backdoor attacks have significantly threatened the models of natural language processing (NLP). However, most textual backdoor attacks exhibit low levels of stealthiness, making them susceptible to detection and removal by defense strategies. In order to improve the performance and stealthiness of such backdoor attacks, this article introduces a novel backdoor attack named Bad Padding (BPad) based on steganography. BPad employs a word\u2010substitution steganographic method to hide triggers in sentences, thereby generating poisoned data. To ensure a high level of stealthiness for these poisoned samples, BPad developed a word substitution strategy that enhances both the diversity of the substituted words and the contextual coherence of the sentences. BPad also modifies the preprocessing stage by extracting triggers from the sentences and padding them as tokens at the end, effectively amplifying the impact of the trigger and making it easier for the model to learn the shortcut from the trigger to the target label, thereby achieving the injection of a backdoor. This article uses various metrics to present experimental measures of the attack performance and stealthiness of BPad. The results find that BPad achieved competitive results compared to baseline methods in non\u2010defense scenarios and outperforms baseline methods under both training and inference defense. Besides that, the attack samples generated by BPad demonstrate strong stealthiness in terms of semantic coherence, perplexity, and grammaticality.",
      "year": 2025,
      "venue": "IET Information Security",
      "authors": [
        "Zhuowei Niu",
        "Qindong Sun",
        "Kai Lin",
        "Mingkai Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/423d917c1f26cf2d0624a9001c5fac5baba6ba73",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b3cecb3cd7fade961f0dfe654f34534782d782e0",
      "title": "SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification",
      "abstract": "The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yingjia Wang",
        "Ting Qiao",
        "Xing Liu",
        "Chongzuo Li",
        "Sixing Wu",
        "Jianbing Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b3cecb3cd7fade961f0dfe654f34534782d782e0",
      "pdf_url": "",
      "publication_date": "2025-10-30",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d2a558980a3f83ab6b81111b4305c0660796be7",
      "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks",
      "abstract": "Website Fingerprinting (WF) is an effective tool for regulating and governing the dark web. However, its performance can be significantly degraded by backdoor poisoning attacks in practical deployments. This paper aims to address the problem of hidden backdoor poisoning attacks faced by Website Fingerprinting attack, and designs a feasible mothed that integrates unlearning technology to realize detection of automatic poisoned points and complete removal of its destructive effects, requiring only a small number of known poisoned test points. Taking Tor onion routing as an example, our method evaluates the influence value of each training sample on these known poisoned test points as the basis for judgment. We optimize the use of influence scores to identify poisoned samples within the training dataset. Furthermore, by quantifying the difference between the contribution of model parameters on the taining data and the clean data, the target parameters are dynamically adjusted to eliminate the impact of the backdoor attacks. Experiments on public datasets under the assumptions of closed-world (CW) and open-world (OW) verify the effectiveness of the proposed method. In complex scenes containing both clean website fingerprinting features and backdoor triggers, the accuracy of the model on the poisoned dataset and the test dataset is stable at about 80%, significantly outperforming the traditional WF attack models. In addition, the proposed method achieves a 2-3 times speedup in runtime efficiency compared to baseline methods. By incorporating machine unlearning, we realize a WF attack model that exhibits enhanced resistance to backdoor poisoning and faster execution speeds in adversarial settings.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ya-Nan Yuan",
        "Kai Xu",
        "Ruolin Ma",
        "Yuchen Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6d2a558980a3f83ab6b81111b4305c0660796be7",
      "pdf_url": "",
      "publication_date": "2025-06-16",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}