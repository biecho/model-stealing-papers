{
  "owasp_id": "ML02",
  "owasp_name": "Data Poisoning Attack",
  "total": 59,
  "updated": "2026-01-28",
  "papers": [
    {
      "paper_id": "seed_68cb4028",
      "title": "A Data-free Backdoor Injection Approach in Neural Networks",
      "abstract": "A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Zewen Li",
        "Fan Liu",
        "Wenjie Yang",
        "Shouheng Peng",
        "Jun Zhou"
      ],
      "author_details": [
        {
          "name": "Zewen Li",
          "h_index": 8,
          "citation_count": 3915,
          "affiliations": []
        },
        {
          "name": "Fan Liu",
          "h_index": 13,
          "citation_count": 4359,
          "affiliations": []
        },
        {
          "name": "Wenjie Yang",
          "h_index": 5,
          "citation_count": 3785,
          "affiliations": []
        },
        {
          "name": "Shouheng Peng",
          "h_index": 1,
          "citation_count": 3747,
          "affiliations": []
        },
        {
          "name": "Jun Zhou",
          "h_index": 31,
          "citation_count": 6372,
          "affiliations": []
        }
      ],
      "max_h_index": 31,
      "url": "https://openalex.org/W3168997536",
      "pdf_url": "http://hdl.handle.net/10072/405164",
      "doi": "https://doi.org/10.1109/tnnls.2021.3084827",
      "citation_count": 3758,
      "influential_citation_count": 93,
      "reference_count": 223,
      "is_open_access": true,
      "publication_date": "2020-04-01",
      "tldr": "This review introduces the history of CNN, some classic and advanced CNN models are introduced, and an overview of various convolutions is provided, including those key points making them reach state-of-the-art results.",
      "fields_of_study": [
        "Computer Science",
        "Engineering",
        "Medicine"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "data-free",
        "backdoor-injection"
      ],
      "open_access_pdf": "https://research-repository.griffith.edu.au/bitstreams/b17b55ad-283a-4b00-9c07-8688d8690e1b/download"
    },
    {
      "paper_id": "1910.03137",
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "abstract": "In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks, but they make undesirable assumptions about the attack strategies or require direct access to the trained models, which restricts their utility in practice.   This paper addresses these challenges by introducing a Meta Neural Trojan Detection (MNTD) pipeline that does not make assumptions on the attack strategies and only needs black-box access to models. The strategy is to train a meta-classifier that predicts whether a given target model is Trojaned. To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution. We then dynamically optimize a query set together with the meta-classifier to distinguish between Trojaned and benign models.   We evaluate MNTD with experiments on vision, speech, tabular data and natural language text datasets, and against different Trojan attacks such as data poisoning attack, model manipulation attack, and latent attack. We show that MNTD achieves 97% detection AUC score and significantly outperforms existing detection approaches. In addition, MNTD generalizes well and achieves high detection performance against unforeseen attacks. We also propose a robust MNTD pipeline which achieves 90% detection AUC even when the attacker aims to evade the detection with full knowledge of the system.",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xiaojun Xu",
        "Qi Wang",
        "Huichen Li",
        "N. Borisov",
        "Carl A. Gunter",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Xiaojun Xu",
          "h_index": 15,
          "citation_count": 2405,
          "affiliations": []
        },
        {
          "name": "Qi Wang",
          "h_index": 12,
          "citation_count": 1694,
          "affiliations": []
        },
        {
          "name": "Huichen Li",
          "h_index": 9,
          "citation_count": 843,
          "affiliations": [
            "University of Illinois at Urbana-Champaign"
          ]
        },
        {
          "name": "N. Borisov",
          "h_index": 50,
          "citation_count": 9490,
          "affiliations": []
        },
        {
          "name": "Carl A. Gunter",
          "h_index": 57,
          "citation_count": 12215,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 19,
          "citation_count": 3103,
          "affiliations": []
        }
      ],
      "max_h_index": 57,
      "url": "https://arxiv.org/abs/1910.03137",
      "citation_count": 365,
      "influential_citation_count": 71,
      "reference_count": 59,
      "is_open_access": true,
      "publication_date": "2019-10-08",
      "tldr": "A Meta Neural Trojan Detection pipeline that does not make assumptions on the attack strategies and only needs black-box access to models is introduced and achieves 97% detection AUC score and significantly outperforms existing detection approaches.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "trojan-detection",
        "meta-learning"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/1910.03137"
    },
    {
      "paper_id": "seed_60201f2f",
      "title": "Blind Backdoors in Deep Learning Models",
      "abstract": "We investigate a new method for injecting backdoors into machine learning models, based compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. \r\nOur attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs on the fly, as the model is training, and uses multi-objective optimization to achieve high accuracy both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Eugene Bagdasarian",
        "Vitaly Shmatikov"
      ],
      "author_details": [
        {
          "name": "Eugene Bagdasarian",
          "h_index": 14,
          "citation_count": 4087,
          "affiliations": [
            "UMass Amherst"
          ]
        },
        {
          "name": "Vitaly Shmatikov",
          "h_index": 69,
          "citation_count": 27582,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://openalex.org/W3195462295",
      "pdf_url": "https://arxiv.org/abs/2005.03823",
      "citation_count": 352,
      "influential_citation_count": 23,
      "reference_count": 115,
      "is_open_access": false,
      "publication_date": "2020-05-08",
      "tldr": "New classes of backdoors strictly more powerful than those in prior literature are demonstrated: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and back Doors that do not require inference-time input modifications.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "code-level",
        "loss-manipulation",
        "single-pixel"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_bbc1dc00",
      "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
      "abstract": "With the prevalent use of Deep Neural Networks (DNNs) in many applications, security of these networks is of importance. Pre-trained DNNs may contain backdoors that are injected through poisoned training. These trojaned models perform well when regular inputs are provided, but misclassify to a target output label when the input is stamped with a unique pattern called trojan trigger. Recently various backdoor detection and mitigation systems for DNN based AI applications have been proposed. However, many of them are limited to trojan attacks that require a specific patch trigger. In this paper, we introduce composite attack, a more flexible and stealthy trojan attack that eludes backdoor scanners using trojan triggers composed from existing benign features of multiple labels. We show that a neural network with a composed backdoor can achieve accuracy comparable to its original version on benign data and misclassifies when the composite trigger is present in the input. Our experiments on 7 different tasks show that this attack poses a severe threat. We evaluate our attack with two state-of-the-art backdoor scanners. The results show none of the injected backdoors can be detected by either scanner. We also study in details why the scanners are not effective. In the end, we discuss the essence of our attack and propose possible defense.",
      "year": 2020,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Junyu Lin",
        "Lei Xu",
        "Yingqi Liu",
        "X. Zhang"
      ],
      "author_details": [
        {
          "name": "Junyu Lin",
          "h_index": 3,
          "citation_count": 310,
          "affiliations": []
        },
        {
          "name": "Lei Xu",
          "h_index": 18,
          "citation_count": 1312,
          "affiliations": []
        },
        {
          "name": "Yingqi Liu",
          "h_index": 13,
          "citation_count": 1832,
          "affiliations": []
        },
        {
          "name": "X. Zhang",
          "h_index": 85,
          "citation_count": 296271,
          "affiliations": []
        }
      ],
      "max_h_index": 85,
      "url": "https://openalex.org/W3106646114",
      "doi": "https://doi.org/10.1145/3372297.3423362",
      "citation_count": 241,
      "influential_citation_count": 23,
      "reference_count": 55,
      "is_open_access": false,
      "publication_date": "2020-10-30",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "composite-trigger",
        "benign-features",
        "stealthy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_2b8ed5cf",
      "title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection",
      "abstract": "A security threat to deep neural networks (DNN) is backdoor contamination, in which an adversary poisons the training data of a target model to inject a Trojan so that images carrying a specific trigger will always be classified into a specific label. Prior research on this problem assumes the dominance of the trigger in an image's representation, which causes any image with the trigger to be recognized as a member in the target class. Such a trigger also exhibits unique features in the representation space and can therefore be easily separated from legitimate images. Our research, however, shows that simple target contamination can cause the representation of an attack image to be less distinguishable from that of legitimate ones, thereby evading existing defenses against the backdoor infection. In our research, we show that such a contamination attack actually subtly changes the representation distribution for the target class, which can be captured by a statistic analysis. More specifically, we leverage an EM algorithm to decompose an image into its identity part (e.g., person, traffic sign) and variation part within a class (e.g., lighting, poses). Then we analyze the distribution in each class, identifying those more likely to be characterized by a mixture model resulted from adding attack samples to the legitimate image pool. Our research shows that this new technique effectively detects data contamination attacks, including the new one we propose, and is also robust against the evasion attempts made by a knowledgeable adversary.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Di Tang",
        "Xiaofeng Wang",
        "Haixu Tang",
        "Kehuan Zhang"
      ],
      "author_details": [
        {
          "name": "Di Tang",
          "h_index": 9,
          "citation_count": 615,
          "affiliations": []
        },
        {
          "name": "Xiaofeng Wang",
          "h_index": 60,
          "citation_count": 12535,
          "affiliations": []
        },
        {
          "name": "Haixu Tang",
          "h_index": 18,
          "citation_count": 1700,
          "affiliations": []
        },
        {
          "name": "Kehuan Zhang",
          "h_index": 32,
          "citation_count": 3942,
          "affiliations": []
        }
      ],
      "max_h_index": 60,
      "url": "https://openalex.org/W2965527544",
      "pdf_url": "https://arxiv.org/pdf/1908.00686",
      "doi": "https://doi.org/10.48550/arxiv.1908.00686",
      "citation_count": 231,
      "influential_citation_count": 49,
      "reference_count": 56,
      "is_open_access": false,
      "publication_date": "2019-08-02",
      "tldr": "This research uses an EM algorithm to decompose an image into its identity part and variation part within a class and identifies those more likely to be characterized by a mixture model resulted from adding attack samples to the legitimate image pool.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "statistical-analysis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2204.05255",
      "title": "NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information",
      "abstract": "Backdoor attacks insert malicious data into a training set so that, during inference time, it misclassifies inputs that have been patched with a backdoor trigger as the malware specified label. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as \"clean-label attacks.\" Existing clean-label backdoor attacks require knowledge of the entire training set to be effective. Obtaining such knowledge is difficult or impossible because training data are often gathered from multiple sources (e.g., face images from different users). It remains a question whether backdoor attacks still present a real threat.   This paper provides an affirmative answer to this question by designing an algorithm to mount clean-label backdoor attacks based only on the knowledge of representative examples from the target class. With poisoning equal to or less than 0.5% of the target-class data and 0.05% of the training set, we can train a model to classify test examples from arbitrary classes into the target class when the examples are patched with a backdoor trigger. Our attack works well across datasets and models, even when the trigger presents in the physical world.   We explore the space of defenses and find that, surprisingly, our attack can evade the latest state-of-the-art defenses in their vanilla form, or after a simple twist, we can adapt to the downstream defenses. We study the cause of the intriguing effectiveness and find that because the trigger synthesized by our attack contains features as persistent as the original semantic features of the target class, any attempt to remove such triggers would inevitably hurt the model accuracy first.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yi Zeng",
        "Minzhou Pan",
        "H. Just",
        "L. Lyu",
        "M. Qiu",
        "R. Jia"
      ],
      "author_details": [
        {
          "name": "Yi Zeng",
          "h_index": 13,
          "citation_count": 1443,
          "affiliations": []
        },
        {
          "name": "Minzhou Pan",
          "h_index": 10,
          "citation_count": 467,
          "affiliations": []
        },
        {
          "name": "H. Just",
          "h_index": 6,
          "citation_count": 474,
          "affiliations": []
        },
        {
          "name": "L. Lyu",
          "h_index": 28,
          "citation_count": 4611,
          "affiliations": []
        },
        {
          "name": "M. Qiu",
          "h_index": 15,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "R. Jia",
          "h_index": 30,
          "citation_count": 4544,
          "affiliations": []
        }
      ],
      "max_h_index": 30,
      "url": "https://arxiv.org/abs/2204.05255",
      "citation_count": 226,
      "influential_citation_count": 19,
      "reference_count": 64,
      "is_open_access": true,
      "publication_date": "2022-04-11",
      "tldr": "An algorithm to launch clean-label backdoor attacks using only samples from the target class and public out-of-distribution data is designed and the synthesized Narcissus trigger contains durable features as persistent as the original target class features.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "clean-label",
        "practical",
        "limited-information"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3576915.3616617"
    },
    {
      "paper_id": "2006.11890",
      "title": "Graph Backdoor",
      "abstract": "One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Zhaohan Xi",
        "Ren Pang",
        "S. Ji",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Zhaohan Xi",
          "h_index": 9,
          "citation_count": 459,
          "affiliations": []
        },
        {
          "name": "Ren Pang",
          "h_index": 11,
          "citation_count": 588,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2006.11890",
      "citation_count": 197,
      "influential_citation_count": 41,
      "reference_count": 83,
      "is_open_access": false,
      "publication_date": "2020-06-21",
      "tldr": "The effectiveness of GTA is demonstrated: for instance, on pre-trained, off-the-shelf GNNs, GTA attains over 99.2% attack success rate with merely less than 0.3% accuracy drop.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "GNN-backdoor",
        "graph-classification"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_7f4a9058",
      "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
      "abstract": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \r\nWe demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), or else by directly fine-tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can teach the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. \r\nWe quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "R. Schuster",
        "Congzheng Song",
        "Eran Tromer",
        "Vitaly Shmatikov"
      ],
      "author_details": [
        {
          "name": "R. Schuster",
          "h_index": 14,
          "citation_count": 2354,
          "affiliations": []
        },
        {
          "name": "Congzheng Song",
          "h_index": 17,
          "citation_count": 8858,
          "affiliations": []
        },
        {
          "name": "Eran Tromer",
          "h_index": 41,
          "citation_count": 14953,
          "affiliations": []
        },
        {
          "name": "Vitaly Shmatikov",
          "h_index": 69,
          "citation_count": 27582,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://openalex.org/W3155981360",
      "pdf_url": "https://arxiv.org/pdf/2007.02220",
      "citation_count": 184,
      "influential_citation_count": 24,
      "reference_count": 69,
      "is_open_access": false,
      "publication_date": "2020-07-05",
      "tldr": "This work quantifies the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "code-completion",
        "poisoning",
        "IDE-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2108.00352",
      "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
      "abstract": "Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jinyuan Jia",
        "Yupei Liu",
        "N. Gong"
      ],
      "author_details": [
        {
          "name": "Jinyuan Jia",
          "h_index": 21,
          "citation_count": 3203,
          "affiliations": []
        },
        {
          "name": "Yupei Liu",
          "h_index": 9,
          "citation_count": 729,
          "affiliations": [
            "Duke University"
          ]
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://arxiv.org/abs/2108.00352",
      "citation_count": 184,
      "influential_citation_count": 31,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2021-08-01",
      "tldr": "This work proposes BadEncoder, the first backdoor attack to self-supervised learning, which injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored imageEncoder for different downstream tasks simultaneously inherit the backdoor behavior.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "self-supervised",
        "encoder-backdoor",
        "transfer-learning"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2108.00352"
    },
    {
      "paper_id": "2003.08904",
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Maurice Weber",
        "Xiaojun Xu",
        "Bojan Karlas",
        "Ce Zhang",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Maurice Weber",
          "h_index": 7,
          "citation_count": 350,
          "affiliations": []
        },
        {
          "name": "Xiaojun Xu",
          "h_index": 15,
          "citation_count": 2405,
          "affiliations": []
        },
        {
          "name": "Bojan Karlas",
          "h_index": 12,
          "citation_count": 582,
          "affiliations": []
        },
        {
          "name": "Ce Zhang",
          "h_index": 59,
          "citation_count": 14868,
          "affiliations": [
            "ETH Zurich"
          ]
        },
        {
          "name": "Bo Li",
          "h_index": 69,
          "citation_count": 27430,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://arxiv.org/abs/2003.08904",
      "citation_count": 183,
      "influential_citation_count": 17,
      "reference_count": 93,
      "is_open_access": false,
      "publication_date": "2020-03-19",
      "tldr": "This paper provides a unified framework via randomized smoothing techniques and proposes the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks, and theoretically proves the robustness bound for machine learning models trained with RAB and proves that the robusts bound is tight.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "backdoor-defense",
        "provable"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2105.00164",
      "title": "Hidden Backdoors in Human-Centric Language Models",
      "abstract": "Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \\textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection, $95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shaofeng Li",
        "Hui Liu",
        "Tian Dong",
        "Benjamin Zi Hao Zhao",
        "Minhui Xue",
        "Haojin Zhu",
        "Jialiang Lu"
      ],
      "author_details": [
        {
          "name": "Shaofeng Li",
          "h_index": 13,
          "citation_count": 939,
          "affiliations": []
        },
        {
          "name": "Hui Liu",
          "h_index": 6,
          "citation_count": 298,
          "affiliations": []
        },
        {
          "name": "Tian Dong",
          "h_index": 10,
          "citation_count": 665,
          "affiliations": []
        },
        {
          "name": "Benjamin Zi Hao Zhao",
          "h_index": 13,
          "citation_count": 980,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 32,
          "citation_count": 4765,
          "affiliations": []
        },
        {
          "name": "Haojin Zhu",
          "h_index": 49,
          "citation_count": 9679,
          "affiliations": []
        },
        {
          "name": "Jialiang Lu",
          "h_index": 19,
          "citation_count": 1446,
          "affiliations": []
        }
      ],
      "max_h_index": 49,
      "url": "https://arxiv.org/abs/2105.00164",
      "citation_count": 169,
      "influential_citation_count": 14,
      "reference_count": 81,
      "is_open_access": true,
      "publication_date": "2021-05-01",
      "tldr": "The proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA).",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "natural-trigger",
        "human-centric",
        "covert"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2105.00164"
    },
    {
      "paper_id": "2101.02644",
      "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
      "abstract": "Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.   In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",
      "year": 2021,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Hai Huang",
        "Jiaming Mu",
        "N. Gong",
        "Qi Li",
        "Bin Liu",
        "Mingwei Xu"
      ],
      "author_details": [
        {
          "name": "Hai Huang",
          "h_index": 3,
          "citation_count": 189,
          "affiliations": []
        },
        {
          "name": "Jiaming Mu",
          "h_index": 4,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        },
        {
          "name": "Qi Li",
          "h_index": 9,
          "citation_count": 535,
          "affiliations": []
        },
        {
          "name": "Bin Liu",
          "h_index": 5,
          "citation_count": 234,
          "affiliations": []
        },
        {
          "name": "Mingwei Xu",
          "h_index": 34,
          "citation_count": 4702,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://arxiv.org/abs/2101.02644",
      "citation_count": 151,
      "influential_citation_count": 15,
      "reference_count": 51,
      "is_open_access": true,
      "publication_date": "2021-01-07",
      "tldr": "This work conducts the first systematic study on data poisoning attacks to deep learning based recommender systems, and formulate the attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "recommender-systems",
        "data-poisoning"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2021.24525"
    },
    {
      "paper_id": "2111.00197",
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "abstract": "Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Lujia Shen",
        "S. Ji",
        "Xuhong Zhang",
        "Jinfeng Li",
        "Jing Chen",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Lujia Shen",
          "h_index": 5,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Xuhong Zhang",
          "h_index": 18,
          "citation_count": 940,
          "affiliations": []
        },
        {
          "name": "Jinfeng Li",
          "h_index": 10,
          "citation_count": 1458,
          "affiliations": [
            "Alibaba Group",
            "Zhejiang University"
          ]
        },
        {
          "name": "Jing Chen",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Jie Shi",
          "h_index": 13,
          "citation_count": 662,
          "affiliations": []
        },
        {
          "name": "Chengfang Fang",
          "h_index": 14,
          "citation_count": 674,
          "affiliations": []
        },
        {
          "name": "Jianwei Yin",
          "h_index": 7,
          "citation_count": 379,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2111.00197",
      "citation_count": 144,
      "influential_citation_count": 30,
      "reference_count": 52,
      "is_open_access": true,
      "publication_date": "2021-10-30",
      "tldr": "A new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a preddefined output representation for the classification token in BERT, instead of a target label, which can introduce backdoor to a wide range of downstream tasks without any prior knowledge.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "pretrained-backdoor",
        "transferable",
        "NLP"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2111.00197"
    },
    {
      "paper_id": "2006.14026",
      "title": "Subpopulation Data Poisoning Attacks",
      "abstract": "Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a \\emph{subpopulation attack}, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Matthew Jagielski",
        "Giorgio Severi",
        "Niklas Pousette Harger",
        "Alina Oprea"
      ],
      "author_details": [
        {
          "name": "Matthew Jagielski",
          "h_index": 36,
          "citation_count": 11099,
          "affiliations": []
        },
        {
          "name": "Giorgio Severi",
          "h_index": 8,
          "citation_count": 517,
          "affiliations": []
        },
        {
          "name": "Niklas Pousette Harger",
          "h_index": 1,
          "citation_count": 138,
          "affiliations": []
        },
        {
          "name": "Alina Oprea",
          "h_index": 32,
          "citation_count": 9229,
          "affiliations": []
        }
      ],
      "max_h_index": 36,
      "url": "https://arxiv.org/abs/2006.14026",
      "citation_count": 138,
      "influential_citation_count": 10,
      "reference_count": 87,
      "is_open_access": true,
      "publication_date": "2020-06-24",
      "tldr": "It is proved that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against the authors' attacks, highlighting the difficulty of protecting machine learning against this threat.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "subpopulation",
        "targeted-poisoning"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2006.14026"
    },
    {
      "paper_id": "2204.00032",
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.   Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8x more precise inference on all other users' otherwise-private data points.   Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Florian Tram\u00e8r",
        "R. Shokri",
        "Ayrton San Joaquin",
        "Hoang M. Le",
        "Matthew Jagielski",
        "Sanghyun Hong",
        "Nicholas Carlini"
      ],
      "author_details": [
        {
          "name": "Florian Tram\u00e8r",
          "h_index": 51,
          "citation_count": 33525,
          "affiliations": [
            "ETH Z\u00fcrich"
          ]
        },
        {
          "name": "R. Shokri",
          "h_index": 45,
          "citation_count": 16637,
          "affiliations": []
        },
        {
          "name": "Ayrton San Joaquin",
          "h_index": 3,
          "citation_count": 148,
          "affiliations": []
        },
        {
          "name": "Hoang M. Le",
          "h_index": 2,
          "citation_count": 159,
          "affiliations": []
        },
        {
          "name": "Matthew Jagielski",
          "h_index": 36,
          "citation_count": 11099,
          "affiliations": []
        },
        {
          "name": "Sanghyun Hong",
          "h_index": 14,
          "citation_count": 1372,
          "affiliations": []
        },
        {
          "name": "Nicholas Carlini",
          "h_index": 35,
          "citation_count": 10308,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2204.00032",
      "citation_count": 137,
      "influential_citation_count": 15,
      "reference_count": 81,
      "is_open_access": true,
      "publication_date": "2022-03-31",
      "tldr": "It is shown that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties, casting doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "privacy-poisoning",
        "membership-inference",
        "active-attack"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560554"
    },
    {
      "paper_id": "seed_048d0c77",
      "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification",
      "abstract": "Deep Neural Network (DNN) classifiers are known to be vulnerable to Trojan or backdoor attacks, where the classifier is manipulated such that it misclassifies any input containing an attacker-determined Trojan trigger. Backdoors compromise a model's integrity, thereby posing a severe threat to the landscape of DNN-based classification. While multiple defenses against such attacks exist for classifiers in the image domain, there have been limited efforts to protect classifiers in the text domain. We present Trojan-Miner (T-Miner) -- a defense framework for Trojan attacks on DNN-based text classifiers. T-Miner employs a sequence-to-sequence (seq-2-seq) generative model that probes the suspicious classifier and learns to produce text sequences that are likely to contain the Trojan trigger. T-Miner then analyzes the text produced by the generative model to determine if they contain trigger phrases, and correspondingly, whether the tested classifier has a backdoor. T-Miner requires no access to the training dataset or clean inputs of the suspicious classifier, and instead uses synthetically crafted \"nonsensical\" text inputs to train the generative model. We extensively evaluate T-Miner on 1100 model instances spanning 3 ubiquitous DNN model architectures, 5 different classification tasks, and a variety of trigger phrases. We show that T-Miner detects Trojan and clean models with a 98.75% overall accuracy, while achieving low false positives on clean models. We also show that T-Miner is robust against a variety of targeted, advanced attacks from an adaptive attacker.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "A. Azizi",
        "I. Tahmid",
        "Asim Waheed",
        "Neal Mangaokar",
        "Jiameng Pu",
        "M. Javed",
        "Chandan K. Reddy",
        "Bimal Viswanath"
      ],
      "author_details": [
        {
          "name": "A. Azizi",
          "h_index": 2,
          "citation_count": 98,
          "affiliations": []
        },
        {
          "name": "I. Tahmid",
          "h_index": 6,
          "citation_count": 183,
          "affiliations": []
        },
        {
          "name": "Asim Waheed",
          "h_index": 3,
          "citation_count": 114,
          "affiliations": []
        },
        {
          "name": "Neal Mangaokar",
          "h_index": 4,
          "citation_count": 206,
          "affiliations": []
        },
        {
          "name": "Jiameng Pu",
          "h_index": 7,
          "citation_count": 374,
          "affiliations": []
        },
        {
          "name": "M. Javed",
          "h_index": 18,
          "citation_count": 1099,
          "affiliations": []
        },
        {
          "name": "Chandan K. Reddy",
          "h_index": 46,
          "citation_count": 7941,
          "affiliations": []
        },
        {
          "name": "Bimal Viswanath",
          "h_index": 24,
          "citation_count": 6491,
          "affiliations": []
        }
      ],
      "max_h_index": 46,
      "url": "https://openalex.org/W3135366566",
      "pdf_url": "https://arxiv.org/pdf/2103.04264",
      "doi": "https://doi.org/10.48550/arxiv.2103.04264",
      "citation_count": 97,
      "influential_citation_count": 16,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2021-03-07",
      "tldr": "T-Miner is presented -- a defense framework for Trojan attacks on DNN-based text classifiers that employs a sequence-to-sequence (seq-2-seq) generative model that probes the suspicious classifier and learns to produce text sequences that are likely to contain the Trojan trigger.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "trojan-defense",
        "text-classification",
        "generative"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_94d1790d",
      "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
      "abstract": "Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data. We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods. We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Nicholas Carlini"
      ],
      "author_details": [
        {
          "name": "Nicholas Carlini",
          "h_index": 2,
          "citation_count": 88,
          "affiliations": []
        }
      ],
      "max_h_index": 2,
      "url": "https://openalex.org/W3157597523",
      "pdf_url": "https://arxiv.org/pdf/2105.01622",
      "doi": "https://doi.org/10.48550/arxiv.2105.01622",
      "citation_count": 78,
      "influential_citation_count": 13,
      "reference_count": 72,
      "is_open_access": false,
      "publication_date": "2021-05-04",
      "tldr": "This work studies a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset, and finds that more accurate methods are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent it.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "semi-supervised",
        "unlabeled-poisoning"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_d2bc1e0d",
      "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
      "abstract": "In adversarial machine learning, new defenses against attacks on deep learning systems are routinely broken soon after their release by more powerful attacks. In this context, forensic tools can offer a valuable complement to existing defenses, by tracing back a successful attack to its root cause, and offering a path forward for mitigation to prevent similar attacks in the future. In this paper, we describe our efforts in developing a forensic traceback tool for poison attacks on deep neural networks. We propose a novel iterative clustering and pruning solution that trims \"innocent\" training samples, until all that remains is the set of poisoned data responsible for the attack. Our method clusters training samples based on their impact on model parameters, then uses an efficient data unlearning method to prune innocent clusters. We empirically demonstrate the efficacy of our system on three types of dirty-label (backdoor) poison attacks and three types of clean-label poison attacks, across domains of computer vision and malware classification. Our system achieves over 98.4% precision and 96.8% recall across all attacks. We also show that our system is robust against four anti-forensics measures specifically designed to attack it.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Shawn Shan",
        "A. Bhagoji",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "author_details": [
        {
          "name": "Shawn Shan",
          "h_index": 18,
          "citation_count": 2943,
          "affiliations": [
            "University of Chicago"
          ]
        },
        {
          "name": "A. Bhagoji",
          "h_index": 23,
          "citation_count": 11117,
          "affiliations": []
        },
        {
          "name": "Haitao Zheng",
          "h_index": 59,
          "citation_count": 14282,
          "affiliations": []
        },
        {
          "name": "Ben Y. Zhao",
          "h_index": 67,
          "citation_count": 25558,
          "affiliations": []
        }
      ],
      "max_h_index": 67,
      "url": "https://openalex.org/W4286904975",
      "pdf_url": "https://arxiv.org/pdf/2110.06904",
      "doi": "https://doi.org/10.48550/arxiv.2110.06904",
      "citation_count": 62,
      "influential_citation_count": 9,
      "reference_count": 90,
      "is_open_access": false,
      "publication_date": "2021-10-13",
      "tldr": "This paper proposes a novel iterative clustering and pruning solution that trims \"innocent\" training samples, until all that remains is the set of poisoned data responsible for the attack.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "forensics",
        "traceback",
        "attribution"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2205.06900",
      "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
      "abstract": "Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type. Our detector leverages the influence of the backdoor attack, independent of the backdoor embedding mechanism, on the landscape of the classifier's outputs prior to the softmax layer. For each class, a maximum margin statistic is estimated. Detection inference is then performed by applying an unsupervised anomaly detector to these statistics. Thus, our detector does not need any legitimate clean samples, and can efficiently detect backdoor attacks with arbitrary numbers of source classes. These advantages over several state-of-the-art methods are demonstrated on four datasets, for three different types of backdoor patterns, and for a variety of attack configurations. Finally, we propose a novel, general approach for backdoor mitigation once a detection is made. The mitigation approach was the runner-up at the first IEEE Trojan Removal Competition. The code is online available.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Hang Wang",
        "Zhen Xiang",
        "David J. Miller",
        "G. Kesidis"
      ],
      "author_details": [
        {
          "name": "Hang Wang",
          "h_index": 5,
          "citation_count": 117,
          "affiliations": []
        },
        {
          "name": "Zhen Xiang",
          "h_index": 13,
          "citation_count": 772,
          "affiliations": [
            "University of Georgia"
          ]
        },
        {
          "name": "David J. Miller",
          "h_index": 33,
          "citation_count": 4004,
          "affiliations": []
        },
        {
          "name": "G. Kesidis",
          "h_index": 41,
          "citation_count": 7135,
          "affiliations": []
        }
      ],
      "max_h_index": 41,
      "url": "https://arxiv.org/abs/2205.06900",
      "citation_count": 61,
      "influential_citation_count": 12,
      "reference_count": 108,
      "is_open_access": true,
      "publication_date": "2022-05-13",
      "tldr": "This paper proposes a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type, and proposes a novel, general approach for backdoor mitigation once a detection is made.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "post-training-detection",
        "maximum-margin"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2205.06900"
    },
    {
      "paper_id": "seed_afd91528",
      "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
      "abstract": "Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users\u2019 privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.",
      "year": 2023,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Adnan Qayyum",
        "M. A. Butt",
        "Hassan Ali",
        "Muhammad Usman",
        "O. Halabi",
        "Ala I. Al-Fuqaha",
        "Q. Abbasi",
        "M. Imran",
        "Junaid Qadir"
      ],
      "author_details": [
        {
          "name": "Adnan Qayyum",
          "h_index": 11,
          "citation_count": 977,
          "affiliations": []
        },
        {
          "name": "M. A. Butt",
          "h_index": 12,
          "citation_count": 403,
          "affiliations": []
        },
        {
          "name": "Hassan Ali",
          "h_index": 5,
          "citation_count": 154,
          "affiliations": []
        },
        {
          "name": "Muhammad Usman",
          "h_index": 11,
          "citation_count": 375,
          "affiliations": []
        },
        {
          "name": "O. Halabi",
          "h_index": 12,
          "citation_count": 703,
          "affiliations": []
        },
        {
          "name": "Ala I. Al-Fuqaha",
          "h_index": 30,
          "citation_count": 8585,
          "affiliations": []
        },
        {
          "name": "Q. Abbasi",
          "h_index": 49,
          "citation_count": 10607,
          "affiliations": []
        },
        {
          "name": "M. Imran",
          "h_index": 30,
          "citation_count": 3691,
          "affiliations": []
        },
        {
          "name": "Junaid Qadir",
          "h_index": 19,
          "citation_count": 1625,
          "affiliations": []
        }
      ],
      "max_h_index": 49,
      "url": "https://openalex.org/W4385724403",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3614426",
      "doi": "https://doi.org/10.1145/3614426",
      "citation_count": 61,
      "influential_citation_count": 2,
      "reference_count": 185,
      "is_open_access": true,
      "publication_date": "2022-10-24",
      "tldr": "This work attempts to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications and presents a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "linguistic-style",
        "hidden-trigger"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3614426"
    },
    {
      "paper_id": "2301.02344",
      "title": "TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models",
      "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "H. Aghakhani",
        "Wei Dai",
        "Andre Manoel",
        "Xavier Fernandes",
        "Anant Kharkar",
        "Christopher Kruegel",
        "Giovanni Vigna",
        "David Evans",
        "B. Zorn",
        "Robert Sim"
      ],
      "author_details": [
        {
          "name": "H. Aghakhani",
          "h_index": 11,
          "citation_count": 721,
          "affiliations": []
        },
        {
          "name": "Wei Dai",
          "h_index": 19,
          "citation_count": 1740,
          "affiliations": []
        },
        {
          "name": "Andre Manoel",
          "h_index": 15,
          "citation_count": 1389,
          "affiliations": [
            "Microsoft Research"
          ]
        },
        {
          "name": "Xavier Fernandes",
          "h_index": 1,
          "citation_count": 60,
          "affiliations": []
        },
        {
          "name": "Anant Kharkar",
          "h_index": 6,
          "citation_count": 471,
          "affiliations": []
        },
        {
          "name": "Christopher Kruegel",
          "h_index": 9,
          "citation_count": 253,
          "affiliations": []
        },
        {
          "name": "Giovanni Vigna",
          "h_index": 93,
          "citation_count": 31561,
          "affiliations": []
        },
        {
          "name": "David Evans",
          "h_index": 49,
          "citation_count": 12330,
          "affiliations": [
            "University of Virginia",
            "MIT"
          ]
        },
        {
          "name": "B. Zorn",
          "h_index": 44,
          "citation_count": 6973,
          "affiliations": []
        },
        {
          "name": "Robert Sim",
          "h_index": 12,
          "citation_count": 1388,
          "affiliations": []
        }
      ],
      "max_h_index": 93,
      "url": "https://arxiv.org/abs/2301.02344",
      "citation_count": 60,
      "influential_citation_count": 13,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2023-01-06",
      "tldr": "This work demonstrates two novel attacks, Covert and TrojanPuzzle, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings and demonstrates that both Covert and TrojanPuzzle have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "model_types": [
        "llm",
        "transformer"
      ],
      "tags": [
        "code-suggestion",
        "covert-poisoning",
        "Copilot"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2301.02344"
    },
    {
      "paper_id": "seed_dddf388a",
      "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
      "abstract": "Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. We also provide procedures to adaptively select the number of suspicious points to remove. In the end-to-end SL setting, ASSET is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios; in particular, it is the only method that can detect the state-of-the-art clean-label attack. Moreover, ASSET's average detection rates are higher than the best existing methods in SSL and TL, respectively, by 69.3% and 33.2%, thus providing the first practical backdoor defense for these new DL settings. We open-source the project to drive further development and encourage engagement: https://github.com/ruoxi-jia-group/ASSET.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minzhou Pan",
        "Yi Zeng",
        "L. Lyu",
        "X. Lin",
        "R. Jia"
      ],
      "author_details": [
        {
          "name": "Minzhou Pan",
          "h_index": 10,
          "citation_count": 467,
          "affiliations": []
        },
        {
          "name": "Yi Zeng",
          "h_index": 7,
          "citation_count": 293,
          "affiliations": []
        },
        {
          "name": "L. Lyu",
          "h_index": 28,
          "citation_count": 4611,
          "affiliations": []
        },
        {
          "name": "X. Lin",
          "h_index": 49,
          "citation_count": 9030,
          "affiliations": []
        },
        {
          "name": "R. Jia",
          "h_index": 30,
          "citation_count": 4544,
          "affiliations": []
        }
      ],
      "max_h_index": 49,
      "url": "https://openalex.org/W4321649939",
      "pdf_url": "https://arxiv.org/pdf/2302.11408",
      "doi": "https://doi.org/10.48550/arxiv.2302.11408",
      "citation_count": 47,
      "influential_citation_count": 9,
      "reference_count": 58,
      "is_open_access": true,
      "publication_date": "2023-02-22",
      "tldr": "A new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation and is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "SSL",
        "TL",
        "cross-paradigm"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2302.11408"
    },
    {
      "paper_id": "seed_a8f190bf",
      "title": "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning",
      "abstract": "Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word \"meaning\" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the \"meaning\" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "R. Schuster",
        "Tal Schuster",
        "Yoav Meri",
        "Vitaly Shmatikov"
      ],
      "author_details": [
        {
          "name": "R. Schuster",
          "h_index": 14,
          "citation_count": 2354,
          "affiliations": []
        },
        {
          "name": "Tal Schuster",
          "h_index": 30,
          "citation_count": 8544,
          "affiliations": [
            "MIT"
          ]
        },
        {
          "name": "Yoav Meri",
          "h_index": 1,
          "citation_count": 42,
          "affiliations": []
        },
        {
          "name": "Vitaly Shmatikov",
          "h_index": 69,
          "citation_count": 27582,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://openalex.org/W2999480335",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152608.pdf",
      "doi": "https://doi.org/10.1109/sp40000.2020.00115",
      "citation_count": 42,
      "influential_citation_count": 3,
      "reference_count": 80,
      "is_open_access": true,
      "publication_date": "2020-01-14",
      "tldr": "This work develops an explicit expression over corpus features that serves as a proxy for distance between words and establishes a causative relationship between its values and embedding distances, and shows how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [],
      "tags": [
        "word-embeddings",
        "corpus-poisoning",
        "semantic-manipulation"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152608.pdf"
    },
    {
      "paper_id": "seed_0c08f38b",
      "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
      "abstract": "Contrastive learning pre-trains an image encoder using a large amount of unlabeled data such that the image encoder can be used as a general-purpose feature extractor for various downstream tasks. In this work, we propose PoisonedEncoder, a data poisoning attack to contrastive learning. In particular, an attacker injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes. We formulate our data poisoning attack as a bilevel optimization problem, whose solution is the set of poisoning inputs; and we propose a contrastive-learning-tailored method to approximately solve it. Our evaluation on multiple datasets shows that PoisonedEncoder achieves high attack success rates while maintaining the testing accuracy of the downstream classifiers built upon the poisoned encoder for non-attacker-chosen inputs. We also evaluate five defenses against PoisonedEncoder, including one pre-processing, three in-processing, and one post-processing defenses. Our results show that these defenses can decrease the attack success rate of PoisonedEncoder, but they also sacrifice the utility of the encoder or require a large clean pre-training dataset.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "author_details": [
        {
          "name": "Hongbin Liu",
          "h_index": 11,
          "citation_count": 494,
          "affiliations": []
        },
        {
          "name": "Jinyuan Jia",
          "h_index": 21,
          "citation_count": 3203,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://openalex.org/W4280514286",
      "pdf_url": "https://arxiv.org/pdf/2205.06401",
      "doi": "https://doi.org/10.48550/arxiv.2205.06401",
      "citation_count": 41,
      "influential_citation_count": 7,
      "reference_count": 60,
      "is_open_access": true,
      "publication_date": "2022-05-13",
      "tldr": "This work proposes PoisonedEncoder, a data poisoning attack to contrastive learning, which injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "contrastive-learning",
        "encoder-poisoning"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2205.06401"
    },
    {
      "paper_id": "seed_c9cfa9dc",
      "title": "Backdooring Multimodal Learning",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xingshuo Han",
        "Yutong Wu",
        "Qingjie Zhang",
        "Yuan Zhou",
        "Yuan Xu",
        "Han Qiu",
        "Guowen Xu",
        "Tianwei Zhang"
      ],
      "author_details": [
        {
          "name": "Xingshuo Han",
          "h_index": 11,
          "citation_count": 381,
          "affiliations": []
        },
        {
          "name": "Yutong Wu",
          "h_index": 3,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Qingjie Zhang",
          "h_index": 4,
          "citation_count": 104,
          "affiliations": []
        },
        {
          "name": "Yuan Zhou",
          "h_index": 3,
          "citation_count": 55,
          "affiliations": []
        },
        {
          "name": "Yuan Xu",
          "h_index": 9,
          "citation_count": 508,
          "affiliations": []
        },
        {
          "name": "Han Qiu",
          "h_index": 10,
          "citation_count": 315,
          "affiliations": []
        },
        {
          "name": "Guowen Xu",
          "h_index": 3,
          "citation_count": 45,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 4,
          "citation_count": 86,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a031/1RjEa7rmaxW",
      "citation_count": 35,
      "influential_citation_count": 1,
      "reference_count": 81,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "multimodal-backdoor",
        "cross-modal"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2301.01197",
      "title": "Backdoor Attacks Against Dataset Distillation",
      "abstract": "Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Zheng Li",
        "M. Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Yugeng Liu",
          "h_index": 9,
          "citation_count": 608,
          "affiliations": []
        },
        {
          "name": "Zheng Li",
          "h_index": 15,
          "citation_count": 1128,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 16,
          "citation_count": 1497,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 82,
          "citation_count": 35162,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://arxiv.org/abs/2301.01197",
      "citation_count": 34,
      "influential_citation_count": 3,
      "reference_count": 89,
      "is_open_access": true,
      "publication_date": "2023-01-03",
      "tldr": "This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain, and proposes two types of backdoor attacks, namely NAIVEATTACK and DOORPING.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "dataset-distillation",
        "backdoor"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2301.01197"
    },
    {
      "paper_id": "seed_2e985c83",
      "title": "Reverse Attack: Black-box Attacks on Collaborative Recommendation",
      "abstract": "Collaborative filtering (CF) recommender systems have been extensively developed and widely deployed in various social websites, promoting products or services to the users of interest. Meanwhile, work has been attempted at poisoning attacks to CF recommender systems for distorting the recommend results to reap commercial or personal gains stealthily. While existing poisoning attacks have demonstrated their effectiveness with the offline social datasets, they are impractical when applied to the real setting on online social websites. This paper develops a novel and practical poisoning attack solution toward the CF recommender systems without knowing involved specific algorithms nor historical social data information a priori. Instead of directly attacking the unknown recommender systems, our solution performs certain operations on the social websites to collect a set of sampling data for use in constructing a surrogate model for deeply learning the inherent recommendation patterns. This surrogate model can estimate the item proximities, learned by the recommender systems. By attacking the surrogate model, the corresponding solutions (for availability and target attacks) can be directly migrated to attack the original recommender systems. Extensive experiments validate the generated surrogate model's reproductive capability and demonstrate the effectiveness of our attack upon various CF recommender algorithms.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yihe Zhang",
        "Xu Yuan",
        "Jin Li",
        "Jiadong Lou",
        "Li Chen",
        "N. Tzeng"
      ],
      "author_details": [
        {
          "name": "Yihe Zhang",
          "h_index": 8,
          "citation_count": 191,
          "affiliations": []
        },
        {
          "name": "Xu Yuan",
          "h_index": 14,
          "citation_count": 643,
          "affiliations": []
        },
        {
          "name": "Jin Li",
          "h_index": 4,
          "citation_count": 155,
          "affiliations": []
        },
        {
          "name": "Jiadong Lou",
          "h_index": 10,
          "citation_count": 263,
          "affiliations": []
        },
        {
          "name": "Li Chen",
          "h_index": 5,
          "citation_count": 91,
          "affiliations": []
        },
        {
          "name": "N. Tzeng",
          "h_index": 28,
          "citation_count": 3064,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://openalex.org/W3214009246",
      "doi": "https://doi.org/10.1145/3460120.3484805",
      "citation_count": 31,
      "influential_citation_count": 2,
      "reference_count": 92,
      "is_open_access": false,
      "publication_date": "2021-11-12",
      "tldr": "This paper develops a novel and practical poisoning attack solution toward the CF recommender systems without knowing involved specific algorithms nor historical social data information a priori, and performs certain operations on the social websites to collect a set of sampling data for use in constructing a surrogate model for deeply learning the inherent recommendation patterns.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "recommender-systems",
        "black-box",
        "collaborative-filtering"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2111.04394",
      "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
      "abstract": "Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.",
      "year": 2022,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "A. Salem",
        "M. Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "A. Salem",
          "h_index": 17,
          "citation_count": 3182,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://arxiv.org/abs/2111.04394",
      "citation_count": 31,
      "influential_citation_count": 5,
      "reference_count": 54,
      "is_open_access": true,
      "publication_date": "2021-11-08",
      "tldr": "This work proposes a new training time attack against computer vision based machine learning models, namely model hijacking attack, which aims to hijack a target model to execute a different task than its original one without the model owner noticing.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "model-hijacking",
        "training-time-attack"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2022.23064"
    },
    {
      "paper_id": "seed_4e4fdabc",
      "title": "REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation",
      "abstract": "Recent works have revealed the vulnerability of deep neural networks to backdoor attacks, where a backdoored model orchestrates targeted or untargeted misclassification when activated by a trigger. A line of purification methods (e.g., fine-pruning, neural attention transfer, MCR [69]) have been proposed to remove the backdoor in a model. However, they either fail to reduce the attack success rate of more advanced backdoor attacks or largely degrade the prediction capacity of the model for clean samples. In this paper, we put forward a new purification defense framework, dubbed SAGE, which utilizes self-attention distillation to purge models of backdoors. Unlike traditional attention transfer mechanisms that require a teacher model to supervise the distillation process, SAGE can realize self-purification with a small number of clean samples. To enhance the defense performance, we further propose a dynamic learning rate adjustment strategy that carefully tracks the prediction accuracy of clean samples to guide the learning rate adjustment. We compare the defense performance of SAGE with 6 state-of-the-art defense approaches against 8 backdoor attacks on 4 datasets. It is shown that SAGE can reduce the attack success rate by as much as 90% with less than 3% decrease in prediction accuracy for clean samples. We will open-source our codes upon publication.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wang Yang",
        "Qianqian Wang",
        "Yuzhe Gu",
        "Huayang Huang",
        "Chao Shen"
      ],
      "author_details": [
        {
          "name": "Xueluan Gong",
          "h_index": 5,
          "citation_count": 92,
          "affiliations": [
            "Wuhan University"
          ]
        },
        {
          "name": "Yanjiao Chen",
          "h_index": 12,
          "citation_count": 473,
          "affiliations": []
        },
        {
          "name": "Wang Yang",
          "h_index": 1,
          "citation_count": 28,
          "affiliations": []
        },
        {
          "name": "Qianqian Wang",
          "h_index": 52,
          "citation_count": 14361,
          "affiliations": []
        },
        {
          "name": "Yuzhe Gu",
          "h_index": 3,
          "citation_count": 47,
          "affiliations": []
        },
        {
          "name": "Huayang Huang",
          "h_index": 7,
          "citation_count": 218,
          "affiliations": []
        },
        {
          "name": "Chao Shen",
          "h_index": 20,
          "citation_count": 1393,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://openalex.org/W4385187298",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179375",
      "citation_count": 28,
      "influential_citation_count": 0,
      "reference_count": 77,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "backdoor-purification",
        "attention-distillation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_27e87237",
      "title": "ODSCAN: Backdoor Scanning for Object Detection Models",
      "abstract": "A backdoor attack in deep learning inserts a hidden backdoor in the model to trigger malicious behavior upon specific input patterns. Existing detection approaches assume a metric space (for either the original inputs or their latent representations) in which normal samples and malicious samples are separable. We show that this assumption has a severe limitation by introducing a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures the difference between normal samples and malicious samples.   To overcome this limitation, we move beyond looking for a perfect metric space that would work for different deep-learning models, and instead resort to more robust topological constructs. We propose TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection. The main idea of TED is to view a deep-learning model as a dynamical system that evolves inputs to outputs. In such a dynamical system, a benign input follows a natural evolution trajectory similar to other benign inputs. In contrast, a malicious sample displays a distinct trajectory, since it starts close to benign samples but eventually shifts towards the neighborhood of attacker-specified target samples to activate the backdoor.   Extensive evaluations are conducted on vision and natural language datasets across different network architectures. The results demonstrate that TED not only achieves a high detection rate, but also significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack. The code to reproduce the results is made public on GitHub.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xiaoxing Mo",
        "Yechao Zhang",
        "L. Zhang",
        "Wei Luo",
        "Nan Sun",
        "Shengshan Hu",
        "Shang Gao",
        "Yang Xiang"
      ],
      "author_details": [
        {
          "name": "Xiaoxing Mo",
          "h_index": 3,
          "citation_count": 197,
          "affiliations": []
        },
        {
          "name": "Yechao Zhang",
          "h_index": 11,
          "citation_count": 486,
          "affiliations": []
        },
        {
          "name": "L. Zhang",
          "h_index": 6,
          "citation_count": 140,
          "affiliations": []
        },
        {
          "name": "Wei Luo",
          "h_index": 3,
          "citation_count": 91,
          "affiliations": []
        },
        {
          "name": "Nan Sun",
          "h_index": 3,
          "citation_count": 36,
          "affiliations": []
        },
        {
          "name": "Shengshan Hu",
          "h_index": 26,
          "citation_count": 1978,
          "affiliations": []
        },
        {
          "name": "Shang Gao",
          "h_index": 3,
          "citation_count": 39,
          "affiliations": []
        },
        {
          "name": "Yang Xiang",
          "h_index": 2,
          "citation_count": 46,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://arxiv.org/abs/2312.02673",
      "citation_count": 28,
      "influential_citation_count": 6,
      "reference_count": 54,
      "is_open_access": true,
      "publication_date": "2023-12-05",
      "tldr": "This work proposes TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection and demonstrates that it significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "object-detection",
        "backdoor-scanning"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2312.02673"
    },
    {
      "paper_id": "seed_c385cdc7",
      "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
      "abstract": "Contains fulltext : 310103.pdf (Publisher\u2019s version ) (Open Access)",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Gorka Abad",
        "O. Ersoy",
        "S. Picek",
        "A. Urbieta"
      ],
      "author_details": [
        {
          "name": "Gorka Abad",
          "h_index": 6,
          "citation_count": 103,
          "affiliations": []
        },
        {
          "name": "O. Ersoy",
          "h_index": 13,
          "citation_count": 604,
          "affiliations": []
        },
        {
          "name": "S. Picek",
          "h_index": 39,
          "citation_count": 5862,
          "affiliations": []
        },
        {
          "name": "A. Urbieta",
          "h_index": 16,
          "citation_count": 1378,
          "affiliations": []
        }
      ],
      "max_h_index": 39,
      "url": "https://openalex.org/W4391725253",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.24334",
      "doi": "https://doi.org/10.14722/ndss.2024.24334",
      "citation_count": 27,
      "influential_citation_count": 3,
      "reference_count": 79,
      "is_open_access": true,
      "publication_date": "2023-02-13",
      "tldr": "Backdoor triggers within neuromorphic data that can manipulate their position and color are explored, providing a broader scope of possibilities than conventional triggers in domains like images, and several state-of-the-art defenses are adapted.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "trigger-intensity",
        "training-inference-gap"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.24334"
    },
    {
      "paper_id": "2302.06279",
      "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.   This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Gorka Abad",
        "O. Ersoy",
        "S. Picek",
        "A. Urbieta"
      ],
      "author_details": [
        {
          "name": "Gorka Abad",
          "h_index": 6,
          "citation_count": 103,
          "affiliations": []
        },
        {
          "name": "O. Ersoy",
          "h_index": 13,
          "citation_count": 604,
          "affiliations": []
        },
        {
          "name": "S. Picek",
          "h_index": 39,
          "citation_count": 5862,
          "affiliations": []
        },
        {
          "name": "A. Urbieta",
          "h_index": 16,
          "citation_count": 1378,
          "affiliations": []
        }
      ],
      "max_h_index": 39,
      "url": "https://arxiv.org/abs/2302.06279",
      "citation_count": 27,
      "influential_citation_count": 3,
      "reference_count": 79,
      "is_open_access": true,
      "publication_date": "2023-02-13",
      "tldr": "Backdoor triggers within neuromorphic data that can manipulate their position and color are explored, providing a broader scope of possibilities than conventional triggers in domains like images, and several state-of-the-art defenses are adapted.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [],
      "tags": [
        "SNN",
        "neuromorphic",
        "stealthy-backdoor"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.24334"
    },
    {
      "paper_id": "seed_2d0aee9f",
      "title": "Disguising Attacks with Explanation-Aware Backdoors",
      "abstract": "Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate how to fully disguise the adversarial operation of a machine learning model. Similar to neural backdoors, we change the model's prediction upon trigger presence but simultaneously fool an explanation method that is applied post-hoc for analysis. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of these explanation-aware backdoors for gradient- and propagation-based explanation methods in the image domain, before we resume to conduct a red-herring attack against malware classification.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Maximilian Noppel",
        "Lukas Peter",
        "Christian Wressnegger"
      ],
      "author_details": [
        {
          "name": "Maximilian Noppel",
          "h_index": 4,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Lukas Peter",
          "h_index": 2,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Christian Wressnegger",
          "h_index": 19,
          "citation_count": 1625,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://openalex.org/W4385080308",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179308",
      "citation_count": 25,
      "influential_citation_count": 4,
      "reference_count": 100,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "XAI-aware",
        "disguised-backdoor",
        "explanation-manipulation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_580ab67c",
      "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Guanhong Tao",
        "Zhenting Wang",
        "Shiwei Feng",
        "Guangyu Shen",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "author_details": [
        {
          "name": "Guanhong Tao",
          "h_index": 27,
          "citation_count": 2840,
          "affiliations": []
        },
        {
          "name": "Zhenting Wang",
          "h_index": 14,
          "citation_count": 804,
          "affiliations": []
        },
        {
          "name": "Shiwei Feng",
          "h_index": 11,
          "citation_count": 434,
          "affiliations": [
            "Purdue University"
          ]
        },
        {
          "name": "Guangyu Shen",
          "h_index": 20,
          "citation_count": 1240,
          "affiliations": []
        },
        {
          "name": "Shiqing Ma",
          "h_index": 13,
          "citation_count": 1238,
          "affiliations": []
        },
        {
          "name": "Xiangyu Zhang",
          "h_index": 16,
          "citation_count": 944,
          "affiliations": []
        }
      ],
      "max_h_index": 27,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a029/1RjEa5rjsHK",
      "citation_count": 24,
      "influential_citation_count": 3,
      "reference_count": 101,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "self-supervised",
        "distribution-preserving"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2311.11225",
      "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
      "abstract": "Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Hengzhi Pei",
        "Jinyuan Jia",
        "Wenbo Guo",
        "Bo Li",
        "D. Song"
      ],
      "author_details": [
        {
          "name": "Hengzhi Pei",
          "h_index": 12,
          "citation_count": 1528,
          "affiliations": []
        },
        {
          "name": "Jinyuan Jia",
          "h_index": 8,
          "citation_count": 473,
          "affiliations": []
        },
        {
          "name": "Wenbo Guo",
          "h_index": 8,
          "citation_count": 156,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 5,
          "citation_count": 171,
          "affiliations": []
        },
        {
          "name": "D. Song",
          "h_index": 8,
          "citation_count": 111,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://arxiv.org/abs/2311.11225",
      "citation_count": 21,
      "influential_citation_count": 0,
      "reference_count": 55,
      "is_open_access": false,
      "publication_date": "2023-11-19",
      "tldr": "Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks, surpassing the certification accuracy of existing certified defenses against backdoor attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "provable-defense",
        "text-classification"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2301.06241",
      "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
      "abstract": "Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Siyuan Cheng",
        "Guanhong Tao",
        "Yingqi Liu",
        "Shengwei An",
        "Xiangzhe Xu",
        "Shiwei Feng",
        "Guangyu Shen",
        "Kaiyuan Zhang",
        "Qiuling Xu",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "author_details": [
        {
          "name": "Siyuan Cheng",
          "h_index": 21,
          "citation_count": 1806,
          "affiliations": []
        },
        {
          "name": "Guanhong Tao",
          "h_index": 27,
          "citation_count": 2840,
          "affiliations": []
        },
        {
          "name": "Yingqi Liu",
          "h_index": 13,
          "citation_count": 1832,
          "affiliations": []
        },
        {
          "name": "Shengwei An",
          "h_index": 17,
          "citation_count": 1032,
          "affiliations": []
        },
        {
          "name": "Xiangzhe Xu",
          "h_index": 12,
          "citation_count": 485,
          "affiliations": []
        },
        {
          "name": "Shiwei Feng",
          "h_index": 11,
          "citation_count": 434,
          "affiliations": [
            "Purdue University"
          ]
        },
        {
          "name": "Guangyu Shen",
          "h_index": 20,
          "citation_count": 1240,
          "affiliations": []
        },
        {
          "name": "Kaiyuan Zhang",
          "h_index": 11,
          "citation_count": 477,
          "affiliations": []
        },
        {
          "name": "Qiuling Xu",
          "h_index": 13,
          "citation_count": 753,
          "affiliations": []
        },
        {
          "name": "Shiqing Ma",
          "h_index": 43,
          "citation_count": 8258,
          "affiliations": []
        },
        {
          "name": "Xiangyu Zhang",
          "h_index": 16,
          "citation_count": 944,
          "affiliations": []
        }
      ],
      "max_h_index": 43,
      "url": "https://arxiv.org/abs/2301.06241",
      "citation_count": 20,
      "influential_citation_count": 1,
      "reference_count": 126,
      "is_open_access": true,
      "publication_date": "2023-01-16",
      "tldr": "This paper proposes a novel model backdoor forensics technique that decomposes a few attack samples such as inputs with backdoor triggers to clean inputs and the corresponding triggers, and clusters the triggers based on their properties to allow automatic attack categorization and summarization.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-forensics",
        "reverse-engineering"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2301.06241"
    },
    {
      "paper_id": "seed_388cc2c1",
      "title": "AI-Guardian: Defeating Adversarial Attacks using Backdoors",
      "abstract": "Deep neural networks (DNNs) have been widely used in many fields due to their increasingly high accuracy. However, they are also vulnerable to adversarial attacks, posing a serious threat to security-critical applications such as autonomous driving, remote diagnosis, etc. Existing solutions are limited in detecting/preventing such attacks, and also impacting the performance on the original tasks. In this paper, we present AI-Guardian, a novel approach to defeating adversarial attacks that leverages intentionally embedded backdoors to fail the adversarial perturbations and maintain the performance of the original main task. We extensively evaluate AI-Guardian using five popular adversarial example generation approaches, and experimental results demonstrate its efficacy in defeating adversarial attacks. Specifically, AI-Guardian reduces the attack success rate from 97.3% to 3.2%, which outperforms the state-of-the-art works by 30.9%, with only a 0.9% decline on the clean data accuracy. Furthermore, AI-Guardian introduces only 0.36% overhead to the model prediction time, almost negligible in most cases.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Hong Zhu",
        "Shengzhi Zhang",
        "Kai Chen"
      ],
      "author_details": [
        {
          "name": "Hong Zhu",
          "h_index": 8,
          "citation_count": 371,
          "affiliations": []
        },
        {
          "name": "Shengzhi Zhang",
          "h_index": 10,
          "citation_count": 283,
          "affiliations": []
        },
        {
          "name": "Kai Chen",
          "h_index": 12,
          "citation_count": 378,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://openalex.org/W4384948583",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179473",
      "citation_count": 20,
      "influential_citation_count": 4,
      "reference_count": 99,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-defense",
        "adversarial-defense",
        "defensive-backdoor"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_e67f1280",
      "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks",
      "abstract": "Deep neural networks have achieved remarkable success on a variety of mission-critical tasks.However, recent studies show that deep neural networks are vulnerable to backdoor attacks, where the attacker releases backdoored models that behave normally on benign samples but misclassify any trigger-imposed samples to a target label.Unlike adversarial examples, backdoor attacks manipulate both the inputs and the model, perturbing samples with the trigger and injecting backdoors into the model.In this paper, we propose a novel attention-based evasive backdoor attack, dubbed ATTEQ-NN.Different from existing works that arbitrarily set the trigger mask, we carefully design an attentionbased trigger mask determination framework, which places the trigger at the crucial region with the most significant influence on the prediction results.To make the trigger-imposed samples appear more natural and imperceptible to human inspectors, we introduce a Quality-of-Experience (QoE) term into the loss function of trigger generation and carefully adjust the transparency of the trigger.During the process of iteratively optimizing the trigger generation and the backdoor injection components, we propose an alternating retraining strategy, which is shown to be effective in improving the clean data accuracy and evading some model-based defense approaches.We evaluate ATTEQ-NN with extensive experiments on VGG-Flower, CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets.The results show that ATTEQ-NN can increase the attack success rate by as much as 82% over baselines when the poison ratio is low while achieving a high QoE of the backdoored samples.We demonstrate that ATTEQ-NN reaches an attack success rate of more than 37.78% in the physical world under different lighting conditions and shooting angles.ATTEQ-NN preserves an attack success rate of more than 92.5% even if the original backdoored model is fine-tuned with clean data.It is shown that ATTEQ-NN is also effective in transfer learning scenarios.Our user studies show that the backdoored samples generated by ATTEQ-NN are indiscernible under visual inspections.ATTEQ-NN is shown to be evasive to state-of-the-art defense methods, including model pruning, NAD, STRIP, NC, and MNTD.We will open-source our codes upon publication.",
      "year": 2022,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Jianshuo Dong",
        "Qian Wang"
      ],
      "author_details": [
        {
          "name": "Xueluan Gong",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Yanjiao Chen",
          "h_index": 32,
          "citation_count": 3532,
          "affiliations": []
        },
        {
          "name": "Jianshuo Dong",
          "h_index": 4,
          "citation_count": 102,
          "affiliations": []
        },
        {
          "name": "Qian Wang",
          "h_index": 19,
          "citation_count": 1266,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://openalex.org/W4226550712",
      "pdf_url": "https://doi.org/10.14722/ndss.2022.24012",
      "doi": "https://doi.org/10.14722/ndss.2022.24012",
      "citation_count": 18,
      "influential_citation_count": 1,
      "reference_count": 83,
      "is_open_access": true,
      "tldr": "A novel attention-based evasive backdoor attack, dubbed A TTEQ -NN, which is shown to be evasive to state-of-the-art defense methods, including model pruning, NAD, STRIP, NC, and MNTD and effective in transfer learning scenarios.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "attention-based",
        "QoE-aware",
        "evasive"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2022.24012"
    },
    {
      "paper_id": "seed_cc1fa34c",
      "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
      "abstract": "Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging \"clean label\" attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Giorgio Severi",
        "J. Meyer",
        "Scott E. Coull",
        "Alina Oprea"
      ],
      "author_details": [
        {
          "name": "Giorgio Severi",
          "h_index": 8,
          "citation_count": 517,
          "affiliations": []
        },
        {
          "name": "J. Meyer",
          "h_index": 2,
          "citation_count": 192,
          "affiliations": []
        },
        {
          "name": "Scott E. Coull",
          "h_index": 23,
          "citation_count": 2884,
          "affiliations": []
        },
        {
          "name": "Alina Oprea",
          "h_index": 32,
          "citation_count": 9229,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://openalex.org/W3120073944",
      "pdf_url": "https://arxiv.org/pdf/2003.01031",
      "doi": "https://doi.org/10.48550/arxiv.2003.01031",
      "citation_count": 18,
      "influential_citation_count": 1,
      "reference_count": 53,
      "is_open_access": false,
      "publication_date": "2020-03-02",
      "tldr": "This work studies for the first time the susceptibility of ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging \"clean label\" attacks where attackers do not control the sample labeling process, and proposes the use of techniques from explainable machine learning to guide the selection of relevant features and their values to create a watermark in a model-agnostic fashion.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "malware-classifier",
        "XAI-guided",
        "clean-label"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_26ed0105",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "S. Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Ren Pang",
          "h_index": 11,
          "citation_count": 588,
          "affiliations": []
        },
        {
          "name": "Zhaohan Xi",
          "h_index": 9,
          "citation_count": 459,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Xiapu Luo",
          "h_index": 59,
          "citation_count": 13207,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W3206584998",
      "pdf_url": "https://arxiv.org/pdf/2110.06018",
      "doi": "https://doi.org/10.48550/arxiv.2110.06018",
      "citation_count": 16,
      "influential_citation_count": 1,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2021-10-12",
      "tldr": "Through an extensive empirical study of 10 popular NAS methods, it is shown that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "AutoML",
        "NAS",
        "security-risks"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2212.04687",
      "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
      "abstract": "In this paper, we present a simple yet surprisingly effective technique to induce \"selective amnesia\" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) within a few minutes (about 30 times faster than training a model from scratch using the MNIST dataset), with only a small amount of clean data (0.1% of training data for TrojAI models).",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Rui Zhu",
        "Di Tang",
        "Siyuan Tang",
        "Xiaofeng Wang",
        "Haixu Tang"
      ],
      "author_details": [
        {
          "name": "Rui Zhu",
          "h_index": 5,
          "citation_count": 115,
          "affiliations": []
        },
        {
          "name": "Di Tang",
          "h_index": 9,
          "citation_count": 615,
          "affiliations": []
        },
        {
          "name": "Siyuan Tang",
          "h_index": 10,
          "citation_count": 285,
          "affiliations": []
        },
        {
          "name": "Xiaofeng Wang",
          "h_index": 9,
          "citation_count": 242,
          "affiliations": []
        },
        {
          "name": "Haixu Tang",
          "h_index": 18,
          "citation_count": 1700,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://arxiv.org/abs/2212.04687",
      "citation_count": 15,
      "influential_citation_count": 3,
      "reference_count": 80,
      "is_open_access": true,
      "publication_date": "2022-12-09",
      "tldr": "This paper presents a simple yet surprisingly effective technique to induce \"selective amnesia\" on a backdoored model, called SEAM, which maximizes the CF on an unknown backdoor in the absence of triggered inputs, and preserves some feature extraction in the network to enable a fast revival of the primary task.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-removal",
        "catastrophic-forgetting",
        "SEAM"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2212.04687"
    },
    {
      "paper_id": "2308.13904",
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Chengkun Wei",
        "Wenlong Meng",
        "Zhikun Zhang",
        "M. Chen",
        "Ming-Hui Zhao",
        "Wenjing Fang",
        "Lei Wang",
        "Zihui Zhang",
        "Wenzhi Chen"
      ],
      "author_details": [
        {
          "name": "Chengkun Wei",
          "h_index": 9,
          "citation_count": 269,
          "affiliations": []
        },
        {
          "name": "Wenlong Meng",
          "h_index": 4,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "M. Chen",
          "h_index": 6,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Ming-Hui Zhao",
          "h_index": 32,
          "citation_count": 3712,
          "affiliations": []
        },
        {
          "name": "Wenjing Fang",
          "h_index": 10,
          "citation_count": 381,
          "affiliations": []
        },
        {
          "name": "Lei Wang",
          "h_index": 4,
          "citation_count": 52,
          "affiliations": []
        },
        {
          "name": "Zihui Zhang",
          "h_index": 6,
          "citation_count": 144,
          "affiliations": []
        },
        {
          "name": "Wenzhi Chen",
          "h_index": 8,
          "citation_count": 165,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://arxiv.org/abs/2308.13904",
      "citation_count": 14,
      "influential_citation_count": 1,
      "reference_count": 94,
      "is_open_access": true,
      "publication_date": "2023-08-26",
      "tldr": "LMSanitator is proposed, a novel approach for detecting and removing task-agnostic backdoors on Transformer models that aims to invert the predefined attack vectors of the pretrained models' output when the input is embedded with triggers, which achieves much better convergence performance and backdoor detection accuracy.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "nlp",
        "llm"
      ],
      "model_types": [
        "llm",
        "transformer"
      ],
      "tags": [
        "prompt-tuning",
        "backdoor-defense",
        "task-agnostic"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.23238"
    },
    {
      "paper_id": "seed_b13c9bc7",
      "title": "LoneNeuron: a Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks",
      "abstract": "The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100% attack success rate, while not affecting the main task performance. With LoneNeuron's unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape state-of-the-art Trojan detectors. LoneNeuron~is also the first effective backdoor attack against vision transformers (ViTs).",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zeyan Liu",
        "Fengjun Li",
        "Zhu Li",
        "Bo Luo"
      ],
      "author_details": [
        {
          "name": "Zeyan Liu",
          "h_index": 5,
          "citation_count": 109,
          "affiliations": []
        },
        {
          "name": "Fengjun Li",
          "h_index": 22,
          "citation_count": 2166,
          "affiliations": []
        },
        {
          "name": "Zhu Li",
          "h_index": 10,
          "citation_count": 462,
          "affiliations": []
        },
        {
          "name": "Bo Luo",
          "h_index": 6,
          "citation_count": 164,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://openalex.org/W4308338624",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560678",
      "doi": "https://doi.org/10.1145/3548606.3560678",
      "citation_count": 13,
      "influential_citation_count": 1,
      "reference_count": 118,
      "is_open_access": true,
      "publication_date": "2022-11-07",
      "tldr": "This work presents a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks and is also the first effective backdoor attack against vision transformers (ViTs).",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "feature-domain",
        "invisible-trigger",
        "polymorphic"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560678"
    },
    {
      "paper_id": "2402.01920",
      "title": "Preference Poisoning Attacks on Reward Model Learning",
      "abstract": "Learning reward models from pairwise comparisons is a fundamental component in a number of domains, including autonomous control, conversational agents, and recommendation systems, as part of a broad goal of aligning automated decisions with user preferences. These approaches entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability by considering an attacker who can flip a small subset of preference comparisons to either promote or demote a target outcome. We propose two classes of algorithmic approaches for these attacks: a gradient-based framework, and several variants of rank-by-distance methods. Next, we evaluate the efficacy of best attacks in both these classes in successfully achieving malicious goals on datasets from three domains: autonomous control, recommendation system, and textual prompt-response preference learning. We find that the best attacks are often highly successful, achieving in the most extreme case 100\\% success rate with only 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary significantly across domains. In addition, we observe that the simpler and more scalable rank-by-distance approaches are often competitive with, and on occasion significantly outperform, gradient-based methods. Finally, we show that state-of-the-art defenses against other classes of poisoning attacks exhibit limited efficacy in our setting.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Junlin Wu",
        "Jiong Wang",
        "Chaowei Xiao",
        "Chenguang Wang",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "author_details": [
        {
          "name": "Junlin Wu",
          "h_index": 7,
          "citation_count": 180,
          "affiliations": []
        },
        {
          "name": "Jiong Wang",
          "h_index": 16,
          "citation_count": 860,
          "affiliations": []
        },
        {
          "name": "Chaowei Xiao",
          "h_index": 15,
          "citation_count": 1294,
          "affiliations": []
        },
        {
          "name": "Chenguang Wang",
          "h_index": 1,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Ning Zhang",
          "h_index": 6,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "Yevgeniy Vorobeychik",
          "h_index": 43,
          "citation_count": 6610,
          "affiliations": []
        }
      ],
      "max_h_index": 43,
      "url": "https://arxiv.org/abs/2402.01920",
      "citation_count": 11,
      "influential_citation_count": 2,
      "reference_count": 213,
      "is_open_access": false,
      "publication_date": "2024-02-02",
      "tldr": "The nature and extent of this vulnerability is investigated by considering an attacker who can flip a small subset of preference comparisons to either promote or demote a target outcome, and two classes of algorithmic approaches are proposed: a gradient-based framework, and several variants of rank-by-distance methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "reward-model",
        "preference-poisoning",
        "RLHF"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_69e81ba3",
      "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
      "abstract": "While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model's performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "W. Zong",
        "Yang-Wai Chow",
        "Willy Susilo",
        "Kien Do",
        "S. Venkatesh"
      ],
      "author_details": [
        {
          "name": "W. Zong",
          "h_index": 8,
          "citation_count": 222,
          "affiliations": []
        },
        {
          "name": "Yang-Wai Chow",
          "h_index": 20,
          "citation_count": 1052,
          "affiliations": []
        },
        {
          "name": "Willy Susilo",
          "h_index": 4,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Kien Do",
          "h_index": 13,
          "citation_count": 628,
          "affiliations": []
        },
        {
          "name": "S. Venkatesh",
          "h_index": 17,
          "citation_count": 883,
          "affiliations": []
        }
      ],
      "max_h_index": 20,
      "url": "https://openalex.org/W4385080316",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179331",
      "citation_count": 11,
      "influential_citation_count": 2,
      "reference_count": 42,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [
        "cnn",
        "rnn"
      ],
      "tags": [
        "ASR-trojan",
        "speech-recognition"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_e8a2ea2f",
      "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts",
      "abstract": "Malicious or manipulated prompts are known to exploit text-to-image models to generate unsafe images. Existing studies, however, focus on the passive exploitation of such harmful capabilities. In this paper, we investigate the proactive generation of unsafe images from benign prompts (e.g., a photo of a cat) through maliciously modified text-to-image models. Our preliminary investigation demonstrates that poisoning attacks are a viable method to achieve this goal but uncovers significant side effects, where unintended spread to non-targeted prompts compromises attack stealthiness. Root cause analysis identifies conceptual similarity as an important contributing factor to these side effects. To address this, we propose a stealthy poisoning attack method that balances covertness and performance. Our findings highlight the potential risks of adopting text-to-image models in real-world scenarios, thereby calling for future research and safety measures in this space.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yixin Wu",
        "Ning Yu",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Yixin Wu",
          "h_index": 8,
          "citation_count": 337,
          "affiliations": []
        },
        {
          "name": "Ning Yu",
          "h_index": 3,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 16,
          "citation_count": 877,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 7,
          "citation_count": 304,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 12,
          "citation_count": 598,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://openalex.org/W4387964003",
      "pdf_url": "https://arxiv.org/pdf/2310.16613",
      "doi": "https://doi.org/10.48550/arxiv.2310.16613",
      "citation_count": 11,
      "influential_citation_count": 0,
      "reference_count": 72,
      "is_open_access": false,
      "publication_date": "2023-10-25",
      "tldr": "This paper investigates the proactive generation of unsafe images from benign prompts through maliciously modified text-to-image models through maliciously modified text-to-image models and proposes a stealthy poisoning attack method that balances covertness and performance.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "proactive-generation",
        "unsafe-images",
        "benign-prompts"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2306.11924",
      "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
      "abstract": "End-to-end encryption (E2EE) provides strong technical protections to individuals from interferences. Governments and law enforcement agencies around the world have however raised concerns that E2EE also allows illegal content to be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH) to detect known illegal content before it is shared, is seen as a promising solution to prevent the diffusion of illegal content while preserving encryption. While these proposals raise strong privacy concerns, proponents of the solutions have argued that the risk is limited as the technology has a limited scope: detecting known illegal content. In this paper, we show that modern perceptual hashing algorithms are actually fairly flexible pieces of technology and that this flexibility could be used by an adversary to add a secondary hidden feature to a client-side scanning system. More specifically, we show that an adversary providing the PH algorithm can ``hide\" a secondary purpose of face recognition of a target individual alongside its primary purpose of image copy detection. We first propose a procedure to train a dual-purpose deep perceptual hashing model by jointly optimizing for both the image copy detection and the targeted facial recognition task. Second, we extensively evaluate our dual-purpose model and show it to be able to reliably identify a target individual 67% of the time while not impacting its performance at detecting illegal content. We also show that our model is neither a general face detection nor a facial recognition model, allowing its secondary purpose to be hidden. Finally, we show that the secondary purpose can be enabled by adding a single illegal looking image to the database. Taken together, our results raise concerns that a deep perceptual hashing-based CSS system could turn billions of user devices into tools to locate targeted individuals.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Shubham Jain",
        "Ana-Maria Cre\u0163u",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "author_details": [
        {
          "name": "Shubham Jain",
          "h_index": 6,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Ana-Maria Cre\u0163u",
          "h_index": 8,
          "citation_count": 238,
          "affiliations": []
        },
        {
          "name": "Antoine Cully",
          "h_index": 28,
          "citation_count": 3699,
          "affiliations": []
        },
        {
          "name": "Yves-Alexandre de Montjoye",
          "h_index": 18,
          "citation_count": 1377,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://arxiv.org/abs/2306.11924",
      "citation_count": 10,
      "influential_citation_count": 2,
      "reference_count": 83,
      "is_open_access": true,
      "publication_date": "2023-05-01",
      "tldr": "It is shown that modern perceptual hashing algorithms are actually fairly flexible pieces of technology and that this flexibility could be used by an adversary to add a secondary hidden feature to a client-side scanning system, which could turn billions of user devices into tools to locate targeted individuals.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "perceptual-hashing",
        "dual-purpose",
        "facial-recognition"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2306.11924"
    },
    {
      "paper_id": "seed_1d4975b3",
      "title": "Double-Cross Attacks: Subverting Active Learning Systems",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jose Rodrigo Sanchez Vicarte",
        "Gang Wang",
        "Christopher W. Fletcher"
      ],
      "author_details": [
        {
          "name": "Jose Rodrigo Sanchez Vicarte",
          "h_index": 6,
          "citation_count": 328,
          "affiliations": []
        },
        {
          "name": "Gang Wang",
          "h_index": 2,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Christopher W. Fletcher",
          "h_index": 39,
          "citation_count": 5166,
          "affiliations": []
        }
      ],
      "max_h_index": 39,
      "url": "https://www.usenix.org/system/files/sec21-vicarte.pdf",
      "citation_count": 8,
      "influential_citation_count": 1,
      "reference_count": 60,
      "is_open_access": false,
      "tldr": "A novel attack called Double Cross is presented, which aims to manipulate data labeling and model training in active learning settings and develops a trigger generation method that simultaneously achieves these three goals.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "active-learning",
        "poisoning",
        "labeling-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_92c470d3",
      "title": "MagBackdoor: Beware of Your Loudspeaker as Backdoor of Magnetic Attack for Malicious Command Injection",
      "abstract": "Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named &#x201C;DirtyFlipping&#x201D;, which uses dirty label techniques, &#x2018;label-on-label&#x2018;, to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Orson Mengara"
      ],
      "author_details": [
        {
          "name": "Orson Mengara",
          "h_index": 4,
          "citation_count": 35,
          "affiliations": []
        }
      ],
      "max_h_index": 4,
      "url": "https://openalex.org/W4393285751",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10483076.pdf",
      "doi": "https://doi.org/10.1109/access.2024.3382839",
      "citation_count": 8,
      "influential_citation_count": 0,
      "reference_count": 74,
      "is_open_access": true,
      "publication_date": "2024-03-29",
      "tldr": "This study proposes a backdoor attack named \u201cDirtyFlipping\u201d, which uses dirty label techniques, \u2018label-on-label\u2018, to input triggers in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.",
      "fields_of_study": [
        "Computer Science",
        "Engineering"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [],
      "tags": [
        "magnetic-attack",
        "command-injection",
        "loudspeaker"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10483076.pdf"
    },
    {
      "paper_id": "seed_48a59125",
      "title": "Persistent Backdoor Attacks in Continual Learning",
      "abstract": "Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Zhen Guo",
        "Abhinav Kumar",
        "R. Tourani"
      ],
      "author_details": [
        {
          "name": "Zhen Guo",
          "h_index": 3,
          "citation_count": 22,
          "affiliations": []
        },
        {
          "name": "Abhinav Kumar",
          "h_index": 4,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "R. Tourani",
          "h_index": 15,
          "citation_count": 970,
          "affiliations": []
        }
      ],
      "max_h_index": 15,
      "url": "https://openalex.org/W4403752757",
      "pdf_url": "https://arxiv.org/pdf/2409.13864",
      "doi": "https://doi.org/10.48550/arxiv.2409.13864",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 59,
      "is_open_access": false,
      "publication_date": "2024-09-20",
      "tldr": "This work introduces two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor- each leveraging minimal adversarial influence, and shows that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "continual-learning",
        "persistent-backdoor"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2406.19466",
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "abstract": "Local differential privacy (LDP) provides a way for an untrusted data collector to aggregate users' data without violating their privacy. Various privacy-preserving data analysis tasks have been studied under the protection of LDP, such as frequency estimation, frequent itemset mining, and machine learning. Despite its privacy-preserving properties, recent research has demonstrated the vulnerability of certain LDP protocols to data poisoning attacks. However, existing data poisoning attacks are focused on basic statistics under LDP, such as frequency estimation and mean/variance estimation. As an important data analysis task, the security of LDP frequent itemset mining has yet to be thoroughly examined. In this paper, we aim to address this issue by presenting novel and practical data poisoning attacks against LDP frequent itemset mining protocols. By introducing a unified attack framework with composable attack operations, our data poisoning attack can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures. We conduct extensive experiments on three datasets to compare the proposed attack with four baseline attacks. The results demonstrate the severity of the threat and the effectiveness of the proposed attack.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Wei Tong",
        "Haoyu Chen",
        "Jiacheng Niu",
        "Sheng Zhong"
      ],
      "author_details": [
        {
          "name": "Wei Tong",
          "h_index": 8,
          "citation_count": 303,
          "affiliations": []
        },
        {
          "name": "Haoyu Chen",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Jiacheng Niu",
          "h_index": 2,
          "citation_count": 32,
          "affiliations": []
        },
        {
          "name": "Sheng Zhong",
          "h_index": 5,
          "citation_count": 376,
          "affiliations": []
        }
      ],
      "max_h_index": 8,
      "url": "https://arxiv.org/abs/2406.19466",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 45,
      "is_open_access": true,
      "publication_date": "2024-06-27",
      "tldr": "This paper presents novel and practical data poisoning attacks against LDP frequent itemset mining protocols, introducing a unified attack framework with composable attack operations that can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "LDP-poisoning",
        "frequent-itemset"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670298"
    },
    {
      "paper_id": "2409.12314",
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "abstract": "Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce \"model implosion,\" where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of \"supervised graph alignment\" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Wenxin Ding",
        "C. Y. Li",
        "Shawn Shan",
        "Ben Y. Zhao",
        "Haitao Zheng"
      ],
      "author_details": [
        {
          "name": "Wenxin Ding",
          "h_index": 4,
          "citation_count": 127,
          "affiliations": []
        },
        {
          "name": "C. Y. Li",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Shawn Shan",
          "h_index": 18,
          "citation_count": 2943,
          "affiliations": [
            "University of Chicago"
          ]
        },
        {
          "name": "Ben Y. Zhao",
          "h_index": 7,
          "citation_count": 623,
          "affiliations": []
        },
        {
          "name": "Haitao Zheng",
          "h_index": 59,
          "citation_count": 14282,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://arxiv.org/abs/2409.12314",
      "citation_count": 6,
      "influential_citation_count": 1,
      "reference_count": 52,
      "is_open_access": true,
      "publication_date": "2024-09-18",
      "tldr": "This work establishes the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models, and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "model-implosion",
        "text-to-image",
        "poisoning"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690205"
    },
    {
      "paper_id": "seed_8a38247d",
      "title": "Backdooring Bias (B^2) into Stable Diffusion Models",
      "abstract": "Recent advances in large text-conditional diffusion models have revolutionized image generation by enabling users to create realistic, high-quality images from textual prompts, significantly enhancing artistic creation and visual communication. However, these advancements also introduce an underexplored attack opportunity: the possibility of inducing biases by an adversary into the generated images for malicious intentions, e.g., to influence public opinion and spread propaganda. In this paper, we study an attack vector that allows an adversary to inject arbitrary bias into a target model. The attack leverages low-cost backdooring techniques using a targeted set of natural textual triggers embedded within a small number of malicious data samples produced with public generative models. An adversary could pick common sequences of words that can then be inadvertently activated by benign users during inference. We investigate the feasibility and challenges of such attacks, demonstrating how modern generative models have made this adversarial process both easier and more adaptable. On the other hand, we explore various aspects of the detectability of such attacks and demonstrate that the model's utility remains intact in the absence of the triggers. Our extensive experiments using over 200,000 generated images and against hundreds of fine-tuned models demonstrate the feasibility of the presented backdoor attack. We illustrate how these biases maintain strong text-image alignment, highlighting the challenges in detecting biased images without knowing that bias in advance. Our cost analysis confirms the low financial barrier (\\$10-\\$15) to executing such attacks, underscoring the need for robust defensive strategies against such vulnerabilities in diffusion models.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ali Naseh",
        "Jaechul Roh",
        "Eugene Bagdasarian",
        "Amir Houmansadr"
      ],
      "author_details": [
        {
          "name": "Ali Naseh",
          "h_index": 5,
          "citation_count": 108,
          "affiliations": []
        },
        {
          "name": "Jaechul Roh",
          "h_index": 5,
          "citation_count": 95,
          "affiliations": []
        },
        {
          "name": "Eugene Bagdasarian",
          "h_index": 14,
          "citation_count": 4087,
          "affiliations": [
            "UMass Amherst"
          ]
        },
        {
          "name": "Amir Houmansadr",
          "h_index": 7,
          "citation_count": 156,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://openalex.org/W4399991166",
      "pdf_url": "https://arxiv.org/pdf/2406.15213",
      "doi": "https://doi.org/10.48550/arxiv.2406.15213",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 40,
      "is_open_access": false,
      "publication_date": "2024-06-21",
      "tldr": "An attack vector that allows an adversary to inject arbitrary bias into a target model using a targeted set of natural textual triggers embedded within a small number of malicious data samples produced with public generative models is studied.",
      "fields_of_study": [
        "Computer Science"
      ],
      "paper_type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "bias-backdoor",
        "text-to-image"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "1911.01559",
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Ren Pang",
        "Hua Shen",
        "Xinyang Zhang",
        "S. Ji",
        "Yevgeniy Vorobeychik",
        "Xiaopu Luo",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Ren Pang",
          "h_index": 11,
          "citation_count": 588,
          "affiliations": []
        },
        {
          "name": "Hua Shen",
          "h_index": 12,
          "citation_count": 713,
          "affiliations": [
            "The PennState University"
          ]
        },
        {
          "name": "Xinyang Zhang",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Yevgeniy Vorobeychik",
          "h_index": 43,
          "citation_count": 6610,
          "affiliations": []
        },
        {
          "name": "Xiaopu Luo",
          "h_index": 2,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/1911.01559",
      "pdf_url": "https://arxiv.org/pdf/1911.01559",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 49,
      "is_open_access": false,
      "publication_date": "2019-11-05",
      "tldr": "This paper develops a new attack model that integrates both adversarial inputs and backdoored models, and reveals that there exists an intricate \"mutual reinforcement\" effect between the two attack vectors.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "adversarial-examples",
        "data-poisoning",
        "unified-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_1fdb3170",
      "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
      "abstract": "This proposal discusses the growing challenges in reverse engineering modern software binaries, particularly those compiled from newer system programming languages such as Rust, Go, and Mojo. Traditional reverse engineering techniques, developed with a focus on C and C++, fall short when applied to these newer languages due to their reliance on outdated heuristics and failure to fully utilize the rich semantic information embedded in binary programs. These challenges are exacerbated by the limitations of current data-driven methods, which are susceptible to generating inaccurate results, commonly referred to as hallucinations. To overcome these limitations, we propose a novel approach that integrates probabilistic binary analysis with fine-tuned large language models (LLMs). Our method systematically models the uncertainties inherent in reverse engineering, enabling more accurate reasoning about incomplete or ambiguous information. By incorporating LLMs, we extend the analysis beyond traditional heuristics, allowing for more creative and context-aware inferences, particularly for binaries from diverse programming languages. This hybrid approach not only enhances the robustness and accuracy of reverse engineering efforts but also offers a scalable solution adaptable to the rapidly evolving landscape of software development.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuo Zhuo",
        "Xiangyu Zhang"
      ],
      "author_details": [
        {
          "name": "Zhuo Zhuo",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Xiangyu Zhang",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        }
      ],
      "max_h_index": 1,
      "url": "https://openalex.org/W4416072512",
      "pdf_url": "https://arxiv.org/pdf/2506.03504",
      "doi": "https://doi.org/10.48550/arxiv.2506.03504",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 35,
      "is_open_access": false,
      "publication_date": "2025-06-04",
      "tldr": "A novel approach that integrates probabilistic binary analysis with fine-tuned large language models (LLMs) is proposed, allowing for more creative and context-aware inferences in reverse engineering, particularly for binaries from diverse programming languages.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "backdoor-exploitation",
        "binary-analysis",
        "natural-backdoors"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_abf32fe4",
      "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
      "abstract": "Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Dorde Popovic",
        "Amin Sadeghi",
        "Ting Yu",
        "Sanjay Chawla",
        "Issa M. Khalil"
      ],
      "author_details": [
        {
          "name": "Dorde Popovic",
          "h_index": 1,
          "citation_count": 51,
          "affiliations": []
        },
        {
          "name": "Amin Sadeghi",
          "h_index": 1,
          "citation_count": 50,
          "affiliations": []
        },
        {
          "name": "Ting Yu",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Sanjay Chawla",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Issa M. Khalil",
          "h_index": 3,
          "citation_count": 28,
          "affiliations": []
        }
      ],
      "max_h_index": 3,
      "url": "https://openalex.org/W4415062401",
      "pdf_url": "https://arxiv.org/pdf/2503.21305",
      "doi": "https://doi.org/10.48550/arxiv.2503.21305",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 82,
      "is_open_access": false,
      "publication_date": "2025-03-27",
      "tldr": "This paper considers a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system, and presents a novel framework for detecting backdoors under realistic restrictions.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "limited-data",
        "deductive"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_03288c70",
      "title": "Pretender: Universal Active Defense against Diffusion Finetuning Attacks",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Zekun Sun",
        "Zijian Liu",
        "Shouling Ji",
        "Chenhao Lin",
        "Na Ruan"
      ],
      "author_details": [
        {
          "name": "Zekun Sun",
          "h_index": 1,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "Zijian Liu",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Shouling Ji",
          "h_index": 1,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Chenhao Lin",
          "h_index": 1,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Na Ruan",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        }
      ],
      "max_h_index": 1,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-sun-zekun.pdf",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 54,
      "is_open_access": false,
      "tldr": "This work innovatively conceptualizes active defense as a bi-level optimization problem, focusing on attackers\u2019 common behaviors to enhance the generalization of defense, and develops a novel algorithm named Pretender, where a surrogate model is adversarially trained a surrogate model to facilitate the generation of more effective protective noise.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "finetuning-defense",
        "active-defense"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_efec62bb",
      "title": "EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Nan Yan",
        "Yuqing Li",
        "Xiong Wang",
        "Jing Chen",
        "Kun He",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Nan Yan",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Yuqing Li",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Xiong Wang",
          "h_index": 3,
          "citation_count": 32,
          "affiliations": []
        },
        {
          "name": "Jing Chen",
          "h_index": 7,
          "citation_count": 253,
          "affiliations": []
        },
        {
          "name": "Kun He",
          "h_index": 19,
          "citation_count": 1068,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-yan-nan.pdf",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "embedding-backdoor",
        "cross-trigger"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2506.24033",
      "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
      "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to provide plausible deniability of their data. However, this also makes LDP vulnerable to poisoning attacks. In this paper, we first introduce novel poisoning attacks for ranking estimation. These attacks are intricate, as fake attackers do not merely adjust the frequency of target items. Instead, they leverage a limited number of fake users to precisely modify frequencies, effectively altering item rankings to maximize gains. To tackle this challenge, we introduce the concepts of attack cost and optimal attack item (set), and propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we iteratively select optimal attack items and allocate suitable fake users. For OUE, we iteratively determine optimal attack item sets and consider the incremental changes in item frequencies across different sets. Regarding OLH, we develop a harmonic cost function based on the pre-image of a hash to select that supporting a larger number of effective attack items. Lastly, we present an attack strategy based on confidence levels to quantify the probability of a successful attack and the number of attack iterations more precisely. We demonstrate the effectiveness of our attacks through theoretical and empirical evidence, highlighting the necessity for defenses against these attacks. The source code and data have been made available at https://github.com/LDP-user/LDP-Ranking.git.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Pei Zhan",
        "Peng Tang",
        "Yangzhuo Li",
        "Puwen Wei",
        "Shanqing Guo"
      ],
      "author_details": [
        {
          "name": "Pei Zhan",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Peng Tang",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Yangzhuo Li",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Puwen Wei",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Shanqing Guo",
          "h_index": 2,
          "citation_count": 25,
          "affiliations": []
        }
      ],
      "max_h_index": 2,
      "url": "https://arxiv.org/abs/2506.24033",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 51,
      "is_open_access": false,
      "publication_date": "2025-06-30",
      "tldr": "Novel poisoning attacks for ranking estimation are introduced, and the concepts of attack cost and optimal attack item (set) are introduced, and corresponding strategies for kRR, OUE, and OLH protocols are proposed.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "LDP-poisoning",
        "ranking",
        "privacy-attack"
      ],
      "open_access_pdf": ""
    }
  ]
}