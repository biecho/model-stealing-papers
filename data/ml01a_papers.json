{
  "updated": "2026-01-06",
  "total": 77,
  "owasp_id": "ML01a",
  "owasp_name": "Prompt Injection",
  "description": "Malicious prompts to manipulate LLM behavior",
  "keywords": [
    "model stealing",
    "model extraction",
    "model theft",
    "steal model",
    "stealing model",
    "extract model",
    "model stealing attack",
    "model extraction attack",
    "neural network extraction attack",
    "knockoff nets",
    "knockoff net",
    "copycat CNN",
    "copycat model",
    "imitation attack",
    "clone model",
    "cloning attack",
    "stealing machine learning",
    "steal ML model",
    "steal ML models",
    "steal neural network",
    "DNN model stealing",
    "DNN extraction",
    "stealing deep learning",
    "LLM stealing",
    "LLM extraction",
    "stealing language model",
    "stealing functionality",
    "functionality stealing",
    "black-box model stealing",
    "blackbox model extraction",
    "model stealing defense",
    "model extraction defense",
    "prevent model stealing",
    "protect model extraction",
    "side-channel model extraction",
    "side-channel neural network",
    "timing attack neural network",
    "cache attack DNN",
    "power analysis neural network",
    "electromagnetic neural network",
    "DNN weights leakage",
    "neural network weight extraction",
    "reverse engineer neural network",
    "reverse engineering DNN",
    "cryptanalytic extraction neural",
    "API model extraction",
    "query-based model stealing",
    "prediction API stealing"
  ],
  "note": "Filtered for Prompt Injection",
  "papers": [
    {
      "paper_id": "eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Y. Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "title": "LoRATEE: A Secure and Efficient Inference Framework for Multi-Tenant LoRA LLMs Based on TEE",
      "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning approach that adaptes pre-trained Large Language Models (LLMs) to multi-tenant tasks by generating a variety of LoRA adapters. However, this approach faces significant security challenges and is particularly susceptible to malicious servers stealing model parameters and sensitive data. Existing research on addressing security risks in multi-tenant environments remains constrained and insufficient. This paper explores the security challenges and proposes the LoRATEE framework, which embeds LoRA adapters within a server-side Trusted Execution Environment (TEE) and employs a lightweight One-Time Pad (OTP) encryption mechanism to ensure secure data transmission. Additionally, we design a dynamic LoRA adapter prefetching mechanism to reduce I/O latency. Moreover, a LoRA adapter module equivalence-sharing strategy based on Parameter-Efficient Fine-Tuning (PEFT) and minimalist design principles was introduced to optimize adapters loading. Experimental results show that LoRATEE maintains inference efficiency while securing multi-tenant LoRA LLMs systems.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Zechao Lin",
        "Sisi Zhang",
        "Xingbin Wang",
        "Yulan Su",
        "Yan Wang",
        "Rui Hou",
        "Dan Meng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
      "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models. To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shiqian Zhao",
        "Chong Wang",
        "Yiming Li",
        "Yihao Huang",
        "Wenjie Qu",
        "Siew-Kei Lam",
        "Yi Xie",
        "Kangjie Chen",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "pdf_url": "",
      "publication_date": "2025-08-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c7de983d4de7ee6c8cce9ca2972876aeb0ee87b6",
      "title": "FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction",
      "abstract": "Extractive reading comprehension systems are designed to locate the correct answer to a question within a given text. However, a persistent challenge lies in ensuring these models maintain high accuracy in answering questions while reliably recognizing unanswerable queries. Despite significant advances in large language models (LLMs) for reading comprehension, this issue remains critical, particularly as the length of supported contexts continues to expand. To address this challenge, we propose an innovative data augmentation methodology grounded in a multi-agent collaborative framework. Unlike traditional methods, such as the costly human annotation process required for datasets like SQuAD 2.0, our method autonomously generates evidence-based question-answer pairs and systematically constructs unanswerable questions. Using this methodology, we developed the FactGuard-Bench dataset, which comprises 25,220 examples of both answerable and unanswerable question scenarios, with context lengths ranging from 8K to 128K. Experimental evaluations conducted on seven popular LLMs reveal that even the most advanced models achieve only 61.79% overall accuracy. Furthermore, we emphasize the importance of a model's ability to reason about unanswerable questions to avoid generating plausible but incorrect answers. By implementing efficient data selection and generation within the multi-agent collaborative framework, our method significantly reduces the traditionally high costs associated with manual annotation and provides valuable insights for the training and optimization of LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qianwen Zhang",
        "Fang Li",
        "Jie Wang",
        "Lingfeng Qiao",
        "Yifei Yu",
        "Di Yin",
        "Xing Sun"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c7de983d4de7ee6c8cce9ca2972876aeb0ee87b6",
      "pdf_url": "",
      "publication_date": "2025-04-08",
      "keywords_matched": [
        "LLM extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5bce864b579b376c028ec40a8fec0f999b005d0e",
      "title": "Attack and defense techniques in large language models: A survey and new perspectives",
      "abstract": "Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attacks, optimized attacks, model theft, as well as attacks on LLM applications, detailing their mechanisms and implications. Consequently, we analyze defense strategies, such as prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open issues, including the need for adaptive scalable defenses, adversarial attack detection, generalized defense mechanisms, and ethical and bias concerns. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",
      "year": 2025,
      "venue": "Neural Networks",
      "authors": [
        "Zhiyu Liao",
        "Kang Chen",
        "Y. Lin",
        "Kangkang Li",
        "Yunxuan Liu",
        "Hefeng Chen",
        "Xingwang Huang",
        "Yuanhui Yu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5bce864b579b376c028ec40a8fec0f999b005d0e",
      "pdf_url": "",
      "publication_date": "2025-05-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c544b7021fc25bcd828d1721da82d25e8a16b407",
      "title": "Information Extraction from Lumbar Spine MRI Radiology Reports Using GPT4: Accuracy and Benchmarking Against Research-Grade Comprehensive Scoring",
      "abstract": "Background/Objectives: This study aimed to create a pipeline for standardized data extraction from lumbar-spine MRI radiology reports using a large language model (LLM) and assess the agreement of the extracted data with research-grade semi-quantitative scoring. Methods: We included a subset of data from a multi-site NIH-funded cohort study of chronic low back pain (cLBP) participants. After initial prompt development, a secure application programming interface (API) deployment of OpenAIs GPT-4 was used to extract different classes of pathology from the clinical radiology report. Unsupervised UMAP and agglomerative clustering of the pathology terms\u2019 embeddings provided insight into model comprehension for optimized prompt design. Model extraction was benchmarked against human extraction (gold standard) with F1 scores and false-positive and false-negative rates (FPR/FNR). Then, an expert MSK radiologist provided comprehensive research-grade scores of the images, and agreement with report-extracted data was calculated using Cohen\u2019s kappa. Results: Data from 230 patients with cLBP were included (mean age 53.2 years, 54% women). The overall model performance for extracting data from clinical reports was excellent, with a mean F1 score of 0.96 across pathologies. The mean FPR was marginally higher than the FNR (5.1% vs. 3.0%). Agreement with comprehensive scoring was moderate (kappa 0.424), and the underreporting of lateral recess stenosis (FNR 63.6%) and overreporting of disc pathology (FPR 42.7%) were noted. Conclusions: LLMs can accurately extract highly detailed information on lumbar spine imaging pathologies from radiology reports. Moderate agreement between the LLM and comprehensive scores underscores the need for less subjective, machine-based data extraction from imaging.",
      "year": 2025,
      "venue": "Diagnostics",
      "authors": [
        "Katharina Ziegeler",
        "Virginie Kreutzinger",
        "Michelle W Tong",
        "C. T. Chin",
        "E. Bahroos",
        "Po-Hung Wu",
        "Noah B. Bonnheim",
        "Aaron J Fields",
        "Jeffrey C. Lotz",
        "Thomas M Link",
        "S. Majumdar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c544b7021fc25bcd828d1721da82d25e8a16b407",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
      "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \\texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \\texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lin Lu",
        "Zhigang Zuo",
        "Ziji Sheng",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah",
        "My T. Thai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "title": "Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel",
      "abstract": "Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-world physical devices. User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive. Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning. However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level. In this paper, we introduce a generic physical side-channel attack, ScaAR, that extracts user interactions with neural networks by leveraging electromagnetic (EM) emissions of physical devices. Our proposed attack is implementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA). Experimental results demonstrate that, through the EM side channel, ScaAR can effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuoran Liu",
        "Senna van Hoek",
        "P'eter Horv'ath",
        "Dirk Lauret",
        "Xiaoyun Xu",
        "L. Batina"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "LLM stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b066eaa019bf6e94540dc54e735defb19d910bf8",
      "title": "Stealix: Model Stealing via Prompt Evolution",
      "abstract": "Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model's data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Hui-Po Wang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b066eaa019bf6e94540dc54e735defb19d910bf8",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a56e4103911d723020bb99bf63f5bb452b236e9",
      "title": "PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization",
      "abstract": "Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner",
      "year": 2025,
      "venue": "",
      "authors": [
        "Mingzhe Li",
        "Renhao Zhang",
        "Zhiyang Wen",
        "Siqi Pan",
        "Bruno Castro da Silva",
        "Juan Zhai",
        "Shiqing Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a56e4103911d723020bb99bf63f5bb452b236e9",
      "pdf_url": "",
      "publication_date": "2025-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-02"
    },
    {
      "paper_id": "f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "title": "Middleware Architecture for the Management and Mitigation of OWASP ML05: Model Theft in IoT Machine Learning Networks",
      "abstract": "The increasing integration of machine learning (ML) models into Internet of Things (IoT) applications has led to notable advancements in automation and decision-making. However, these models are vulnerable to modern attack vectors recognized by the OWASP Top 10 for Large Language Model Applications, specifically ML05: Model Theft, where adversaries gain unauthorized access to model parameters and training data, compromising intellectual property and sensitive information. Such threats are particularly concerning in IoT environments due to their distributed nature and resource limitations. This paper proposes a middleware architecture for the management and mitigation of model theft risks by incorporating encryption, access control, obfuscation, watermarking, continuous monitoring, and service assurance programmability. By strengthening the security management framework of ML models deployed in IoT, the proposed architecture aims to protect against theft, ensure data confidentiality, and maintain network resilience. The approach includes detailed mathematical models and an evaluation of existing security measures, demonstrating the architecture's effectiveness in diverse IoT deployments, such as telemedicine and smart cities.",
      "year": 2025,
      "venue": "Global",
      "authors": [
        "Yair Enrique Rivera Julio",
        "\u00c1ngel Pinto",
        "Nelson A. P\u00e9rez-Garc\u00eda",
        "M\u00f3nica-Karel Huerta",
        "C\u00e9sar Viloria-N\u00fa\u00f1ez",
        "Marvin Luis P\u00e9rez Cabrera",
        "Frank Ibarra Hern\u00e1ndez",
        "Juan Manuel Torres Tovio",
        "Erwin J. Sacoto-Cabrera"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a744498909536ee4a0752f3fe2430c13268a2b82",
      "title": "Siamese: Stealing Fine-Tuned Visual Foundation Models via Diversified Prompting",
      "abstract": "Visual foundation models, characterized by their robust generalization and adaptability, serve as the basis for a wide array of downstream tasks. When fine-tuned for specific tasks, these models encapsulate confidential and valuable task-specific knowledge, making them prime targets for model stealing (MS) attacks. While recent efforts have exposed MS threats in practical scenarios such as data-free and hard-label contexts, these attacks predominantly target traditional victim models trained from scratch. Fine-tuned visual foundation models, pre-trained on vast and diverse datasets and then fine-tuned on downstream tasks, present significant challenges for traditional MS attacks to extract task-specific knowledge. In this paper, we introduce an innovative MS attack, named SIAMESE, to steal fine-tuned visual foundation models under black-box, data-free, and hard-label settings. The core approach of SIAMESE involves constructing a stolen model using a foundation model that is efficiently and concurrently fine-tuned with multiple diversified soft prompts. To integrate the knowledge derived from these prompts, we propose a novel and tractable loss function that analyzes the output distributions while enforcing orthogonality among the prompts to minimize interference. Additionally, a unique alignment module enhances SIAMESE by synchronizing interpretations between the victim and stolen models. Extensive experiments validate that SIAMESE outperforms state-of-the-art baseline attacks over 10% in accuracy, exposing the heightened vulnerability of fine-tuned visual foundation models to MS threats.",
      "year": 2025,
      "venue": "Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing",
      "authors": [
        "Madhureeta Das",
        "Gaurav Bagwe",
        "Miao Pan",
        "Kaichen Yang",
        "X. Yuan",
        "Lan Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a744498909536ee4a0752f3fe2430c13268a2b82",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
      "abstract": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuochen Yang",
        "Fok Kar Wai",
        "V. Thing"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d66b4910d4c16463e1859035cca94bbefbd76f92",
      "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
      "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhengchunmin Dai",
        "Jiaxiong Tang",
        "Peng Sun",
        "Honglong Chen",
        "Liantao Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d66b4910d4c16463e1859035cca94bbefbd76f92",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8cf823eb4c9309221953480a9b98c7700136c546",
      "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment",
      "abstract": "With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tushar Nayan",
        "Ziqi Zhang",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8cf823eb4c9309221953480a9b98c7700136c546",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "title": "BarkBeetle: Stealing Decision Tree Models with Fault Injection",
      "abstract": "Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qifan Wang",
        "Jonas Sander",
        "Minmin Jiang",
        "Thomas Eisenbarth",
        "David Oswald"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9d0f0fb82a339debb181382f725baa278bb202d7",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d0f0fb82a339debb181382f725baa278bb202d7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
      "abstract": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Sohely Jahan",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-12"
    },
    {
      "paper_id": "5e91b9b928b33ce05273507cf8dea1ade2450598",
      "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
      "abstract": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Xuebin Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5e91b9b928b33ce05273507cf8dea1ade2450598",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "abstract": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "year": 2025,
      "venue": "",
      "authors": [
        "A. Foundjem",
        "L. Tidjon",
        "L. D. Silva",
        "Foutse Khomh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-31"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b719f3b74f5025aeaa0ed3ed8707e8557bce6654",
      "title": "Performance of two large language models for data extraction in evidence synthesis",
      "abstract": "Accurate data extraction is a key component of evidence synthesis and critical to valid results. The advent of publicly available large language models (LLMs) has generated interest in these tools for evidence synthesis and created uncertainty about the choice of LLM. We compare the performance of two widely available LLMs (Claude 2 and GPT\u20104) for extracting pre\u2010specified data elements from 10 published articles included in a previously completed systematic review. We use prompts and full study PDFs to compare the outputs from the browser versions of Claude 2 and GPT\u20104. GPT\u20104 required use of a third\u2010party plugin to upload and parse PDFs. Accuracy was high for Claude 2 (96.3%). The accuracy of GPT\u20104 with the plug\u2010in was lower (68.8%); however, most of the errors were due to the plug\u2010in. Both LLMs correctly recognized when prespecified data elements were missing from the source PDF and generated correct information for data elements that were not reported explicitly in the articles. A secondary analysis demonstrated that, when provided selected text from the PDFs, Claude 2 and GPT\u20104 accurately extracted 98.7% and 100% of the data elements, respectively. Limitations include the narrow scope of the study PDFs used, that prompt development was completed using only Claude 2, and that we cannot guarantee the open\u2010source articles were not used to train the LLMs. This study highlights the potential for LLMs to revolutionize data extraction but underscores the importance of accurate PDF parsing. For now, it remains essential for a human investigator to validate LLM extractions.",
      "year": 2024,
      "venue": "Research Synthesis Methods",
      "authors": [
        "Amanda Konet",
        "Ian Thomas",
        "Gerald Gartlehner",
        "Leila C Kahwati",
        "Rainer Hilscher",
        "Shannon Kugley",
        "Karen Crotty",
        "Meera Viswanathan",
        "Robert Chew"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/b719f3b74f5025aeaa0ed3ed8707e8557bce6654",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "LLM extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "abstract": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "model stealing",
        "stealing language model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3434f582cc9c2818785e2920241d43d932625539",
      "title": "ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack",
      "abstract": "Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai",
        "Minghu Jiang",
        "Yongfeng Huang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3434f582cc9c2818785e2920241d43d932625539",
      "pdf_url": "",
      "publication_date": "2024-05-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks",
      "abstract": "Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Qiongkai Xu",
        "Jey Han Lau"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "pdf_url": "",
      "publication_date": "2024-08-29",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c9a4bce17712ecc82c7a9dcda85dfbaa1a7c2391",
      "title": "A Comparative Study of Large Language Models for Goal Model Extraction",
      "abstract": "User stories, expressed in snippets of natural language text, are commonly used to elicit stakeholder's needs in agile software development. Requirement engineers model user stories to interpret the relations among goals and requirements. Manual transformation of goal models has challenges such as, difficulty of converting lower-abstraction user stories into higher-level goals, and extraction of goals embedded in user stories depends on the skill of requirements engineers. In this paper we introduce a technique that leverages Large Language Models (LLMs) to automatically generate goal models from user stories. The approach uses Iterative Prompt Engineering that guides LLM to extract intentional elements and generate its XML-compatible representation in Goal-oriented Requirements Language (GRL). The generated models can be visualized using jUCMNav tool. We evaluated our approach using three LLMs: GPT-4, Llama and Cohere. Our qualitative evaluation indicates that GPT-4 or Llama can be used to assist requirements engineers in modeling as they can produce GRL goal models that are understandable. Additionally, these LLMs are capable of exposing soft goals that are not apparent to stakeholders who are new to the domain.",
      "year": 2024,
      "venue": "ACM/IEEE International Conference on Model Driven Engineering Languages and Systems",
      "authors": [
        "Vaishali Siddeshwar",
        "Sanaa A. Alwidian",
        "Masoud Makrehchi"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c9a4bce17712ecc82c7a9dcda85dfbaa1a7c2391",
      "pdf_url": "",
      "publication_date": "2024-09-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "title": "Protecting Confidential Virtual Machines from Hardware Performance Counter Side Channels",
      "abstract": "In modern cloud platforms, it is becoming more important to preserve the privacy of guest virtual machines (VMs) from the untrusted host. To this end, Secure Encrypted Virtualization (SEV) is developed as a hardware extension to protect VMs by encrypting their memory pages and register states. Unfortunately, such confidential VMs are still vulnerable to micro-architectural side channels, and Hardware Performance Counters (HPCs) are a prominent information leakage source. To make matters worse, currently there is no systematic defense against the HPC side channels. We introduce Aegis, a unified framework for demystifying the inherent relations between the instruction execution and HPC event statistics, and defending VMs against HPC side channels with provable privacy guarantee and minimal performance overhead. Aegis consists of three modules. Application Profiler profiles the application offline and adopts information theory to quantitatively estimate the vulnerability of HPC events. Event Fuzzer leverages the fuzzing technique to automatically generate interesting inputs, i.e., instruction sequences, that can effectively alter the HPC observations. Event Obfuscator injects noisy instructions into the protected VM based on the differential privacy mechanisms for high efficiency and privacy. We present three case studies to demonstrate that Aegis can defeat different types of HPC side-channel attacks (i.e., website fingerprinting, DNN model extraction, keystroke sniffing). Evaluations show that Aegis can effectively decrease the attack accuracy from 90% to 2%, with only 3% overhead on the application execution time and 7% overhead on the CPU usage.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Xiaoxuan Lou",
        "Kangjie Chen",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a6854f91dbad0b39a0289872ac838545a9c18225",
      "title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225",
      "pdf_url": "",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "title": "Secure AI Systems: Emerging Threats and Defense Mechanisms",
      "abstract": "The capability of artificial intelligence (AI), increasingly embedded in critical domains, faces a complex array of security threats. It has motivated researchers to explore the security vulnerability of AI solutions and propose effective countermeasures. This article offers a comprehensive exploration of diverse attacks on AI models, including backdoors (Trojans), adversarial, fault injection, data poisoning, model inversion, model extraction, membership inference attacks, etc. These security vulnerabilities are classified into two broad categories, namely, Supply Chain Attacks and Runtime Attacks. We highlight threat models, attack strategies, and defenses to secure AI systems against these attacks. The work also underscores the significance of developing secure and robust AI models and their implementation to safeguard sensitive data and embedded systems. We present some emerging research directions on secure AI systems.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "S. Bhunia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "befa5df856721b10c50d034594fbc7194d96e386",
      "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
      "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hanbo Huang",
        "Yihan Li",
        "Bowen Jiang",
        "Bowen Jiang",
        "Lin Liu",
        "Ruoyu Sun",
        "Zhuotao Liu",
        "Shiyu Liang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/befa5df856721b10c50d034594fbc7194d96e386",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation",
      "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "title": "Genetic Improvement for DNN Security",
      "abstract": "Genetic improvement (GI) in Deep Neural Networks (DNNs) has traditionally enhanced neural architecture and trained DNN parameters. Recently, GI has supported large language models by optimizing DNN operator scheduling on accelerator clusters. However, with the rise of adversarial AI, particularly model extraction attacks, there is an unexplored potential for GI in fortifying Machine Learning as a Service (MLaaS) models. We suggest a novel application of GI \u2014 not to improve model performance, but to diversify operator parallelism for the purpose of a moving target defense against model extraction attacks. We discuss an application of GI to create a DNN model defense strategy that uses probabilistic isolation, offering unique benefits not present in current DNN defense systems.",
      "year": 2024,
      "venue": "International Genetic Improvement Workshop",
      "authors": [
        "Hunter Baxter",
        "Yu Huang",
        "Kevin Leach"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "pdf_url": "",
      "publication_date": "2024-04-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21c62f2ef68323099480c80318e48baae8b9098f",
      "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
      "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Liaoyaqi Wang",
        "Minhao Cheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "title": "TrustZoneTunnel: A Cross-World Pattern History Table-Based Microarchitectural Side-Channel Attack",
      "abstract": "ARM's TrustZone is a hardware-based trusted execution environment (TEE), prevalent in mobile devices, IoT edge systems, and autonomous systems. Within TrustZone, security-sensitive applications reside in a hardware-isolated secure world, protected from the normal-world's applications, OS, debugger, peripherals, and memory. However, microarchitectural side-channel vulnerabilities have been discovered on shared on-chip resources, such as caches and branch prediction unit (BPU). In this paper, we propose TrustZoneTunnel, the first Pattern History Table (PHT)-based side-channel attack on TrustZone, which is able to reveal the complete control flow of a trusted application in the secure world. We reverse-engineer the PHT indexing for ARM processors and develop key primitives for cross-world attacks, including well-controlled world-switching, PHT collision construction between two worlds, and precise PHT state-setting and checking functions. Furthermore, we introduce a novel model extraction attack against TrustZone based deep neural network, which can recover model parameters using only the side-channel leakage of vital branch instructions, obviating the need for model output or logits while prior research work requires such knowledge for model extraction.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Tianhong Xu",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "title": "Stealing Brains: From English to Czech Language Model",
      "abstract": ": We present a simple approach for efficiently adapting pre-trained English language models to generate text in lower-resource language, specifically Czech. We propose a vocabulary swap method that leverages parallel corpora to map tokens between languages, allowing the model to retain much of its learned capabilities. Experiments conducted on a Czech translation of the TinyStories dataset demonstrate that our approach significantly outperforms baseline methods, especially when using small amounts of training data. With only 10% of the data, our method achieves a perplexity of 17.89, compared to 34.19 for the next best baseline. We aim to contribute to work in the field of cross-lingual transfer in natural language processing and we propose a simple to implement, computationally efficient method tested in a controlled environment.",
      "year": 2024,
      "venue": "International Joint Conference on Computational Intelligence",
      "authors": [
        "Petr Hyner",
        "Petr Marek",
        "D. Adamczyk",
        "Jan Hula",
        "Jan \u0160ediv\u00fd"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "title": "Late Breaking Results: Extracting QNNs from NISQ Computers via Ensemble Learning",
      "abstract": "The recent success of Quantum Neural Networks (QNNs) prompts model extraction attacks on cloud platforms, even under black-box constraints. These attacks repeatedly query the victim QNN with malicious inputs. However, existing extraction attacks tailored for classical models yield local substitute QNNs with limited performance due to NISQ computer noise. Drawing from bagging-based ensemble learning, which uses independent weak learners to learn from noisy data, we introduce a novel QNN extraction approach. Our experimental results show this quantum ensemble learning approach improves local QNN accuracy by up to 15.09% compared to previous techniques.",
      "year": 2024,
      "venue": "Design Automation Conference",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "48861a4166786313ff97a3c946c08574716aabce",
      "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
      "abstract": "With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs, employing low-frequency words as triggers and embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks, robustness against model extraction, pruning, fusion-based backdoor removal attacks, and high efficiency with 10-second embedding. Our code is available at https://github.com/edu-yinzhaoxia/Med-PLMW.",
      "year": 2024,
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "authors": [
        "Cong Kong",
        "Rui Xu",
        "Jiawei Chen",
        "Zhaoxia Yin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/48861a4166786313ff97a3c946c08574716aabce",
      "pdf_url": "",
      "publication_date": "2024-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
      "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \u2018METHOD\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.Our code is available at https://github.com/yjw1029/EmbMarker.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Zhu",
        "L. Lyu",
        "Binxing Jiao",
        "Tongye Xu",
        "Guangzhong Sun",
        "Xing Xie"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "pdf_url": "http://arxiv.org/pdf/2305.10036",
      "publication_date": "2023-05-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d032a269b465df9116f080ff9c56049bc581acb4",
      "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
      "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Shuai Wang",
        "Cuiyun Gao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d032a269b465df9116f080ff9c56049bc581acb4",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "68b560859078171978f2c040b1522f4e7668c38e",
      "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
      "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \u201ccode synthesis\u201d and \u201ccode translation:\u2019 We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Pingchuan Ma",
        "Chaowei Liu",
        "Shuai Wang",
        "Daoyuan Wu",
        "Cuiyun Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/68b560859078171978f2c040b1522f4e7668c38e",
      "pdf_url": "https://arxiv.org/pdf/2303.03012",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "title": "Stealing the Decoding Algorithms of Language Models",
      "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., 0.8, 1, 4, and 40 for the four versions of GPT-3.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "A. Naseh",
        "Kalpesh Krishna",
        "Mohit Iyyer",
        "Amir Houmansadr"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "pdf_url": "https://arxiv.org/pdf/2303.04729",
      "publication_date": "2023-03-08",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
      "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wen",
        "Tianhao Wang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fa9d49f32440aff7417ce46419d1073239b58b5b",
      "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
      "abstract": "Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.",
      "year": 2023,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Qing Huang",
        "Yanbang Sun",
        "Zhenchang Xing",
        "Mingming Yu",
        "Xiwei Xu",
        "Qinghua Lu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b",
      "pdf_url": "http://arxiv.org/pdf/2301.03987",
      "publication_date": "2023-01-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "title": "Hardware-Software Co-design for Side-Channel Protected Neural Network Inference",
      "abstract": "Physical side-channel attacks are a major threat to stealing confidential data from devices. There has been a recent surge in such attacks on edge machine learning (ML) hardware to extract the model parameters. Consequently, there has also been work, although limited, on building corresponding defenses against such attacks. Current solutions take either fully software-or fully hardware-centric approaches, which are limited in performance and flexibility, respectively. In this paper, we propose the first hardware-software co-design solution for building side-channel-protected ML hardware. Our solution targets edge devices and addresses both performance and flexibility needs. To that end, we develop a secure RISCV-based coprocessor design that can execute a neural network implemented in C/C++. Our coprocessor uses masking to execute various neural network operations like weighted summations, activation functions, and output layer computation in a sidechannel secure fashion. We extend the original RV32I instruction set with custom instructions to control the masking gadgets inside the secure coprocessor. We further use the custom instructions to implement easy-to-use APIs that are exposed to the end-user as a shared library. Finally, we demonstrate the empirical sidechannel security of the design up to 1M traces.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Avinash L. Varna",
        "Raghavan Kumar",
        "Aydin Aysu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
      "abstract": "Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanmin Tang",
        "Jing Yu",
        "Keke Gai",
        "Xiangyang Qu",
        "Yue Hu",
        "Gang Xiong",
        "Qi Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "pdf_url": "https://arxiv.org/pdf/2309.01786",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning (ML) paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the ML model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "pdf_url": "",
      "publication_date": "2022-01-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "pdf_url": "",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
      "abstract": "Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Cato Pauling",
        "Michael Gimson",
        "Muhammed Qaid",
        "Ahmad Kida",
        "Basel Halak"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "pdf_url": "",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
      "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals\u2019 sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users\u2019 privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
      "year": 2021,
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "authors": [
        "A. R. Javed",
        "S. Rehman",
        "M. Khan",
        "M. Alazab",
        "H. Khan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "pdf_url": "http://qspace.qu.edu.qa/bitstream/10576/37656/2/3460392.pdf",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "title": "Side-Channel based Disassembler for AVR Micro-Controllers using Convolutional Neural Networks",
      "abstract": "Reverse engineering using Side Channel Attacks (SCA) have been known as a serious menace against embedded devices. The attacker could employ side channel data to retrieve some sensitive information from the device, security analysis, existence of a library in the device or execution of a special stream of codes. Side channel data could be gathered from the power consumption or electromagnetic radiations by the device. In this paper, we propose a disassembler to extract the instructions of the device under attack. A deep convolutional neural network is employed to make templates of the target to use it for real-time scenarios. Short Time Fourier Transform (STFT), and Mel-Frequency Cepstrum Coefficients (MFCC) are utilized as feature extractors. The proposed method consists of two different parts: 1) Hierarchical scenario and 2) Sole model. Atmel 8-bit AVR micro-controller is employed as the target device under attack. Our results indicate that, even with an experimental and low cost setup a vast number of instructions are detectable. The proposed method reaches 98.21% accuracy on the real code, outperforms state-of-the-art methods on the proposed dataset.",
      "year": 2021,
      "venue": "ISC Conference on Information Security and Cryptology",
      "authors": [
        "Pouya Narimani",
        "M. Akhaee",
        "Seyedamin Habibi"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8c9027b106cee47d9012bff1a29e212207d4d1bc",
      "title": "Daps: A Dynamic Asynchronous Progress Stealing Model for MPI Communication",
      "abstract": "MPI provides nonblocking point-to-point and one-sided communication models to help applications achieve communication and computation overlap. These models provide the opportunity for MPI to offload data transfer to low level network hardware while the user process is computing. In practice, however, MPI implementations have to often handle complex data transfer in software due to limited capability of network hardware. Therefore, additional asynchronous progress is necessary to ensure prompt progress of these software-handled communication. Traditional mechanisms either spawn an additional background thread on each MPI process or launch a fixed number of helper processes on each node. Both mechanisms may degrade performance in user computation due to statically occupied CPU resources. The user has to fine-tune the progress resource deployment to gain overall performance. For complex multiphase applications, unfortunately, severe performance degradation may occur due to dynamically changing communication characteristics and thus changed progress requirement. This paper proposes a novel Dynamic Asynchronous Progress Stealing model, called Daps, to completely address the asynchronous progress complication. Daps is implemented inside the MPI runtime. It dynamically leverages idle MPI processes to steal communication progress tasks from other busy computing processes located on the same node. The basic concept of Daps is straightforward; however, various implementation challenges have to be resolved due to the unique requirements of interprocess data and code sharing. We present our design that ensures high performance while maintaining strict program correctness. We compare Daps with state-of-the-art asynchronous progress approaches by utilizing both microbenchmarks and HPC proxy applications.",
      "year": 2021,
      "venue": "IEEE International Conference on Cluster Computing",
      "authors": [
        "Kaiming Ouyang",
        "Min Si",
        "A. Hori",
        "Zizhong Chen",
        "P. Balaji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8c9027b106cee47d9012bff1a29e212207d4d1bc",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "title": "SNIFF: Reverse Engineering of Neural Networks With Fault Attacks",
      "abstract": "Neural networks have been shown to be vulnerable against fault injection attacks. These attacks change the physical behavior of the device during the computation, resulting in a change of value that is currently being computed. They can be realized by various techniques, ranging from clock/voltage glitching to application of lasers to rowhammer. Previous works have mostly explored fault attacks for output misclassification, thus affecting the reliability of neural networks. In this article, we investigate the possibility to reverse engineer neural networks with fault attacks. Sign bit flip fault attack enables the reverse engineering by changing the sign of intermediate values. We develop the first exact extraction method on deep-layer feature extractor networks that provably allows the recovery of proprietary model parameters. Our experiments with Keras library show that the precision error for the parameter recovery for the tested networks is less than $10^{-13}$ with the usage of 64-bit floats, which improves the current state of the art by six orders of magnitude.",
      "year": 2020,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin",
        "Yang Liu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/155678/2/_IEEE_TREL__Reverse_Engineering_of_Neural_Networks_with_Fault_Attacks.pdf",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a391a74b82dbb86b120c78976f21546c08d7116e",
      "title": "Analysis of Information Leakage from MCU using Neural Network",
      "abstract": "Electromagnetic leakage from an operating microcontroller unit (MCU) can be intercepted and analyzed to deduce the instructions being processed. This is helpful to understanding eavesdropping and to later protecting against it. In this work, harnessing a deep neural network, we analyze the massive electromagnetic (EM) leakage information to extract the instructions run in a microcontroller unit (MCU). An EM leakage acquisition environment is built, and the leakage signal of an MCU is collected. A multi-layer convolutional neural network is constructed for the side channel analysis and identification. The recognition accuracy is over 95% in the training phase, and more than 75% in the prediction phase. This experiment proves that the electromagnetic leakage emitted can be collected by appropriate methods and analyzed by deep learning technology. It can effectively deduce the instructions running in an MCU, which lays a foundation for the protection against eavesdropping.",
      "year": 2019,
      "venue": "International Workshop on Electromagnetic Compatibility of Integrated Circuits",
      "authors": [
        "Siping Gao",
        "Yingkai Guo",
        "Z. Aung",
        "Yongxin Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a391a74b82dbb86b120c78976f21546c08d7116e",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    }
  ]
}