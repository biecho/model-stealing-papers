{
  "metadata": {
    "classifier": "claude-opus-4-5",
    "created_at": "2025-01-28",
    "source": "Awesome-ML-SP-Papers (seed papers)",
    "owasp_version": "2023",
    "notes": "Verified against official OWASP ML Top 10 definitions from owasp.org"
  },
  "papers": [
    {
      "id": 1,
      "title": "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "black-box",
        "query-efficient",
        "transfer-attack",
        "adversarial-examples"
      ]
    },
    {
      "id": 3,
      "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "decision-based",
        "black-box",
        "query-efficient",
        "boundary-attack"
      ]
    },
    {
      "id": 6,
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "owasp": [
        "ML01",
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "adversarial-examples",
        "data-poisoning",
        "unified-attack"
      ]
    },
    {
      "id": 7,
      "title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "defense-bypass",
        "trapdoor-evasion",
        "adversarial-examples"
      ]
    },
    {
      "id": 12,
      "title": "AutoDA: Automated Decision-based Iterative Adversarial Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "decision-based",
        "automated",
        "program-synthesis"
      ]
    },
    {
      "id": 14,
      "title": "Physical Hijacking Attacks against Object Trackers",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "physical-attack",
        "object-tracking",
        "autonomous-systems"
      ]
    },
    {
      "id": 27,
      "title": "Self-interpreting Adversarial Images",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision",
        "llm"
      ],
      "models": [
        "transformer",
        "llm"
      ],
      "tags": [
        "VLM-attack",
        "cross-modal",
        "prompt-injection",
        "meta-instructions"
      ]
    },
    {
      "id": 29,
      "title": "Bad Characters: Imperceptible NLP Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "unicode-attack",
        "imperceptible",
        "text-adversarial"
      ]
    },
    {
      "id": 32,
      "title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "ASR-defense",
        "audio-adversarial",
        "detection"
      ]
    },
    {
      "id": 33,
      "title": "Dompteur: Taming Audio Adversarial Examples",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "ASR-defense",
        "psychoacoustic",
        "adversarial-mitigation"
      ]
    },
    {
      "id": 34,
      "title": "EarArray: Defending against DolphinAttack via Acoustic Attenuation",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "ultrasonic-defense",
        "voice-assistant",
        "DolphinAttack"
      ]
    },
    {
      "id": 41,
      "title": "SpecPatch: Human-in-the-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "spectrogram-attack",
        "human-in-loop",
        "ASR-attack",
        "physical"
      ]
    },
    {
      "id": 43,
      "title": "Understanding and Benchmarking the Commonality of Adversarial Examples",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "adversarial-commonality",
        "benchmark",
        "transferability"
      ]
    },
    {
      "id": 44,
      "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "black-box",
        "linguistic-features",
        "low-cost",
        "ASR-attack"
      ]
    },
    {
      "id": 45,
      "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "speaker-recognition",
        "black-box",
        "practical-attack"
      ]
    },
    {
      "id": 46,
      "title": "When Translators Refuse to Translate: A Novel Attack to Speech Translation Systems",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "audio",
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "speech-translation",
        "denial-of-service",
        "evasion"
      ]
    },
    {
      "id": 47,
      "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "video-attack",
        "3D-perturbations",
        "black-box",
        "universal"
      ]
    },
    {
      "id": 48,
      "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "video-attack",
        "style-transfer",
        "black-box",
        "unrestricted"
      ]
    },
    {
      "id": 49,
      "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "hard-label",
        "black-box",
        "graph-classification"
      ]
    },
    {
      "id": 50,
      "title": "Evading Classifiers by Morphing in the Dark",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "black-box",
        "minimal-knowledge",
        "morphing",
        "evasion"
      ]
    },
    {
      "id": 51,
      "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [],
      "tags": [
        "authorship-attribution",
        "code-transformation",
        "MCTS",
        "evasion"
      ]
    },
    {
      "id": 52,
      "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "problem-space",
        "formalization",
        "real-evasive-objects"
      ]
    },
    {
      "id": 53,
      "title": "Structural Attack against Graph Based Android Malware Detection",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "malware-detection",
        "FCG-attack",
        "Android",
        "problem-space"
      ]
    },
    {
      "id": 54,
      "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
      "owasp": [
        "ML01"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "evaluation-toolkit",
        "evasion",
        "robustness-testing"
      ]
    },
    {
      "id": 55,
      "title": "Adversarial Training for Raw-Binary Malware Classifiers",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "malware-detection",
        "adversarial-training",
        "raw-binary"
      ]
    },
    {
      "id": 56,
      "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "backdoor-exploitation",
        "binary-analysis",
        "natural-backdoors"
      ]
    },
    {
      "id": 57,
      "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [],
      "tags": [
        "malware-detection",
        "FCG",
        "black-box",
        "Android"
      ]
    },
    {
      "id": 58,
      "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "malware-detection",
        "query-based",
        "zero-knowledge",
        "Android"
      ]
    },
    {
      "id": 59,
      "title": "Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "code-auditing",
        "LLM-attack",
        "evasion"
      ]
    },
    {
      "id": 60,
      "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "hardware-trojan",
        "RL-attack",
        "evasion",
        "detection-bypass"
      ]
    },
    {
      "id": 61,
      "title": "DeepShuffle: A Lightweight Defense Framework against Adversarial Fault Injection Attacks on Deep Neural Networks in Multi-Tenant Cloud-FPGA",
      "owasp": [
        "ML10"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "fault-injection",
        "FPGA",
        "cloud-security",
        "weight-protection"
      ]
    },
    {
      "id": 62,
      "title": "Interpretable Deep Learning under Fire",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "XAI-attack",
        "interpretability",
        "explanation-manipulation"
      ]
    },
    {
      "id": 64,
      "title": "AIRS: Explanation for Deep Reinforcement Learning based Security Applications",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "XAI",
        "DRL-explanation",
        "security-applications"
      ]
    },
    {
      "id": 65,
      "title": "SoK: Explainable Machine Learning in Adversarial Environments",
      "owasp": [
        "ML01"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "XAI",
        "adversarial-environments",
        "systematization"
      ]
    },
    {
      "id": 66,
      "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "physical-attack",
        "projector",
        "short-lived",
        "real-world"
      ]
    },
    {
      "id": 67,
      "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "Android",
        "real-world-threats",
        "mobile-DL"
      ]
    },
    {
      "id": 68,
      "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "X-ray",
        "physical-attack",
        "prohibited-items",
        "security-screening"
      ]
    },
    {
      "id": 69,
      "title": "That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "autonomous-systems",
        "attack-detection",
        "spatiotemporal",
        "smart-city"
      ]
    },
    {
      "id": 70,
      "title": "You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "LiDAR",
        "autonomous-vehicles",
        "spoofing",
        "removal-attack"
      ]
    },
    {
      "id": 73,
      "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "traffic-sign",
        "infrared",
        "autonomous-vehicles",
        "physical"
      ]
    },
    {
      "id": 74,
      "title": "Avara: A Uniform Evaluation System for Perceptibility Analysis Against Adversarial Object Evasion Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "benchmark",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "evaluation-system",
        "perceptibility",
        "object-detection"
      ]
    },
    {
      "id": 75,
      "title": "VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "autonomous-vehicles",
        "visual-perception",
        "practical-defense"
      ]
    },
    {
      "id": 76,
      "title": "Invisible but Detected: Physical Adversarial Shadow Attack and Defense on LiDAR Object Detection",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "LiDAR",
        "shadow-attack",
        "autonomous-vehicles",
        "detection"
      ]
    },
    {
      "id": 77,
      "title": "From Threat to Trust: Exploiting Attention Mechanisms for Attacks and Defenses in Cooperative Perception",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "attention-mechanism",
        "cooperative-perception",
        "V2X"
      ]
    },
    {
      "id": 78,
      "title": "Adversarial Policy Training against Deep Reinforcement Learning",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "adversarial-policy",
        "DRL-attack",
        "multi-agent"
      ]
    },
    {
      "id": 79,
      "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "MARL",
        "adversarial-policy",
        "partial-observation"
      ]
    },
    {
      "id": 80,
      "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "certified-robustness",
        "DRL-defense",
        "provable"
      ]
    },
    {
      "id": 81,
      "title": "Cost-Aware Robust Tree Ensembles for Security Applications",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "tree",
        "ensemble"
      ],
      "tags": [
        "cost-aware",
        "domain-knowledge",
        "robustness"
      ]
    },
    {
      "id": 82,
      "title": "CADE: Detecting and Explaining Concept Drift Samples for Security Applications",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "detection",
        "explainability"
      ]
    },
    {
      "id": 83,
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "global-robustness",
        "certified",
        "security-classifiers"
      ]
    },
    {
      "id": 84,
      "title": "On the Robustness of Domain Constraints",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "domain-constraints",
        "realistic-attacks",
        "feature-constraints"
      ]
    },
    {
      "id": 85,
      "title": "Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "nlp"
      ],
      "models": [
        "rnn"
      ],
      "tags": [
        "certified-robustness",
        "RNN",
        "verification"
      ]
    },
    {
      "id": 86,
      "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "semantic-transformations",
        "smoothing"
      ]
    },
    {
      "id": 87,
      "title": "Transcend: Detecting Concept Drift in Malware Classification Models",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "malware-detection",
        "conformal-prediction"
      ]
    },
    {
      "id": 88,
      "title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "malware-detection",
        "retraining"
      ]
    },
    {
      "id": 89,
      "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "robustness-transfer",
        "representation-matching",
        "adversarial-training"
      ]
    },
    {
      "id": 90,
      "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "diffusion",
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "diffusion-purification",
        "smoothing"
      ]
    },
    {
      "id": 91,
      "title": "Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "anomaly-detection",
        "open-world"
      ]
    },
    {
      "id": 92,
      "title": "BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "traffic-analysis",
        "local-certification"
      ]
    },
    {
      "id": 93,
      "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "encoder-service",
        "cloud-ML",
        "downstream-robustness"
      ]
    },
    {
      "id": 94,
      "title": "Continuous Learning for Android Malware Detection",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "continuous-learning",
        "malware-detection",
        "Android"
      ]
    },
    {
      "id": 95,
      "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "object-detection",
        "patch-hiding"
      ]
    },
    {
      "id": 96,
      "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "realistic-attacks",
        "adversarial-hardening",
        "robustness-evaluation"
      ]
    },
    {
      "id": 97,
      "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "certified-robustness",
        "text-adversarial",
        "synonym-substitution"
      ]
    },
    {
      "id": 98,
      "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "evaluation-metrics",
        "disaggregation"
      ]
    },
    {
      "id": 99,
      "title": "SoK: Efficiency Robustness of Dynamic Deep Learning Systems",
      "owasp": [
        "ML01"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "efficiency-robustness",
        "dynamic-DL",
        "systematization"
      ]
    },
    {
      "id": 100,
      "title": "AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "certified-robustness",
        "GNN-defense",
        "deterministic"
      ]
    },
    {
      "id": 101,
      "title": "Robustifying ML-powered Network Classifiers with PANTS",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "network-classification",
        "robustness",
        "traffic-analysis"
      ]
    },
    {
      "id": 102,
      "title": "CertTA: Certified Robustness Made Practical for Learning-Based Traffic Analysis",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "certified-robustness",
        "traffic-analysis",
        "practical"
      ]
    },
    {
      "id": 103,
      "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "personalized-defense",
        "fine-tuning",
        "pretrained-models"
      ]
    },
    {
      "id": 104,
      "title": "Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "traffic-analysis",
        "real-time",
        "blind-perturbations"
      ]
    },
    {
      "id": 105,
      "title": "Pryde: A Modular Generalizable Workflow for Uncovering Evasion Attacks Against Stateful Firewall Deployments",
      "owasp": [
        "ML01"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "firewall-evasion",
        "workflow",
        "network-security"
      ]
    },
    {
      "id": 106,
      "title": "Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "malicious-domain",
        "multi-instance",
        "black-box"
      ]
    },
    {
      "id": 107,
      "title": "Swallow: A Transfer-Robust Website Fingerprinting Attack via Consistent Feature Learning",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "website-fingerprinting",
        "transfer-robust",
        "traffic-analysis"
      ]
    },
    {
      "id": 108,
      "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "wireless",
        "UAP",
        "robust-attack",
        "GAN"
      ]
    },
    {
      "id": 110,
      "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "model-poisoning",
        "Byzantine-robust",
        "FL-attack"
      ]
    },
    {
      "id": 111,
      "title": "Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "model-poisoning",
        "optimization",
        "FL-attack"
      ]
    },
    {
      "id": 112,
      "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "backdoor-defense",
        "model-inspection",
        "FL-defense"
      ]
    },
    {
      "id": 113,
      "title": "FLAME: Taming Backdoors in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "backdoor-defense",
        "clustering",
        "FL-defense"
      ]
    },
    {
      "id": 114,
      "title": "EIFFeL: Ensuring Integrity for Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "integrity",
        "secure-aggregation",
        "FL-defense"
      ]
    },
    {
      "id": 115,
      "title": "Eluding Secure Aggregation in Federated Learning via Model Inconsistency",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "secure-aggregation-bypass",
        "model-inconsistency",
        "FL-attack"
      ]
    },
    {
      "id": 116,
      "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "recovery",
        "historical-info",
        "FL-defense"
      ]
    },
    {
      "id": 117,
      "title": "Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "ranking-based",
        "poisoning-defense",
        "FL-defense"
      ]
    },
    {
      "id": 118,
      "title": "Securing Federated Sensitive Topic Classification against Poisoning Attacks",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning",
        "nlp"
      ],
      "models": [],
      "tags": [
        "sensitive-topic",
        "poisoning-defense",
        "FL-defense"
      ]
    },
    {
      "id": 119,
      "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "Bayesian",
        "backdoor-defense",
        "FL-defense"
      ]
    },
    {
      "id": 120,
      "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "vertical-FL",
        "dominating-inputs",
        "FL-attack"
      ]
    },
    {
      "id": 121,
      "title": "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "backdoor",
        "covert-attack",
        "adaptive"
      ]
    },
    {
      "id": 122,
      "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "validation-based",
        "poisoning-defense",
        "FL-defense"
      ]
    },
    {
      "id": 123,
      "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "vertical-FL",
        "backdoor",
        "FL-attack"
      ]
    },
    {
      "id": 124,
      "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "backdoor-detection",
        "crowdsourced",
        "FL-defense"
      ]
    },
    {
      "id": 125,
      "title": "Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "stealthy-poisoning",
        "adaptive",
        "FL-attack"
      ]
    },
    {
      "id": 126,
      "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "frequency-analysis",
        "poisoning-defense",
        "FL-defense"
      ]
    },
    {
      "id": 127,
      "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning \u2013 A Survey and Taxonomy",
      "owasp": [
        "ML03"
      ],
      "type": "survey",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "gradient-inversion",
        "taxonomy",
        "privacy-attack"
      ]
    },
    {
      "id": 128,
      "title": "Byzantine-Robust Decentralized Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "Byzantine-robust",
        "decentralized",
        "FL-defense"
      ]
    },
    {
      "id": 129,
      "title": "PoiSAFL: Scalable Poisoning Attack Framework to Byzantine-resilient Semi-asynchronous Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "scalable-poisoning",
        "semi-async",
        "FL-attack"
      ]
    },
    {
      "id": 130,
      "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "limited-data",
        "deductive"
      ]
    },
    {
      "id": 131,
      "title": "Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation Agent",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "gradient-aggregation",
        "Byzantine-robust",
        "FL-defense"
      ]
    },
    {
      "id": 132,
      "title": "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [],
      "tags": [
        "word-embeddings",
        "corpus-poisoning",
        "semantic-manipulation"
      ]
    },
    {
      "id": 133,
      "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "code-completion",
        "poisoning",
        "IDE-attack"
      ]
    },
    {
      "id": 134,
      "title": "TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "code-suggestion",
        "covert-poisoning",
        "Copilot"
      ]
    },
    {
      "id": 135,
      "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "semi-supervised",
        "unlabeled-poisoning"
      ]
    },
    {
      "id": 136,
      "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "recommender-systems",
        "data-poisoning"
      ]
    },
    {
      "id": 137,
      "title": "Reverse Attack: Black-box Attacks on Collaborative Recommendation",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "recommender-systems",
        "black-box",
        "collaborative-filtering"
      ]
    },
    {
      "id": 138,
      "title": "Subpopulation Data Poisoning Attacks",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "subpopulation",
        "targeted-poisoning"
      ]
    },
    {
      "id": 139,
      "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "model-hijacking",
        "training-time-attack"
      ]
    },
    {
      "id": 140,
      "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "contrastive-learning",
        "encoder-poisoning"
      ]
    },
    {
      "id": 141,
      "title": "Preference Poisoning Attacks on Reward Model Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "reward-model",
        "preference-poisoning",
        "RLHF"
      ]
    },
    {
      "id": 142,
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "owasp": [
        "ML02",
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "privacy-poisoning",
        "membership-inference",
        "active-attack"
      ]
    },
    {
      "id": 143,
      "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "test-time-adaptation",
        "inference-attack",
        "distribution-shift"
      ]
    },
    {
      "id": 144,
      "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "forensics",
        "traceback",
        "attribution"
      ]
    },
    {
      "id": 145,
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "owasp": [
        "ML02"
      ],
      "type": "empirical",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "model-implosion",
        "text-to-image",
        "poisoning"
      ]
    },
    {
      "id": 146,
      "title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "statistical-analysis"
      ]
    },
    {
      "id": 147,
      "title": "Double-Cross Attacks: Subverting Active Learning Systems",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "active-learning",
        "poisoning",
        "labeling-attack"
      ]
    },
    {
      "id": 148,
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "trojan-detection",
        "meta-learning"
      ]
    },
    {
      "id": 149,
      "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
      "owasp": [
        "ML02",
        "ML07"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "self-supervised",
        "encoder-backdoor",
        "transfer-learning"
      ]
    },
    {
      "id": 150,
      "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "composite-trigger",
        "benign-features",
        "stealthy"
      ]
    },
    {
      "id": 151,
      "title": "AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "debugging",
        "error-localization",
        "neuron-analysis"
      ]
    },
    {
      "id": 152,
      "title": "LoneNeuron: a Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "feature-domain",
        "invisible-trigger",
        "polymorphic"
      ]
    },
    {
      "id": 153,
      "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "attention-based",
        "QoE-aware",
        "evasive"
      ]
    },
    {
      "id": 154,
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "backdoor-defense",
        "provable"
      ]
    },
    {
      "id": 155,
      "title": "A Data-free Backdoor Injection Approach in Neural Networks",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "data-free",
        "backdoor-injection"
      ]
    },
    {
      "id": 156,
      "title": "Backdoor Attacks Against Dataset Distillation",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "dataset-distillation",
        "backdoor"
      ]
    },
    {
      "id": 157,
      "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-forensics",
        "reverse-engineering"
      ]
    },
    {
      "id": 158,
      "title": "Disguising Attacks with Explanation-Aware Backdoors",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "XAI-aware",
        "disguised-backdoor",
        "explanation-manipulation"
      ]
    },
    {
      "id": 159,
      "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-removal",
        "catastrophic-forgetting",
        "SEAM"
      ]
    },
    {
      "id": 160,
      "title": "AI-Guardian: Defeating Adversarial Attacks using Backdoors",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-defense",
        "adversarial-defense",
        "defensive-backdoor"
      ]
    },
    {
      "id": 161,
      "title": "REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "backdoor-purification",
        "attention-distillation"
      ]
    },
    {
      "id": 162,
      "title": "NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "clean-label",
        "practical",
        "limited-information"
      ]
    },
    {
      "id": 163,
      "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "backdoor-detection",
        "SSL",
        "TL",
        "cross-paradigm"
      ]
    },
    {
      "id": 164,
      "title": "ODSCAN: Backdoor Scanning for Object Detection Models",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "object-detection",
        "backdoor-scanning"
      ]
    },
    {
      "id": 165,
      "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "post-training-detection",
        "maximum-margin"
      ]
    },
    {
      "id": 166,
      "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "self-supervised",
        "distribution-preserving"
      ]
    },
    {
      "id": 167,
      "title": "Backdooring Bias (B^2) into Stable Diffusion Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "bias-backdoor",
        "text-to-image"
      ]
    },
    {
      "id": 168,
      "title": "Watch the Watchers! On the Security Risks of Robustness-Enhancing Diffusion Models",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "diffusion-purification",
        "security-risks"
      ]
    },
    {
      "id": 169,
      "title": "Pretender: Universal Active Defense against Diffusion Finetuning Attacks",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "finetuning-defense",
        "active-defense"
      ]
    },
    {
      "id": 170,
      "title": "Rowhammer-Based Trojan Injection: One Bit Flip Is Sufficient for Backdooring DNNs",
      "owasp": [
        "ML10"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "rowhammer",
        "bit-flip",
        "hardware-attack"
      ]
    },
    {
      "id": 172,
      "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
      "owasp": [
        "ML02"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "trigger-intensity",
        "training-inference-gap"
      ]
    },
    {
      "id": 173,
      "title": "Persistent Backdoor Attacks in Continual Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "continual-learning",
        "persistent-backdoor"
      ]
    },
    {
      "id": 174,
      "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "trojan-defense",
        "text-classification",
        "generative"
      ]
    },
    {
      "id": 175,
      "title": "Hidden Backdoors in Human-Centric Language Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "natural-trigger",
        "human-centric",
        "covert"
      ]
    },
    {
      "id": 176,
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "owasp": [
        "ML02",
        "ML07"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "pretrained-backdoor",
        "transferable",
        "NLP"
      ]
    },
    {
      "id": 177,
      "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "linguistic-style",
        "hidden-trigger"
      ]
    },
    {
      "id": 178,
      "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "provable-defense",
        "text-classification"
      ]
    },
    {
      "id": 179,
      "title": "Graph Backdoor",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "GNN-backdoor",
        "graph-classification"
      ]
    },
    {
      "id": 180,
      "title": "Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses",
      "owasp": [
        "ML08"
      ],
      "type": "attack",
      "domains": [
        "federated-learning",
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "federated-graph",
        "distributed-backdoor"
      ]
    },
    {
      "id": 181,
      "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "malware-classifier",
        "XAI-guided",
        "clean-label"
      ]
    },
    {
      "id": 182,
      "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [
        "cnn",
        "rnn"
      ],
      "tags": [
        "ASR-trojan",
        "speech-recognition"
      ]
    },
    {
      "id": 183,
      "title": "MagBackdoor: Beware of Your Loudspeaker as Backdoor of Magnetic Attack for Malicious Command Injection",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "magnetic-attack",
        "command-injection",
        "loudspeaker"
      ]
    },
    {
      "id": 184,
      "title": "Backdooring Multimodal Learning",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "multimodal-backdoor",
        "cross-modal"
      ]
    },
    {
      "id": 185,
      "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [],
      "tags": [
        "SNN",
        "neuromorphic",
        "stealthy-backdoor"
      ]
    },
    {
      "id": 186,
      "title": "Blind Backdoors in Deep Learning Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "code-level",
        "loss-manipulation",
        "single-pixel"
      ]
    },
    {
      "id": 187,
      "title": "IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "fuzzing",
        "DL-framework",
        "vulnerability-discovery"
      ]
    },
    {
      "id": 188,
      "title": "Towards Understanding and Detecting Cyberbullying in Real-world Images",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "cyberbullying",
        "content-moderation",
        "AI-for-security"
      ]
    },
    {
      "id": 189,
      "title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "toxic-content",
        "prompt-learning",
        "AI-for-security"
      ]
    },
    {
      "id": 190,
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "owasp": [
        "NONE"
      ],
      "type": "benchmark",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "hate-speech",
        "benchmark",
        "AI-for-security"
      ]
    },
    {
      "id": 191,
      "title": "FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "attack-categorization",
        "low-quality-labels",
        "AI-for-security"
      ]
    },
    {
      "id": 192,
      "title": "From Grim Reality to Practical Solution: Malware Classification in Real-World Noise",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "malware-classification",
        "noisy-labels",
        "AI-for-security"
      ]
    },
    {
      "id": 193,
      "title": "Decoding the Secrets of Machine Learning in Windows Malware Classification: A Deep Dive into Datasets, Features, and Model Performance",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "malware-classification",
        "datasets",
        "AI-for-security"
      ]
    },
    {
      "id": 194,
      "title": "KAIROS: Practical Intrusion Detection and Investigation using Whole-system Provenance",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "gnn"
      ],
      "tags": [
        "intrusion-detection",
        "provenance-graph",
        "AI-for-security"
      ]
    },
    {
      "id": 195,
      "title": "FLASH: A Comprehensive Approach to Intrusion Detection via Provenance Graph Representation Learning",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "gnn"
      ],
      "tags": [
        "intrusion-detection",
        "provenance-graph",
        "AI-for-security"
      ]
    },
    {
      "id": 196,
      "title": "Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "network-representation",
        "security-analytics",
        "AI-for-security"
      ]
    },
    {
      "id": 197,
      "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "browser-fingerprinting",
        "federated-detection",
        "AI-for-security"
      ]
    },
    {
      "id": 198,
      "title": "GNNIC: Finding Long-Lost Sibling Functions with Abstract Similarity",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "gnn"
      ],
      "tags": [
        "function-similarity",
        "call-graph",
        "binary-analysis"
      ]
    },
    {
      "id": 199,
      "title": "Experimental Analyses of the Physical Surveillance Risks in Client-Side Content Scanning",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [],
      "tags": [
        "perceptual-hashing",
        "content-scanning",
        "surveillance-risk"
      ]
    },
    {
      "id": 200,
      "title": "Attributions for ML-based ICS Anomaly Detection: From Theory to Practice",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "XAI",
        "ICS",
        "anomaly-detection",
        "attributions"
      ]
    },
    {
      "id": 201,
      "title": "DRAINCLoG: Detecting Rogue Accounts with Illegally-obtained NFTs using Classifiers Learned on Graphs",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "NFT-fraud",
        "phishing-detection",
        "AI-for-security"
      ]
    },
    {
      "id": 202,
      "title": "Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "encrypted-traffic",
        "malware-detection",
        "AI-for-security"
      ]
    },
    {
      "id": 203,
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "deepfake-detection",
        "privacy-preserving",
        "AI-for-security"
      ]
    },
    {
      "id": 204,
      "title": "USD: NSFW Content Detection for Text-to-Image Models via Scene Graph",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "NSFW-detection",
        "content-moderation",
        "AI-for-security"
      ]
    },
    {
      "id": 205,
      "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "proactive-generation",
        "unsafe-images",
        "benign-prompts"
      ]
    },
    {
      "id": 206,
      "title": "VoiceWukong: Benchmarking Deepfake Voice Detection",
      "owasp": [
        "NONE"
      ],
      "type": "benchmark",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "deepfake-detection",
        "benchmark",
        "AI-for-security"
      ]
    },
    {
      "id": 207,
      "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "voice-protection",
        "anti-cloning",
        "adversarial-perturbation"
      ]
    },
    {
      "id": 208,
      "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "graph",
        "reinforcement-learning"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "APT-detection",
        "provenance-graph",
        "AI-for-security"
      ]
    },
    {
      "id": 209,
      "title": "Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "concept-drift",
        "malware-classification",
        "AI-for-security"
      ]
    },
    {
      "id": 210,
      "title": "MM4flow: A Pre-trained Multi-modal Model for Versatile Network Traffic Analysis",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "transformer"
      ],
      "tags": [
        "traffic-analysis",
        "multimodal",
        "AI-for-security"
      ]
    },
    {
      "id": 211,
      "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [],
      "models": [
        "transformer"
      ],
      "tags": [
        "PDF-malware",
        "adversarial-robustness",
        "IR-analysis"
      ]
    },
    {
      "id": 215,
      "title": "A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",
      "owasp": [
        "NONE"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "captcha-breaking",
        "self-supervised",
        "AI-for-security"
      ]
    },
    {
      "id": 216,
      "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "binary-analysis",
        "instruction-embedding"
      ]
    },
    {
      "id": 217,
      "title": "CALLEE: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "call-graph",
        "binary-analysis",
        "indirect-calls"
      ]
    },
    {
      "id": 218,
      "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "vulnerability-repair",
        "zero-shot",
        "AI-for-security"
      ]
    },
    {
      "id": 219,
      "title": "Raconteur: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "shell-command",
        "explanation",
        "AI-for-security"
      ]
    },
    {
      "id": 220,
      "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "chatbot-attack",
        "toxic-behavior",
        "trigger"
      ]
    },
    {
      "id": 221,
      "title": "Towards a General Video-based Keystroke Inference Attack",
      "owasp": [
        "NONE"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "keystroke-inference",
        "side-channel",
        "video-analysis"
      ]
    },
    {
      "id": 222,
      "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "perceptual-hashing",
        "dual-purpose",
        "facial-recognition"
      ]
    },
    {
      "id": 226,
      "title": "CERBERUS: Exploring Federated Prediction of Security Events",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "security-prediction",
        "federated",
        "AI-for-security"
      ]
    },
    {
      "id": 227,
      "title": "VulChecker: Graph-based Vulnerability Localization in Source Code",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "vulnerability-detection",
        "source-code",
        "AI-for-security"
      ]
    },
    {
      "id": 228,
      "title": "On the Security Risks of AutoML",
      "owasp": [
        "ML01",
        "ML02"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "AutoML",
        "NAS",
        "security-risks"
      ]
    },
    {
      "id": 229,
      "title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "runtime-verification",
        "adversarial-defense",
        "safety"
      ]
    },
    {
      "id": 230,
      "title": "NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks",
      "owasp": [
        "ML10"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "bit-flip-defense",
        "proactive",
        "honeypot"
      ]
    },
    {
      "id": 231,
      "title": "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks",
      "owasp": [
        "ML10"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "bit-flip-defense",
        "targeted-attack"
      ]
    },
    {
      "id": 232,
      "title": "DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications",
      "owasp": [
        "ML01"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "XAI",
        "anomaly-detection",
        "interpretability"
      ]
    },
    {
      "id": 233,
      "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "XAI-evaluation",
        "faithfulness",
        "explanation-testing"
      ]
    },
    {
      "id": 234,
      "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
      "owasp": [
        "ML01"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "cnn"
      ],
      "tags": [
        "feature-attribution",
        "security-analysis",
        "XAI"
      ]
    },
    {
      "id": 235,
      "title": "Who Are You (I Really Wanna Know)? Detecting Audio DeepFakes Through Vocal Tract Reconstruction",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "deepfake-detection",
        "vocal-tract",
        "AI-for-security"
      ]
    },
    {
      "id": 236,
      "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "face-recognition",
        "impersonation",
        "physical-attack"
      ]
    },
    {
      "id": 237,
      "title": "DepthFake: Spoofing 3D Face Authentication with a 2D Photo",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "face-authentication",
        "3D-spoofing",
        "liveness-bypass"
      ]
    },
    {
      "id": 238,
      "title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "face-verification",
        "mobile-security",
        "system-analysis"
      ]
    },
    {
      "id": 239,
      "title": "Deepfake Text Detection: Limitations and Opportunities",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "deepfake-text",
        "detection",
        "AI-for-security"
      ]
    },
    {
      "id": 240,
      "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
      "owasp": [
        "NONE"
      ],
      "type": "benchmark",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "MGT-detection",
        "benchmark",
        "AI-for-security"
      ]
    },
    {
      "id": 241,
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "adversarial-prompt-defense",
        "content-safety",
        "T2I"
      ]
    },
    {
      "id": 242,
      "title": "SoK: The Good, The Bad, and The Unbalanced: Measuring Structural Limitations of Deepfake Media Datasets",
      "owasp": [
        "NONE"
      ],
      "type": "survey",
      "domains": [
        "vision",
        "audio"
      ],
      "models": [],
      "tags": [
        "SoK",
        "deepfake-datasets",
        "AI-for-security"
      ]
    },
    {
      "id": 244,
      "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "code-generation",
        "security-hardening",
        "adversarial-testing"
      ]
    },
    {
      "id": 245,
      "title": "DeGPT: Optimizing Decompiler Output with LLM",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "decompiler",
        "reverse-engineering",
        "LLM-tool"
      ]
    },
    {
      "id": 246,
      "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "secure-code-generation",
        "prompt-optimization"
      ]
    },
    {
      "id": 247,
      "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs",
      "owasp": [
        "ML06"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "package-hallucination",
        "supply-chain",
        "code-generation"
      ]
    },
    {
      "id": 248,
      "title": "Transferable Multimodal Attack on Vision-Language Pre-training Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "VLP-attack",
        "transferable",
        "multimodal"
      ]
    },
    {
      "id": 249,
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "jailbreak",
        "T2I",
        "safety-bypass"
      ]
    },
    {
      "id": 250,
      "title": "SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "NSFW-mitigation",
        "T2I-defense"
      ]
    },
    {
      "id": 251,
      "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "safety-bypass",
        "substitution",
        "T2I-attack"
      ]
    },
    {
      "id": 252,
      "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "policy-based",
        "content-moderation",
        "T2I-defense"
      ]
    },
    {
      "id": 253,
      "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "VLM-safety",
        "cross-modal",
        "unsafe-concepts"
      ]
    },
    {
      "id": 254,
      "title": "Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Language Model",
      "owasp": [
        "NONE"
      ],
      "type": "attack",
      "domains": [
        "vision",
        "llm"
      ],
      "models": [
        "transformer",
        "llm"
      ],
      "tags": [
        "captcha-breaking",
        "VLM",
        "AI-for-security"
      ]
    },
    {
      "id": 255,
      "title": "From Meme to Threat: On the Hateful Meme Understanding and Induced Hateful Content Generation in Open-Source Vision Language Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "models": [
        "transformer",
        "llm"
      ],
      "tags": [
        "hateful-meme",
        "content-generation",
        "VLM-attack"
      ]
    },
    {
      "id": 256,
      "title": "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "automated-jailbreak",
        "chatbot-attack"
      ]
    },
    {
      "id": 257,
      "title": "Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "alignment-weakness",
        "refusal-bypass"
      ]
    },
    {
      "id": 258,
      "title": "Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "safety-unlearning",
        "alignment-attack"
      ]
    },
    {
      "id": 259,
      "title": "Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "activation-approximation",
        "safety-vulnerability"
      ]
    },
    {
      "id": 261,
      "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "twin-prompts",
        "jailbreak"
      ]
    },
    {
      "id": 262,
      "title": "Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs",
      "owasp": [
        "ML01"
      ],
      "type": "benchmark",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "task-level",
        "jailbreak-benchmark"
      ]
    },
    {
      "id": 263,
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "fuzzing",
        "stealthy-jailbreak"
      ]
    },
    {
      "id": 264,
      "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "multi-turn",
        "crescendo",
        "jailbreak"
      ]
    },
    {
      "id": 265,
      "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "self-defense",
        "practical",
        "jailbreak-defense"
      ]
    },
    {
      "id": 266,
      "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "concept-analysis",
        "jailbreak-defense"
      ]
    },
    {
      "id": 267,
      "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "dynamic-attention",
        "robustness"
      ]
    },
    {
      "id": 268,
      "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "ChatGPT-detection",
        "AI-for-security"
      ]
    },
    {
      "id": 269,
      "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [
        "vision",
        "generative"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "AI-art-detection",
        "AI-for-security"
      ]
    },
    {
      "id": 270,
      "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "ChatGPT-detection",
        "academic-writing",
        "AI-for-security"
      ]
    },
    {
      "id": 271,
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "detector-evasion",
        "gradient-based"
      ]
    },
    {
      "id": 274,
      "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "synthetic-data",
        "privacy-leakage",
        "fine-tuning"
      ]
    },
    {
      "id": 275,
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "owasp": [
        "ML02"
      ],
      "type": "defense",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "prompt-tuning",
        "backdoor-defense",
        "task-agnostic"
      ]
    },
    {
      "id": 276,
      "title": "EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "embedding-backdoor",
        "cross-trigger"
      ]
    },
    {
      "id": 277,
      "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "web-enabled",
        "agentic-LLM",
        "emerging-threat"
      ]
    },
    {
      "id": 278,
      "title": "Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "LLM-agent",
        "vulnerability-detection",
        "taint-analysis"
      ]
    },
    {
      "id": 279,
      "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "attribution",
        "long-context",
        "traceability"
      ]
    },
    {
      "id": 280,
      "title": "Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "AI-search",
        "safety-risks",
        "AIPSE"
      ]
    },
    {
      "id": 281,
      "title": "Cloak, Honey, Trap: Proactive Defenses Against LLM Agents",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "LLM-agent-defense",
        "proactive",
        "honeypot"
      ]
    },
    {
      "id": 282,
      "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "privacy-audit",
        "tracking",
        "GenAI-assistants"
      ]
    },
    {
      "id": 283,
      "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "computer-use-agent",
        "real-time-defense"
      ]
    },
    {
      "id": 284,
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-injection-defense",
        "structured-queries"
      ]
    },
    {
      "id": 285,
      "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "RAG-attack",
        "DoS",
        "blocker-documents"
      ]
    },
    {
      "id": 286,
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-injection-defense",
        "preference-optimization"
      ]
    },
    {
      "id": 287,
      "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
      "owasp": [
        "ML01"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "llm",
        "vision"
      ],
      "models": [
        "llm",
        "transformer"
      ],
      "tags": [
        "hallucination-attack",
        "attention-sink",
        "MLLM"
      ]
    },
    {
      "id": 288,
      "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "online-learning",
        "dataset-inference",
        "reconstruction"
      ]
    },
    {
      "id": 289,
      "title": "Extracting Training Data from Large Language Models",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "training-data-extraction",
        "memorization"
      ]
    },
    {
      "id": 290,
      "title": "Analyzing Information Leakage of Updates to Natural Language Models",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "model-updates",
        "information-leakage"
      ]
    },
    {
      "id": 291,
      "title": "TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "tabular",
        "generative"
      ],
      "models": [
        "gan"
      ],
      "tags": [
        "membership-collision",
        "tabular-GAN"
      ]
    },
    {
      "id": 292,
      "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "gan"
      ],
      "tags": [
        "privacy-preserving",
        "gradient-compression"
      ]
    },
    {
      "id": 293,
      "title": "Property Inference Attacks Against GANs",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "generative"
      ],
      "models": [
        "gan"
      ],
      "tags": [
        "property-inference",
        "GAN-attack"
      ]
    },
    {
      "id": 294,
      "title": "MIRROR: Model Inversion for Deep Learning Network with High Fidelity",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn",
        "gan"
      ],
      "tags": [
        "model-inversion",
        "StyleGAN",
        "high-fidelity"
      ]
    },
    {
      "id": 295,
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "owasp": [
        "ML03"
      ],
      "type": "empirical",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "PII-leakage",
        "language-models"
      ]
    },
    {
      "id": 296,
      "title": "Timing Channels in Adaptive Neural Networks",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "timing-channel",
        "adaptive-NN",
        "side-channel"
      ]
    },
    {
      "id": 297,
      "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "model-inversion-defense",
        "facial-features",
        "edge-computing"
      ]
    },
    {
      "id": 298,
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "dataset-stealing",
        "bidirectional-training",
        "rogue-model"
      ]
    },
    {
      "id": 299,
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "generative",
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "data-boundary",
        "trustworthiness",
        "GenAI"
      ]
    },
    {
      "id": 300,
      "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "privacy-defense-evaluation",
        "misleading-metrics"
      ]
    },
    {
      "id": 302,
      "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
      "owasp": [
        "ML03"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "data-reconstruction",
        "benchmark"
      ]
    },
    {
      "id": 303,
      "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "data-auditing",
        "privacy",
        "facial-images"
      ]
    },
    {
      "id": 304,
      "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "LDP-poisoning",
        "ranking",
        "privacy-attack"
      ]
    },
    {
      "id": 305,
      "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "white-box",
        "memorization",
        "calibrated"
      ]
    },
    {
      "id": 306,
      "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "privacy-risk",
        "systematic-evaluation"
      ]
    },
    {
      "id": 307,
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "blind-attack",
        "differential-comparison"
      ]
    },
    {
      "id": 308,
      "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "generative"
      ],
      "models": [
        "gan"
      ],
      "tags": [
        "taxonomy",
        "GAN-privacy"
      ]
    },
    {
      "id": 309,
      "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "contrastive-learning",
        "privacy-risk"
      ]
    },
    {
      "id": 310,
      "title": "Membership Inference Attacks Against Recommender Systems",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "recommender-systems",
        "membership-inference"
      ]
    },
    {
      "id": 311,
      "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "encoder-attack",
        "contrastive-learning"
      ]
    },
    {
      "id": 312,
      "title": "Auditing Membership Leakages of Multi-Exit Networks",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "multi-exit",
        "membership-leakage"
      ]
    },
    {
      "id": 313,
      "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "loss-trajectory",
        "training-dynamics"
      ]
    },
    {
      "id": 314,
      "title": "On the Privacy Risks of Cell-Based NAS Architectures",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "NAS",
        "privacy-risk"
      ]
    },
    {
      "id": 315,
      "title": "Membership Inference Attacks and Defenses in Neural Network Pruning",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "pruning",
        "membership-inference"
      ]
    },
    {
      "id": 316,
      "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "self-distillation",
        "ensemble",
        "MI-defense"
      ]
    },
    {
      "id": 317,
      "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "hypothesis-testing",
        "enhanced-attack"
      ]
    },
    {
      "id": 318,
      "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "causal-analysis",
        "generalization"
      ]
    },
    {
      "id": 319,
      "title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "speaker-recognition",
        "speaker-level"
      ]
    },
    {
      "id": 320,
      "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "overconfidence",
        "MI-defense",
        "calibration"
      ]
    },
    {
      "id": 321,
      "title": "Membership Inference Attacks Against Vision-Language Models",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "VLM",
        "membership-inference"
      ]
    },
    {
      "id": 322,
      "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "label-only",
        "LLM-privacy"
      ]
    },
    {
      "id": 323,
      "title": "Enhanced Label-Only Membership Inference Attacks with Fewer Queries",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "label-only",
        "query-efficient"
      ]
    },
    {
      "id": 324,
      "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "data-obfuscation",
        "fine-tuning-privacy"
      ]
    },
    {
      "id": 325,
      "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "privacy-auditing",
        "reliability",
        "ensemble"
      ]
    },
    {
      "id": 327,
      "title": "The Value of Collaboration in Convex Machine Learning with Differential Privacy",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "differential-privacy",
        "multi-party-ML"
      ]
    },
    {
      "id": 328,
      "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "multi-party-ML",
        "property-leakage"
      ]
    },
    {
      "id": 329,
      "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "split-learning",
        "inference-attack"
      ]
    },
    {
      "id": 330,
      "title": "Local and Central Differential Privacy for Robustness and Privacy in Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "differential-privacy",
        "robustness"
      ]
    },
    {
      "id": 331,
      "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "gradient-obfuscation",
        "false-security"
      ]
    },
    {
      "id": 332,
      "title": "PPA: Preference Profiling Attack Against Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "preference-profiling",
        "FL-privacy"
      ]
    },
    {
      "id": 333,
      "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "decentralized-ML",
        "P2P-privacy"
      ]
    },
    {
      "id": 334,
      "title": "RoFL: Robustness of Secure Federated Learning",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "secure-aggregation",
        "robustness-analysis"
      ]
    },
    {
      "id": 335,
      "title": "Scalable and Privacy-Preserving Federated Principal Component Analysis",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "federated-PCA",
        "privacy-preserving"
      ]
    },
    {
      "id": 336,
      "title": "Protecting Label Distribution in Cross-Silo Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "label-distribution",
        "cross-silo"
      ]
    },
    {
      "id": 337,
      "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "data-reconstruction",
        "model-manipulation"
      ]
    },
    {
      "id": 338,
      "title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning",
      "owasp": [
        "ML03"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "gradient-privacy",
        "systematic-analysis"
      ]
    },
    {
      "id": 339,
      "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "gradient-leakage",
        "realistic-settings"
      ]
    },
    {
      "id": 340,
      "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "gradient-leakage-defense",
        "data-refining"
      ]
    },
    {
      "id": 341,
      "title": "Aion: Robust and Efficient Multi-Round Single-Mask Secure Aggregation Against Malicious Participants",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "secure-aggregation",
        "malicious-participants"
      ]
    },
    {
      "id": 342,
      "title": "SoK: On Gradient Leakage in Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "survey",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "SoK",
        "gradient-leakage"
      ]
    },
    {
      "id": 343,
      "title": "DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning with Client Momentum",
      "owasp": [
        "ML08"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "differential-privacy",
        "Byzantine-robust"
      ]
    },
    {
      "id": 345,
      "title": "Sharpness-Aware Initialization: Improving Differentially Private Machine Learning from First Principles",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "differential-privacy",
        "initialization"
      ]
    },
    {
      "id": 346,
      "title": "Task-Oriented Training Data Privacy Protection for Cloud-based Model Training",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "cloud-training",
        "data-privacy"
      ]
    },
    {
      "id": 347,
      "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "empirical",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "DRA-risk",
        "risk-assessment"
      ]
    },
    {
      "id": 348,
      "title": "SoK: Gradient Inversion Attacks in Federated Learning",
      "owasp": [
        "ML03"
      ],
      "type": "survey",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "SoK",
        "gradient-inversion"
      ]
    },
    {
      "id": 349,
      "title": "Privacy Risks of General-Purpose Language Models",
      "owasp": [
        "ML03"
      ],
      "type": "empirical",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "LM-privacy",
        "embedding-privacy"
      ]
    },
    {
      "id": 350,
      "title": "Information Leakage in Embedding Models",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [
        "transformer"
      ],
      "tags": [
        "embedding-leakage",
        "pre-training"
      ]
    },
    {
      "id": 351,
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "secret-encoding",
        "attribute-leakage"
      ]
    },
    {
      "id": 352,
      "title": "Stealing Links from Graph Neural Networks",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "link-stealing",
        "GNN-privacy"
      ]
    },
    {
      "id": 353,
      "title": "Inference Attacks Against Graph Neural Networks",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "GNN-inference",
        "membership-inference"
      ]
    },
    {
      "id": 354,
      "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "link-recovery",
        "influence-analysis"
      ]
    },
    {
      "id": 355,
      "title": "Locally Private Graph Neural Networks",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "local-DP",
        "GNN-privacy"
      ]
    },
    {
      "id": 356,
      "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "node-embedding",
        "memory-recovery"
      ]
    },
    {
      "id": 357,
      "title": "Group Property Inference Attacks Against Graph Neural Networks",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "property-inference",
        "group-level"
      ]
    },
    {
      "id": 358,
      "title": "LPGNet: Link Private Graph Networks for Node Classification",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "link-privacy",
        "node-classification"
      ]
    },
    {
      "id": 359,
      "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "data-misuse-detection",
        "MLaaS"
      ]
    },
    {
      "id": 360,
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "link-stealing-defense",
        "training-graph"
      ]
    },
    {
      "id": 361,
      "title": "Machine Unlearning",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "machine-unlearning",
        "SISA",
        "right-to-be-forgotten"
      ]
    },
    {
      "id": 362,
      "title": "When Machine Unlearning Jeopardizes Privacy",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "unlearning-privacy",
        "membership-inference"
      ]
    },
    {
      "id": 363,
      "title": "Graph Unlearning",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "graph-unlearning",
        "partition"
      ]
    },
    {
      "id": 364,
      "title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "unlearning-definitions",
        "auditability"
      ]
    },
    {
      "id": 365,
      "title": "Machine Unlearning of Features and Labels",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "feature-unlearning",
        "label-unlearning"
      ]
    },
    {
      "id": 366,
      "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services",
      "owasp": [
        "ML04"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "MLaaS-unlearning",
        "vulnerabilities"
      ]
    },
    {
      "id": 367,
      "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "MLaaS-unlearning",
        "inference-serving"
      ]
    },
    {
      "id": 368,
      "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "unlearning-evaluation",
        "inference-attack"
      ]
    },
    {
      "id": 370,
      "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "unlearning-completeness",
        "lifecycle"
      ]
    },
    {
      "id": 371,
      "title": "Split Unlearning",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "split-learning",
        "SISA-unlearning"
      ]
    },
    {
      "id": 372,
      "title": "Rethinking Machine Unlearning in Image Generation Models",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "generative-unlearning",
        "image-generation"
      ]
    },
    {
      "id": 373,
      "title": "Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "prototype-unlearning",
        "efficient"
      ]
    },
    {
      "id": 374,
      "title": "Are Attribute Inference Attacks Just Imputation?",
      "owasp": [
        "ML03"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "attribute-inference",
        "imputation"
      ]
    },
    {
      "id": 375,
      "title": "Feature Inference Attack on Shapley Values",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "Shapley-values",
        "feature-inference",
        "XAI-attack"
      ]
    },
    {
      "id": 376,
      "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "query-based-systems",
        "automated-discovery"
      ]
    },
    {
      "id": 377,
      "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "targeted-attack",
        "disparate-privacy"
      ]
    },
    {
      "id": 378,
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "property-inference",
        "poisoning-assisted"
      ]
    },
    {
      "id": 379,
      "title": "SoK: Privacy-Preserving Data Synthesis",
      "owasp": [
        "ML04"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "data-synthesis",
        "privacy-preserving"
      ]
    },
    {
      "id": 380,
      "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "reinforcement-learning"
      ],
      "models": [],
      "tags": [
        "dataset-auditing",
        "offline-DRL"
      ]
    },
    {
      "id": 381,
      "title": "SoK: Dataset Copyright Auditing in Machine Learning Systems",
      "owasp": [
        "ML05"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "dataset-copyright",
        "auditing"
      ]
    },
    {
      "id": 382,
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "owasp": [
        "ML05"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "active-learning",
        "model-extraction"
      ]
    },
    {
      "id": 383,
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "model-extraction",
        "accuracy-fidelity"
      ]
    },
    {
      "id": 384,
      "title": "DRMI: A Dataset Reduction Technology based on Mutual Information for Black-box Attacks",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "dataset-reduction",
        "black-box-extraction"
      ]
    },
    {
      "id": 385,
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "entangled-watermarks",
        "extraction-defense"
      ]
    },
    {
      "id": 386,
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "MLaaS-stealing",
        "adversarial-assisted"
      ]
    },
    {
      "id": 387,
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "teacher-fingerprinting",
        "transfer-learning"
      ]
    },
    {
      "id": 388,
      "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "encoder-stealing",
        "EaaS"
      ]
    },
    {
      "id": 389,
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "defense-penetrating",
        "model-extraction"
      ]
    },
    {
      "id": 390,
      "title": "SoK: Neural Network Extraction Through Physical Side Channels",
      "owasp": [
        "ML05"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "side-channel",
        "physical-extraction"
      ]
    },
    {
      "id": 391,
      "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
      "owasp": [
        "ML05"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "on-device",
        "research-practice-gap"
      ]
    },
    {
      "id": 392,
      "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "text-watermarking",
        "provenance"
      ]
    },
    {
      "id": 393,
      "title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation",
      "owasp": [
        "ML05"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "white-box-watermark",
        "obfuscation"
      ]
    },
    {
      "id": 394,
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "watermark-robustness",
        "extraction-defense"
      ]
    },
    {
      "id": 395,
      "title": "SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-Supervised Learning",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "SSL-watermarking",
        "encoder-protection"
      ]
    },
    {
      "id": 396,
      "title": "Watermarking Language Models for Many Adaptive Users",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "transformer",
        "llm"
      ],
      "tags": [
        "LM-watermarking",
        "adaptive-users"
      ]
    },
    {
      "id": 397,
      "title": "SoK: Watermarking for AI-Generated Content",
      "owasp": [
        "ML05"
      ],
      "type": "survey",
      "domains": [
        "generative"
      ],
      "models": [],
      "tags": [
        "SoK",
        "AIGC-watermarking"
      ]
    },
    {
      "id": 398,
      "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "multi-bit-watermark",
        "provable-robustness"
      ]
    },
    {
      "id": 399,
      "title": "AUDIO WATERMARK: Dynamic and Harmless Watermark for Black-box Voice Dataset Copyright Protection",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "audio-watermark",
        "voice-dataset"
      ]
    },
    {
      "id": 400,
      "title": "AudioMarkNet: Audio Watermarking for Deepfake Speech Detection",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "audio-watermark",
        "deepfake-detection"
      ]
    },
    {
      "id": 401,
      "title": "Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification",
      "owasp": [
        "ML05"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "proof-of-training",
        "ownership-verification"
      ]
    },
    {
      "id": 402,
      "title": "LightShed: Defeating Perturbation-based Image Copyright Protections",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "copyright-protection-bypass"
      ]
    },
    {
      "id": 403,
      "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "models": [
        "diffusion"
      ],
      "tags": [
        "watermark-removal",
        "Tree-Ring"
      ]
    },
    {
      "id": 404,
      "title": "Proof-of-Learning: Definitions and Practice",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "proof-of-learning",
        "ownership"
      ]
    },
    {
      "id": 405,
      "title": "SoK: How Robust is Image Classification Deep Neural Network Watermarking?",
      "owasp": [
        "ML05"
      ],
      "type": "survey",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "SoK",
        "watermark-robustness"
      ]
    },
    {
      "id": 406,
      "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "copyright-testing",
        "framework"
      ]
    },
    {
      "id": 407,
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "SSL-watermarking",
        "encoder-protection"
      ]
    },
    {
      "id": 408,
      "title": "RAI2: Responsible Identity Audit Governing the Artificial Intelligence",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "identity-audit",
        "responsible-AI"
      ]
    },
    {
      "id": 409,
      "title": "ActiveDaemon: Unconscious DNN Dormancy and Waking Up via User-specific Invisible Token",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "dormancy",
        "invisible-token",
        "IP-protection"
      ]
    },
    {
      "id": 410,
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "on-device",
        "IP-protection",
        "post-deployment"
      ]
    },
    {
      "id": 411,
      "title": "PublicCheck: Public Integrity Verification for Services of Run-time Deep Models",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "integrity-verification",
        "runtime",
        "public"
      ]
    },
    {
      "id": 412,
      "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-inversion",
        "collaborative-inference"
      ]
    },
    {
      "id": 413,
      "title": "On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts",
      "owasp": [
        "ML05"
      ],
      "type": "empirical",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-stealing",
        "in-the-wild"
      ]
    },
    {
      "id": 414,
      "title": "PRSA: Prompt Stealing Attacks against Real-World Prompt Services",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-stealing",
        "real-world"
      ]
    },
    {
      "id": 415,
      "title": "Cross-Modal Prompt Inversion: Unifying Threats to Text and Image Generative AI Models",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "multimodal",
        "generative"
      ],
      "models": [
        "llm",
        "diffusion"
      ],
      "tags": [
        "cross-modal",
        "prompt-inversion"
      ]
    },
    {
      "id": 416,
      "title": "Prompt Obfuscation for Large Language Models",
      "owasp": [
        "ML05"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-obfuscation",
        "IP-protection"
      ]
    },
    {
      "id": 417,
      "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-inference",
        "distributed-inference"
      ]
    },
    {
      "id": 418,
      "title": "Codebreaker: Dynamic Extraction Attacks on Code Language Models",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "code-LM",
        "dynamic-extraction"
      ]
    },
    {
      "id": 419,
      "title": "LLMmap: Fingerprinting for Large Language Models",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "LLM-fingerprinting",
        "version-detection"
      ]
    },
    {
      "id": 420,
      "title": "Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "differential-privacy",
        "zeroth-order",
        "fine-tuning"
      ]
    },
    {
      "id": 421,
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "internal-state-inversion",
        "collaborative-inference"
      ]
    },
    {
      "id": 422,
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "PII-extraction",
        "countermeasures"
      ]
    },
    {
      "id": 423,
      "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
      "owasp": [
        "ML03"
      ],
      "type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "privacy-breach-detection",
        "semantic-consistency"
      ]
    },
    {
      "id": 424,
      "title": "Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications",
      "owasp": [
        "ML04"
      ],
      "type": "tool",
      "domains": [
        "llm"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "synthetic-data",
        "auditing",
        "tracing"
      ]
    },
    {
      "id": 425,
      "title": "Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "audio"
      ],
      "models": [],
      "tags": [
        "speech-privacy",
        "adversarial-protection",
        "ASR"
      ]
    },
    {
      "id": 426,
      "title": "Effective PII Extraction from LLMs through Augmented Few-Shot Learning",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "PII-extraction",
        "few-shot"
      ]
    },
    {
      "id": 427,
      "title": "Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts",
      "owasp": [
        "ML03"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "PII-extraction",
        "optimized-prompts"
      ]
    },
    {
      "id": 428,
      "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "face-cloaking",
        "privacy-protection"
      ]
    },
    {
      "id": 429,
      "title": "Automatically Detecting Bystanders in Photos to Reduce Privacy Risks",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "bystander-detection",
        "photo-privacy"
      ]
    },
    {
      "id": 430,
      "title": "Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks",
      "owasp": [
        "NONE"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "non-consensual-sharing",
        "photo-abuse"
      ]
    },
    {
      "id": 431,
      "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
      "owasp": [
        "ML01"
      ],
      "type": "empirical",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "face-obfuscation",
        "fairness"
      ]
    },
    {
      "id": 432,
      "title": "SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "MPC",
        "PPML",
        "secure-computation"
      ]
    },
    {
      "id": 433,
      "title": "BLAZE: Blazing Fast Privacy-Preserving Machine Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "MPC",
        "PPML",
        "efficient"
      ]
    },
    {
      "id": 434,
      "title": "Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "3PC",
        "non-linear",
        "PPML"
      ]
    },
    {
      "id": 435,
      "title": "Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "tree"
      ],
      "tags": [
        "3PC",
        "decision-trees",
        "communication"
      ]
    },
    {
      "id": 436,
      "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "4PC",
        "PPML",
        "actively-secure"
      ]
    },
    {
      "id": 437,
      "title": "Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "MPC",
        "collaborative-learning",
        "platform"
      ]
    },
    {
      "id": 438,
      "title": "Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "MPL",
        "differential-privacy"
      ]
    },
    {
      "id": 439,
      "title": "MPCDiff: Testing and Repairing MPC-Hardened Deep Learning Models",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "MPC-testing",
        "model-repair"
      ]
    },
    {
      "id": 440,
      "title": "Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption",
      "owasp": [
        "NONE"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "collaborative-learning",
        "no-non-collusion"
      ]
    },
    {
      "id": 441,
      "title": "Securely Training Decision Trees Efficiently",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [
        "tree"
      ],
      "tags": [
        "secure-training",
        "decision-trees"
      ]
    },
    {
      "id": 442,
      "title": "CoGNN: Towards Secure and Efficient Collaborative Graph Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "graph"
      ],
      "models": [
        "gnn"
      ],
      "tags": [
        "secure-GNN",
        "collaborative"
      ]
    },
    {
      "id": 443,
      "title": "SoK: Cryptographic Neural-Network Computation",
      "owasp": [
        "NONE"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "cryptographic-NN",
        "PPML"
      ]
    },
    {
      "id": 444,
      "title": "From Individual Computation to Allied Optimization: Remodeling Privacy-Preserving Neural Inference with Function Input Tuning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "PPML",
        "inference-optimization"
      ]
    },
    {
      "id": 445,
      "title": "BOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "nlp"
      ],
      "models": [
        "transformer"
      ],
      "tags": [
        "MPC",
        "transformer-inference"
      ]
    },
    {
      "id": 446,
      "title": "Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "secure-aggregation",
        "single-server"
      ]
    },
    {
      "id": 447,
      "title": "ELSA: Secure Aggregation for Federated Learning with Malicious Actors",
      "owasp": [
        "NONE"
      ],
      "type": "tool",
      "domains": [
        "federated-learning"
      ],
      "models": [],
      "tags": [
        "secure-aggregation",
        "malicious-actors"
      ]
    },
    {
      "id": 448,
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "owasp": [
        "ML04"
      ],
      "type": "empirical",
      "domains": [],
      "models": [],
      "tags": [
        "holistic-assessment",
        "inference-attacks"
      ]
    },
    {
      "id": 449,
      "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
      "owasp": [
        "ML04"
      ],
      "type": "survey",
      "domains": [],
      "models": [],
      "tags": [
        "SoK",
        "privacy-games",
        "inference-privacy"
      ]
    },
    {
      "id": 450,
      "title": "Federated Boosted Decision Trees with Differential Privacy",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [
        "federated-learning"
      ],
      "models": [
        "tree"
      ],
      "tags": [
        "differential-privacy",
        "boosted-trees"
      ]
    },
    {
      "id": 452,
      "title": "Bounded and Unbiased Composite Differential Privacy",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "composite-DP",
        "bounded",
        "unbiased"
      ]
    },
    {
      "id": 453,
      "title": "Cohere: Managing Differential Privacy in Large Scale Systems",
      "owasp": [
        "ML04"
      ],
      "type": "tool",
      "domains": [],
      "models": [],
      "tags": [
        "DP-management",
        "large-scale"
      ]
    },
    {
      "id": 454,
      "title": "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks",
      "owasp": [
        "ML01"
      ],
      "type": "defense",
      "domains": [
        "vision"
      ],
      "models": [
        "cnn"
      ],
      "tags": [
        "visual-privacy",
        "privacy-preserving"
      ]
    },
    {
      "id": 455,
      "title": "Locally Differentially Private Frequency Estimation Based on Convolution Framework",
      "owasp": [
        "ML04"
      ],
      "type": "defense",
      "domains": [],
      "models": [],
      "tags": [
        "LDP",
        "frequency-estimation"
      ]
    },
    {
      "id": 456,
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "owasp": [
        "ML02"
      ],
      "type": "attack",
      "domains": [],
      "models": [],
      "tags": [
        "LDP-poisoning",
        "frequent-itemset"
      ]
    },
    {
      "id": 457,
      "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
      "owasp": [
        "ML05"
      ],
      "type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "models": [
        "llm"
      ],
      "tags": [
        "prompt-leaking",
        "LLM-applications"
      ]
    }
  ]
}