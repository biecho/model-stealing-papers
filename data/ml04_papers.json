{
  "updated": "2026-01-06",
  "total": 272,
  "owasp_id": "ML04",
  "owasp_name": "Membership Inference Attack",
  "description": "Attacks that determine whether specific data points were used to train a\n        machine learning model. This includes membership inference attacks, privacy\n        attacks that reveal training set membership, and techniques to distinguish\n        training data from non-training data based on model behavior.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "37f5d1c5cd25583882f85568d5b409e873fbf2aa",
      "title": "MANDA: On Adversarial Example Detection for Network Intrusion Detection System",
      "abstract": "With the rapid advancement in machine learning (ML), ML-based Intrusion Detection Systems (IDSs) are widely deployed to protect networks from various attacks. Yet one of the biggest challenges is that ML-based IDSs suffer from adversarial example (AE) attacks. By applying small perturbations (e.g. slightly increasing packet inter-arrival time) to the intrusion traffic, an AE attack can flip the prediction of a well-trained IDS. We address this challenge by proposing MANDA, a MANifold and Decision boundary-based AE detection system. Through analyzing AE attacks, we notice that 1) an AE tends to be close to its original manifold (i.e., the cluster of samples in its original class) regardless which class it is misclassified into; and 2) AEs tend to be close to the decision boundary so as to minimize the perturbation scale. Based on the two observations, we design MANDA for accurate AE detection by exploiting inconsistency between manifold evaluation and IDS model inference and evaluating model uncertainty on small perturbations. We evaluate MANDA on NSL-KDD under three state-of-the-art AE attacks. Our experimental results show that MANDA achieves as high as 98.41% true-positive rate with 5% false-positive rate and can be applied to other problem spaces such as image recognition.",
      "year": 2021,
      "venue": "IEEE Conference on Computer Communications",
      "authors": [
        "Ning Wang",
        "Yimin Chen",
        "Yang Hu",
        "Wenjing Lou",
        "Yiwei Thomas Hou"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/37f5d1c5cd25583882f85568d5b409e873fbf2aa",
      "pdf_url": "",
      "publication_date": "2021-05-10",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8bac121226a963cc094d594d62de3945ad1b9742",
      "title": "Differentially Private Range Queries with Correlated Input Perturbation",
      "abstract": "This work proposes a class of differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our theoretical and empirical analysis demonstrates that we achieve near-optimal utility, effectively compete with other methods, and retain all the favorable statistical properties discussed earlier.",
      "year": 2024,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Prathamesh Dharangutte",
        "Jie Gao",
        "Ruobin Gong",
        "Guanyang Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8bac121226a963cc094d594d62de3945ad1b9742",
      "pdf_url": "",
      "publication_date": "2024-02-10",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6af8e52f1848abf03e3e945247d5611508cd622a",
      "title": "Fortifying Federated Learning against Membership Inference Attacks via Client-level Input Perturbation",
      "abstract": "Membership inference (MI) attacks are more diverse in a Federated Learning (FL) setting, because an adversary may be either an FL client, a server, or an external attacker. Existing defenses against MI attacks rely on perturbations to either the model's output predictions or the training process. However, output perturbations are ineffective in an FL setting, because a malicious server can access the model without output perturbation while training perturbations struggle to achieve a good utility. This paper proposes a novel defense, called CIP, to fortify FL against MI attacks via a client-level input perturbation during training and inference procedures. The key insight is to shift each client's local data distribution via a personalized perturbation to get a shifted model. CIP achieves a good balance between privacy and utility. Our evaluation shows that CIP causes accuracy to drop at most 0.7% while reducing attacks to random guessing.",
      "year": 2023,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Yuchen Yang",
        "Haolin Yuan",
        "Bo Hui",
        "N. Gong",
        "Neil Fendley",
        "P. Burlina",
        "Yinzhi Cao"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/6af8e52f1848abf03e3e945247d5611508cd622a",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9c34b67e936fd16dffcb7913b27dbeab79892a6e",
      "title": "Machine-Learning-Based DDoS Attack Detection Using Mutual Information and Random Forest Feature Importance Method",
      "abstract": "Cloud computing facilitates the users with on-demand services over the Internet. The services are accessible from anywhere at any time. Despite the valuable services, the paradigm is, also, prone to security issues. A Distributed Denial of Service (DDoS) attack affects the availability of cloud services and causes security threats to cloud computing. Detection of DDoS attacks is necessary for the availability of services for legitimate users. The topic has been studied by many researchers, with better accuracy for different datasets. This article presents a method for DDoS attack detection in cloud computing. The primary objective of this article is to reduce misclassification error in DDoS detection. In the proposed work, we select the most relevant features, by applying two feature selection techniques, i.e., the Mutual Information (MI) and Random Forest Feature Importance (RFFI) methods. Random Forest (RF), Gradient Boosting (GB), Weighted Voting Ensemble (WVE), K Nearest Neighbor (KNN), and Logistic Regression (LR) are applied to selected features. The experimental results show that the accuracy of RF, GB, WVE, and KNN with 19 features is 0.99. To further study these methods, misclassifications of the methods are analyzed, which lead to more accurate measurements. Extensive experiments conclude that the RF performed well in DDoS attack detection and misclassified only one attack as normal. Comparative results are presented to validate the proposed method.",
      "year": 2022,
      "venue": "Symmetry",
      "authors": [
        "Mona A. Alduailij",
        "Qazi Waqas Khan",
        "Muhammad Tahir",
        "Muhammad Sardaraz",
        "Mai A. Alduailij",
        "Fazila Malik"
      ],
      "citation_count": 139,
      "url": "https://www.semanticscholar.org/paper/9c34b67e936fd16dffcb7913b27dbeab79892a6e",
      "pdf_url": "https://www.mdpi.com/2073-8994/14/6/1095/pdf?version=1653623569",
      "publication_date": "2022-05-27",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b4a55f248dcb496bb0f0bb2d3fd12b4e467ba020",
      "title": "Enhancing power security: Attack detection based on cost-sensitive SVM and attention mechanism",
      "abstract": "In order to proactively block attack paths and secure the power monitoring host, a cost-sensitive SVM-based method for detecting attacks on power monitoring hosts is proposed. This method aims to accurately identify network and user attacks. A model for detecting power monitoring host attack behavior using a cost-sensitive SVM approach is developed. The data on power monitoring host attack behavior, encompassing both user and network attack behaviors, is gathered, and the data characteristics are used to create training samples for the model. By resolving the misclassification cost of each sample using a dynamic cost function based on class membership, the cost-sensitive SVM classifier is trained to detect attacks. The classifier then processes the data for classification and other operations to output the detected instances of attack behavior on the power monitoring host. Experimental results demonstrate that this approach can effectively reduce the overall misclassification cost of power monitoring host attacks. It can detect network attacks and user attacks by analyzing abnormal network traffic and user keystrokes, as well as accurately identify other types of attacks on power monitoring hosts. This method enhances the detection capabilities of power monitoring host attacks, thereby ensuring the security of power systems.",
      "year": 2024,
      "venue": "2024 6th International Conference on Energy, Power and Grid (ICEPG)",
      "authors": [
        "Hao Sun",
        "Dapeng Li",
        "Yang Yu",
        "Bingyu Han"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b4a55f248dcb496bb0f0bb2d3fd12b4e467ba020",
      "pdf_url": "",
      "publication_date": "2024-09-27",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82e8c2fe7764c42992a546a2c1364bc197e824d3",
      "title": "Multiscale Evolutionary Perturbation Attack on Community Detection",
      "abstract": "Community detection, aiming to group nodes based on their connections, plays an important role in network analysis since communities, treated as meta-nodes, allow us to create a large-scale map of a network to simplify its analysis. However, for privacy reasons, we may want to prevent communities from being discovered in certain cases, leading to the topics on community deception. In this article, we formalize this community detection attack problem in three scales, including global attack (macroscale), target community attack (mesoscale), and target node attack (microscale). We treat this as an optimization problem and further propose a novel evolutionary perturbation attack (EPA) method, where we generate adversarial networks to realize the community detection attack. Numerical experiments validate that our EPA can successfully attack network community algorithms in all three scales, i.e., hide target nodes or communities and further disturb the community structure of the whole network by only changing a small fraction of links. By comparison, our EPA behaves better than a number of baseline attack methods on six synthetic networks and three real-world networks. More interestingly, although our EPA is based on the Louvain algorithm, it is also effective in attacking other community detection algorithms, validating its good transferability.",
      "year": 2019,
      "venue": "IEEE Transactions on Computational Social Systems",
      "authors": [
        "Jinyin Chen",
        "Yixian Chen",
        "Lihong Chen",
        "M. Zhao",
        "Qi Xuan"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/82e8c2fe7764c42992a546a2c1364bc197e824d3",
      "pdf_url": "https://arxiv.org/pdf/1910.09741",
      "publication_date": "2019-10-22",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7206e3e20bd7f01e997d2702f6745684639ebcef",
      "title": "Privacy Preserving Through Fireworks Algorithm Based Model for Image Perturbation in Big Data",
      "abstract": null,
      "year": 2015,
      "venue": "International Journal of Swarm Intelligence Research",
      "authors": [
        "Amine Rahmani",
        "Abdelmalek Amine",
        "R. Hamou",
        "M. Rahmani",
        "Hadj Ahmed Bouarara"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/7206e3e20bd7f01e997d2702f6745684639ebcef",
      "pdf_url": "",
      "publication_date": "2015-07-01",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3660f8661700695bebe70cbf0ad508696090f9e7",
      "title": "EdgeInfer: Robust Truth Inference under Data Poisoning Attack",
      "abstract": "As crowdsourcing is becoming more widely used for annotating data from a large group of users, attackers have strong incentives to manipulate the system. Deriving the true answer of tasks in crowdsourcing systems based on user-provided data is susceptible to data poisoning attacks, whereby malicious users may intentionally or strategically report incorrect information to mislead the system into inferring the wrong truth for a set of tasks. Recent work has proposed several attacks on the crowdsourcing systems and showed that existing truth inference methods may be vulnerable to such attacks. In this paper, we propose solutions to enhance the robustness of existing truth inference methods. Our solutions base on 1) detecting and augmenting the answers for the boundary tasks in which users could not reach a strong consensus and hence are subjective to potential manipulation, and 2) enhancing inference method with a stronger prior. We empirically evaluate these defense mechanisms by designing attack scenarios that aim to decrease the accuracy of the system. Experiments show that our method is effective and significantly improves the robustness of the system under attack.",
      "year": 2020,
      "venue": "International Conference on Smart Data Services",
      "authors": [
        "Farnaz Tahmasebian",
        "Li Xiong",
        "Mani Sotoodeh",
        "V. Sunderam"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/3660f8661700695bebe70cbf0ad508696090f9e7",
      "pdf_url": "",
      "publication_date": "2020-10-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f1a37492f3555ed3aeff97babcdf1362277f9e3",
      "title": "APDPFL: Anti-Poisoning Attack Decentralized Privacy Enhanced Federated Learning Scheme for Flight Operation Data Sharing",
      "abstract": "The sharing of flight operation data brings huge benefits to all participants, but for the privacy protection and data security, it is difficult to directly share flight operation data. Federated learning (FL) enables participants to jointly train machine learning models without exposing local data. However, due to the centralization of FL and the unreliability of FL participants, FL is vulnerable to malicious client and server attacks. In this paper, an anti-poisoning attack decentralized privacy enhanced federated learning (APDPFL) scheme is designed to mitigate the impact of server and malicious clients. Specifically, a local R\u00e9nyi differential privacy is designed to protect client data privacy. Then, a verification method based on K-Means clustering is proposed to select models to participate in aggregation, which improves the anti-poisoning attack performance. Finally, a federated grouping practical Byzantine fault tolerance (FGPBFT) consensus algorithm based on consortium blockchain is proposed to dynamically change server and consensus clients, to decentralize server and improve the consensus efficiency. The theoretical analysis proves that the APDPFL achieves better convergence and provides data privacy protection and security protection. The experimental results on public datasets and flight operation datasets show that the APDPFL is robust and effective for sharing flight operation data.",
      "year": 2024,
      "venue": "IEEE Transactions on Wireless Communications",
      "authors": [
        "Xinyan Li",
        "Huimin Zhao",
        "Junjie Xu",
        "Guangtian Zhu",
        "Wu Deng"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/1f1a37492f3555ed3aeff97babcdf1362277f9e3",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "77e23f0a314801ebdd92de2110f8af7a5b99cfce",
      "title": "Efficiently Achieving Privacy Preservation and Poisoning Attack Resistance in Federated Learning",
      "abstract": "Federated learning enables clients to train models locally and provide local updates to the server instead of raw dataset, thereby preserving data privacy to some extent. However, adversaries can still pry users\u2019 privacy by inferring updates, and compromise the integrity of the global model through poisoning attack. Therefore, many related works have integrated poisoning attack detection method with secure computation to address both issues. Nevertheless, they still encounter two major challenges: 1) the efficiency is too low to be applied in practice; and 2) the privacy is still at risk of being leaked, e.g., the distance of two local updates for detecting poisoning attack could be exposed to the server. Aiming at the challenges, in this paper, we propose an Efficient Privacy-preserving and Poisoning attack Resistant scheme for Federated Learning, named EPPRFL, which preserves the privacy for local updates and some intermediate information used to detect poisoning attack. In particular, we design an efficient poisoning attack detection method based on Euclidean distance filtering & clipping technique, named F&C. Then, considering the privacy preservation of the F&C method, we efficiently customize secure comparison, secure median, secure distance computation and secure clipping protocols based on additive secret sharing. Experimental results and theoretical analysis show that compared with existing schemes, EPPRFL can better resist poisoning attack and has lower computational and communication overheads on the client side.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xueyang Li",
        "Xue Yang",
        "Zhengchun Zhou",
        "Rongxing Lu"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/77e23f0a314801ebdd92de2110f8af7a5b99cfce",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c28a01ec4a8f3b83aca039d7c5dc99a6f21d24a",
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "abstract": "Local differential privacy (LDP) provides a way for an untrusted data collector to aggregate users' data without violating their privacy. Various privacy-preserving data analysis tasks have been studied under the protection of LDP, such as frequency estimation, frequent itemset mining, and machine learning. Despite its privacy-preserving properties, recent research has demonstrated the vulnerability of certain LDP protocols to data poisoning attacks. However, existing data poisoning attacks are focused on basic statistics under LDP, such as frequency estimation and mean/variance estimation. As an important data analysis task, the security of LDP frequent itemset mining has yet to be thoroughly examined. In this paper, we aim to address this issue by presenting novel and practical data poisoning attacks against LDP frequent itemset mining protocols. By introducing a unified attack framework with composable attack operations, our data poisoning attack can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures. We conduct extensive experiments on three datasets to compare the proposed attack with four baseline attacks. The results demonstrate the severity of the threat and the effectiveness of the proposed attack.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Wei Tong",
        "Haoyu Chen",
        "Jiacheng Niu",
        "Sheng Zhong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7c28a01ec4a8f3b83aca039d7c5dc99a6f21d24a",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670298",
      "publication_date": "2024-06-27",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "84e0045dac1a21787c179d0006ca740da0108565",
      "title": "RFed: Robustness-Enhanced Privacy-Preserving Federated Learning Against Poisoning Attack",
      "abstract": "Federated learning not only realizes collaborative training of models, but also effectively maintains user privacy. However, with the widespread application of privacy-preserving federated learning, poisoning attacks threaten the model utility. Existing defense schemes suffer from a series of problems, including low accuracy, low robustness and reliance on strong assumptions, which limit the practicability of federated learning. To solve these problems, we propose a Robustness-enhanced privacy-preserving Federated learning with scaled dot-product attention (RFed) under dual-server model. Specifically, we design a highly robust defense mechanism that uses a dual-server model instead of traditional single-server model to significantly improve model accuracy and completely eliminate the reliance on strong assumptions. Formal security analysis proves that our scheme achieves convergence and provides privacy protection, and extensive experiments demonstrate that our scheme reduces high computational overhead while guaranteeing privacy preservation and model accuracy, and ensures that the failure rate of poisoning attacks is higher than 96%.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yinbin Miao",
        "Xinru Yan",
        "Xinghua Li",
        "Shujiang Xu",
        "Ximeng Liu",
        "Hongwei Li",
        "Robert H. Deng"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/84e0045dac1a21787c179d0006ca740da0108565",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d27811e2cfe875f6d552a4c050248416e5f8400",
      "title": "Personalized federated learning-based intrusion detection system: Poisoning attack and defense",
      "abstract": null,
      "year": 2023,
      "venue": "Future generations computer systems",
      "authors": [
        "Thin Tharaphe Thein",
        "Yoshiaki Shiraishi",
        "M. Morii"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7d27811e2cfe875f6d552a4c050248416e5f8400",
      "pdf_url": "https://doi.org/10.1016/j.future.2023.10.005",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "88b1ca7cfc47bcdf95ab66a7952b8ff105d932e7",
      "title": "ADFL: A Poisoning Attack Defense Framework for Horizontal Federated Learning",
      "abstract": "Recently, federated learning has received widespread attention, which will promote the implementation of artificial intelligence technology in various fields. Privacy-preserving technologies are applied to users\u2019 local models to protect users\u2019 privacy. Such operations make the server not see the true model parameters of each user, which opens wider door for a malicious user to upload malicious parameters and make the training result converge to an ineffective model. To solve this problem, in this article, we propose a poisoning attack defense framework for horizontal federated learning systems called ADFL. Specifically, we design a proof generation method for users to generate proofs to verify whether it is malicious or not. An aggregation rule is also proposed to make sure the global model has a high accuracy. Several verification experiments were conducted and the results show that our method can detect malicious user effectively and ensure the global model has a high accuracy.",
      "year": 2022,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Jingjing Guo",
        "Haiyang Li",
        "Feiran Huang",
        "Zhiquan Liu",
        "Yanguo Peng",
        "Xinghua Li",
        "J. Ma",
        "Varun G. Menon",
        "Konstantin Kostromitin Igorevich"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/88b1ca7cfc47bcdf95ab66a7952b8ff105d932e7",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "769c8168f6f380fe72bceed043da8d03a13cb1f6",
      "title": "SecFedNIDS: Robust defense for poisoning attack against federated learning-based network intrusion detection system",
      "abstract": null,
      "year": 2022,
      "venue": "Future generations computer systems",
      "authors": [
        "Zhao Zhang",
        "Yong Zhang",
        "Da Guo",
        "Lei Yao",
        "Zhao Li"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/769c8168f6f380fe72bceed043da8d03a13cb1f6",
      "pdf_url": "",
      "publication_date": "2022-04-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be1116f6c918860adec0973a028839f6d33cbe47",
      "title": "Poisoning-Assisted Property Inference Attack Against Federated Learning",
      "abstract": "Federated learning (FL) has emerged as an ideal privacy-preserving learning technique which can train a global model in a collaborative way while preserving the private data in the local. However, recent advances have demonstrated that FL is still vulnerable to inference attacks, such as reconstruction attack and membership inference. Among these attacks, the property inference attack, aiming to infer properties of the training data that are irrelevant with the learning objective, has not received too much attention while resulting in severe privacy leakage. Existing property inference attack approaches either cannot achieve satisfactory performance when the global model has converged or under dynamic FL where participants can drop in and drop out freely. In this paper, we propose a novel poisoning-assisted property inference attack (PAPI-attack) against FL. The key insight is that there exists underlying discriminative ability in the periodic model updates, which reflects the change of the data distribution, especially the occurrence of the sensitive property. Thus, a binary attack model can be constructed by a malicious participant for inferring the unintended information. More importantly, we present a property-specific poisoning mechanism by modifying the label of training data from the adversary to distort the decision boundary of shared (global) model in FL. Consequently, benign participants are induced to disclose more information about the sensitive property. Extensive experiments on real-world datasets demonstrate that PAPI-attack outperforms the state-of-the-art property inference attacks against FL.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhibo Wang",
        "Yuting Huang",
        "Mengkai Song",
        "Libing Wu",
        "Feng Xue",
        "Kui Ren"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/be1116f6c918860adec0973a028839f6d33cbe47",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "48c3c2d1a9dfe5cf4f3b480aedf546d026533590",
      "title": "Untargeted Poisoning Attack Detection in Federated Learning via Behavior AttestationAl",
      "abstract": "Federated Learning (FL) is a paradigm in Machine Learning (ML) that addresses data privacy, security, access rights and access to heterogeneous information issues by training a global model using distributed nodes. Despite its advantages, there is an increased potential for cyberattacks on FL-based ML techniques that can undermine the benefits. Model-poisoning attacks on FL target the availability of the model. The adversarial objective is to disrupt the training. We propose attestedFL, a defense mechanism that monitors the training of individual nodes through state persistence in order to detect a malicious worker in small to medium federation size. A fine-grained assessment of the history of the worker permits the evaluation of its behavior in time and results in innovative detection strategies. We present three lines of defense that aim at assessing if the worker is reliable by observing if the node is truly training, while advancing towards a goal. Our defense exposes an attacker\u2019s malicious behavior and removes unreliable nodes from the aggregation process so that the FL process converge faster. attestedFL increased the accuracy of the model in different FL settings, under different attacking patterns, and scenarios e.g., attacks performed at different stages of the convergence, colluding attackers, and continuous attacks.",
      "year": 2021,
      "venue": "IEEE Access",
      "authors": [
        "Ranwa Al Mallah",
        "David L\u00f3pez",
        "Godwin Badu-Marfo",
        "B. Farooq"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/48c3c2d1a9dfe5cf4f3b480aedf546d026533590",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10309113.pdf",
      "publication_date": "2021-01-24",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "99fc856dd2800e429919a6a4bffb418cb6c615e1",
      "title": "Exploration and Exploitation in Federated Learning to Exclude Clients with Poisoned Data",
      "abstract": "Federated Learning (FL) is one of the hot research topics, and it utilizes Machine Learning (ML) in a distributed manner without directly accessing private data on clients. How-ever, FL faces many challenges, including the difficulty to obtain high accuracy, high communication cost between clients and the server, and security attacks related to adversarial ML. To tackle these three challenges, we propose an FL algorithm inspired by evolutionary techniques. The proposed algorithm groups clients randomly in many clusters, each with a model selected randomly to explore the performance of different models. The clusters are then trained in a repetitive process where the worst performing cluster is removed in each iteration until one cluster remains. In each iteration, some clients are expelled from clusters either due to using poisoned data or low performance. The surviving clients are exploited in the next iteration. The remaining cluster with surviving clients is then used for training the best FL model (i.e., remaining FL model). Communication cost is reduced since fewer clients are used in the final training of the FL model. To evaluate the performance of the proposed algorithm, we conduct a number of experiments using FEMNIST dataset and compare the result against the random FL algorithm. The experimental results show that the proposed algorithm outperforms the baseline algorithm in terms of accuracy, communication cost, and security.",
      "year": 2022,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Shadha Tabatabai",
        "Ihab Mohammed",
        "Basheer Qolomany",
        "Abdullatif Albaseer",
        "Kashif Ahmad",
        "M. Abdallah",
        "Ala I. Al-Fuqaha"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/99fc856dd2800e429919a6a4bffb418cb6c615e1",
      "pdf_url": "http://arxiv.org/pdf/2204.14020",
      "publication_date": "2022-04-29",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c8cab4284efc395d20ae4502ef81315c4e2f1ad9",
      "title": "Systematic Analysis of Label-flipping Attacks against Federated Learning in Collaborative Intrusion Detection Systems",
      "abstract": "With the emergence of federated learning (FL) and its promise of privacy-preserving knowledge sharing, the field of intrusion detection systems (IDSs) has seen a renewed interest in the development of collaborative models. However, the distributed nature of FL makes it vulnerable to malicious contributions from its participants, including data poisoning attacks. The specific case of label-flipping attacks, where the labels of a subset of the training data are flipped, has been overlooked in the context of IDSs that leverage FL primitives. This study aims to close this gap by providing a systematic and comprehensive analysis of the impact of label-flipping attacks on FL for IDSs. We show that such attacks can still have a significant impact on the performance of FL models, especially targeted ones, depending on parameters and dataset characteristics. Additionally, the provided tools and methodology can be used to extend our findings to other models and datasets, and benchmark the efficiency of existing countermeasures.",
      "year": 2024,
      "venue": "ARES",
      "authors": [
        "L\u00e9o Lavaur",
        "Yann Busnel",
        "F. Autrel"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c8cab4284efc395d20ae4502ef81315c4e2f1ad9",
      "pdf_url": "https://hal.science/hal-04559018/file/main.pdf",
      "publication_date": "2024-07-30",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f7ec7fdb6361e6b5049de1a3f8acd6c5b1f53bc2",
      "title": "Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees",
      "abstract": "Malicious URLs present significant threats to businesses, such as transportation and banking, causing disruptions in business operations. It is essential to identify these URLs; however, existing Machine Learning models are vulnerable to backdoor attacks. These attacks involve manipulating a small portion of the training data labels, such as Label Flipping, which can lead to misclassification. Therefore, it is crucial to incorporate defense mechanisms into machine-learning models to protect against such attacks. The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify machine-learning models against adversarial threats within the machine-learning domain in network security. We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers. We conducted a case study using the Alexa and Phishing Site URL datasets and showed that label-flipping attacks can be addressed using our proposed defense mechanism. Our experimental results prove that the Label Flipping attack achieved an Attack Success Rate between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%.",
      "year": 2024,
      "venue": "IEEE Transactions on Network and Service Management",
      "authors": [
        "Ehsan Nowroozi",
        "Nada Jadalla",
        "Samaneh Ghelichkhani",
        "A. Jolfaei"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/f7ec7fdb6361e6b5049de1a3f8acd6c5b1f53bc2",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a9cad5c2ef74f83bc1cba74dec4d15aa4ec63f04",
      "title": "Edge-Assisted Label-Flipping Attack Detection in Federated Learning",
      "abstract": "Federated Learning (FL) has transformed machine learning by facilitating decentralized, privacy-focused data processing. Despite its advantages, FL remains vulnerable to data poisoning attacks, particularly Label-Flipping Attacks (LFA). In LFA, malicious clients deliberately mislabel local data, causing the global model to misclassify certain classes, thus undermining its integrity. Although centralized detection methods have been explored, there is a notable gap in addressing LFA within the decentralized Client-Edge-Cloud architecture, which is crucial for FL systems. This study introduces an innovative edge-assisted framework for early detection of LFA, crucial for real-time applications. To our knowledge, this is the first study to propose such an edge-assisted LFA detection mechanism. Through detailed conceptual and empirical analyses of LFA behavior, we identified a key characteristic: class-wise accuracy, particularly recall for specific classes, decreases due to label flipping, significantly increases the delta discrepancy with the edge model. Our method remains effective across varying numbers of malicious clients and model sizes, without requiring prior knowledge about the malicious clients. We developed two mitigation strategies: (1) the Zero Tolerance approach, which excludes entire client updates upon detecting adversarial behavior, and (2) the Zero Masking approach, which zeros out gradients for the flipped class while preserving others. This method leverages the direct influence of final layer gradients on class predictions. Extensive evaluation using three benchmark datasets shows that the proposed edge-assisted LFA detection framework outperforms traditional cloud-based methods. We demonstrate its superiority in latency, resource efficiency, and accuracy in detecting malicious clients, outperforming state-of-the-art defenses.",
      "year": 2024,
      "venue": "IEEE Open Journal of the Communications Society",
      "authors": [
        "Nourah S. AlOtaibi",
        "Muhamad Felemban",
        "Sajjad Mahmood"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a9cad5c2ef74f83bc1cba74dec4d15aa4ec63f04",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2f94e4aa47aa2787caf605ab5eafbce73ec89e5e",
      "title": "Active Source Inference Attack Based on Label-Flipping in Federated Learning",
      "abstract": "Federated Learning (FL) is a distributed learning framework that creates a global model by periodically exchanging model parameters through a central server, without directly sharing local data. We introduce a novel Active Source Inference Attack (ASI) aimed at FL, which identifies the originating client of datasets within the FL system. ASI specifically includes a minor label-flipping operation on a small portion of data at the server side to train the global model, while concurrently using a supervised learning model to perform the inference attack. Extensive experiments on multiple datasets confirm the effectiveness of ASI, showing significant enhancements in attack success rate, recall, and precision. Additionally, ASI has minimal impact on the FL model, maintaining high attack stealth.",
      "year": 2024,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Lening Zhang",
        "Hui Xia"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/2f94e4aa47aa2787caf605ab5eafbce73ec89e5e",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6145e47e74195272688db6d08d608df9d098f639",
      "title": "Defending against label-flipping attacks in federated learning systems using uniform manifold approximation and projection",
      "abstract": "The user experience can be greatly improved by using learning models that have been trained using data from mobile devices and other internet of things (IoT) devices. Numerous efforts have been made to implement federated learning (FL) algorithms in order to facilitate the success of machine learning models. Researchers have been working on various privacy-preserving methodologies, such as deep neural networks (DNN), support vector machines (SVM), logistic regression, and gradient boosted decision trees, to support a wider range of machine learning models. The capacity for computing and storage has increased over time, emphasizing the growing significance of data mining in engineering. Artificial intelligence and machine learning have recently achieved remarkable progress. We carried out research on data poisoning attacks in the FL system and proposed defence technique using uniform manifold approximation and projection (UMAP). We compare the efficiency by using UMAP, principal component analysis (PCA), Kernel principal component analysis (KPCA) and k-mean clustering algorithm. We make clear in the paper that UMAP performs better than PCA, KPCA and k-mean, and gives excellent performance in detection and mitigating against data-poisoning attacks.",
      "year": 2024,
      "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)",
      "authors": [
        "Deepak Upreti",
        "Hyunil Kim",
        "Eunmok Yang",
        "Changho Seo"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6145e47e74195272688db6d08d608df9d098f639",
      "pdf_url": "https://ijai.iaescore.com/index.php/IJAI/article/download/22757/13848",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "998733da6d3ba4e9be3b50cf39343e9de47aff9c",
      "title": "Robustness of Quantum Federated Learning (QFL) Against \u201cLabel Flipping Attacks\u201d for Lithography Hotspot Detection in Semiconductor Manufacturing",
      "abstract": "The geographical dispersion of manufacturing units introduces challenges related to latency, network connectivity, data synchronization, communication overhead, scalability, data security, and resource allocation. Manufacturing data often includes sensitive information related to proprietary processes, designs, and technologies. Companies are hesitant to share this data such as lithography hotspot data (LHD) directly due to concerns about privacy and the risk of intellectual property theft. Federated learning provides a solution to address these challenges by allowing companies to collaboratively train models without sharing raw data. It allows companies to adhere to privacy laws while benefiting from shared insights. In recent years, the convergence of quantum machine learning has demonstrated transformative potential by leveraging quantum principles to address complex problems across various industries. In this work, a concept of quantum federated learning consisting of variational quantum circuit is proposed to address the lithography hotspot sharing challenges by providing a privacy-preserving, centralized, and quantum-enhanced framework. It combines the collaborative advantages of federated learning with the unique capabilities of quantum computing, offering a solution that is secure, scalable, and adaptable to the complexities of modern manufacturing environments. The performance evaluations involve scalability and robustness, particularly in the context of label-flipping attacks, which is crucial for validating, optimizing, and enhancing the quantum federated learning framework's applicability to real-world challenges in the manufacturing sector.",
      "year": 2024,
      "venue": "IEEE International Reliability Physics Symposium",
      "authors": [
        "A. Bhatia",
        "S. Kais",
        "M. Alam"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/998733da6d3ba4e9be3b50cf39343e9de47aff9c",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a7509214f416904047cf534343b6712ab74a6746",
      "title": "Detection and Mitigation of Label-Flipping Attacks in Federated Learning Systems with KPCA and K-Means",
      "abstract": "Federated learning is a popular machine-learning technique that is often preferred due to its efficiency and data privacy. However, federated-learning systems face a serious threat of data poisoning that can cause the systems and predictions to fail if not treated in time. This study extends another study of data-poisoning attacks in federated-learning systems conducted by Tolpegin et al. We first investigate the effectiveness of the defense strategy suggested in Tolpegin\u2019s study. Then we propose an improved defense strategy that emphasizes employing KPCA and K-mean clustering. It is demonstrated in this paper that our defense strategy, when combined with improved dimensionality-reduction algorithms, produces better results in defending against data-poisoning attacks in federated-learning systems.",
      "year": 2021,
      "venue": "International Conferences on Dependable Systems and Their Applications",
      "authors": [
        "Dongcheng Li",
        "W. Wong",
        "Wei Wang",
        "Yao Yao",
        "Matthew Chau"
      ],
      "citation_count": 68,
      "url": "https://www.semanticscholar.org/paper/a7509214f416904047cf534343b6712ab74a6746",
      "pdf_url": "",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "81e5423f3a7d31a9abd0966a21eda564339c754d",
      "title": "Collective Data-Sanitization for Preventing Sensitive Information Inference Attacks in Social Networks",
      "abstract": null,
      "year": 2018,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhipeng Cai",
        "Zaobo He",
        "Xin Guan",
        "Yingshu Li"
      ],
      "citation_count": 490,
      "url": "https://www.semanticscholar.org/paper/81e5423f3a7d31a9abd0966a21eda564339c754d",
      "pdf_url": "https://doi.org/10.1109/tdsc.2016.2613521",
      "publication_date": "2018-07-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "174d7917d06f4adfa3d7f0575b1557fb7914d38a",
      "title": "Lossless and robust privacy preservation of association rules in data sanitization",
      "abstract": null,
      "year": 2018,
      "venue": "Cluster Computing",
      "authors": [
        "G. Navale",
        "S. Mali"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/174d7917d06f4adfa3d7f0575b1557fb7914d38a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "692f9d30068c501d35a625121ba7e14b379800ee",
      "title": "A Swarm-based Data Sanitization Algorithm in Privacy-Preserving Data Mining",
      "abstract": "In recent decades, data protection (PPDM), which not only hides information, but also provides information that is useful to make decisions, has become a critical concern. We present a sanitization algorithm with the consideration of four side effects based on multi-objective PSO and hierarchical clustering methods to find optimized solutions for PPDM. Experiments showed that compared to existing approaches, the designed sanitization algorithm based on the hierarchical clustering method achieves satisfactory performance in terms of hiding failure, missing cost, and artificial cost.",
      "year": 2019,
      "venue": "IEEE Congress on Evolutionary Computation",
      "authors": [
        "J. Wu",
        "Chun-Wei Lin",
        "Y. Djenouri",
        "Philippe Fournier-Viger",
        "Yuyu Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/692f9d30068c501d35a625121ba7e14b379800ee",
      "pdf_url": "",
      "publication_date": "2019-06-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c47122ad2907222ba4d927c2394ff6ffb1ba0317",
      "title": "Lclean: A Plausible Approach to Individual Trajectory Data Sanitization",
      "abstract": "In recent years, with the continuous development of significant data industrialization, trajectory data have more and more critical analytical value for urban construction and environmental monitoring. However, the trajectory contains a lot of personal privacy, and rashly publishing trajectory data set will cause serious privacy leakage risk. At present, the privacy protection of trajectory data mainly uses the methods of data anonymity and generalization, without considering the background knowledge of attackers and ignores the risk of adjacent location points may leak sensitive location points. In this paper, based on the above problems, combined with the location correlation of trajectory data, we proposed a plausible replacement method. First, the correlation of trajectory points is proposed to classify the individual trajectories containing sensitive points. Then, according to the relevance of location points and the randomized response mechanism, a reasonable candidate set is selected to replace the sensitive points in the trajectory to satisfy the locally differential privacy. Theoretical and experimental results show that the proposed method not only protects the sensitive information of individuals but also does not affect the overall data distribution.",
      "year": 2018,
      "venue": "IEEE Access",
      "authors": [
        "Qilong Han",
        "Dan. Lu",
        "Kejia Zhang",
        "Xiaojiang Du",
        "M. Guizani"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/c47122ad2907222ba4d927c2394ff6ffb1ba0317",
      "pdf_url": "https://doi.org/10.1109/access.2018.2833163",
      "publication_date": "2018-04-05",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "30ef4f053ffe261d72a9992dff053bd3006ddf9a",
      "title": "Data sanitization in association rule mining based on impact factor",
      "abstract": "Data sanitization process is used to promote the sharing of transactional databases among organizations and businesses, and alleviates concerns for individuals and organizations regarding the disclosure of sensitive patterns. It transforms the source database into a released database so that counterparts cannot discover the sensitive patterns and so data confidentiality is preserved against association rule mining method. This process strongly relies on the minimizing the impact of data sanitization on the data utility by minimizing the number of lost patterns in the form of non-sensitive patterns which are not mined from sanitized database. This study proposes a data sanitization algorithm to hide sensitive patterns in the form of frequent itemsets from the database while controlling the impact of sanitization on the data utility using estimation of impact factor of each modification on non-sensitive itemsets. The proposed algorithm has been compared with Sliding Window size Algorithm (SWA) and Max-Min1 in terms of execution time, data utility and data accuracy. The data accuracy is defined as the ratio of deleted items to the total support values of sensitive itemsets in the source dataset. Experimental results demonstrate that the proposed algorithm outperforms SWA and Max-Min1 in terms of maximizing the data utility and data accuracy and it provides better execution time over SWA and Max-Min1 in high scalability for sensitive itemsets and transactions.",
      "year": 2015,
      "venue": "",
      "authors": [
        "A. Telikani",
        "A. Shahbahrami",
        "Reza Tavoli"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/30ef4f053ffe261d72a9992dff053bd3006ddf9a",
      "pdf_url": "http://jad.shahroodut.ac.ir/article_499_b19c5d39580de5d1c96ed584708a8002.pdf",
      "publication_date": "2015-10-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
      "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
      "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Siwon Kim",
        "Sangdoo Yun",
        "Hwaran Lee",
        "Martin Gubri",
        "Sung-Hoon Yoon",
        "Seong Joon Oh"
      ],
      "citation_count": 162,
      "url": "https://www.semanticscholar.org/paper/b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
      "pdf_url": "https://arxiv.org/pdf/2307.01881",
      "publication_date": "2023-07-04",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f5e020ca9ad8339f1f2026e9a93f1a70da324e2",
      "title": "Analysis of Privacy Leakage in Federated Large Language Models",
      "abstract": "With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking. To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets. Additionally, we conduct thorough experiments to evaluate the privacy leakage of these models when data is protected by state-of-the-art differential privacy (DP) mechanisms.",
      "year": 2024,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Minh N. Vu",
        "Truc Nguyen",
        "Tre' R. Jeter",
        "My T. Thai"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/4f5e020ca9ad8339f1f2026e9a93f1a70da324e2",
      "pdf_url": "",
      "publication_date": "2024-03-02",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c0d198dd9bad1bfaae71a51759eabf6e3e609f9e",
      "title": "MIGAN: A Privacy Leakage Evaluation Scheme for CIoT-Based Federated Learning Users",
      "abstract": "Federated Learning (FL) in Consumer Internet of Things (CIoT) encounters significant privacy security threats when collaborative training Machine Learning models using data distributed among numerous smart CIoT devices. This paper\u2019s objective is to evaluate the potential privacy risks associated with FL within CIoT context, specifically through the lens of passive malicious users. We propose an evaluation scheme (MIGAN) for privacy leakage, which incorporates various privacy inference attacks. First, this paper analyzes the characteristics of privacy inference attacks in FL and summarizes privacy security threats to guide the design of the MIGAN evaluation scheme. Then, to fully utilize the privacy information in FL, this paper proposes faster model inversion and white-box membership inference. Integrating these two modules into the MIGAN framework facilitates fast and high-quality privacy data reconstruction. Finally, this paper constructs the evaluation function to measure the correlation between compromised privacy data and users\u2019 training data. Experiments conducted on the MNIST and CelebA datasets reveal that MIGAN\u2019s data privacy reconstruction outperforms comparable methods. Notably, MIGAN quantifies the likelihood of potential privacy leakage at 14.7% for MNIST and 8.3% for CelebA in FL with 30 clients. These results validates the practicality and relevance of MIGAN in evaluating privacy leakage risks.",
      "year": 2024,
      "venue": "IEEE transactions on consumer electronics",
      "authors": [
        "Shuo Xu",
        "H. Xia",
        "Lijuan Xu",
        "Rui Zhang",
        "Chunqiang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c0d198dd9bad1bfaae71a51759eabf6e3e609f9e",
      "pdf_url": "",
      "publication_date": "2024-02-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e3a1e74d77652ad5567db8451fd3575b0db8e06d",
      "title": "A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency",
      "abstract": "Federated learning (FL) has emerged as a secure paradigm for collaborative training among clients. Without data centralization, FL allows clients to share local information in a privacy-preserving manner. This approach has gained considerable attention, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on FL methods that share model parameters during the training process, while overlooking the possibility of sharing local information in other forms. In this paper, we present a systematic survey from a new perspective of what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. First, we present a new taxonomy of FL methods in terms of three sharing methods, which respectively share model, synthetic data, and knowledge. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms. Third, we conduct extensive experiments to compare the learning performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we identify future research directions and conclude the survey.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jiawei Shao",
        "Zijian Li",
        "Wenqiang Sun",
        "Tailin Zhou",
        "Yuchang Sun",
        "Lumin Liu",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e3a1e74d77652ad5567db8451fd3575b0db8e06d",
      "pdf_url": "https://arxiv.org/pdf/2307.10655",
      "publication_date": "2023-07-20",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c6fbe66f962908a6cf7cd771268d9a161000ce4d",
      "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
      "abstract": "The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.",
      "year": 2023,
      "venue": "Findings",
      "authors": [
        "Hanyin Shao",
        "Jie Huang",
        "Shen Zheng",
        "K. Chang"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c6fbe66f962908a6cf7cd771268d9a161000ce4d",
      "pdf_url": "http://arxiv.org/pdf/2305.12707",
      "publication_date": "2023-05-22",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b348d5e43c32a7a97f75cf7437526e19b6d6eefd",
      "title": "Beyond Class-Level Privacy Leakage: Breaking Record-Level Privacy in Federated Learning",
      "abstract": "Federated learning (FL) enables multiple clients to collaboratively build a global learning model without sharing their own raw data for privacy protection. Unfortunately, recent research still found privacy leakage in FL, especially on image classification tasks, such as the reconstruction of class representatives. Nevertheless, such analysis on image classification tasks is not applicable to uncover the privacy threats against natural language processing (NLP) tasks, whose records composed of sequential texts cannot be grouped as class representatives. The finer (record-level) granularity in NLP tasks not only makes it more challenging to extract individual text records, but also exposes more serious threats. This article presents the first attempt to explore the record-level privacy leakage against NLP tasks in FL. We propose a framework to investigate the exposure of the records of interest in federated aggregations by leveraging the perplexity of language modeling. Through monitoring the exposure patterns, we propose two correlation attacks to identify the corresponding clients when extracting their specific records. Extensive experimental results demonstrate the effectiveness of the proposed attacks. We have also examined several countermeasures and shown that they are ineffective to mitigate such attacks, and hence further research is expected.",
      "year": 2022,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xiaoyong Yuan",
        "Xiyao Ma",
        "Lan Zhang",
        "Yuguang Fang",
        "Dapeng Wu"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/b348d5e43c32a7a97f75cf7437526e19b6d6eefd",
      "pdf_url": "https://doi.org/10.1109/jiot.2021.3089713",
      "publication_date": "2022-02-15",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f3044b037f4ab067ea6e488977c2b8e78b8257f",
      "title": "Quantifying Privacy Leakage in Graph Embedding",
      "abstract": "Graph embeddings have been proposed to map graph data to low dimensional space for downstream processing (e.g., node classification or link prediction). With the increasing collection of personal data, graph embeddings can be trained on private and sensitive data. For the first time, we quantify the privacy leakage in graph embeddings through three inference attacks targeting Graph Neural Networks. Our membership inference attack aims to infer whether a graph node corresponding to an individual user\u2019s data was a member of the model\u2019s private training data or not. We consider a blackbox setting where the adversary exploits the output prediction scores and a whitebox setting where the adversary has also access to the released node embeddings. Our attack provides accuracy up to 28% (blackbox) and 36% (whitebox) beyond the random guess by exploiting the distinguishable footprint between train and test data records left by the graph embedding. In our graph reconstruction attack, the adversary aims to reconstruct the target graph given the corresponding graph embeddings. Here, the adversary can reconstruct the graph with more than 80% of accuracy and infer the link between two nodes with \u223c 30% more accuracy than the random guess. Finally, we propose an attribute inference attack where the adversary aims to infer the sensitive node attributes corresponding to an individual user. We show that the strong correlation between the graph embeddings and node attributes allows the adversary to infer sensitive information (e.g., gender or location).",
      "year": 2020,
      "venue": "International Conference on Mobile and Ubiquitous Systems: Networking and Services",
      "authors": [
        "Vasisht Duddu",
        "A. Boutet",
        "Virat Shejwalkar"
      ],
      "citation_count": 145,
      "url": "https://www.semanticscholar.org/paper/5f3044b037f4ab067ea6e488977c2b8e78b8257f",
      "pdf_url": "https://arxiv.org/pdf/2010.00906",
      "publication_date": "2020-10-02",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2cb10b11951246878ad3fbd26313014e7d55b863",
      "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning",
      "abstract": "Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users.\nTo address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees.\nIn particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over any number of training rounds). \nOur framework also carefully accounts for the fairness and the average number of participating users at each round.\nOur experiments on MNIST, CIFAR-10 and CIFAR-100 datasets in the IID and the non-IID settings demonstrate the performance improvement over the baselines, both in terms of privacy protection and test accuracy.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Jinhyun So",
        "Ramy E. Ali",
        "Basak Guler",
        "Jiantao Jiao",
        "S. Avestimehr"
      ],
      "citation_count": 102,
      "url": "https://www.semanticscholar.org/paper/2cb10b11951246878ad3fbd26313014e7d55b863",
      "pdf_url": "https://doi.org/10.1609/aaai.v37i8.26177",
      "publication_date": "2021-06-07",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cddc77dad0aef234b66e79297bcf39516f8337b3",
      "title": "AttrLeaks on the Edge: Exploiting Information Leakage from Privacy-Preserving Co-inference",
      "abstract": null,
      "year": 2023,
      "venue": "Chinese journal of electronics",
      "authors": [
        "Zhibo Wang",
        "Kai-yan Liu",
        "Jiahui Hu",
        "Ju Ren",
        "Hengchang Guo",
        "Weiting Yuan"
      ],
      "citation_count": 68,
      "url": "https://www.semanticscholar.org/paper/cddc77dad0aef234b66e79297bcf39516f8337b3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9970761/10038784/10038798.pdf",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7086b7e31a4e91af436415f35f4ae6805efc7cdf",
      "title": "Privacy Leakage of Real-World Vertical Federated Learning",
      "abstract": null,
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Haiqin Weng",
        "Juntao Zhang",
        "Feng Xue",
        "Tao Wei",
        "S. Ji",
        "Zhiyuan Zong"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/7086b7e31a4e91af436415f35f4ae6805efc7cdf",
      "pdf_url": "",
      "publication_date": "2020-11-18",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6711f5193c901287e61a264953c4c72946251a65",
      "title": "Awakening the Web's Sleeper Agents: Misusing Service Workers for Privacy Leakage",
      "abstract": ",",
      "year": 2021,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Soroush Karami",
        "Panagiotis Ilia",
        "Jason Polakis"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/6711f5193c901287e61a264953c4c72946251a65",
      "pdf_url": "https://doi.org/10.14722/ndss.2021.23104",
      "publication_date": null,
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "faaee27d0bed50c9bfa97de9369a96b908b9dc42",
      "title": "A Quantitative Metric for Privacy Leakage in Federated Learning",
      "abstract": "In the federated learning system, parameter gradients are shared among participants and the central modulator, while the original data never leave their protected source domain. However, the gradient itself might carry enough information for precise inference of the original data. By reporting their parameter gradients to the central server, client datasets are exposed to inference attacks from adversaries. In this paper, we propose a quantitative metric based on mutual information for clients to evaluate the potential risk of information leakage in their gradients. Mutual information has received increasing attention in the machine learning and data mining community over the past few years. However, existing mutual information estimation methods cannot handle high-dimensional variables. In this paper, we propose a novel method to approximate the mutual information between the high-dimensional gradients and batched input data. Experimental results show that the proposed metric reliably reflect the extent of information leakage in federated learning. In addition, using the proposed metric, we investigate the influential factors of risk level. It is proven that, the risk of information leakage is related to the status of the task model, as well as the inherent data distribution.",
      "year": 2021,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Y. Liu",
        "Xinghua Zhu",
        "Jianzong Wang",
        "Jing Xiao"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/faaee27d0bed50c9bfa97de9369a96b908b9dc42",
      "pdf_url": "https://arxiv.org/pdf/2102.13472",
      "publication_date": "2021-02-24",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7dbc8ba8cd6c179852480c903c9970ab9ef9c5e7",
      "title": "Variational Leakage: The Role of Information Complexity in Privacy Leakage",
      "abstract": "We study the role of information complexity in privacy leakage about an attribute of an adversary's interest, which is not known a priori to the system designer. Considering the supervised representation learning setup and using neural networks to parameterize the variational bounds of information quantities, we study the impact of the following factors on the amount of information leakage: information complexity regularizer weight, latent space dimension, the cardinalities of the known utility and unknown sensitive attribute sets, the correlation between utility and sensitive attributes, and a potential bias in a sensitive attribute of adversary's interest. We conduct extensive experiments on Colored-MNIST and CelebA datasets to evaluate the effect of information complexity on the amount of intrinsic leakage.",
      "year": 2021,
      "venue": "WiseML@WiSec",
      "authors": [
        "A. A. Atashin",
        "Behrooz Razeghi",
        "Deniz Gunduz",
        "S. Voloshynovskiy"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/7dbc8ba8cd6c179852480c903c9970ab9ef9c5e7",
      "pdf_url": "https://arxiv.org/pdf/2106.02818",
      "publication_date": "2021-06-05",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e1eb9a172c6c285180d1cc1ed626b80c4e5fedd4",
      "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
      "abstract": "Autonomous AI agents that can follow instructions and perform complex multi-step tasks have tremendous potential to boost human productivity. However, to perform many of these tasks, the agents need access to personal information from their users, raising the question of whether they are capable of using it appropriately. In this work, we introduce a new benchmark AgentDAM that measures if AI web-navigation agents follow the privacy principle of ``data minimization''. For the purposes of our benchmark, data minimization means that the agent uses a piece of potentially sensitive information only if it is ``necessary''to complete a particular task. Our benchmark simulates realistic web interaction scenarios end-to-end and is adaptable to all existing web navigation agents. We use AgentDAM to evaluate how well AI agents built on top of GPT-4, Llama-3 and Claude can limit processing of potentially private information, and show that they are prone to inadvertent use of unnecessary sensitive information. We also propose a prompting-based defense that reduces information leakage, and demonstrate that our end-to-end benchmarking provides a more realistic measure than probing LLMs about privacy. Our results highlight that further research is needed to develop AI agents that can prioritize data minimization at inference time.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Arman Zharmagambetov",
        "Chuan Guo",
        "Ivan Evtimov",
        "Maya Pavlova",
        "Ruslan Salakhutdinov",
        "Kamalika Chaudhuri"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/e1eb9a172c6c285180d1cc1ed626b80c4e5fedd4",
      "pdf_url": "",
      "publication_date": "2025-03-12",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "859219f193880c8b597f6fcd34fbc012575033dc",
      "title": "Beyond Model-Level Membership Privacy Leakage: an Adversarial Approach in Federated Learning",
      "abstract": "With the rise of privacy concerns in traditional centralized machine learning services, the federated learning, which incorporates multiple participants to train a global model across their localized training data, has lately received signifi-cant attention in both industry and academia. However, recent researches reveal the inherent vulnerabilities of the federated learning for the membership inference attacks that the adversary could infer whether a given data record belongs to the model\u2019s training set. Although the state-of-the-art techniques could successfully deduce the membership information from the centralized machine learning models, it is still challenging to infer the membership to a more confined level, user-level. In this paper, We propose a novel user-level inference attack mechanism in federated learning. Specifically, we first give a comprehensive analysis of active and targeted membership inference attacks in the context of the federated learning. Then, by considering a more complicated scenario that the adversary can only passively observe the updating models from different iterations, we incorporate the generative adversarial networks into our method, which can enrich the training set for the final membership inference model. The extensive experimental results demonstrate the effectiveness of our proposed attacking approach in the case of single-label and multi-label.",
      "year": 2020,
      "venue": "International Conference on Computer Communications and Networks",
      "authors": [
        "Jiale Chen",
        "Jiale Zhang",
        "Yanchao Zhao",
        "Hao Han",
        "Kun Zhu",
        "Bing Chen"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/859219f193880c8b597f6fcd34fbc012575033dc",
      "pdf_url": "",
      "publication_date": "2020-08-01",
      "keywords_matched": [
        "privacy leakage",
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e74635db3e988425a97b017c68a9f2eaf35adc3b",
      "title": "KART: Parameterization of Privacy Leakage Scenarios from Pre-trained Language Models",
      "abstract": "For the safe sharing pre-trained language models, no guidelines exist at present owing to the difficulty in estimating the upper bound of the risk of privacy leakage. One problem is that previous studies have assessed the risk for different real-world privacy leakage scenarios and attack methods, which reduces the portability of the findings. To tackle this problem, we represent complex real-world privacy leakage scenarios under a universal parameterization, \\textit{Knowledge, Anonymization, Resource, and Target} (KART). KART parameterization has two merits: (i) it clarifies the definition of privacy leakage in each experiment and (ii) it improves the comparability of the findings of risk assessments. We show that previous studies can be simply reviewed by parameterizing the scenarios with KART. We also demonstrate privacy risk assessments in different scenarios under the same attack method, which suggests that KART helps approximate the upper bound of risk under a specific attack or scenario. We believe that KART helps integrate past and future findings on privacy risk and will contribute to a standard for sharing language models.",
      "year": 2020,
      "venue": "",
      "authors": [
        "Yuta Nakamura",
        "S. Hanaoka",
        "Y. Nomura",
        "N. Hayashi",
        "O. Abe",
        "Shuntaro Yada",
        "Shoko Wakamiya",
        "Eiji Aramaki The University of Tokyo",
        "The Department of Radiology",
        "The University of Tokyo Hospital",
        "The Department of Radiology",
        "Preventive Medicine",
        "C. University",
        "Nara Institute of Science",
        "Technology"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e74635db3e988425a97b017c68a9f2eaf35adc3b",
      "pdf_url": "",
      "publication_date": "2020-12-31",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04b74ff9161bdd3a89a7b08216927f957e1eda80",
      "title": "Information-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning",
      "abstract": "Machine learning algorithms operating on mobile networks can be characterized into three different categories. First is the classical situation in which the end-user devices send their data to a central server where this data is used to train a model. Second is the distributed setting in which each device trains its own model and send its model parameters to a central server where these model parameters are aggregated to create one final model. Third is the federated learning setting in which, at any given time t, a certain number of active end users train with their own local data along with feedback provided by the central server and then send their newly estimated model parameters to the central server. The server, then, aggregates these new parameters, updates its own model, and feeds the updated parameters back to all the end users, continuing this process until it converges.The main objective of this work is to provide an information-theoretic framework for all of the aforementioned learning paradigms. Moreover, using the provided framework, we develop upper and lower bounds on the generalization error together with bounds on the privacy leakage in the classical, distributed and federated learning settings.",
      "year": 2020,
      "venue": "International Workshop on Signal Processing Advances in Wireless Communications",
      "authors": [
        "Semih Yagli",
        "Alex Dytso",
        "H. Poor"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/04b74ff9161bdd3a89a7b08216927f957e1eda80",
      "pdf_url": "http://arxiv.org/pdf/2005.02503",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8aeab1c4a0ecf3dbcb277fa23b42c9012dfa8ba6",
      "title": "Mind your privacy: Privacy leakage through BCI applications using machine learning methods",
      "abstract": null,
      "year": 2020,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Ofir Landau",
        "Aviad Cohen",
        "S. Gordon",
        "N. Nissim"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/8aeab1c4a0ecf3dbcb277fa23b42c9012dfa8ba6",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa92979942c6d68897bddfc95dfef5ebd3ee9f09",
      "title": "Debiasing Training Data for Inductive Expert System Construction",
      "abstract": null,
      "year": 2001,
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": [
        "V. Mookerjee"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/fa92979942c6d68897bddfc95dfef5ebd3ee9f09",
      "pdf_url": "",
      "publication_date": "2001-05-01",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3330d839fff9f49c1aa4d3f59cabc8fb1e3c4f14",
      "title": "Targeted Training Data Extraction\u2014Neighborhood Comparison-Based Membership Inference Attacks in Large Language Models",
      "abstract": "A large language model refers to a deep learning model characterized by extensive parameters and pretraining on a large-scale corpus, utilized for processing natural language text and generating high-quality text output. The increasing deployment of large language models has brought significant attention to their associated privacy and security issues. Recent experiments have demonstrated that training data can be extracted from these models due to their memory effect. Initially, research on large language model training data extraction focused primarily on non-targeted methods. However, following the introduction of targeted training data extraction by Carlini et al., prefix-based extraction methods to generate suffixes have garnered considerable interest, although current extraction precision remains low. This paper focuses on the targeted extraction of training data, employing various methods to enhance the precision and speed of the extraction process. Building on the work of Yu et al., we conduct a comprehensive analysis of the impact of different suffix generation methods on the precision of suffix generation. Additionally, we examine the quality and diversity of text generated by various suffix generation strategies. The study also applies membership inference attacks based on neighborhood comparison to the extraction of training data in large language models, conducting thorough evaluations and comparisons. The effectiveness of membership inference attacks in extracting training data from large language models is assessed, and the performance of different membership inference attacks is compared. Hyperparameter tuning is performed on multiple parameters to enhance the extraction of training data. Experimental results indicate that the proposed method significantly improves extraction precision compared to previous approaches.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Huan Xu",
        "Zhanhao Zhang",
        "Xiaodong Yu",
        "Yingbo Wu",
        "Zhiyong Zha",
        "Bo Xu",
        "Wenfeng Xu",
        "Menglan Hu",
        "Kai Peng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3330d839fff9f49c1aa4d3f59cabc8fb1e3c4f14",
      "pdf_url": "https://doi.org/10.3390/app14167118",
      "publication_date": "2024-08-14",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8dc9a864c19413234023f7ead304827f16d41b8a",
      "title": "Efficient training data extraction framework for intrusion detection systems",
      "abstract": null,
      "year": 2015,
      "venue": "International Conference on Network of the Future",
      "authors": [
        "Abdelhamid Makiou",
        "A. Serhrouchni"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8dc9a864c19413234023f7ead304827f16d41b8a",
      "pdf_url": "https://hal.archives-ouvertes.fr/hal-01358229/file/07333298.pdf",
      "publication_date": "2015-09-01",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "74b0abc4ef9fd993abdf57ef4d387b773cc8d162",
      "title": "Training Data Extraction Attack from Large Language Models in Federated Learning Through Frequent Sequence Mining",
      "abstract": null,
      "year": 0,
      "venue": "",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei",
        "Binyuan Huang",
        "Luo Hui",
        "Mei Ji",
        "Junyang Li",
        "Lin",
        "Runji Lin",
        "Dayiheng Liu",
        "Gao Liu",
        "Chengqiang Lu",
        "guang Wu",
        "Benfeng Xu",
        "Jin Xu",
        "An Yang",
        "Hao Yang",
        "H. Brown",
        "Katherine Lee Fatemehsadat",
        "Reza Mireshghallah",
        "Shokri Florian",
        "Tram\u00e8r",
        "Nicholas Carlini",
        "Daphne Ippolito",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Wonsuk Oh",
        "Girish N. Nadkarni. 2023",
        "Federated",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Benjamin Chess",
        "Chester Cho",
        "Hyung Casey Chu",
        "Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Yunxing Dai",
        "Tarun Goel",
        "Gabriel Gogineni",
        "Rapha Goh",
        "Jonathan Gontijo-663 Lopes",
        "Morgan Gordon",
        "Scott Grafstein",
        "Ryan Gray",
        "Joshua Greene",
        "Shixiang Shane Gross",
        "Yufei Gu",
        "Chris Guo",
        "Jesse Hallacy",
        "Jeff Han",
        "Harris Yuchen",
        "Mike He",
        "Johannes Heaton",
        "C. Heidecke",
        "Alan Hesse",
        "W. Hickey",
        "Peter Hickey",
        "Hoeschele Brandon",
        "Kenny Houghton",
        "Shengli Hsu",
        "Xin Hu",
        "Joost Hu",
        "S. Huizinga",
        "Shawn Jain",
        "Jain Joanne",
        "Angela Jang",
        "Roger Jiang",
        "Haozhun Jiang",
        "Denny Jin",
        "Shino Jin",
        "Billie Jomoto",
        "Hee-woo Jonn",
        "Tomer Jun",
        "\u0141ukasz Kaftan",
        "Ali Kaiser",
        "Ingmar Ka-673 mali",
        "Kanitscheider",
        "Nitish Shirish",
        "Keskar Tabarak",
        "Logan Khan",
        "J. Kilpatrick",
        "Kim Christina",
        "Yongjik Kim",
        "J. Kim",
        "Jamie Kirch-676 ner",
        "Matt Kiros",
        "Daniel Knight",
        "Kokotajlo \u0141ukasz",
        "A. Kondraciuk",
        "Aris Kondrich",
        "Kyle Kon-678 stantinidis",
        "G. Kosic",
        "Vishal Krueger",
        "Michael Kuo",
        "Ikai Lampe",
        "Teddy Lan",
        "Jan Lee",
        "J. Leike",
        "Daniel Leung",
        "Chak Ming Levy",
        "Li Rachel",
        "Molly Lim",
        "Stephanie Lin",
        "Mateusz Lin",
        "Theresa Litwin",
        "Ryan Lopez",
        "Patricia Lowe",
        "Lue Anna",
        "Kim Makanju",
        "S. Malfacini",
        "Todor Manning",
        "Yaniv Markov",
        "B. Markovski",
        "Katie Martin",
        "Andrew Mayer",
        "Bob Mayne",
        "S. McGrew",
        "Chris-tine McKinney",
        "Paul McLeavey",
        "M. Jake",
        "David McNeil",
        "Aalok Medina",
        "Jacob Mehta",
        "Luke Menick",
        "An-drey Metz",
        "Pamela Mishchenko",
        "Vinnie Mishkin",
        "Evan Monaco",
        "Daniel P Morikawa",
        "T. Mossing",
        "Mira Mu",
        "Oleg Murati",
        "D. Murk",
        "Ashvin M\u00e9ly",
        "Reiichiro Nair",
        "Rajeev Nakano",
        "Nayak Arvind",
        "R. Neelakantan",
        "Hyeonwoo Ngo",
        "Noh Long",
        "Cullen Ouyang",
        "Jakub O\u2019Keefe",
        "Alex Pachocki",
        "J. Paino",
        "Ashley Palermo",
        "Giambat-tista Pantuliano",
        "Joel Parascandolo",
        "Emy Parish",
        "Alex Parparita",
        "Mikhail Passos",
        "Andrew Pavlov",
        "Adam Peng",
        "Filipe Perel-697 man",
        "de Avila Belbute",
        "Michael Peres",
        "Petrov Henrique",
        "Pond\u00e9",
        "Michael Oliveira Pinto",
        "Michelle Pokrass",
        "Vitchyr H. Pong",
        "Tolly Pow-672 ell",
        "Alethea Power",
        "Boris Power",
        "Elizabeth Proehl",
        "Raul Puri",
        "Alec Radford",
        "Jack W. Rae",
        "Aditya Ramesh",
        "Carl Ross",
        "Bob Rotsted",
        "Henri Roussez",
        "Nick Ry-704",
        "Mario D. Saltarelli",
        "Ted Sanders",
        "Shibani Santurkar",
        "Jessica Sherbakov",
        "Sarah Shieh",
        "Shoker",
        "Pranav",
        "Szymon Shyam",
        "Eric Sidor",
        "Mad-die Sigler",
        "Simens",
        "P. Tillet",
        "Amin Tootoonchian",
        "Elizabeth Tseng",
        "Preston Tuggle",
        "Nick Turley",
        "Jerry Tworek",
        "Juan Fe-715",
        "Cer\u00f3n Uribe",
        "Andrea Vallone",
        "Arun Vijayvergiya",
        "CJ Weinmann",
        "Akila Welihinda",
        "Peter Welinder",
        "Jiaxuan Weng",
        "Lilian Weng",
        "Matt Wiethoff",
        "Dave Willner",
        "Lauren Workman",
        "Sherwin Wu",
        "Jeff Wu",
        "Kai Wu",
        "Tao Xiao",
        "Sarah Xu",
        "Kevin Yoo",
        "Qim-695 ing Yu",
        "Wojciech Yuan",
        "Rowan Zaremba",
        "Zellers",
        "Chong",
        "Marvin Zhang",
        "Shengjia Zhang",
        "Tianhao Zhao",
        "Jacob Pfau",
        "Alex Infanger",
        "A. Sheshadri",
        "Ayush",
        "Md. Rafi",
        "U. Rashid",
        "Vishnu Asutosh Dasu",
        "Kang Gu",
        "Avi Schwarzschild",
        "Zhili Feng",
        "Pratyush Maini",
        "Minh N. Vu",
        "Truc D. T. Nguyen",
        "Tre' R. Jeter",
        "Yijia Xiao",
        "Yiqiao Jin",
        "Yushi Bai",
        "Xianjun Yue Wu",
        "Xiao Yang",
        "Wenchao Luo",
        "Xujiang Yu",
        "Yanchi",
        "Haifeng Liu",
        "Wei Chen",
        "Wang Wei",
        "Cheng",
        "Guangwei Liu",
        "Guosheng Ai",
        "Haizhou Dong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/74b0abc4ef9fd993abdf57ef4d387b773cc8d162",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ef2e105835fe555300e67fe9f8389bf869adfe88",
      "title": "Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge",
      "abstract": "Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications. The construction of data extraction attacks is challenging, current attacks are quite inefficient, and there exists a significant gap in the extraction capabilities of untargeted attacks and memorization. Thus, targeted attacks are proposed, which identify if a given sample from the training data, is extractable from a model. In this work, we apply a targeted data extraction attack to the SATML2023 Language Model Training Data Extraction Challenge. We apply a two-step approach. In the first step, we maximise the recall of the model and are able to extract the suffix for 69% of the samples. In the second step, we use a classifier-based Membership Inference Attack on the generations. Our AutoSklearn classifier achieves a precision of 0.841. The full approach reaches a score of 0.405 recall at a 10% false positive rate, which is an improvement of 34% over the baseline of 0.301.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Ali Al-Kaswan",
        "M. Izadi",
        "A. Deursen"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/ef2e105835fe555300e67fe9f8389bf869adfe88",
      "pdf_url": "http://arxiv.org/pdf/2302.07735",
      "publication_date": "2023-02-13",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8ebd76125296bc3837f0fb73f2df0b6ad7c3b457",
      "title": "Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction",
      "abstract": "Cybersecurity has emerged as a critical global concern. Intrusion Detection Systems (IDS) play a critical role in protecting interconnected networks by detecting malicious actors and activities. Machine Learning (ML)-based behavior analysis within the IDS has considerable potential for detecting dynamic cyber threats, identifying abnormalities, and identifying malicious conduct within the network. However, as the number of data grows, dimension reduction becomes an increasingly difficult task when training ML models. Addressing this, our paper introduces a novel ML-based network intrusion detection model that uses Random Oversampling (RO) to address data imbalance and Stacking Feature Embedding based on clustering results, as well as Principal Component Analysis (PCA) for dimension reduction and is specifically designed for large and imbalanced datasets. This model\u2019s performance is carefully evaluated using three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and CIC-IDS-2018. On the UNSW-NB15 dataset, our trials show that the RF and ET models achieve accuracy rates of 99.59% and 99.95%, respectively. Furthermore, using the CIC-IDS2017 dataset, DT, RF, and ET models reach 99.99% accuracy, while DT and RF models obtain 99.94% accuracy on CIC-IDS2018. These performance results continuously outperform the state-of-art, indicating significant progress in the field of network intrusion detection. This achievement demonstrates the efficacy of the suggested methodology, which can be used practically to accurately monitor and identify network traffic intrusions, thereby blocking possible threats.",
      "year": 2024,
      "venue": "Journal of Big Data",
      "authors": [
        "Md. Alamin Talukder",
        "Md. Manowarul Islam",
        "Md. Ashraf Uddin",
        "Khondokar Fida Hasan",
        "Selina Sharmin",
        "S. Alyami",
        "M. A. Moni"
      ],
      "citation_count": 180,
      "url": "https://www.semanticscholar.org/paper/8ebd76125296bc3837f0fb73f2df0b6ad7c3b457",
      "pdf_url": "https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-024-00886-w",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
      "year": 2016,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "R. Shokri",
        "M. Stronati",
        "Congzheng Song",
        "Vitaly Shmatikov"
      ],
      "citation_count": 4754,
      "url": "https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
      "pdf_url": "https://arxiv.org/pdf/1610.05820",
      "publication_date": "2016-10-18",
      "keywords_matched": [
        "membership inference",
        "membership privacy",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54d7ae7cfee56b0e19fd42c45d365f760a41794d",
      "title": "Membership Inference Attacks From First Principles",
      "abstract": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nicholas Carlini",
        "Steve Chien",
        "Milad Nasr",
        "Shuang Song",
        "A. Terzis",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 896,
      "url": "https://www.semanticscholar.org/paper/54d7ae7cfee56b0e19fd42c45d365f760a41794d",
      "pdf_url": "https://arxiv.org/pdf/2112.03570",
      "publication_date": "2021-12-07",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f16c669c126c800dac38d445ddd178ffb9af5b7a",
      "title": "Do Membership Inference Attacks Work on Large Language Models?",
      "abstract": "Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs). We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing MIAs, supporting future work.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Michael Duan",
        "Anshuman Suri",
        "Niloofar Mireshghallah",
        "Sewon Min",
        "Weijia Shi",
        "Luke S. Zettlemoyer",
        "Yulia Tsvetkov",
        "Yejin Choi",
        "David Evans",
        "Hanna Hajishirzi"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/f16c669c126c800dac38d445ddd178ffb9af5b7a",
      "pdf_url": "",
      "publication_date": "2024-02-12",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bc942c52f9da27dfff30e3fa68bc9726e9548888",
      "title": "Blind Baselines Beat Membership Inference Attacks for Foundation Models",
      "abstract": "Membership inference (MI) attacks try to determine if a data sample was used to train a machine learning model. For foundation models trained on unknown Web data, MI attacks are often used to detect copyrighted training materials, measure test set contamination, or audit machine unlearning. Unfortunately, we find that evaluations of MI attacks for foundation models are flawed, because they sample members and non-members from different distributions. For 8 pub-lished MI evaluation datasets, we show that blind attacks-that distinguish the member and non-member distributions without looking at any trained model-outperform state-of-the-art MI attacks. Existing evaluations thus tell us nothing about membership leakage of a foundation model's training data.",
      "year": 2024,
      "venue": "2025 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Debeshee Das",
        "Jie Zhang",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/bc942c52f9da27dfff30e3fa68bc9726e9548888",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "21dc5aa608721f7e8a09e7dd9de8e48a71d9f0c9",
      "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
      "abstract": "Retrieval Augmented Generation (RAG) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the RAG system, an attack known as a Membership Inference Attack (MIA). Despite the significance of this threat, MIAs against RAG systems have yet remained under-explored. This study addresses this gap by introducing an efficient and easy-to-use method for conducting MIA against RAG systems. We demonstrate the effectiveness of our attack using two benchmark datasets and multiple generative models, showing that the membership of a document in the retrieval database can be efficiently determined through the creation of an appropriate prompt in both black-box and gray-box settings. Moreover, we introduce an initial defense strategy based on adding instructions to the RAG template, which shows high effectiveness for some datasets and models. Our findings highlight the importance of implementing security countermeasures in deployed RAG systems and developing more advanced defenses to protect the privacy and security of retrieval databases.",
      "year": 2024,
      "venue": "International Conference on Information Systems Security and Privacy",
      "authors": [
        "Maya Anderson",
        "Guy Amit",
        "Abigail Goldsteen"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/21dc5aa608721f7e8a09e7dd9de8e48a71d9f0c9",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "membership inference",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2237d4ef1aaaf411b4e3d9dc04a4d5ac1df4b11d",
      "title": "ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods",
      "abstract": "The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs\u2019 pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs\u2019 behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Roy Xie",
        "Junlin Wang",
        "Ruomin Huang",
        "Minxing Zhang",
        "Rong Ge",
        "Jian Pei",
        "Neil Zhenqiang Gong",
        "Bhuwan Dhingra"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/2237d4ef1aaaf411b4e3d9dc04a4d5ac1df4b11d",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d18c790f1ba81b7f51a17142a4b2d129c373035e",
      "title": "Position: Membership Inference Attacks Cannot Prove That a Model was Trained on Your Data",
      "abstract": "We consider the problem of a training data proof, where a data creator or owner wants to demonstrate to a third party that some machine learning model was trained on their data. Training data proofs play a key role in recent lawsuits against foundation models trained on Web-scale data. Many prior works suggest to instantiate training data proofs using membership inference attacks. We argue that this approach is statistically unsound: to provide convincing evidence, the data creator needs to demonstrate that their attack has a low false positive rate, i.e., that the attack's output is unlikely under the null hypothesis that the model was not trained on the target data. Yet, sampling from this null hypothesis is impossible, as we do not know the exact contents of the training set, nor can we (efficiently) retrain a large foundation model. We conclude by offering three paths forward, by showing that membership inference on special canary data, watermarked training data, and data extraction attacks can be used to create sound training data proofs.",
      "year": 2024,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Jie Zhang",
        "Debeshee Das",
        "Gautam Kamath",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d18c790f1ba81b7f51a17142a4b2d129c373035e",
      "pdf_url": "",
      "publication_date": "2024-09-29",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9016617a9176942aa532d762461c89d3e605ccf5",
      "title": "Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack",
      "abstract": "Explainability is increasingly recognized as an enabling technology for the broader adoption of machine learning (ML), particularly for safety-critical applications. This has given rise to explainable ML, which seeks to enhance the explainability of neural networks through the use of explanators. Yet, the pursuit for better explainability inadvertently leads to increased security and privacy risks. While there has been considerable research into the security risks of explainable ML, its potential privacy risks remain under-explored.To bridge this gap, we present a systematic study of privacy risks in explainable ML through the lens of membership inference. Building on the observation that, besides the accuracy of the model, robustness also exhibits observable differences among member samples and non-member samples, we develop a new membership inference attack. This attack extracts additional membership features from changes in model confidence under different levels of perturbations guided by the importance highlighted by the attribution maps in the explanators. Intuitively, perturbing important features generally results in a bigger loss in confidence for member samples. Using the member-non-member differences in both model performance and robustness, an attack model is trained to distinguish the membership. We evaluated our approach with seven popular explanators across various benchmark models and datasets. Our attack demonstrates there is non-trivial privacy leakage in current explainable ML methods. Furthermore, such leakage issue persists even if the attacker lacks the knowledge of training datasets or target model architectures. Lastly, we also found existing model and output-based defense mechanisms are not effective in mitigating this new attack.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Han Liu",
        "Yuhao Wu",
        "Zhiyuan Yu",
        "Ning Zhang"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/9016617a9176942aa532d762461c89d3e605ccf5",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "membership inference",
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c438dcb2b3d7e1a9cee8d9b1035b96c39b53941c",
      "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)",
      "abstract": "Whether Large Language models (LLMs) memorize their training data and what this means, from measuring privacy leakage to detecting copyright violations, has become a rapidly growing area of research. In the last few months, more than 10 new methods have been proposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed-but randomized-records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization raises concerns of a distribution shift between members and non-members. In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered. We then quantify distribution shifts present in 6 datasets used in the literature, ranging from books to papers using a model-less bag of word classifier and compare the model-less results to the MIA. Our analysis shows that all of these datasets constructed post-hoc suffer from strong distribution shifts. These shifts invalidate the claims of LLMs memorizing strongly in real-world scenarios and, potentially, also the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. In the second part of this work, we introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized finetuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide the development of MIA methods and study LLM memorization. We conclude with an overview of recommended approaches to benchmark sequence-level and document-level MIAs against LLMs.",
      "year": 2024,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Matthieu Meeus",
        "Igor Shilov",
        "Shubham Jain",
        "Manuel Faysse",
        "Marek Rei",
        "Y. Montjoye"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/c438dcb2b3d7e1a9cee8d9b1035b96c39b53941c",
      "pdf_url": "",
      "publication_date": "2024-06-25",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c154acf1e1518d78cd6df13f5272832b53844ed4",
      "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
      "abstract": "Most existing membership inference attacks (MIAs) utilize metrics (e.g., loss) calculated on the model's final state, while recent advanced attacks leverage metrics computed at various stages, including both intermediate and final stages, throughout the model training. Nevertheless, these attacks often process multiple intermediate states of the metric independently, ignoring their time-dependent patterns. Consequently, they struggle to effectively distinguish between members and non-members who exhibit similar metric values, particularly resulting in a high false-positive rate. In this study, we delve deeper into the new membership signals in the black-box scenario. We identify a new, more integrated membership signal: the Pattern of Metric Sequence, derived from the various stages of model training. We contend that current signals provide only partial perspectives of this new signal: the new one encompasses both the model's multiple intermediate and final states, with a greater emphasis on temporal patterns among them. Building upon this signal, we introduce a novel attack method called Sequential-metric based Membership Inference Attack (SeqMIA). Specifically, we utilize knowledge distillation to obtain a set of distilled models representing various stages of the target model's training. We then assess multiple metrics on these distilled models in chronological order, creating distilled metric sequence. We finally integrate distilled multi-metric sequences as a sequential multiformat and employ an attention-based RNN attack model for inference. Empirical results show SeqMIA outperforms all baselines, especially can achieve an order of magnitude improvement in terms of TPR @ 0.1% FPR. Furthermore, we delve into the reasons why this signal contributes to SeqMIA's high attack performance, and assess various defense mechanisms against SeqMIA.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Hao Li",
        "Zheng Li",
        "Siyuan Wu",
        "Chengrui Hu",
        "Yutong Ye",
        "Min Zhang",
        "Dengguo Feng",
        "Yang Zhang"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/c154acf1e1518d78cd6df13f5272832b53844ed4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690335",
      "publication_date": "2024-07-21",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20dd6bc45dc186f7e5f24678f69e856413ca9499",
      "title": "Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy",
      "abstract": "Text-to-image diffusion models have achieved tremendous success in the field of controllable image generation, while also coming along with issues of privacy leakage and data copyrights. Membership inference arises in these contexts as a potential auditing method for detecting unauthorized data usage. While some efforts have been made on diffusion models, they are not applicable to text-to-image diffusion models due to the high computation overhead and enhanced generalization capabilities. In this paper, we first identify a conditional overfitting phenomenon in text-to-image diffusion models, indicating that these models tend to overfit the conditional distribution of images given the corresponding text rather than the marginal distribution of images only. Based on this observation, we derive an analytical indicator, namely Conditional Likelihood Discrepancy (CLiD), to perform membership inference, which reduces the stochasticity in estimating memorization of individual samples. Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and dataset scales. Additionally, our method shows superior resistance to overfitting mitigation strategies, such as early stopping and data augmentation.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Shengfang Zhai",
        "Huanran Chen",
        "Yinpeng Dong",
        "Jiajun Li",
        "Qingni Shen",
        "Yansong Gao",
        "Hang Su",
        "Yang Liu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/20dd6bc45dc186f7e5f24678f69e856413ca9499",
      "pdf_url": "",
      "publication_date": "2024-05-23",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "99784f2814cbab4a9e3fb263843c3fa91b78a370",
      "title": "Is Difficulty Calibration All We Need? Towards More Practical Membership Inference Attacks",
      "abstract": "The vulnerability of machine learning models to Membership Inference Attacks (MIAs) has garnered considerable attention in recent years. These attacks determine whether a data sample belongs to the model's training set or not. Recent research has focused on reference-based attacks, which leverage difficulty calibration with independently trained reference models. While empirical studies have demonstrated its effectiveness, there is a notable gap in our understanding of the circumstances under which it succeeds or fails. In this paper, we take a further step towards a deeper understanding of the role of difficulty calibration. Our observations reveal inherent limitations in calibration methods, leading to the misclassification of non-members and suboptimal performance, particularly on high-loss samples. We further identify that these errors stem from an imperfect sampling of the potential distribution and a strong dependence of membership scores on the model parameters. By shedding light on these issues, we propose RAPID: a query-efficient and computation-efficient MIA that directly Re-leverAges the original membershiP scores to mItigate the errors in Difficulty calibration. Our experimental results, spanning 9 datasets and 5 model architectures, demonstrate that RAPID outperforms previous state-of-the-art attacks (e.g., LiRA and Canary offline) across different metrics while remaining computationally efficient. Our observations and analysis challenge the current de facto paradigm of difficulty calibration in high-precision inference, encouraging greater attention to the persistent risks posed by MIAs in more practical scenarios.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yu He",
        "Boheng Li",
        "Yao Wang",
        "Mengda Yang",
        "Juan Wang",
        "Hongxin Hu",
        "Xingyu Zhao"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/99784f2814cbab4a9e3fb263843c3fa91b78a370",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690316",
      "publication_date": "2024-08-31",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f7ef135cf14f35da75311ecde77800737a65510d",
      "title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models",
      "abstract": "Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable\"cheating.\"In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Haritz Puerto",
        "Martin Gubri",
        "Sangdoo Yun",
        "Seong Joon Oh"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/f7ef135cf14f35da75311ecde77800737a65510d",
      "pdf_url": "",
      "publication_date": "2024-10-31",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "852f307fb0cd2ccd3edd4ee00dc60af64655841c",
      "title": "Mask-based Membership Inference Attacks for Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has been an effective approach to mitigate hallucinations in large language models (LLMs) by incorporating up-to-date and domain-specific knowledge. Recently, there has been a trend of storing up-to-date or copyrighted data in RAG knowledge databases instead of using it for LLM training. This practice has raised concerns about Membership Inference Attacks (MIAs), which aim to detect if a specific target document is stored in the RAG system's knowledge database so as to protect the rights of data producers. While research has focused on enhancing the trustworthiness of RAG systems, existing MIAs for RAG systems remain largely insufficient. Previous work either relies solely on the RAG system's judgment or is easily influenced by other documents or the LLM's internal knowledge, which is unreliable and lacks explainability. To address these limitations, we propose a Mask-Based Membership Inference Attacks (MBA) framework. Our framework first employs a masking algorithm that effectively masks a certain number of words in the target document. The masked text is then used to prompt the RAG system, and the RAG system is required to predict the mask values. If the target document appears in the knowledge database, the masked text will retrieve the complete target document as context, allowing for accurate mask prediction. Finally, we adopt a simple yet effective threshold-based method to infer the membership of target document by analyzing the accuracy of mask prediction. Our mask-based approach is more document-specific, making the RAG system's generation less susceptible to distractions from other documents or the LLM's internal knowledge. Extensive experiments demonstrate the effectiveness of our approach compared to existing baseline models.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Mingrui Liu",
        "Sixiao Zhang",
        "Cheng Long"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/852f307fb0cd2ccd3edd4ee00dc60af64655841c",
      "pdf_url": "",
      "publication_date": "2024-10-26",
      "keywords_matched": [
        "membership inference",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33b81b3b25c84674936e83bf91bef7d5af870ee2",
      "title": "Sampling-based Pseudo-Likelihood for Membership Inference Attacks",
      "abstract": "Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text. This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data. Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention. Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs. However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user. In this study, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for MIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks. The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data. Even without likelihoods, SaMIA performed on par with existing likelihood-based methods.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Masahiro Kaneko",
        "Youmi Ma",
        "Yuki Wata",
        "Naoaki Okazaki"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/33b81b3b25c84674936e83bf91bef7d5af870ee2",
      "pdf_url": "",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "99b364eaf55295f53f6afaf6fce7c61dcd567eb9",
      "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG\u2019s external database, with the aim of determining whether a given sample is part of the RAG\u2019s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S2MIA, a Membership Inference Attack that utilizes the Semantic Similarity between a given sample and the content generated by the RAG system. With our proposed S2MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experimental results demonstrate that S2MIA outperforms five existing MIAs, even when the system is protected by three representative defenses.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Yuying Li",
        "Gaoyang Liu",
        "Chen Wang",
        "Yang Yang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/99b364eaf55295f53f6afaf6fce7c61dcd567eb9",
      "pdf_url": "",
      "publication_date": "2024-06-27",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8cf8d81ce2a52dab7aa185941e631b3c706fd845",
      "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
      "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.",
      "year": 2024,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Qizhang Feng",
        "Siva Rajesh Kasa",
        "Hyokun Yun",
        "C. Teo",
        "S. Bodapati"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/8cf8d81ce2a52dab7aa185941e631b3c706fd845",
      "pdf_url": "",
      "publication_date": "2024-07-08",
      "keywords_matched": [
        "membership inference",
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "00664799a2debc6a0b67d128daf88c0cedad21a8",
      "title": "Membership Inference Attacks against Large Vision-Language Models",
      "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\\'enyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhan Li",
        "Yongtao Wu",
        "Yihang Chen",
        "Francesco Tonin",
        "El\u00edas Abad-Rocamora",
        "V. Cevher"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/00664799a2debc6a0b67d128daf88c0cedad21a8",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "38c14931cf5dc7781b4f24af15e8938dfb898317",
      "title": "Are Diffusion Models Vulnerable to Membership Inference Attacks?",
      "abstract": "Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Jinhao Duan",
        "Fei Kong",
        "Shiqi Wang",
        "Xiaoshuang Shi",
        "Kaidi Xu"
      ],
      "citation_count": 154,
      "url": "https://www.semanticscholar.org/paper/38c14931cf5dc7781b4f24af15e8938dfb898317",
      "pdf_url": "http://arxiv.org/pdf/2302.01316",
      "publication_date": "2023-02-02",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cb754310302086dfbbcd098263200e2a03f65874",
      "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
      "abstract": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Justus Mattern",
        "Fatemehsadat Mireshghallah",
        "Zhijing Jin",
        "B. Scholkopf",
        "Mrinmaya Sachan",
        "Taylor Berg-Kirkpatrick"
      ],
      "citation_count": 262,
      "url": "https://www.semanticscholar.org/paper/cb754310302086dfbbcd098263200e2a03f65874",
      "pdf_url": "https://arxiv.org/pdf/2305.18462",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e61a616ef46444f1943763daa004af2079b98b5b",
      "title": "Membership Inference Attacks and Defenses in Federated Learning: A Survey",
      "abstract": "Federated learning is a decentralized machine learning approach where clients train models locally and share model updates to develop a global model. This enables low-resource devices to collaboratively build a high-quality model without requiring direct access to the raw training data. However, despite only sharing model updates, federated learning still faces several privacy vulnerabilities. One of the key threats is membership inference attacks, which target clients\u2019 privacy by determining whether a specific example is part of the training set. These attacks can compromise sensitive information in real-world applications, such as medical diagnoses within a healthcare system. Although there has been extensive research on membership inference attacks, a comprehensive and up-to-date survey specifically focused on it within federated learning is still absent. To fill this gap, we categorize and summarize membership inference attacks and their corresponding defense strategies based on their characteristics in this setting. We introduce a unique taxonomy of existing attack research and provide a systematic overview of various countermeasures. For these studies, we thoroughly analyze the strengths and weaknesses of different approaches. Finally, we identify and discuss key future research directions for readers interested in advancing the field.",
      "year": 2024,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Li Bai",
        "Haibo Hu",
        "Qingqing Ye",
        "Haoyang Li",
        "Leixia Wang",
        "Jianliang Xu"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/e61a616ef46444f1943763daa004af2079b98b5b",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3704633",
      "publication_date": "2024-11-14",
      "keywords_matched": [
        "membership inference",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6e8e815f6f0c5370e86651ed959f1c77657d607b",
      "title": "Membership Inference Attacks Against In-Context Learning",
      "abstract": "Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Rui Wen",
        "Zheng Li",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/6e8e815f6f0c5370e86651ed959f1c77657d607b",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690306",
      "publication_date": "2024-09-02",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "25de9e07d1b1d0f09995347bb6dd193e1ef52e91",
      "title": "Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models",
      "abstract": "Machine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner\u2019s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model\u2019s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model\u2019s training data in an unsupervised manner, without any priori knowledge about the target model\u2019s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Gaoyang Liu",
        "Tianlong Xu",
        "Rui Zhang",
        "Zixiong Wang",
        "Chen Wang",
        "Ling Liu"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/25de9e07d1b1d0f09995347bb6dd193e1ef52e91",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership inference",
        "membership information",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af3056123820635635479f65b90022021823354f",
      "title": "Noisy Neighbors: Efficient membership inference attacks against LLMs",
      "abstract": "The potential of transformer-based LLMs risks being hindered by privacy concerns due to their reliance on extensive datasets, possibly including sensitive information. Regulatory measures like GDPR and CCPA call for using robust auditing tools to address potential privacy issues, with Membership Inference Attacks (MIA) being the primary method for assessing LLMs\u2019 privacy risks. Differently from traditional MIA approaches, often requiring computationally intensive training of additional models, this paper introduces an efficient methodology that generates noisy neighbors for a target sample by adding stochastic noise in the embedding space, requiring operating the target model in inference mode only. Our findings demonstrate that this approach closely matches the effectiveness of employing shadow models, showing its usability in practical privacy auditing scenarios.",
      "year": 2024,
      "venue": "PRIVATENLP",
      "authors": [
        "Filippo Galli",
        "Luca Melis",
        "Tommaso Cucinotta"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/af3056123820635635479f65b90022021823354f",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a68fccc152d238f62848de1be8522ccd71137ac0",
      "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
      "abstract": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
      "year": 2018,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "A. Salem",
        "Yang Zhang",
        "Mathias Humbert",
        "Mario Fritz",
        "M. Backes"
      ],
      "citation_count": 1062,
      "url": "https://www.semanticscholar.org/paper/a68fccc152d238f62848de1be8522ccd71137ac0",
      "pdf_url": "https://doi.org/10.14722/ndss.2019.23119",
      "publication_date": "2018-06-04",
      "keywords_matched": [
        "membership inference",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a9619d3286cef8bf57d291da5fb0007db6dc2e44",
      "title": "Semantic Membership Inference Attack against Large Language Models",
      "abstract": "Membership Inference Attacks (MIAs) determine whether a specific data point was included in the training set of a target model. In this paper, we introduce the Semantic Membership Inference Attack (SMIA), a novel approach that enhances MIA performance by leveraging the semantic content of inputs and their perturbations. SMIA trains a neural network to analyze the target model's behavior on perturbed inputs, effectively capturing variations in output probability distributions between members and non-members. We conduct comprehensive evaluations on the Pythia and GPT-Neo model families using the Wikipedia dataset. Our results show that SMIA significantly outperforms existing MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B, compared to 58.90% by the second-best attack.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hamid Mozaffari",
        "Virendra J. Marathe"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/a9619d3286cef8bf57d291da5fb0007db6dc2e44",
      "pdf_url": "",
      "publication_date": "2024-06-14",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40edc8e3cd3c4d410f656fbae079fc4b6b655b0f",
      "title": "A survey on membership inference attacks and defenses in machine learning",
      "abstract": null,
      "year": 2024,
      "venue": "Journal of Information and Intelligence",
      "authors": [
        "Jun Niu",
        "Peng Liu",
        "Xiaoyan Zhu",
        "Kuo Shen",
        "Yuecong Wang",
        "Haotian Chi",
        "Yulong Shen",
        "Xiaohong Jiang",
        "Jianfeng Ma",
        "Yuqing Zhang"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40edc8e3cd3c4d410f656fbae079fc4b6b655b0f",
      "pdf_url": "https://doi.org/10.1016/j.jiixd.2024.02.001",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a14d6a842ac94cc561f39a5cab6ec133862d5f42",
      "title": "Label-Only Membership Inference Attacks",
      "abstract": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Christopher A. Choquette-Choo",
        "Florian Tram\u00e8r",
        "Nicholas Carlini",
        "Nicolas Papernot"
      ],
      "citation_count": 592,
      "url": "https://www.semanticscholar.org/paper/a14d6a842ac94cc561f39a5cab6ec133862d5f42",
      "pdf_url": "",
      "publication_date": "2020-07-28",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b1856118892a5e4258629a1ac420074d55393f5e",
      "title": "Context-Aware Membership Inference Attacks against Pre-trained Large Language Models",
      "abstract": "Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs) aim at determining if a data point was part of the model's training set. Prior MIAs that are built for classification models fail at LLMs, due to ignoring the generative nature of LLMs across token sequences. In this paper, we present a novel attack on pre-trained LLMs that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior approaches, revealing context-dependent memorization patterns in pre-trained LLMs.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hongyan Chang",
        "A. Shamsabadi",
        "Kleomenis Katevas",
        "Hamed Haddadi",
        "Reza Shokri"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/b1856118892a5e4258629a1ac420074d55393f5e",
      "pdf_url": "",
      "publication_date": "2024-09-11",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e22cc31d0cfeb3dcc12d053e991d0322cf717be6",
      "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
      "abstract": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensivehypothesis testing framework that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explainwhy different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform adifferential analysis between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of thePrivacy Meter software tool.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiayuan Ye",
        "Aadyaa Maddi",
        "S. K. Murakonda",
        "R. Shokri"
      ],
      "citation_count": 325,
      "url": "https://www.semanticscholar.org/paper/e22cc31d0cfeb3dcc12d053e991d0322cf717be6",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560675",
      "publication_date": "2021-11-18",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b48a8c226efb52844c9b97309a654a383f33b823",
      "title": "Interaction-level Membership Inference Attack Against Federated Recommender Systems",
      "abstract": "The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy concerns in personalized recommendation services. In FedRecs, users\u2019 attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it is considered a fairly secure approach to protect user privacy. As a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to user attribute inference attacks, highlighting the privacy concerns of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs. Specifically, we perform the first systematic study on interaction-level membership inference attacks on FedRecs. An interaction-level membership inference attacker is first designed, and then the classical privacy protection mechanism, Local Differential Privacy (LDP), is adopted to defend against the membership inference attack. Unfortunately, the empirical analysis shows that LDP is not effective against such new attacks unless the recommendation performance is largely compromised. To mitigate the interaction-level membership attack threats, we design a simple yet effective defense method to significantly reduce the attacker\u2019s inference accuracy without losing recommendation performance. Extensive experiments are conducted with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on three real-world recommendation datasets (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the experimental results show the effectiveness of our solutions.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Wei Yuan",
        "Chao-Peng Yang",
        "Q. Nguyen",
        "Li-zhen Cui",
        "Tieke He",
        "Hongzhi Yin"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/b48a8c226efb52844c9b97309a654a383f33b823",
      "pdf_url": "https://arxiv.org/pdf/2301.10964",
      "publication_date": "2023-01-26",
      "keywords_matched": [
        "membership inference",
        "membership information",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05de8d869abe43d166e591f83bc8dd050a6b05ce",
      "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
      "abstract": "Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Guy Amit",
        "Abigail Goldsteen",
        "Ariel Farkash"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/05de8d869abe43d166e591f83bc8dd050a6b05ce",
      "pdf_url": "",
      "publication_date": "2024-03-13",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "71622c5fb66d949e6b8c0610d321c1b310946d29",
      "title": "A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability",
      "abstract": "Self-supervised learning shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision. In this paper, we aim to perform membership inference on visual self-supervised models in a more realistic setting: self-supervised training method and details are unknown for an adversary when attacking as he usually faces a black-box system in practice. In this setting, considering that self-supervised model could be trained by completely different self-supervised paradigms, e.g., masked image modeling and contrastive learning, with complex training details, we propose a unified membership inference method called PartCrop. It is motivated by the shared part-aware capability among models and stronger part response on the training data. Specifically, PartCrop crops parts of objects in an image to query responses with the image in representation space. We conduct extensive attacks on self-supervised models with different training protocols and structures using three widely used image datasets. The results verify the effectiveness and generalization of PartCrop. Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and differential privacy, and propose a tailored method called shrinking crop scale range. The defense experiments indicate that all of them are effective. Our code is available at https://github.com/JiePKU/PartCrop.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jie Zhu",
        "Jirong Zha",
        "Ding Li",
        "Leye Wang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/71622c5fb66d949e6b8c0610d321c1b310946d29",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690202",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c74f7c2e906f1f12707735d40d810472406d5a84",
      "title": "Low-Cost High-Power Membership Inference Attacks",
      "abstract": "Membership inference attacks aim to detect if a particular data point was used in training a model. We design a novel statistical test to perform robust membership inference attacks (RMIA) with low computational overhead. We achieve this by a fine-grained modeling of the null hypothesis in our likelihood ratio tests, and effectively leveraging both reference models and reference population data samples. RMIA has superior test power compared with prior methods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0). Under computational constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack (e.g., data distribution), our method performs exceptionally well, unlike prior attacks that approach random guessing. RMIA lays the groundwork for practical yet accurate data privacy risk assessment in machine learning.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Sajjad Zarifzadeh",
        "Philippe Liu",
        "Reza Shokri"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/c74f7c2e906f1f12707735d40d810472406d5a84",
      "pdf_url": "",
      "publication_date": "2023-12-06",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5b5e8b284cdf9d6474447c09a42292513338935a",
      "title": "Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
      "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs. Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Wenjie Fu",
        "Huandong Wang",
        "Chen Gao",
        "Guanghua Liu",
        "Yong Li",
        "Tao Jiang"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/5b5e8b284cdf9d6474447c09a42292513338935a",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "495086152953cb3ef08a83a399230141cb31b0a3",
      "title": "Membership Inference Attacks against Diffusion Models",
      "abstract": "Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.",
      "year": 2023,
      "venue": "2023 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Tomoya Matsumoto",
        "Takayuki Miura",
        "Naoto Yanai"
      ],
      "citation_count": 83,
      "url": "https://www.semanticscholar.org/paper/495086152953cb3ef08a83a399230141cb31b0a3",
      "pdf_url": "https://arxiv.org/pdf/2302.03262",
      "publication_date": "2023-02-07",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85b6d229b5ee21840116d9b0520ede1e73c25174",
      "title": "Scalable Membership Inference Attacks via Quantile Regression",
      "abstract": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Mart\u00edn Bertr\u00e1n",
        "Shuai Tang",
        "Michael Kearns",
        "Jamie Morgenstern",
        "Aaron Roth",
        "Zhiwei Steven Wu"
      ],
      "citation_count": 68,
      "url": "https://www.semanticscholar.org/paper/85b6d229b5ee21840116d9b0520ede1e73c25174",
      "pdf_url": "https://arxiv.org/pdf/2307.03694",
      "publication_date": "2023-07-07",
      "keywords_matched": [
        "membership inference",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "de1f64522b5594a9f572a92f8816cb1f36adcbac",
      "title": "Membership Inference Attacks against Synthetic Data through Overfitting Detection",
      "abstract": "Data is the foundation of most science. Unfortunately, sharing data can be obstructed by the risk of violating data privacy, impeding research in fields like healthcare. Synthetic data is a potential solution. It aims to generate data that has the same distribution as the original data, but that does not disclose information about individuals. Membership Inference Attacks (MIAs) are a common privacy attack, in which the attacker attempts to determine whether a particular real sample was used for training of the model. Previous works that propose MIAs against generative models either display low performance -- giving the false impression that data is highly private -- or need to assume access to internal generative model parameters -- a relatively low-risk scenario, as the data publisher often only releases synthetic data, not the model. In this work we argue for a realistic MIA setting that assumes the attacker has some knowledge of the underlying data distribution. We propose DOMIAS, a density-based MIA model that aims to infer membership by targeting local overfitting of the generative model. Experimentally we show that DOMIAS is significantly more successful at MIA than previous work, especially at attacking uncommon samples. The latter is disconcerting since these samples may correspond to underrepresented groups. We also demonstrate how DOMIAS' MIA performance score provides an interpretable metric for privacy, giving data publishers a new tool for achieving the desired privacy-utility trade-off in their synthetic data.",
      "year": 2023,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "B. V. Breugel",
        "Hao Sun",
        "Zhaozhi Qian",
        "M. Schaar"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/de1f64522b5594a9f572a92f8816cb1f36adcbac",
      "pdf_url": "http://arxiv.org/pdf/2302.12580",
      "publication_date": "2023-02-24",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "62749fbe7bfb0cd3abe0a8897efd3989ef867941",
      "title": "Membership Inference Attacks Against Machine Learning Models via Prediction Sensitivity",
      "abstract": "Machine learning (ML) has achieved huge success in recent years, but is also vulnerable to various attacks. In this article, we concentrate on membership inference attacks and propose Aster, which merely requires the target model's black-box API and a data sample to determine whether this sample was used to train the given ML model or not. The key idea of Aster is that the training data of a fully trained ML model usually has lower prediction sensitivities compared with that of the non-training data (i.e., testing data). Less sensitivity means that when perturbing a training sample's feature value in the corresponding feature space, the prediction of the perturbed sample obtained from the target model tends to be consistent with the original prediction. In this article, we quantify the prediction sensitivity with the Jacobian matrix which could reflect the relationship between each feature's perturbation and the corresponding prediction's change. Then we regard the samples with a lower as training data. Aster can breach the membership privacy of the target model's training data with no prior knowledge about the target model or its training data. The experiment results on four datasets show that our method outperforms three state-of-the-art inference attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Lan Liu",
        "Yi Wang",
        "Gaoyang Liu",
        "Kai Peng",
        "Chen Wang"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/62749fbe7bfb0cd3abe0a8897efd3989ef867941",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bc9705f7b05bc415a0c6514c4873750ed1fdbe36",
      "title": "Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models",
      "abstract": "With large language models (LLMs) poised to become embedded in our daily lives, questions are starting to be raised about the data they learned from. These questions range from potential bias or misinformation LLMs could retain from their training data to questions of copyright and fair use of human-generated text. However, while these questions emerge, developers of the recent state-of-the-art LLMs become increasingly reluctant to disclose details on their training corpus. We here introduce the task of document-level membership inference for real-world LLMs, i.e. inferring whether the LLM has seen a given document during training or not. First, we propose a procedure for the development and evaluation of document-level membership inference for LLMs by leveraging commonly used data sources for training and the model release date. We then propose a practical, black-box method to predict document-level membership and instantiate it on OpenLLaMA-7B with both books and academic papers. We show our methodology to perform very well, reaching an AUC of 0.856 for books and 0.678 for papers. We then show our approach to outperform the sentence-level membership inference attacks used in the privacy literature for the document-level membership task. We further evaluate whether smaller models might be less sensitive to document-level inference and show OpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach. Finally, we consider two mitigation strategies and find the AUC to slowly decrease when only partial documents are considered but to remain fairly high when the model precision is reduced. Taken together, our results show that accurate document-level membership can be inferred for LLMs, increasing the transparency of technology poised to change our lives.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Matthieu Meeus",
        "Shubham Jain",
        "Marek Rei",
        "Yves-Alexandre de Montjoye"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/bc9705f7b05bc415a0c6514c4873750ed1fdbe36",
      "pdf_url": "",
      "publication_date": "2023-10-23",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ecb0cb2fceb713d8d760baf979fc9f3190f965db",
      "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization",
      "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. {Code is available at \\url{https://github.com/kong13661/PIA}}.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Fei Kong",
        "Jinhao Duan",
        "Ruipeng Ma",
        "Hengtao Shen",
        "Xiaofeng Zhu",
        "Xiaoshuang Shi",
        "Kaidi Xu"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/ecb0cb2fceb713d8d760baf979fc9f3190f965db",
      "pdf_url": "https://arxiv.org/pdf/2305.18355",
      "publication_date": "2023-05-26",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7cb2c4103247646f6e1e1902b69f57cee9aa260d",
      "title": "Membership Inference of Diffusion Models",
      "abstract": ". Recent years have witnessed the tremendous success of diffusion models in data synthesis. However, when di\ufb00usion models are applied to sensitive data, they also give rise to severe privacy concerns. In this paper, we systematically present the \ufb01rst study about membership inference attacks against di\ufb00usion models, which aims to infer whether a sample was used to train the model. Two attack methods are proposed, namely loss-based and likelihood-based attacks. Our attack methods are evaluated on several state-of-the-art di\ufb00usion models, over di\ufb00erent datasets in relation to privacy-sensitive data. Extensive experimental evaluations show that our attacks can achieve remarkable performance. Furthermore, we exhaustively investigate various factors which can af-fect attack performance. Finally, we also evaluate the performance of our attack methods on di\ufb00usion models trained with di\ufb00erential privacy.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/7cb2c4103247646f6e1e1902b69f57cee9aa260d",
      "pdf_url": "http://arxiv.org/pdf/2301.09956",
      "publication_date": "2023-01-24",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0e62d8661b8a599bca55d233c31e7b20d8c258c",
      "title": "Secure Aggregation is Not Private Against Membership Inference Attacks",
      "abstract": "Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round. Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension. Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning.",
      "year": 2024,
      "venue": "ECML/PKDD",
      "authors": [
        "K. Ngo",
        "Johan \u00d6stman",
        "Giuseppe Durisi",
        "Alexandre Graell i Amat"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/d0e62d8661b8a599bca55d233c31e7b20d8c258c",
      "pdf_url": "",
      "publication_date": "2024-03-26",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0aee8500fd2bb33c69ee3b05a20a75fbfc5bb3aa",
      "title": "Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study",
      "abstract": "Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks.This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that leverages ground-truth non-members (e.g., obtained by using the publication date of a target model and the timestamps of the open data) to further enhance the attack. Our evaluation shows that CLIP models are susceptible to our attack strategies, with our simple baseline achieving over 75% membership identification accuracy. Furthermore, our enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of 17% and being at least 7X more effective at low false-positive rates. These findings highlight the importance of protecting the privacy of multi-modal foundational models, which were previously assumed to be less susceptible to MIAs due to less overfitting. Our code is available at https://github.com/ruoxi-jia-group/CLIP-MIA.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Myeongseob Ko",
        "Ming Jin",
        "Chenguang Wang",
        "Ruoxi Jia"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/0aee8500fd2bb33c69ee3b05a20a75fbfc5bb3aa",
      "pdf_url": "https://arxiv.org/pdf/2310.00108",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04a880d1f01e773e3f739d27d388f6100874933a",
      "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
      "abstract": "Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Jan Dubinski",
        "A. Kowalczuk",
        "Stanislaw Pawlak",
        "Przemyslaw Rokita",
        "Tomasz Trzci'nski",
        "P. Morawiecki"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/04a880d1f01e773e3f739d27d388f6100874933a",
      "pdf_url": "https://arxiv.org/pdf/2306.12983",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "60e53d9f34e71cbbfe87ab75ca3a909233a6b356",
      "title": "White-box Membership Inference Attacks against Diffusion Models",
      "abstract": "Diffusion models have begun to overshadow GANs and other generative models in industrial applications due to their superior image generation performance. The complex architecture of these models furnishes an extensive array of attack features. In light of this, we aim to design membership inference attacks (MIAs) catered to diffusion models. We first conduct an exhaustive analysis of existing MIAs on diffusion models, taking into account factors such as black-box/white-box models and the selection of attack features. We found that white-box attacks are highly applicable in real-world scenarios, and the most effective attacks presently are white-box. Departing from earlier research, which employs model loss as the attack feature for white-box MIAs, we employ model gradients in our attack, leveraging the fact that these gradients provide a more profound understanding of model responses to various samples. We subject these models to rigorous testing across a range of parameters, including training steps, timestep sampling frequency, diffusion steps, and data variance. Across all experimental settings, our method consistently demonstrated near-flawless attack performance, with attack success rate approaching 100% and attack AUCROC near 1.0. We also evaluated our attack against common defense mechanisms, and observed our attacks continue to exhibit commendable performance.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Yan Pang",
        "Tianhao Wang",
        "Xu Kang",
        "Mengdi Huai",
        "Yang Zhang"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/60e53d9f34e71cbbfe87ab75ca3a909233a6b356",
      "pdf_url": "https://arxiv.org/pdf/2308.06405",
      "publication_date": "2023-08-11",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c06e364790020996f4b605ee44812ebdc759d389",
      "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion Models",
      "abstract": "With the rapid advancement of diffusion-based image-generative models, the quality of generated images has become increasingly photorealistic. Moreover, with the release of high-quality pre-trained image-generative models, a growing number of users are downloading these pre-trained models to fine-tune them with downstream datasets for various image-generation tasks. However, employing such powerful pre-trained models in downstream tasks presents significant privacy leakage risks. In this paper, we propose the first reconstruction-based membership inference attack framework, tailored for recent diffusion models, and in the more stringent black-box access setting. Considering four distinct attack scenarios and three types of attacks, this framework is capable of targeting any popular conditional generator model, achieving high precision, evidenced by an impressive AUC of $0.95$.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Yan Pang",
        "Tianhao Wang"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/c06e364790020996f4b605ee44812ebdc759d389",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
      "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrey V. Galichin",
        "Mikhail Aleksandrovich Pautov",
        "Alexey Zhavoronkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e2c0c0bce9320ca14b6e936e24375f1682d1fb97",
      "title": "Range Membership Inference Attacks",
      "abstract": "Machine learning models can leak private information about their training data. The standard methods to measure this privacy risk, based on membership inference attacks (MIAs), only check if a given data point exactly matches a training point, neglecting the potential of similar or partially overlapping memorized data revealing the same private information. To address this issue, we introduce the class of range membership inference attacks (RaMIAs), testing if the model was trained on any data in a specified range (defined based on the semantics of privacy). We formulate the RaMIAs game and design a principled statistical test for its composite hypotheses. We show that RaMIAs can capture privacy loss more accurately and comprehensively than MIAs on various types of data, such as tabular, image, and language. RaMIA paves the way for more comprehensive and meaningful privacy auditing of machine learning algorithms.",
      "year": 2024,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Jiashu Tao",
        "Reza Shokri"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e2c0c0bce9320ca14b6e936e24375f1682d1fb97",
      "pdf_url": "",
      "publication_date": "2024-08-09",
      "keywords_matched": [
        "membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e5663313b95f3abe96188aa1d53135ad29ddfa1",
      "title": "Machine Learning with Membership Privacy using Adversarial Regularization",
      "abstract": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%).",
      "year": 2018,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Milad Nasr",
        "R. Shokri",
        "Amir Houmansadr"
      ],
      "citation_count": 516,
      "url": "https://www.semanticscholar.org/paper/0e5663313b95f3abe96188aa1d53135ad29ddfa1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3243734.3243855",
      "publication_date": "2018-01-15",
      "keywords_matched": [
        "membership privacy",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a63aaaf7a50a3db87ac218ddb4dba6937f841abc",
      "title": "Secure Decentralized Aggregation to Prevent Membership Privacy Leakage in Edge-Based Federated Learning",
      "abstract": "Federated Learning (FL) is a machine learning approach that enables multiple users to share their local models for the aggregation of a global model, protecting data privacy by avoiding the sharing of raw data. However, frequent parameter sharing between users and the aggregator can incur high risk of membership privacy leakage. In this paper, we propose LiPFed, a computationally lightweight privacy preserving FL scheme using secure decentralized aggregation for edge networks. Under this scheme, we ensure privacy preservation on the aggregation side, and promote lightweight computation on the user side. By incorporating blockchain and additive secret sharing algorithm, we effectively protect the membership privacy of both local models and global models. Furthermore, the secure decentralized aggregation mechanism safeguards against potential compromises of the aggregator. Meanwhile, smart contract is introduced to identify malicious models uploaded by edge nodes and return trustworthy global models to users. Rigorous security analysis shows the effectiveness of this scheme in privacy preservation. Extensive experiments verify that LiPFed outperforms the state-of-the-art schemes in terms of training efficiency, model accuracy, and privacy preservation.",
      "year": 2024,
      "venue": "IEEE Transactions on Network Science and Engineering",
      "authors": [
        "Meng Shen",
        "Jing Wang",
        "Jie Zhang",
        "Qinglin Zhao",
        "Bohan Peng",
        "Tong Wu",
        "Liehuang Zhu",
        "Ke Xu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/a63aaaf7a50a3db87ac218ddb4dba6937f841abc",
      "pdf_url": "",
      "publication_date": "2024-05-01",
      "keywords_matched": [
        "membership privacy",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6b411ea66c700a1b2e37366ceedc9360a8b22895",
      "title": "Membership Privacy Evaluation in Deep Spiking Neural Networks",
      "abstract": "Artificial Neural Networks (ANNs), commonly mimicking neurons with non-linear functions to output floating-point numbers, consistently receive the same signals of a data point during its forward time. Unlike ANNs, Spiking Neural Networks (SNNs) get various input signals in the forward time of a data point and simulate neurons in a biologically plausible way, i.e., producing a spike (a binary value) if the accumulated membrane potential of a neuron is larger than a threshold. Even though ANNs have achieved remarkable success in multiple tasks, e.g., face recognition and object detection, SNNs have recently obtained attention due to their low power consumption, fast inference, and event-driven properties. While privacy threats against ANNs are widely explored, much less work has been done on SNNs. For instance, it is well-known that ANNs are vulnerable to the Membership Inference Attack (MIA), but whether the same applies to SNNs is not explored. In this paper, we evaluate the membership privacy of SNNs by considering eight MIAs, seven of which are inspired by MIAs against ANNs. Our evaluation results show that SNNs are more vulnerable (maximum 10% higher in terms of balanced attack accuracy) than ANNs when both are trained with neuromorphic datasets (with time dimension). On the other hand, when training ANNs or SNNs with static datasets (without time dimension), the vulnerability depends on the dataset used. If we convert ANNs trained with static datasets to SNNs, the accuracy of MIAs drops (maximum 11.5% with a reduction of 7.6% on the test accuracy of the target model). Next, we explore the impact factors of MIAs on SNNs by conducting a hyperparameter study. Finally, we show that the basic data augmentation method for static data and two recent data augmentation methods for neuromorphic data can considerably (maximum reduction of 25.7%) decrease MIAs' performance on SNNs.",
      "year": 2024,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Jiaxin Li",
        "Gorka Abad",
        "S. Picek",
        "Mauro Conti"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/6b411ea66c700a1b2e37366ceedc9360a8b22895",
      "pdf_url": "",
      "publication_date": "2024-09-28",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "371dd00ba604fe34c8deffca5181871328906af9",
      "title": "SEDMA: Self-Distillation with Model Aggregation for Membership Privacy",
      "abstract": "Membership inference attacks (MIAs) are important measures to evaluate potential risks of privacy leakage from machine learning (ML) models. State-of-the-art MIA defenses have achieved favorable privacy-utility trade-offs using knowledge distillation on split training datasets. However, such defenses increase computational costs as a large number of the ML models must be trained on the split datasets. In this study, we proposed a new MIA defense, called SEDMA, based on self-distillation using model aggregation to mitigate the MIAs, inspired by the model parameter averaging as used in federated learning. The key idea of SEDMA is to split the training dataset into several parts and aggregate multiple ML models trained on each split for self-distillation. The intuitive explanation of SEDMA is that model aggregation prevents model over-fitting by smoothing information related to the training data among the multiple ML models and preserving the model utility, such as in federated learning. Through our experiments on major benchmark datasets (Purchase100, Texas100, and CIFAR100), we show that SEDMA outperforms state-of-the-art MIA defenses in terms of membership privacy (MIA accuracy), model accuracy, and computational costs. Specifically, SEDMA incurs at most approximately 3 - 5% model accuracy drop, while achieving the lowest MIA accuracy in state-of-the-art empirical MIA defenses. For computational costs, SEDMA takes significantly less processing time than a defense with the state-of-the-art privacy-utility trade-offs in previous defenses. SEDMA achieves both favorable privacy-utility trade-offs and low computational costs.",
      "year": 2024,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Tsunato Nakai",
        "Ye Wang",
        "Kota Yoshida",
        "Takeshi Fujino"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/371dd00ba604fe34c8deffca5181871328906af9",
      "pdf_url": "https://petsymposium.org/popets/2024/popets-2024-0029.pdf",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abcf2d721678b041373be7d27f664590cfe08e24",
      "title": "Regularization Mixup Adversarial Training: A Defense Strategy for Membership Privacy with Model Availability Assurance",
      "abstract": "Neural network models face two highly destructive threats in real-world applications: membership inference attacks (MIAs) and adversarial attacks (AAs). One compromises the model's confidentiality, leading to membership privacy breaches, while the other disrupts the model's availability, rendering it dysfunctional. Recent work has shown that adversarial examples can pose dual threats to neural network models, both MIAs and AAs, so that these two different types of attacks have a certain degree of intersection. However, existing defense methods typically focus on defending against one of the two attacks and cannot simultaneously address both. To address the above issues, we propose a defense method called regularization mixup adversarial training (RMAT), aiming to protect model membership privacy while ensuring model availability. The core idea of RMAT is to use a continuous augmentation strategy during training to generate mixup adversarial examples, weakening attackers' understanding of membership data, and updating the model with a new membership-weighted loss strategy to improve its generalization ability for handling unknown samples. This approach can further protect the model's membership privacy and ensure model availability. Experimental results demonstrate that compared to single defense strategies, RMAT can simultaneously protect the model's membership privacy and availability.",
      "year": 2024,
      "venue": "2024 2nd International Conference on Big Data and Privacy Computing (BDPC)",
      "authors": [
        "Zehua Ding",
        "Youliang Tian",
        "Guorong Wang",
        "Jinbo Xiong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/abcf2d721678b041373be7d27f664590cfe08e24",
      "pdf_url": "",
      "publication_date": "2024-01-10",
      "keywords_matched": [
        "membership privacy",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cc01fc6ff8e727e110278198ef3d7633f60629f3",
      "title": "Understanding Practical Membership Privacy of Deep Learning",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Marlon Tobaben",
        "Gauri Pradhan",
        "Yuan He",
        "Joonas J\u00e4lk\u00f6",
        "Antti Honkela"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/cc01fc6ff8e727e110278198ef3d7633f60629f3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0cf009d078867258d0b446baec215aadf93dfe4",
      "title": "A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy",
      "abstract": "Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.",
      "year": 2023,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Enyan Dai",
        "Limeng Cui",
        "Zhengyang Wang",
        "Xianfeng Tang",
        "Yinghan Wang",
        "Mo Cheng",
        "Bin Yin",
        "Suhang Wang"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/d0cf009d078867258d0b446baec215aadf93dfe4",
      "pdf_url": "https://arxiv.org/pdf/2306.08604",
      "publication_date": "2023-06-14",
      "keywords_matched": [
        "membership privacy",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "66448ec9148eac58f0f45dcdaef71b355c6fd756",
      "title": "MemberShield: A framework for federated learning with membership privacy",
      "abstract": null,
      "year": 2024,
      "venue": "Neural Networks",
      "authors": [
        "Faisal Ahmed",
        "David S\u00e1nchez",
        "Zouhair Haddi",
        "Josep Domingo-Ferrer"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/66448ec9148eac58f0f45dcdaef71b355c6fd756",
      "pdf_url": "https://doi.org/10.1016/j.neunet.2024.106768",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c16fc2283c10bdf3f64ddacb10413aa1015d5563",
      "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
      "abstract": "In recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (>0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of our method, highlighting the importance of considering privacy and intellectual property risks when using diffusion models in image generation tasks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Derui Zhu",
        "Dingfan Chen",
        "Jens Grossklags",
        "Mario Fritz"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/c16fc2283c10bdf3f64ddacb10413aa1015d5563",
      "pdf_url": "https://arxiv.org/pdf/2302.07801",
      "publication_date": "2023-02-15",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "65307e54b3aed44c6f9941a20b391ceddea1cf28",
      "title": "Local Differential Privacy Based Membership-Privacy-Preserving Federated Learning for Deep-Learning-Driven Remote Sensing",
      "abstract": "With the development of deep learning, image recognition based on deep learning is now widely used in remote sensing. As we know, the effectiveness of deep learning models significantly benefits from the size and quality of the dataset. However, remote sensing data are often distributed in different parts. They cannot be shared directly for privacy and security reasons, and this has motivated some scholars to apply federated learning (FL) to remote sensing. However, research has found that federated learning is usually vulnerable to white-box membership inference attacks (MIAs), which aim to infer whether a piece of data was participating in model training. In remote sensing, the MIA can lead to the disclosure of sensitive information about the model trainers, such as their location and type, as well as time information about the remote sensing equipment. To solve this issue, we consider embedding local differential privacy (LDP) into FL and propose LDP-Fed. LDP-Fed performs local differential privacy perturbation after properly pruning the uploaded parameters, preventing the central server from obtaining the original local models from the participants. To achieve a trade-off between privacy and model performance, LDP-Fed adds different noise levels to the parameters for various layers of the local models. This paper conducted comprehensive experiments to evaluate the framework\u2019s effectiveness on two remote sensing image datasets and two machine learning benchmark datasets. The results demonstrate that remote sensing image classification models are susceptible to MIAs, and our framework can successfully defend against white-box MIA while achieving an excellent global model.",
      "year": 2023,
      "venue": "Remote Sensing",
      "authors": [
        "Zheng Zhang",
        "Xindi Ma",
        "Jianfeng Ma"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/65307e54b3aed44c6f9941a20b391ceddea1cf28",
      "pdf_url": "https://www.mdpi.com/2072-4292/15/20/5050/pdf?version=1697812247",
      "publication_date": "2023-10-20",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "30ea201f49e6a1f23da476224b0fa80eda57afc1",
      "title": "Membership Privacy Risks of Sharpness Aware Minimization",
      "abstract": "Optimization algorithms that seek flatter minima such as Sharpness-Aware Minimization (SAM) are widely credited with improved generalization. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to membership inference attacks than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This is an intriguing phenomenon as conventional belief posits that higher membership privacy risk is associated with poor generalization. We conjecture that SAM is capable of memorizing atypical subpatterns more, leading to better generalization but higher privacy risk. We empirically validate our hypothesis by running extensive analysis on memorization and influence scores. Finally, we theoretically show how a model that captures minority subclass features more can effectively generalize better \\emph{and} have higher membership privacy risk.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Young In Kim",
        "Andrea Agiollo",
        "Pratiksha Agrawal",
        "J. Royset",
        "Rajiv Khanna"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/30ea201f49e6a1f23da476224b0fa80eda57afc1",
      "pdf_url": "",
      "publication_date": "2023-09-30",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd82555d32409564c86597b9e052a06ce723515c",
      "title": "Diffence: Fencing Membership Privacy With Diffusion Models",
      "abstract": "Deep learning models, while achieving remarkable performances, are vulnerable to membership inference attacks (MIAs). Although various defenses have been proposed, there is still substantial room for improvement in the privacy-utility trade-off. In this work, we introduce a novel defense framework against MIAs by leveraging generative models. The key intuition of our defense is to remove the differences between member and non-member inputs, which is exploited by MIAs, by re-generating input samples before feeding them to the target model. Therefore, our defense, called DIFFENCE, works pre inference, which is unlike prior defenses that are either training-time or post-inference time. A unique feature of DIFFENCE is that it works on input samples only, without modifying the training or inference phase of the target model. Therefore, it can be cascaded with other defense mechanisms as we demonstrate through experiments. DIFFENCE is designed to preserve the model's prediction labels for each sample, thereby not affecting accuracy. Furthermore, we have empirically demonstrated it does not reduce the usefulness of confidence vectors. Through extensive experimentation, we show that DIFFENCE can serve as a robust plug-n-play defense mechanism, enhancing membership privacy without compromising model utility. For instance, DIFFENCE reduces MIA accuracy against an undefended model by 15.8\\% and attack AUC by 14.0\\% on average across three datasets, all without impacting model utility. By integrating DIFFENCE with prior defenses, we can achieve new state-of-the-art performances in the privacy-utility trade-off. For example, when combined with the state-of-the-art SELENA defense it reduces attack accuracy by 9.3\\%, and attack AUC by 10.0\\%. DIFFENCE achieves this by imposing a negligible computation overhead, adding only 57ms to the inference time per sample processed on average.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Yuefeng Peng",
        "Ali Naseh",
        "Amir Houmansadr"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/cd82555d32409564c86597b9e052a06ce723515c",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7750e0f88603992999b8079fb624b83a0f508741",
      "title": "Membership Privacy for Machine Learning Models Through Knowledge Transfer",
      "abstract": "Large capacity machine learning (ML) models are prone to membership inference attacks (MIAs), which aim to infer whether the target sample is a member of the target model's training dataset. The serious privacy concerns due to the membership inference have motivated multiple defenses against MIAs, e.g., differential privacy and adversarial regularization. Unfortunately, these defenses produce ML models with unacceptably low classification performances.\n\nOur work proposes a new defense, called distillation for membership privacy (DMP), against MIAs that preserves the utility of the resulting models significantly better than prior defenses. DMP leverages knowledge distillation to train ML models with membership privacy. We provide a novel criterion to tune the data used for knowledge transfer in order to amplify the membership privacy of DMP.\n\nOur extensive evaluation shows that DMP provides significantly better tradeoffs between membership privacy and classification accuracies compared to state-of-the-art MIA defenses. For instance, DMP achieves ~100% accuracy improvement over adversarial regularization for DenseNet trained on CIFAR100, for similar membership privacy (measured using MIA risk): when the MIA risk is 53.7%, adversarially regularized DenseNet is 33.6% accurate, while DMP-trained DenseNet is 65.3% accurate. We have released our code at github.com/vrt1shjwlkr/AAAI21-MIA-Defense.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "citation_count": 120,
      "url": "https://www.semanticscholar.org/paper/7750e0f88603992999b8079fb624b83a0f508741",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17150/16957",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b15823abc5766f95e7d9107d444a6548db07ebce",
      "title": "MKD: Mutual Knowledge Distillation for Membership Privacy Protection",
      "abstract": null,
      "year": 2023,
      "venue": "AIS&P",
      "authors": [
        "Sihao Huang",
        "Zhongxiang Liu",
        "Jiafu Yu",
        "Yongde Tang",
        "Zidan Luo",
        "Yuan Rao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b15823abc5766f95e7d9107d444a6548db07ebce",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5430b97b6404700829a7f365cae2532d524ddb07",
      "title": "Membership Privacy-Preserving GAN",
      "abstract": null,
      "year": 2022,
      "venue": "British Machine Vision Conference",
      "authors": [
        "Heonseok Ha",
        "Uiwon Hwang",
        "Jaehee Jang",
        "Ho Bae",
        "Sungroh Yoon"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5430b97b6404700829a7f365cae2532d524ddb07",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership privacy",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a5f9f8c838f02a4f29cc80d12862233a1a991b47",
      "title": "Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation",
      "abstract": "Image-to-image translation models are shown to be vulnerable to the Membership Inference Attack (MIA), in which the adversary's goal is to identify whether a sample is used to train the model or not. With daily increasing applications based on image-to-image translation models, it is crucial to protect the privacy of these models against MIAs. We propose adversarial knowledge distillation (AKD) as a defense method against MIAs for image-to-image translation models. The proposed method protects the privacy of the training samples by improving the generalizability of the model. We conduct experiments on the image-to-image translation models and show that AKD achieves the state-of-the-art utility-privacy tradeoff by reducing the attack performance up to 38.9% compared with the regular training model at the cost of a slight drop in the quality of the generated output images. The experimental results also indicate that the models trained by AKD generalize better than the regular training models. Furthermore, compared with existing defense methods, the results show that at the same privacy protection level, image translation models trained by AKD generate outputs with higher quality; while at the same quality of outputs, AKD enhances the privacy protection over 30%.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Saeed Ranjbar Alvar",
        "Lanjun Wang",
        "Jiangbo Pei",
        "Yong Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a5f9f8c838f02a4f29cc80d12862233a1a991b47",
      "pdf_url": "http://arxiv.org/pdf/2203.05212",
      "publication_date": "2022-03-10",
      "keywords_matched": [
        "membership privacy",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0417245ab6dc859728985a87f9fc1418a36c3017",
      "title": "Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?",
      "abstract": "For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP theory cannot explain these empirical findings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are essentially vacuous. In this paper, we aim to close this gap between theory and practice and understand why a large DP parameter can prevent practical MIAs. To tackle this problem, we propose a new privacy notion called practical membership privacy (PMP). PMP models a practical attacker's uncertainty about the contents of the private data. The PMP parameter has a natural interpretation in terms of the success rate of a practical MIA on a given data set. We quantitatively analyze the PMP parameter of two fundamental DP mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis reveals that a large DP parameter often translates into a much smaller PMP parameter, which guarantees strong privacy against practical MIAs. Using our findings, we offer principled guidance for practitioners in choosing the DP parameter.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrew Lowy",
        "Zhuohang Li",
        "Jing Liu",
        "T. Koike-Akino",
        "K. Parsons",
        "Ye Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/0417245ab6dc859728985a87f9fc1418a36c3017",
      "pdf_url": "",
      "publication_date": "2024-02-14",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4c2117639eb51ee8474124be890d2c1f95ad1c62",
      "title": "SHAPr: An Efficient and Versatile Membership Privacy Risk Metric for Machine Learning",
      "abstract": "Data used to train machine learning (ML) models can be sensitive. Membership inference attacks (MIAs), attempting to determine whether a particular data record was used to train an ML model, risk violating membership privacy. ML model builders need a principled definition of a metric to quantify the membership privacy risk of (a) individual training data records, (b) computed independently of specific MIAs, (c) which assesses susceptibility to different MIAs, (d) can be used for different applications, and (e) efficiently. None of the prior membership privacy risk metrics simultaneously meet all these requirements. We present SHAPr, a membership privacy metric based on Shapley values which is a leave-one-out (LOO) technique, originally intended to measure the contribution of a training data record on model utility. We conjecture that contribution to model utility can act as a proxy for memorization, and hence represent membership privacy risk. Using ten benchmark datasets, we show that SHAPr is indeed effective in estimating susceptibility of training data records to MIAs. We also show that, unlike prior work, SHAPr is significantly better in estimating susceptibility to newer, and more effective MIA. We apply SHAPr to evaluate the efficacy of several defenses against MIAs: using regularization and removing high risk training data records. Moreover, SHAPr is versatile: it can be used for estimating vulnerability of different subgroups to MIAs, and inherits applications of Shapley values (e.g., data valuation). We show that SHAPr has an acceptable computational cost (compared to naive LOO), varying from a few minutes for the smallest dataset to ~92 minutes for the largest dataset.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "Sebastian Szyller",
        "N. Asokan"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/4c2117639eb51ee8474124be890d2c1f95ad1c62",
      "pdf_url": "",
      "publication_date": "2021-12-04",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e106189176e3e2e98b480cfdee803a0514c18ad",
      "title": "Privacy-Preserving Low-Rank Adaptation Against Membership Inference Attacks for Latent Diffusion Models",
      "abstract": "Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a private dataset to generate specific images by minimizing the adaptation loss. However, the LoRA-adapted LDMs are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to the private dataset, thus leading to the privacy leakage. To defend against MI attacks, we first propose a straightforward solution: Membership-Privacy-preserving LoRA (MP-LoRA). MP-LoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the MI gain of the proxy attack model. However, we empirically find that MP-LoRA has the issue of unstable optimization, and theoretically analyze that the potential reason is the unconstrained local smoothness, which impedes the privacy-preserving adaptation. To mitigate this issue, we further propose a Stable Membership-Privacy-preserving LoRA (SMP-LoRA) that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain. Besides, we theoretically prove that the local smoothness of SMP-LoRA can be constrained by the gradient norm, leading to improved convergence. Our experimental results corroborate that SMP-LoRA can indeed defend against MI attacks and generate high-quality images.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Zihao Luo",
        "Xilie Xu",
        "Feng Liu",
        "Yun Sing Koh",
        "Di Wang",
        "Jingfeng Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/9e106189176e3e2e98b480cfdee803a0514c18ad",
      "pdf_url": "https://doi.org/10.1609/aaai.v39i6.32628",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0bfd98032594f9199cb48989bb32ed55bb8c553b",
      "title": "Quantifying Membership Privacy via Information Leakage",
      "abstract": "Machine learning models are known to memorize the unique properties of individual data points in a training set. This memorization capability can be exploited by several types of attacks to infer information about the training data, most notably, membership inference attacks. In this paper, we propose an approach based on information leakage for guaranteeing membership privacy. Specifically, we propose to use a conditional form of the notion of maximal leakage to quantify the information leaking about individual data entries in a dataset, i.e., the entrywise information leakage. We apply our privacy analysis to the Private Aggregation of Teacher Ensembles (PATE) framework for privacy-preserving classification of sensitive data and prove that the entrywise information leakage of its aggregation mechanism is Schur-concave when the injected noise has a log-concave probability density. The Schur-concavity of this leakage implies that increased consensus among teachers in labeling a query reduces its associated privacy cost. Finally, we derive upper bounds on the entrywise information leakage when the aggregation mechanism uses Laplace distributed noise.",
      "year": 2020,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Sara Saeidian",
        "Giulia Cervia",
        "T. Oechtering",
        "M. Skoglund"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/0bfd98032594f9199cb48989bb32ed55bb8c553b",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/10206/9151439/09406982.pdf",
      "publication_date": "2020-10-12",
      "keywords_matched": [
        "membership privacy",
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6eb50dbb59ebb64df3ad425d6d959ea08595ad6b",
      "title": "Knowledge Cross-Distillation for Membership Privacy",
      "abstract": "Abstract A membership inference attack (MIA) poses privacy risks for the training data of a machine learning model. With an MIA, an attacker guesses if the target data are a member of the training dataset. The state-of-the-art defense against MIAs, distillation for membership privacy (DMP), requires not only private data for protection but a large amount of unlabeled public data. However, in certain privacy-sensitive domains, such as medicine and finance, the availability of public data is not guaranteed. Moreover, a trivial method for generating public data by using generative adversarial networks significantly decreases the model accuracy, as reported by the authors of DMP. To overcome this problem, we propose a novel defense against MIAs that uses knowledge distillation without requiring public data. Our experiments show that the privacy protection and accuracy of our defense are comparable to those of DMP for the benchmark tabular datasets used in MIA research, Purchase100 and Texas100, and our defense has a much better privacy-utility trade-off than those of the existing defenses that also do not use public data for the image dataset CIFAR10.",
      "year": 2021,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "R. Chourasia",
        "Batnyam Enkhtaivan",
        "Kunihiro Ito",
        "Junki Mori",
        "Isamu Teranishi",
        "Hikaru Tsuchida"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/6eb50dbb59ebb64df3ad425d6d959ea08595ad6b",
      "pdf_url": "https://petsymposium.org/popets/2022/popets-2022-0050.pdf",
      "publication_date": "2021-11-02",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b45b55c208200f9590f280a7c11f6519ed32effe",
      "title": "A Novel Dynamic Group Signature with Membership Privacy",
      "abstract": "Group signature is a cryptography primitive that has been widely researched. It strikes a balance between digital signature and the user's demand for anonymity. A valid member in the group can generate a signature on behalf of the whole group. The public can only know that it was provided by a valid group member and learn nothing about the actual identity of the signer when verifying a group signature. Backes et al pointed out that the existing dynamic group signature schemes implicitly assume that the membership of everyone in the group is open to the public. Thus, they put forward a property called membership privacy for dynamic group signature. In this paper, we design a dynamic group signature scheme with membership privacy on top of Signature Proofs of Knowledge (SPK) and BBS+ signature. Further more, dynamic accumulator mechanism is adopted to revoke a group member's authority to sign. Then, a security analysis demonstrates that the proposed group signature scheme satisfies join-leave privacy. Finally, quantitative analysis and experimental results show that the proposed group signature scheme achieves the fewer signature size and less computation overhead compared with Backes's scheme.",
      "year": 2021,
      "venue": "International Conference on Data Science in Cyberspace",
      "authors": [
        "Junqing Lu",
        "Rongxin Qi",
        "Jian Shen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b45b55c208200f9590f280a7c11f6519ed32effe",
      "pdf_url": "",
      "publication_date": "2021-01-30",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e57aee28b70104dcbb8d969b37f6a56439a0d90d",
      "title": "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning",
      "abstract": "Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.",
      "year": 2024,
      "venue": "International Conference on Computing, Networking and Communications",
      "authors": [
        "Sayyed Farid Ahamed",
        "Soumya Banerjee",
        "Sandip Roy",
        "Devin Quinn",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/e57aee28b70104dcbb8d969b37f6a56439a0d90d",
      "pdf_url": "",
      "publication_date": "2024-07-26",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a3420c63fda34fa1dd03f7e6e716be66d7020317",
      "title": "Privacy-Preserving Membership Queries for Federated Anomaly Detection",
      "abstract": "In this work, we propose a new privacy-preserving membership query protocol that lets a centralized entity privately query datasets held by one or more other parties to check if they contain a given element. This protocol, based on elliptic curve-based ElGamal and oblivious key-value stores, ensures that those 'data-augmenting' parties only have to send their encrypted data to the centralized entity once, making the protocol particularly efficient when the centralized entity repeatedly queries the same sets of data. We apply this protocol to detect anomalies in cross-silo federations. Data anomalies across such cross-silo federations are challenging to detect because (1) the centralized entities have little knowledge of the actual users, (2) the data-augmenting entities do not have a global view of the system, and (3) privacy concerns and regulations prevent pooling all the data. Our protocol allows for anomaly detection even in strongly separated distributed systems while protecting users' privacy. Specifically, we propose a cross-silo federated architecture in which a centralized entity (the backbone) has labeled data to train a machine learning model for detecting anomalous instances. The other entities in the federation are data-augmenting clients (the user-facing entities) who collaborate with the centralized entity to extract feature values to improve the utility of the model. These feature values are computed using our privacy-preserving membership query protocol. The model can be trained with an off-the-shelf machine learning algorithm that provides differential privacy to prevent it from memorizing instances from the training data, thereby providing output privacy. However, it is not straightforward to also efficiently provide input privacy, which ensures that none of the entities in the federation ever see the data of other entities in an unencrypted form. We demonstrate the effectiveness of our approach in the financial domain, motivated by the PETs Prize Challenge, which is a collaborative effort between the US and UK governments to combat international fraudulent transactions. We show that the private queries significantly increase the precision and recall of the otherwise centralized system and argue that this improvement translates to other use cases as well.",
      "year": 2024,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Jelle Vos",
        "Sikha Pentyala",
        "Steven Golob",
        "Ricardo Maia",
        "Dean Kelley",
        "Z. Erkin",
        "Martine De Cock",
        "Anderson Nascimento"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a3420c63fda34fa1dd03f7e6e716be66d7020317",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8578811fd1f7fcac5a1b4145b89077e6c4c0c6eb",
      "title": "Membership Privacy for Fully Dynamic Group Signatures",
      "abstract": "Group signatures present a compromise between the traditional goals of digital signatures and the need for signer privacy, allowing for the creation of unforgeable signatures in the name of a group which reveal nothing about the actual signer's identity beyond their group membership. An important consideration that is absent in prevalent models is that group membership itself may be sensitive information, especially if group membership is dynamic, i.e. membership status may change over time. We address this issue by introducing formal notions of membership privacy for fully dynamic group signature schemes, which can be easily integrated into the most expressive models of group signature security to date. We then propose a generic construction for a fully dynamic group signature scheme with membership privacy that is based on signatures with flexible public key (SFPK) and signatures on equivalence classes (SPSEQ). Finally, we devise novel techniques for SFPK to construct a highly efficient standard model scheme (i.e. without random oracles) that provides shorter signatures than even the non-private state-of-the-art from standard assumptions. This shows that, although the strictly stronger security notions we introduce have been completely unexplored in the study of fully dynamic group signatures so far, they do not come at an additional cost in practice.",
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "M. Backes",
        "L. Hanzlik",
        "J. Schneider"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/8578811fd1f7fcac5a1b4145b89077e6c4c0c6eb",
      "pdf_url": "",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5e30735a69677e4df202e6a12c9122ab8104c518",
      "title": "Characterizing Membership Privacy in Stochastic Gradient Langevin Dynamics",
      "abstract": "Bayesian deep learning is recently regarded as an intrinsic way to characterize the weight uncertainty of deep neural networks~(DNNs). Stochastic Gradient Langevin Dynamics~(SGLD) is an effective method to enable Bayesian deep learning on large-scale datasets. Previous theoretical studies have shown various appealing properties of SGLD, ranging from the convergence properties to the generalization bounds. In this paper, we study the properties of SGLD from a novel perspective of membership privacy protection (i.e., preventing the membership attack). The membership attack, which aims to determine whether a specific sample is used for training a given DNN model, has emerged as a common threat against deep learning algorithms. To this end, we build a theoretical framework to analyze the information leakage (w.r.t. the training dataset) of a model trained using SGLD. Based on this framework, we demonstrate that SGLD can prevent the information leakage of the training dataset to a certain extent. Moreover, our theoretical analysis can be naturally extended to other types of Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods. Empirical results on different datasets and models verify our theoretical findings and suggest that the SGLD algorithm can not only reduce the information leakage but also improve the generalization ability of the DNN models in real-world applications.",
      "year": 2019,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bingzhe Wu",
        "Chaochao Chen",
        "Shiwan Zhao",
        "Cen Chen",
        "Yuan Yao",
        "Guangyu Sun",
        "Li Wang",
        "Xiaolu Zhang",
        "Jun Zhou"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/5e30735a69677e4df202e6a12c9122ab8104c518",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6107/5963",
      "publication_date": "2019-10-05",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "070171862d08fc83798bdeab468a6b77ee80947b",
      "title": "On the Privacy of Multi-Versioned Approximate Membership Check Filters",
      "abstract": "Approximate membership filters are increasingly used in many computing and networking applications and new filter designs are being continuously presented to improve one or more performance metrics. Therefore, understanding their security and privacy is an important issue. Previous works have considered attackers that only have access to an individual filter in isolation. For applications that generate many related filters, such as a filter for a deny list that evolves over time, that analysis is insufficient. This article considers an attacker with access to several versions of a filter that share most of the same input elements. We find that for typical implementations of Bloom, cuckoo, and quotient filters, the attacker gains little or no advantage with access to multiple versions of a filter. However, typical xor filters do reveal more information about their input elements by querying multiple versions of a filter, and we propose techniques to enhance the privacy of xor filters and others.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "P. Reviriego",
        "A. S\u00e1nchez-Maci\u00e1n",
        "P. Dillinger",
        "Stefan Walzer"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/070171862d08fc83798bdeab468a6b77ee80947b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "65656158b9ae07066560d7281ef5e44f93f9aeab",
      "title": "Active Membership Inference Attack under Local Differential Privacy in Federated Learning",
      "abstract": "Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.",
      "year": 2023,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Truc Nguyen",
        "Phung Lai",
        "K. Tran",
        "Nhathai Phan",
        "M. Thai"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/65656158b9ae07066560d7281ef5e44f93f9aeab",
      "pdf_url": "https://arxiv.org/pdf/2302.12685",
      "publication_date": "2023-02-24",
      "keywords_matched": [
        "membership privacy",
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "00fa213314d8e7ee20201a051c3293c16ff549f9",
      "title": "Mace: A flexible framework for membership privacy estimation in generative models",
      "abstract": "Generative machine learning models are being increasingly viewed as a way to share sensitive data between institutions. While there has been work on developing differentially private generative modeling approaches, these approaches generally lead to sub-par sample quality, limiting their use in real world applications. Another line of work has focused on developing generative models which lead to higher quality samples but currently lack any formal privacy guarantees. In this work, we propose the first formal framework for membership privacy estimation in generative models. We formulate the membership privacy risk as a statistical divergence between training samples and hold-out samples, and propose sample-based methods to estimate this divergence. Compared to previous works, our framework makes more realistic and flexible assumptions. First, we offer a generalizable metric as an alternative to the accuracy metric especially for imbalanced datasets. Second, we loosen the assumption of having full access to the underlying distribution from previous studies , and propose sample-based estimations with theoretical guarantees. Third, along with the population-level membership privacy risk estimation via the optimal membership advantage, we offer the individual-level estimation via the individual privacy risk. Fourth, our framework allows adversaries to access the trained model via a customized query, while prior works require specific attributes.",
      "year": 2020,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Yixi Xu",
        "S. Mukherjee",
        "Xiyang Liu",
        "Shruti Tople",
        "R. Dodhia",
        "J. Ferres"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/00fa213314d8e7ee20201a051c3293c16ff549f9",
      "pdf_url": "",
      "publication_date": "2020-09-11",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "71a1bc401c7e11b60f830b800c32c86936cc5b15",
      "title": "Towards Measuring Membership Privacy",
      "abstract": "Machine learning models are increasingly made available to the masses through public query interfaces. Recent academic work has demonstrated that malicious users who can query such models are able to infer sensitive information about records within the training data. Differential privacy can thwart such attacks, but not all models can be readily trained to achieve this guarantee or to achieve it with acceptable utility loss. As a result, if a model is trained without differential privacy guarantee, little is known or can be said about the privacy risk of releasing it. In this work, we investigate and analyze membership attacks to understand why and how they succeed. Based on this understanding, we propose Differential Training Privacy (DTP), an empirical metric to estimate the privacy risk of publishing a classier when methods such as differential privacy cannot be applied. DTP is a measure of a classier with respect to its training dataset, and we show that calculating DTP is efficient in many practical cases. We empirically validate DTP using state-of-the-art machine learning models such as neural networks trained on real-world datasets. Our results show that DTP is highly predictive of the success of membership attacks and therefore reducing DTP also reduces the privacy risk. We advocate for DTP to be used as part of the decision-making process when considering publishing a classifier. To this end, we also suggest adopting the DTP-1 hypothesis: if a classifier has a DTP value above 1, it should not be published.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Yunhui Long",
        "Vincent Bindschaedler",
        "Carl A. Gunter"
      ],
      "citation_count": 88,
      "url": "https://www.semanticscholar.org/paper/71a1bc401c7e11b60f830b800c32c86936cc5b15",
      "pdf_url": "",
      "publication_date": "2017-12-25",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "075de81bdc1d51c1ddcf2eb7836ad2c9271db7bd",
      "title": "Synthetic data for enhanced privacy: A VAE-GAN approach against membership inference attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Jianen Yan",
        "Haihui Huang",
        "Kairan Yang",
        "Haiyan Xu",
        "Yanling Li"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/075de81bdc1d51c1ddcf2eb7836ad2c9271db7bd",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd2711cfe890675e8d885df88f3f76b5be5b39a6",
      "title": "Membership Privacy in MicroRNA-based Studies",
      "abstract": null,
      "year": 2016,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "M. Backes",
        "Pascal Berrang",
        "Mathias Humbert",
        "Praveen Manoharan"
      ],
      "citation_count": 151,
      "url": "https://www.semanticscholar.org/paper/fd2711cfe890675e8d885df88f3f76b5be5b39a6",
      "pdf_url": "",
      "publication_date": "2016-10-24",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb59c9f118734aeca03163155406dec4fb15b446",
      "title": "From Membership-Privacy Leakage to Quantum Machine Unlearning",
      "abstract": "Quantum Machine Learning (QML) has the potential to achieve quantum advantage for specific tasks by combining quantum computation with classical Machine Learning (ML). In classical ML, a significant challenge is membership privacy leakage, whereby an attacker can infer from model outputs whether specific data were used in training. When specific data are required to be withdrawn, removing their influence from the trained model becomes necessary. Machine Unlearning (MU) addresses this issue by enabling the model to forget the withdrawn data, thereby preventing membership privacy leakage. However, this leakage remains underexplored in QML. This raises two research questions: do QML models leak membership privacy about their training data, and can MU methods efficiently mitigate such leakage in QML models? We investigate these questions using two QNN architectures, a basic Quantum Neural Network (basic QNN) and a Hybrid QNN (HQNN), evaluated in noiseless simulations and on quantum hardware. For the first question, we design a Membership Inference Attack (MIA) tailored to QNN in a gray-box setting. Our experiments indicate clear evidence of leakage of membership privacy in both QNNs. For the second question, we propose a Quantum Machine Unlearning (QMU) framework, comprising three MU mechanisms. Experiments on two QNN architectures show that QMU removes the influence of the withdrawn data while preserving accuracy on retained data. A comparative analysis further characterizes the three MU mechanisms with respect to data dependence, computational cost, and robustness. Overall, this work provides a potential path towards privacy-preserving QML.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Junjian Su",
        "Runze He",
        "Guanghui Li",
        "Sujuan Qin",
        "Zhimin He",
        "Haozhen Situ",
        "Fei Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eb59c9f118734aeca03163155406dec4fb15b446",
      "pdf_url": "",
      "publication_date": "2025-09-07",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e33ae42f40196754c0189cd6e30c4c246da012df",
      "title": "Gaussian Membership Inference Privacy",
      "abstract": "We propose a novel and practical privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model. Consequently, $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). In particular, we derive a parametric family of $f$-MIP guarantees that we refer to as $\\mu$-Gaussian Membership Inference Privacy ($\\mu$-GMIP) by theoretically analyzing likelihood ratio-based membership inference attacks on stochastic gradient descent (SGD). Our analysis highlights that models trained with standard SGD already offer an elementary level of MIP. Additionally, we show how $f$-MIP can be amplified by adding noise to gradient updates. Our analysis further yields an analytical membership inference attack that offers two distinct advantages over previous approaches. First, unlike existing state-of-the-art attacks that require training hundreds of shadow models, our attack does not require any shadow model. Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g., batch size, number of model parameters) and specific data characteristics determine an attacker's ability to accurately infer a point's membership in the training set. We demonstrate the effectiveness of our method on models trained on vision and tabular datasets.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Tobias Leemann",
        "Martin Pawelczyk",
        "Gjergji Kasneci"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/e33ae42f40196754c0189cd6e30c4c246da012df",
      "pdf_url": "http://arxiv.org/pdf/2306.07273",
      "publication_date": "2023-06-12",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2f8e8c6ad803c297d01635722a5d093e3750dae",
      "title": "Measuring Membership Privacy on Aggregate Location Time-Series",
      "abstract": "While location data is extremely valuable for various applications, disclosing it prompts serious threats to individuals' privacy. To limit such concerns, organizations often provide analysts with aggregate time-series that indicate, e.g., how many people are in a location at a time interval, rather than raw individual traces. In this paper, we perform a measurement study to understand Membership Inference Attacks (MIAs) on aggregate location time-series, where an adversary tries to infer whether a specific user contributed to the aggregates. We find that the volume of contributed data, as well as the regularity and particularity of users' mobility patterns, play a crucial role in the attack's success. We experiment with a wide range of defenses based on generalization, hiding, and perturbation, and evaluate their ability to thwart the attack vis-\u00e0-vis the utility loss they introduce for various mobility analytics tasks. Our results show that some defenses fail across the board, while others work for specific tasks on aggregate location time-series. For instance, suppressing small counts can be used for ranking hotspots, data generalization for forecasting traffic, hotspot discovery, and map inference, while sampling is effective for location labeling and anomaly detection when the dataset is sparse. Differentially private techniques provide reasonable accuracy only in very specific settings, e.g., discovering hotspots and forecasting their traffic, and more so when using weaker privacy notions like crowd-blending privacy. Overall, our measurements show that there does not exist a unique generic defense that can preserve the utility of the analytics for arbitrary applications, and provide useful insights regarding the disclosure of sanitized aggregate location time-series.",
      "year": 2019,
      "venue": "Measurement and Modeling of Computer Systems",
      "authors": [
        "Apostolos Pyrgelis",
        "C. Troncoso",
        "Emiliano De Cristofaro"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/c2f8e8c6ad803c297d01635722a5d093e3750dae",
      "pdf_url": "https://arxiv.org/pdf/1902.07456",
      "publication_date": "2019-02-20",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a1f342c529494898675002c969038a10944eca9a",
      "title": "Measuring Membership Privacy on Aggregate Location Time-Series",
      "abstract": "While location data is extremely valuable for various applications, disclosing it prompts serious threats to individuals' privacy. To limit such concerns, organizations often provide analysts with aggregate time-series that indicate, e.g., how many people are in a location at a time interval, rather than raw individual traces. In this paper, we perform a measurement study to understand Membership Inference Attacks (MIAs) on aggregate location time-series, where an adversary tries to infer whether a specific user contributed to the aggregates. We find that the volume of contributed data, as well as the regularity and particularity of users' mobility patterns, play a crucial role in the attack's success. We experiment with a wide range of defenses based on generalization, hiding, and perturbation, and evaluate their ability to thwart the attack vis-\u00e0-vis the utility loss they introduce for various mobility analytics tasks. Our results show that some defenses fail across the board, while others work for specific tasks on aggregate location time-series. For instance, suppressing small counts can be used for ranking hotspots, data generalization for forecasting traffic, hotspot discovery, and map inference, while sampling is effective for location labeling and anomaly detection when the dataset is sparse. Differentially private techniques provide reasonable accuracy only in very specific settings, e.g., discovering hotspots and forecasting their traffic, and more so when using weaker privacy notions like crowd-blending privacy. Overall, our measurements show that there does not exist a unique generic defense that can preserve the utility of the analytics for arbitrary applications, and provide useful insights regarding the disclosure of sanitized aggregate location time-series.",
      "year": 2020,
      "venue": "Abstracts of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems",
      "authors": [
        "Apostolos Pyrgelis",
        "Carmela Troncoso",
        "Emiliano De Cristofaro"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1f342c529494898675002c969038a10944eca9a",
      "pdf_url": "https://arxiv.org/pdf/1902.07456",
      "publication_date": "2020-06-08",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "939d99d610869a7489fff0d74660be1c093a1fe5",
      "title": "Enable membership privacy-preserving group signature scheme",
      "abstract": null,
      "year": 2025,
      "venue": "Int. J. Secur. Networks",
      "authors": [
        "Zhihong Chen",
        "Huiying Hou",
        "Zhenyu Liu",
        "Zhiyuan Ren",
        "Yucong Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/939d99d610869a7489fff0d74660be1c093a1fe5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a8f6430be660a5fc34d589806622cc73c209731",
      "title": "Reconciling Utility and Membership Privacy via Knowledge Distillation",
      "abstract": "Large capacity machine learning models are prone to membership inference attacks in which an adversary aims to infer whether a particular data sample is a member of the target model's training dataset. Such membership inferences can lead to serious privacy violations as machine learning models are often trained using privacy-sensitive data such as medical records and controversial user opinions. Recently, defenses against membership inference attacks are developed, in particular, based on differential privacy and adversarial regularization; unfortunately, such defenses highly impact the classification accuracy of the underlying machine learning models. In this work, we present a new defense against membership inference attacks that preserves the utility of the target machine learning models significantly better than prior defenses. Our defense, called distillation for membership privacy (DMP), leverages knowledge distillation to train machine learning models with membership privacy. We analyze the key requirements for membership privacy and provide a novel criterion to select data used for knowledge transfer, in order to improve membership privacy of the final models. DMP works effectively against the attackers with either a whitebox or blackbox access to the target model. We evaluate DMP's performance through extensive experiments on different deep neural networks and using various benchmark datasets. We show that DMP provides a significantly better tradeoff between inference resistance and classification performance than state-of-the-art membership inference defenses. For instance, a DMP-trained DenseNet provides a classification accuracy of 65.3% for a 54.4% blackbox membership inference attack accuracy, while an adversarially regularized DenseNet provides a classification accuracy of only 53.7% for a (much worse) 68.7% blackbox membership inference attack accuracy.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/3a8f6430be660a5fc34d589806622cc73c209731",
      "pdf_url": "",
      "publication_date": "2019-06-15",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cc3b41c4d2ededb94b481089633ecb43d6b2162f",
      "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks",
      "abstract": "The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying potential robustness of MLMs to privacy attacks.In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM\u2019s model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are indeed susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Fatemehsadat Mireshghallah",
        "Kartik Goyal",
        "Archit Uniyal",
        "Taylor Berg-Kirkpatrick",
        "R. Shokri"
      ],
      "citation_count": 202,
      "url": "https://www.semanticscholar.org/paper/cc3b41c4d2ededb94b481089633ecb43d6b2162f",
      "pdf_url": "http://arxiv.org/pdf/2203.03929",
      "publication_date": "2022-03-08",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "022f66e8339b0a49ff3d230cabeb89ec8971e227",
      "title": "Machine Learning with Membership Privacy via Knowledge Transfer",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/022f66e8339b0a49ff3d230cabeb89ec8971e227",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d95a678903347b8764e04d287c672242dae29dc",
      "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
      "abstract": "The use of personal data for training machine learning systems comes with a privacy threat and measuring the level of privacy of a model is one of the major challenges in machine learning today. Identifying training data based on a trained model is a standard way of measuring the privacy risks induced by the model. We develop a novel approach to address the problem of membership inference in pattern recognition models, relying on information provided by adversarial examples. The strategy we propose consists of measuring the magnitude of a perturbation necessary to build an adversarial example. Indeed, we argue that this quantity reflects the likelihood of belonging to the training data. Extensive numerical experiments on multivariate data and an array of state-of-the-art target models show that our method performs comparable or even outperforms state-of-the-art strategies, but without requiring any additional training samples.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Ganesh Del Grosso",
        "Hamid Jalalzai",
        "Georg Pichler",
        "C. Palamidessi",
        "P. Piantanida"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/6d95a678903347b8764e04d287c672242dae29dc",
      "pdf_url": "https://arxiv.org/pdf/2203.09566",
      "publication_date": "2022-03-17",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "763cf9b653f3c5899525d0f916069015c73acc19",
      "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
      "abstract": "Machine learning models have been shown to be vulnerable to membership inference attacks, i.e., inferring whether individuals' data have been used for training models. The lack of understanding about factors contributing success of these attacks motivates the need for modelling membership information leakage using information theory and for investigating properties of machine learning models and training algorithms that can reduce membership information leakage. We use conditional mutual information leakage to measure the amount of information leakage from the trained machine learning model about the presence of an individual in the training dataset. We devise an upper bound for this measure of information leakage using Kullback--Leibler divergence that is more amenable to numerical computation. We prove a direct relationship between the Kullback--Leibler membership information leakage and the probability of success for a hypothesis-testing adversary examining whether a particular data record belongs to the training dataset of a machine learning model. We show that the mutual information leakage is a decreasing function of the training dataset size and the regularization weight. We also prove that, if the sensitivity of the machine learning model (defined in terms of the derivatives of the fitness with respect to model parameters) is high, more membership information is potentially leaked. This illustrates that complex models, such as deep neural networks, are more susceptible to membership inference attacks in comparison to simpler models with fewer degrees of freedom. We show that the amount of the membership information leakage is reduced by $\\mathcal{O}(\\log^{1/2}(\\delta^{-1})\\epsilon^{-1})$ when using Gaussian $(\\epsilon,\\delta)$-differentially-private additive noises.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "F. Farokhi",
        "M. K\u00e2afar"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/763cf9b653f3c5899525d0f916069015c73acc19",
      "pdf_url": "",
      "publication_date": "2020-01-29",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "12562355aec7c0e82a84f11c35de1fc7df5805e2",
      "title": "Membership Information",
      "abstract": null,
      "year": 2021,
      "venue": "Geostandards and Geoanalytical Research",
      "authors": [
        "J. Enzweiler"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/12562355aec7c0e82a84f11c35de1fc7df5805e2",
      "pdf_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/ggr.12337",
      "publication_date": "2021-08-26",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9fc3eba09985dde57735d3c0b692ab80383558e5",
      "title": "Membership Information",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9fc3eba09985dde57735d3c0b692ab80383558e5",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7698ad870dc0a74001ca579fab61233cdc23e1cd",
      "title": "Rethinking Membership Inference Attacks Against Transfer Learning",
      "abstract": "Transfer learning, successful in knowledge translation across related tasks, faces a substantial privacy threat from membership inference attacks (MIAs). These attacks, despite posing significant risk to ML model\u2019s training data, remain limited-explored in transfer learning. The interaction between teacher and student models in transfer learning has not been thoroughly explored in MIAs, potentially resulting in an under-examined aspect of privacy vulnerabilities within transfer learning. In this paper, we propose a new MIA vector against transfer learning, to determine whether a specific data point was used to train the teacher model while only accessing the student model in a white-box setting. Our method delves into the intricate relationship between teacher and student models, analyzing the discrepancies in hidden layer representations between the student model and its shadow counterpart. These identified differences are then adeptly utilized to refine the shadow model\u2019s training process and to inform membership inference decisions effectively. Our method, evaluated across four datasets in diverse transfer learning tasks, reveals that even when an attacker only has access to the student model, the teacher model\u2019s training data remains susceptible to MIAs. We believe our work unveils the unexplored risk of membership inference in transfer learning.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Cong Wu",
        "Jing Chen",
        "Qianru Fang",
        "Kun He",
        "Ziming Zhao",
        "Hao Ren",
        "Guowen Xu",
        "Yang Liu",
        "Yang Xiang"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/7698ad870dc0a74001ca579fab61233cdc23e1cd",
      "pdf_url": "",
      "publication_date": "2025-01-20",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be39eab7eab50964443eee7e36d473bd50f4ca36",
      "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
      "abstract": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yiyong Liu",
        "Zhengyu Zhao",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/be39eab7eab50964443eee7e36d473bd50f4ca36",
      "pdf_url": "https://arxiv.org/pdf/2208.14933",
      "publication_date": "2022-08-31",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f22230876e57d4cbab64bf94109d1ca33831e8da",
      "title": "Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models",
      "abstract": "Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats.",
      "year": 2023,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Zhou Yang",
        "Zhipeng Zhao",
        "Chenyu Wang",
        "Jieke Shi",
        "Dongsum Kim",
        "Donggyun Han",
        "David Lo"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/f22230876e57d4cbab64bf94109d1ca33831e8da",
      "pdf_url": "",
      "publication_date": "2023-10-02",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "faf42ee6ba0f4eb95e2191461c19ab29b9f51ad3",
      "title": "TEAR: Exploring Temporal Evolution of Adversarial Robustness for Membership Inference Attacks Against Federated Learning",
      "abstract": "Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple clients to train a unified model without disclosing their private data. However, susceptibility to membership inference attacks (MIAs) arises due to the natural inclination of FL models to overfit on the training data during the training process, thereby enabling MIAs to exploit the subtle differences in the FL model\u2019s parameters, activations, or predictions between the training and testing data to infer membership information. It is worth noting that most if not all existing MIAs against FL require access to the model\u2019s internal information or modification of the training process, yielding them unlikely to be performed in practice. In this paper, we present with TEAR the first evidence that it is possible for an honest-but-curious federated client to perform MIA against an FL system, by exploring the Temporal Evolution of the Adversarial Robustness between the training and non-training data. We design a novel adversarial example generation method to quantify the target sample\u2019s adversarial robustness, which can be utilized to obtain the membership features to train the inference model in a supervised manner. Extensive experiment results on five realistic datasets demonstrate that TEAR can achieve a strong inference performance compared with two existing MIAs, and is able to escape from the protection of two representative defenses.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Gaoyang Liu",
        "Zehao Tian",
        "Jian Chen",
        "Chen Wang",
        "Jiangchuan Liu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/faf42ee6ba0f4eb95e2191461c19ab29b9f51ad3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "166804591abf73b79d9f8e8153299a1a2fa2a1f8",
      "title": "Membership Inference Attacks Against Text-to-image Generation Models",
      "abstract": "Text-to-image generation models have recently attracted unprecedented attention as they unlatch imaginative applications in all areas of life. However, developing such models requires huge amounts of data that might contain privacy-sensitive information, e.g., face identity. While privacy risks have been extensively demonstrated in the image classification and GAN generation domains, privacy risks in the text-to-image generation domain are largely unexplored. In this paper, we perform the first privacy analysis of text-to-image generation models through the lens of membership inference. Specifically, we propose three key intuitions about membership information and design four attack methodologies accordingly. We conduct comprehensive evaluations on two mainstream text-to-image generation models including sequence-to-sequence modeling and diffusion-based modeling. The empirical results show that all of the proposed attacks can achieve significant performance, in some cases even close to an accuracy of 1, and thus the corresponding risk is much more severe than that shown by existing membership inference attacks. We further conduct an extensive ablation study to analyze the factors that may affect the attack performance, which can guide developers and researchers to be alert to vulnerabilities in text-to-image generation models. All these findings indicate that our proposed attacks pose a realistic privacy threat to the text-to-image generation models.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yixin Wu",
        "Ning Yu",
        "Zheng Li",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 77,
      "url": "https://www.semanticscholar.org/paper/166804591abf73b79d9f8e8153299a1a2fa2a1f8",
      "pdf_url": "http://arxiv.org/pdf/2210.00968",
      "publication_date": "2022-10-03",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "580bba032938bdc88754490c8525eff99ded4a8d",
      "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
      "abstract": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Dingfan Chen",
        "Ning Yu",
        "Yang Zhang",
        "Mario Fritz"
      ],
      "citation_count": 459,
      "url": "https://www.semanticscholar.org/paper/580bba032938bdc88754490c8525eff99ded4a8d",
      "pdf_url": "https://arxiv.org/pdf/1909.03935",
      "publication_date": "2019-09-09",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1064cdd25d6193d0283536c947c173104ccc7a98",
      "title": "Securing dynamic membership information in multicast communications",
      "abstract": null,
      "year": 2004,
      "venue": "IEEE INFOCOM 2004",
      "authors": [
        "Yan Sun",
        "K.J.R. Liu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/1064cdd25d6193d0283536c947c173104ccc7a98",
      "pdf_url": "http://www.ele.uri.edu/muri/publications/GDI_Infocom04.pdf",
      "publication_date": "2004-03-07",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0f44898d09750eb037bdb51c6938bd140fc24ab",
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "abstract": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
      "year": 2021,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Bo Hui",
        "Yuchen Yang",
        "Haolin Yuan",
        "P. Burlina",
        "N. Gong",
        "Yinzhi Cao"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/f0f44898d09750eb037bdb51c6938bd140fc24ab",
      "pdf_url": "https://doi.org/10.14722/ndss.2021.24293",
      "publication_date": "2021-01-05",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16121a757e8fc7d66da979cf34ab1542f0fdfc48",
      "title": "Analysis and Protection of Dynamic Membership Information for Group Key Distribution Schemes",
      "abstract": null,
      "year": 2007,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Y. Sun",
        "K.J.R. Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/16121a757e8fc7d66da979cf34ab1542f0fdfc48",
      "pdf_url": "http://sig.umd.edu/publications/Sun_2007_TIFS.pdf",
      "publication_date": "2007-06-01",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7aea0ac86e90ea2c6a38d0b1e5f90d18a42add73",
      "title": "M^4I: Multi-modal Models Membership Inference",
      "abstract": "With the development of machine learning techniques, the attention of research has been moved from single-modal learning to multi-modal learning, as real-world data exist in the form of different modalities. However, multi-modal models often carry more information than single-modal models and they are usually applied in sensitive scenarios, such as medical report generation or disease identification. Compared with the existing membership inference against machine learning classifiers, we focus on the problem that the input and output of the multi-modal models are in different modalities, such as image captioning. This work studies the privacy leakage of multi-modal models through the lens of membership inference attack, a process of determining whether a data record involves in the model training process or not. To achieve this, we propose Multi-modal Models Membership Inference (M^4I) with two attack methods to infer the membership status, named metric-based (MB) M^4I and feature-based (FB) M^4I, respectively. More specifically, MB M^4I adopts similarity metrics while attacking to infer target data membership. FB M^4I uses a pre-trained shadow multi-modal feature extractor to achieve the purpose of data inference attack by comparing the similarities from extracted input and output features. Extensive experimental results show that both attack methods can achieve strong performances. Respectively, 72.5% and 94.83% of attack success rates on average can be obtained under unrestricted scenarios. Moreover, we evaluate multiple defense mechanisms against our attacks. The source code of M^4I attacks is publicly available at https://github.com/MultimodalMI/Multimodal-membership-inference.git.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pingyi Hu",
        "Zihan Wang",
        "Ruoxi Sun",
        "Hu Wang",
        "Minhui Xue"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/7aea0ac86e90ea2c6a38d0b1e5f90d18a42add73",
      "pdf_url": "http://arxiv.org/pdf/2209.06997",
      "publication_date": "2022-09-15",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32842a656099e34bd68c41b2a90a2d635a4dbaf0",
      "title": "Membership Inference Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), which generalize traditional deep neural networks on graph data, have achieved state-of-the-art performance on several graph analytical tasks. We focus on how trained GNN models could leak information about the member nodes that they were trained on. We introduce two realistic settings for performing a membership inference (MI) attack on GNNs. While choosing the simplest possible attack model that utilizes the posteriors of the trained model (black-box access), we thoroughly analyze the properties of GNNs and the datasets which dictate the differences in their robustness towards MI attack. While in traditional machine learning models, overfitting is considered the main cause of such leakage, we show that in GNNs the additional structural information is the major contributing factor. We support our findings by extensive experiments on four representative GNN models. To prevent MI attacks on GNN, we propose two effective defenses that significantly decreases the attacker's inference by up to 60% without degradation to the target model's performance. Our code is available at https://github.com/iyempissy/rebMIGraph.",
      "year": 2021,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Iyiola E. Olatunji",
        "W. Nejdl",
        "Megha Khosla"
      ],
      "citation_count": 126,
      "url": "https://www.semanticscholar.org/paper/32842a656099e34bd68c41b2a90a2d635a4dbaf0",
      "pdf_url": "https://arxiv.org/pdf/2101.06570",
      "publication_date": "2021-01-17",
      "keywords_matched": [
        "membership information",
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1d2658dd920d8a0a97bd47e3238bb3a4cf8252d4",
      "title": "Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting",
      "abstract": "Balancing strong privacy guarantees with high predictive performance is critical for time series forecasting (TSF) tasks involving Electronic Health Records (EHR). In this study, we explore how data augmentation can mitigate Membership Inference Attacks (MIA) on TSF models. We show that retraining with synthetic data can substantially reduce the effectiveness of loss-based MIAs by reducing the attacker's true-positive to false-positive ratio. The key challenge is generating synthetic samples that closely resemble the original training data to confuse the attacker, while also introducing enough novelty to enhance the model's ability to generalize to unseen data. We examine multiple augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to strengthen model resilience without sacrificing accuracy. Our experimental results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA attacks without sacrificing performance on test data.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Marius Fracarolli",
        "Michael Staniek",
        "Stefan Riezler"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d2658dd920d8a0a97bd47e3238bb3a4cf8252d4",
      "pdf_url": "",
      "publication_date": "2025-11-07",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39420a8321d7d1d183e40552037e263075e42709",
      "title": "GanNoise: Defending against black-box membership inference attacks by countering noise generation",
      "abstract": "In recent years, data privacy in deep learning has seen a notable surge of interest. Pretrained large-scale data-driven models are potential to be attacked risky with membership inference attacks. However, the current corresponding defenses to prevent the leak of data may reduce the performance of pre-trained models. In this paper, we propose a novel training framework called GanNoise that preserves privacy by maintaining the accuracy of classification tasks. Through utilizing adversarial regularization to train a noise generation model, we generate noise that adds randomness to private data during model training, effectively preventing excessive memorization of the actual training data. Our experimental results illustrate the efficacy of the framework against existing attack schemes on various datasets while outperforming advanced MIA defense solutions in terms of efficiency.",
      "year": 2023,
      "venue": "2023 International Conference on Data Security and Privacy Protection (DSPP)",
      "authors": [
        "Jiaming Liang",
        "Teng Huang",
        "Zidan Luo",
        "Dan Li",
        "Yunhao Li",
        "Ziyu Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/39420a8321d7d1d183e40552037e263075e42709",
      "pdf_url": "",
      "publication_date": "2023-10-16",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a3a15fe57d1bf6d9ff7cce4276701eecd61c37b7",
      "title": "Optimizing Region of Convergence to Preserve Model Privacy from Membership Inference Attack",
      "abstract": "Recently, membership attacks are a well-known thread to disclose the training data of deep learning models, which can leak sensitive data under several circumstances. To prevent such attacks, improving the model's generalization is one of the key approaches. Such generalization capability of the deep model can be achieved through the careful examination of loss landscape and the usage of large learning rate. Based on the popular stochastic gradient optimizer, our work explores the connection between the training learning rate and the resulting model's loss landscape in defensing against the membership attacks. We found out that flat surfaces in the loss landscape which come from a large learning rate tend to preserve the model's privacy better while maintains a good prediction accuracy. We validate our findings with three model architectures, ResNet-18, VGG-11 and 2-layers MLP on the two popular datasets, FashionMNIST and CIFAR10. The results show that a large learning rate can be used to improve the model privacy from 2\u20134% with better models' accuracy.",
      "year": 2023,
      "venue": "International Conference on Knowledge and Systems Engineering",
      "authors": [
        "Bao Dung Nguyen",
        "Tuan Dung Pham",
        "Viet-Cuong Ta"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a3a15fe57d1bf6d9ff7cce4276701eecd61c37b7",
      "pdf_url": "",
      "publication_date": "2023-10-18",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "609c9647dd19da2aac4d6a623c010bbbcc76af9d",
      "title": "Membership inference attack on differentially private block coordinate descent",
      "abstract": "The extraordinary success of deep learning is made possible due to the availability of crowd-sourced large-scale training datasets. Mostly, these datasets contain personal and confidential information, thus, have great potential of being misused, raising privacy concerns. Consequently, privacy-preserving deep learning has become a primary research interest nowadays. One of the prominent approaches adopted to prevent the leakage of sensitive information about the training data is by implementing differential privacy during training for their differentially private training, which aims to preserve the privacy of deep learning models. Though these models are claimed to be a safeguard against privacy attacks targeting sensitive information, however, least amount of work is found in the literature to practically evaluate their capability by performing a sophisticated attack model on them. Recently, DP-BCD is proposed as an alternative to state-of-the-art DP-SGD, to preserve the privacy of deep-learning models, having low privacy cost and fast convergence speed with highly accurate prediction results. To check its practical capability, in this article, we analytically evaluate the impact of a sophisticated privacy attack called the membership inference attack against it in both black box as well as white box settings. More precisely, we inspect how much information can be inferred from a differentially private deep model\u2019s training data. We evaluate our experiments on benchmark datasets using AUC, attacker advantage, precision, recall, and F1-score performance metrics. The experimental results exhibit that DP-BCD keeps its promise to preserve privacy against strong adversaries while providing acceptable model utility compared to state-of-the-art techniques.",
      "year": 2023,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Shazia Riaz",
        "Saqib Ali",
        "Guojun Wang",
        "Muhammad Ahsan Latif",
        "Muhammad Zafar Iqbal"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/609c9647dd19da2aac4d6a623c010bbbcc76af9d",
      "pdf_url": "https://doi.org/10.7717/peerj-cs.1616",
      "publication_date": "2023-10-05",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "144c15893154d53bc7cdf494791298948fbf3ebe",
      "title": "An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models",
      "abstract": "Tabular data typically contains private and important information; thus, precautions must be taken before they are shared with others. Although several methods (e.g., differential privacy and k-anonymity) have been proposed to prevent information leakage, in recent years, tabular data synthesis models have become popular because they can well trade-off between data utility and privacy. However, recent research has shown that generative models for image data are susceptible to the membership inference attack, which can determine whether a given record was used to train a victim synthesis model. In this paper, we investigate the membership inference attack in the context of tabular data synthesis. We conduct experiments on 4 state-of-the-art tabular data synthesis models under two attack scenarios (i.e., one black-box and one white-box attack), and find that the membership inference attack can seriously jeopardize these models. We next conduct experiments to evaluate how well two popular differentially-private deep learning training algorithms, DP-SGD and DP-GAN, can protect the models against the attack. Our key finding is that both algorithms can largely alleviate this threat by sacrificing the generation quality.",
      "year": 2022,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "J. Hyeong",
        "Jayoung Kim",
        "Noseong Park",
        "S. Jajodia"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/144c15893154d53bc7cdf494791298948fbf3ebe",
      "pdf_url": "https://arxiv.org/pdf/2208.08114",
      "publication_date": "2022-08-17",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ff9fab1f02eddd9d2bf61bfd69841c9498c2bef7",
      "title": "MemDefense: Defending Against Membership Inference Attacks in IoT-Based Federated Learning via Pruning Perturbations",
      "abstract": "Depending on large-scale devices, the Internet of Things (IoT) provides massive data support for resource sharing and intelligent decision, but privacy risks also increase. As a popular distributed learning framework, Federated Learning (FL) is widely used because it does not need to share raw data while only parameters to collaboratively train models. However, Federated Learning is not spared by some emerging attacks, e.g., membership inference attack. Therefore, for IoT devices with limited resources, it is challenging to design a defense scheme against the membership inference attack ensuring high model utility, strong membership privacy and acceptable time efficiency. In this article, we propose MemDefense, a lightweight defense mechanism to prevent membership inference attack from local models and global models in IoT-based FL, while maintaining high model utility. MemDefense adds crafted pruning perturbations to local models at each round of FL by deploying two key components, i.e., parameter filter and noise generator. Specifically, the parameter filter selects the apposite model parameters which have little impact on the model test accuracy and contribute more to member inference attacks. Then, the noise generator is used to find the pruning noise that can reduce the attack accuracy while keeping high model accuracy, protecting each participant's membership privacy. We comprehensively evaluate MemDefense with different deep learning models and multiple benchmark datasets. The experimental results show that low-cost MemDefense drastically reduces the attack accuracy within limited drop of classification accuracy, meeting the requirements for model utility, membership privacy and time efficiency.",
      "year": 2025,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Meng Shen",
        "Jin Meng",
        "Ke Xu",
        "Shui Yu",
        "Liehuang Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ff9fab1f02eddd9d2bf61bfd69841c9498c2bef7",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "545e10ed7c80c6f1d870dc6fc7ddaf1983946147",
      "title": "meMIA: Multilevel Ensemble Membership Inference Attack",
      "abstract": "Leakage of private information in machine learning models can lead to breaches of confidentiality, identity theft, and unauthorized access to personal data. Ensuring the safe and trustworthy deployment of AI systems necessitates addressing privacy concerns to prevent unintentional disclosure and discrimination. One significant threat, membership inference (MI) attacks, exploit vulnerabilities in target learning models to determine if a given sample was part of the training set. However, the effectiveness of existing MI attacks is often limited by the number of classes in the dataset or the need for diverse multilevel adversarial features to exploit overfitted models. To enhance MI attack performance, we propose meMIA, a novel framework based on stacked ensemble learning. meMIA integrates embeddings from a neural network (NN) and a long short-term memory (LSTM) model, training a subsequent NN, termed the meta-model, on the concatenated embeddings. This method leverages the complementary strengths of NN and LSTM models; the LSTM captures order differences in confidence scores, while the NN discerns probability distribution differences between member and nonmember samples. We extensively evaluate meMIA on seven benchmark datasets, demonstrating that it surpasses current state-of-the-art MI attacks, achieving accuracy up to 94.6% and near-perfect recall. meMIA's superior performance, especially on datasets with fewer classes, underscores the urgent need for robust defenses against privacy attacks in machine learning, contributing to the safer and more ethical use of AI technologies.",
      "year": 2025,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Najeeb Ullah",
        "M. Aman",
        "Biplab Sikdar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/545e10ed7c80c6f1d870dc6fc7ddaf1983946147",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f8bdae3d747d3aee5cfd99cbc88d1de6d35bdee",
      "title": "LossControl: Defending Membership Inference Attacks by Controlling the Loss",
      "abstract": "Machine learning models are vulnerable to membership inference attacks (MIAs), where adversaries attempt to predict whether specific samples are part of the model\u2019s training set. Previous studies have demonstrated a strong correlation between the distinguishability of training and testing loss distributions and the model\u2019s susceptibility to MIAs. Motivated by existing results, we propose a novel training framework called LossControl, which focuses on manipulating loss to mitigate privacy leaks. In LossControl, we first utilize Soft-label Training to replace the general learning process, which facilitates model training while improving generalization. Next, we monitor overfitting samples during the training process and prevent further loss reduction by applying our designed Loss Ascent to these samples without sacrificing model performance. Through extensive evaluations across four diverse datasets (including images, medical data, and transaction records), our method consistently outperforms defense mechanisms against state-of-the-art attacks and achieves optimal model performance in most experiments, demonstrating LossControl\u2019s superior resilience against MIAs and its ability to strike an unparalleled balance between privacy and utility.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Bo Yang",
        "Hongwei Yang",
        "Renhao Lu",
        "Hui He",
        "Weizhe Zhang",
        "Haoyu He",
        "Rahul Yadav"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f8bdae3d747d3aee5cfd99cbc88d1de6d35bdee",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c93c4c3fcaf37cba4068bb800f657aed32d098d1",
      "title": "Do LLMs Offer a Robust Defense Mechanism Against Membership Inference Attacks on Graph Neural Networks?",
      "abstract": "Graph neural networks (GNNs) are deep learning models that process structured graph data. By leveraging their graphs/node classification and link prediction capabilities, they have been effectively applied in multiple domains such as community detection, location sharing services, and drug discovery. These powerful applications and the vast availability of graphs in diverse fields have facilitated the adoption of GNNs in privacy-sensitive contexts (e.g., banking systems and healthcare). Unfortunately, GNNs are vulnerable to the leakage of sensitive information through well-defined attacks. Our main focus is on membership inference attacks (MIAs) that allow the attacker to infer whether a given sample belongs to the training dataset. To prevent this, we introduce three LLM-guided defense mechanisms applied at the posterior level: posterior encoding with noise, knowledge distillation, and secure aggregation. Our proposed approaches not only successfully reduce MIA accuracy but also maintain the model\u2019s performance on the node classification task. Our findings, validated through extensive experiments on widely used GNN architectures, offer insights into balancing privacy preservation with predictive performance.",
      "year": 2025,
      "venue": "De Computis",
      "authors": [
        "Abdellah Jnaini",
        "M. Koulali"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c93c4c3fcaf37cba4068bb800f657aed32d098d1",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05617a596270b560b4d5c93ff39ee3225c305d89",
      "title": "Against Membership Inference Attack: Pruning is All You Need",
      "abstract": "The large model size, high computational operations, and vulnerability against membership inference attack (MIA) have impeded deep learning or deep neural networks (DNNs) popularity, especially on mobile devices. To address the challenge, we envision that the weight pruning technique will help DNNs against MIA while reducing model storage and computational operation. In this work, we propose a pruning algorithm, and we show that the proposed algorithm can find a subnetwork that can prevent privacy leakage from MIA and achieves competitive accuracy with the original DNNs. We also verify our theoretical insights with experiments. Our experimental results illustrate that the attack accuracy using model compression is up to 13.6% and 10% lower than that of the baseline and Min-Max game, accordingly.",
      "year": 2020,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Yijue Wang",
        "Chenghong Wang",
        "Zigeng Wang",
        "Shangli Zhou",
        "Hang Liu",
        "J. Bi",
        "Caiwen Ding",
        "S. Rajasekaran"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/05617a596270b560b4d5c93ff39ee3225c305d89",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0432.pdf",
      "publication_date": "2020-08-28",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8394909a8b99c02cbb79d3c6813424ab6f36348e",
      "title": "Privacy-Preserving in Defending against Membership Inference Attacks",
      "abstract": "The membership inference attack refers to the attacker's purpose to infer whether the data sample is in the target classifier training dataset. The ability of an adversary to ascertain the presence of an individual constitutes an obvious privacy threat if relate to a group of users that share a sensitive characteristic. Many defense methods have been proposed for membership inference attack, but they have not achieved the expected privacy effect. In this paper, we quantify the impact of these choices on privacy in experiments using logistic regression and neural network models. Using both formal and empirical analyses, we illustrate that differential privacy and L2 regularization can effectively prevent member inference attacks.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Zuobin Ying",
        "Yun Zhang",
        "Ximeng Liu"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/8394909a8b99c02cbb79d3c6813424ab6f36348e",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b814a569f67f116dd3ee19d2b033d6df4e736fa8",
      "title": "Use the Spear as a Shield: An Adversarial Example Based Privacy-Preserving Technique Against Membership Inference Attacks",
      "abstract": "Recent researches demonstrate that deep learning models are vulnerable to membership inference attacks. Few defenses have been proposed, but suffer from compromising the performance or quality of the target model, or cannot effectively resist membership inference attacks. This paper proposes an adversarial example based privacy-preserving technique (AEPPT), which adds crafted adversarial perturbations to the prediction of the target model to mislead the adversary's membership inference model. The added adversarial perturbations do not affect the accuracy of the target model, while can prevent the adversary from inferring whether a specific data is in the training set of the target model. Since AEPPT only modifies the original output of the target model, the proposed method does not require to modify or retrain the target model. Experimental results show that the proposed method can reduce the inference accuracy and precision of the membership inference model to around 50%, which is close to a random guess. The recall of the membership inference model drops from 88.24% to 6.48% on TinyImageNet dataset, drops from 98.5% to 17.1% on Purchase dataset, drops from 97.70% to 40.30% on ImageNet dataset, and drops from 88.7% to 51.9% on the CIFAR100 dataset, respectively. Besides, the performances of the proposed method under various factors (i.e., perturbation step size, number of adversary's data, proportion of member data and non-member data used to train substitute membership inference model, number of target model's output classes, and different membership inference models) are evaluated, which demonstrate that the proposed method can resist membership inference attacks under different conditions. Moreover, for those adaptive attacks where the adversary knows the defense mechanism, the proposed AEPPT is also demonstrated to be effective. Compared with the state-of-the-art defense methods, the proposed defense can significantly degrade the accuracy and precision of membership inference attacks to 50% (i.e., random guess), while the normal performance and utility of the target model are not affected.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Mingfu Xue",
        "Chengxiang Yuan",
        "Can He",
        "Zhiyu Wu",
        "Yushu Zhang",
        "Zhe Liu",
        "Weiqiang Liu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/b814a569f67f116dd3ee19d2b033d6df4e736fa8",
      "pdf_url": "http://arxiv.org/pdf/2011.13696",
      "publication_date": "2020-11-27",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9e21ef10806b61716f24fadfb14e1d4ce86de9f",
      "title": "Disparate Vulnerability to Membership Inference Attacks",
      "abstract": "Abstract A membership inference attack (MIA) against a machine-learning model enables an attacker to determine whether a given data record was part of the model\u2019s training data or not. In this paper, we provide an in-depth study of the phenomenon of disparate vulnerability against MIAs: unequal success rate of MIAs against different population subgroups. We first establish necessary and sufficient conditions for MIAs to be prevented, both on average and for population subgroups, using a notion of distributional generalization. Second, we derive connections of disparate vulnerability to algorithmic fairness and to differential privacy. We show that fairness can only prevent disparate vulnerability against limited classes of adversaries. Differential privacy bounds disparate vulnerability but can significantly reduce the accuracy of the model. We show that estimating disparate vulnerability by na\u00efvely applying existing attacks can lead to overestimation. We then establish which attacks are suitable for estimating disparate vulnerability, and provide a statistical framework for doing so reliably. We conduct experiments on synthetic and real-world data finding significant evidence of disparate vulnerability in realistic settings.",
      "year": 2019,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Bogdan Kulynych",
        "Mohammad Yaghini",
        "Giovanni Cherubin",
        "Michael Veale",
        "C. Troncoso"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/f9e21ef10806b61716f24fadfb14e1d4ce86de9f",
      "pdf_url": "https://petsymposium.org/popets/2022/popets-2022-0023.pdf",
      "publication_date": "2019-06-02",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd4ccf695be157c925a62c163d7552124721e501",
      "title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper, we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation, we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content, thereby addressing a critical concern in the deployment of LLMs. However, we also find that watermarking can have unintended consequences on Membership Inference Attacks (MIAs), which aim to discern whether a sample was part of the pretraining dataset and may be used to detect copyright violations. Surprisingly, we find that watermarking adversely affects the success rate of MIAs, complicating the task of detecting copyrighted text in the pretraining dataset. These results reveal the complex interplay between different regulatory measures, which may impact each other in unforeseen ways. Finally, we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Michael-Andrei Panaitescu-Liess",
        "Zora Che",
        "Bang An",
        "Yuancheng Xu",
        "Pankayaraj Pathmanathan",
        "Souradip Chakraborty",
        "Sicheng Zhu",
        "Tom Goldstein",
        "Furong Huang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/fd4ccf695be157c925a62c163d7552124721e501",
      "pdf_url": "",
      "publication_date": "2024-07-24",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "91cfd28a0b609fdedf48959f7846ec2caf425ed2",
      "title": "VLIA: Navigating Shadows with Proximity for Highly Accurate Visited Location Inference Attack against Federated Recommendation Models",
      "abstract": "Personalized location recommendation allows users to enjoy a seamless travel experience by suggesting the optimal travel locations/routes based on user preferences. Most service providers collect users' location data centrally to develop accurate route recommendation applications. Federated learning (FL) can be used as an inherent privacy-preserving mechanism in these applications to prevent users from sharing private data. However, recent research shows that FL is still vulnerable to privacy leakages. Therefore, many FL-based recommendation systems use Local Differential Privacy (LDP) to defend against such attacks. In this paper, we propose the Visited Location Inference Attack (VLIA), a novel attack for federated location recommendation systems through the lens of Membership Inference Attack (MIA). Specifically, we focus on inferring user behaviour data (visited locations) even when the federated recommendation system is protected with LDP. We design and implement VLIA leveraging both embedding and proximity information of locations, making the inference more accurate. Our extensive experiments with two state-of-the-art personalized route recommendation (PRR) systems implemented in the FL setting and two real-world trajectory datasets showcase the effectiveness of the VLIA attack. Our results show that LDP cannot defend VLIA unless the recommendation performance is significantly compromised.",
      "year": 2024,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Thirasara Ariyarathna",
        "Meisam Mohammady",
        "Hye-young Paik",
        "S. Kanhere"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/91cfd28a0b609fdedf48959f7846ec2caf425ed2",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2add26695bc1ff136327191e41be43466f18245c",
      "title": "To Detect and Prevent Sophisticated Cyber-Attacks in 5G-Enabled Networks using AI Models",
      "abstract": "Approximate membership query (AMQ) structures represented by the Bloom Filter and its variants have been\npopularly researched in recent years. Researchers have recently combined machine learning with this type of structure to reduce\nspace consumption and computation overhead further and make remarkable progress. However, with the booming performance\nin space or other metrics, researchers tend to ignore the security of the trained model. The machine learning model is vulnerable\nto poisoning attacks, and naturally, we infer that the learning-based filters also have the same deficiencies. Hence, in this paper,\nto confirm the inference mentioned above, experiments on the real-world datasets of URLs are conducted and prove that it is\nnecessary to consider the security issue when using learning-based filters. We show that by data poisoning, the attacker can\ndeflect learned Bloom Filters to make a false identification, which can lead to a significant loss in some cases. Aiming to solve\nthis issue, we put forward a method named Defensive Learned Bloom Filter (DLBF) to diminish the influence of data poisoning\nand achieve a better performance compared to types of learned Bloom Filters.",
      "year": 2025,
      "venue": "International Journal for Research in Applied Science and Engineering Technology",
      "authors": [
        "Dr. C. Parthasarathy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2add26695bc1ff136327191e41be43466f18245c",
      "pdf_url": "",
      "publication_date": "2025-11-30",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ae1391f06430d3e91e1ade8e201f089bec82b317",
      "title": "NSPFL: A Novel Secure and Privacy-Preserving Federated Learning With Data Integrity Auditing",
      "abstract": "Federated learning (FL) is a new distributed machine learning framework that emerged in recent years, which can protect the participants\u2019 data privacy to a certain extent without exchanging the participants\u2019 original data. Unfortunately, it can still be vulnerable to privacy attacks (e.g. membership inference attacks) or security attacks (e.g. model poisoning attacks), which can compromise participants\u2019 data or corrupt the trained model. Inspired by the existing works, we propose a novel federated learning framework with data integrity auditing called NSPFL. First, NSPFL protects against privacy attacks by using a single mask to hide the participants\u2019 original data. Second, NSPFL constructs a novel reputation evaluation method to resist security attacks by measuring the distance between the previous and current aggregated gradients. Third, NSPFL utilizes the data stored on the cloud to prevent malicious Byzantine participants from denying behaviors. Finally, sufficient theoretical analysis proves the reliability of the scheme, and a large number of experiments demonstrate the effectiveness of the NSPFL.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Zehu Zhang",
        "Yanping Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/ae1391f06430d3e91e1ade8e201f089bec82b317",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35c5e3f1c5116698d0805350217a0d5067bc573a",
      "title": "FACE-AUDITOR: Data Auditing in Facial Recognition Systems",
      "abstract": "Few-shot-based facial recognition systems have gained increasing attention due to their scalability and ability to work with a few face images during the model deployment phase. However, the power of facial recognition systems enables entities with moderate resources to canvas the Internet and build well-performed facial recognition models without people's awareness and consent. To prevent the face images from being misused, one straightforward approach is to modify the raw face images before sharing them, which inevitably destroys the semantic information, increases the difficulty of retroactivity, and is still prone to adaptive attacks. Therefore, an auditing method that does not interfere with the facial recognition model's utility and cannot be quickly bypassed is urgently needed. In this paper, we formulate the auditing process as a user-level membership inference problem and propose a complete toolkit FACE-AUDITOR that can carefully choose the probing set to query the few-shot-based facial recognition model and determine whether any of a user's face images is used in training the model. We further propose to use the similarity scores between the original face images as reference information to improve the auditing performance. Extensive experiments on multiple real-world face image datasets show that FACE-AUDITOR can achieve auditing accuracy of up to $99\\%$. Finally, we show that FACE-AUDITOR is robust in the presence of several perturbation mechanisms to the training images or the target models. The source code of our experiments can be found at \\url{https://github.com/MinChen00/Face-Auditor}.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/35c5e3f1c5116698d0805350217a0d5067bc573a",
      "pdf_url": "http://arxiv.org/pdf/2304.02782",
      "publication_date": "2023-04-05",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "69dcea2925b1adead718fafbc0407885befff708",
      "title": "CLDP=FATD: Secure Federated Averaging Threat Detection Framework for Intelligent Vehicle Sensor Networks Based on Client-Level Differential Privacy",
      "abstract": "The certification of real-time information in vehicles depends on threat detection. Intelligent vehicle sensor networks (IVSNs) have revolutionized modern transportation systems, enhancing traffic management and providing greater comfort. However, the increased use of smart sensing technologies has made connected and intelligent vehicles (CIVs) an attractive target for unauthorized access. Consequently, CIV owners are keen to ensure the security of their vehicle information, particularly the positioning, timing, and navigation of their vehicles. This article proposes a federated framework that utilizes client-level differential privacy (CLDP) to prevent privacy attacks, such as model inversion and membership inference attacks. In these attacks, an unauthorized party attempts to extract sensitive data from the model\u2019s outputs to exploit its predictive capabilities. The CLDP-federated averaging threat detection (CLDP-FATD) approach utilizes R\u00e9nyi-DP-Fed-Avg (RDP)/ $(\\alpha, \\epsilon)$ -DP, as an alternative to traditional DP algorithms to safeguard privacy and prevent data leakage within the federated learning (FL) framework. The efficacy of the proposed framework was evaluated using a GPS spoofing attack dataset. The findings demonstrate that the proposed scheme ensures collaborative privacy-utility tradeoff for CIV, achieving a minimal privacy budget $(\\epsilon)$ of 0.99 at 94.27% and 2.0 at 88.42% for binary and multiclass, respectively, outperforming existing approaches.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Goodness Oluchi Anyanwu",
        "Hadis Karimipour"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/69dcea2925b1adead718fafbc0407885befff708",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6646dc916aedee24cf1f76a5e69c8b70db8d3cac",
      "title": "A Secure Federated Learning Framework for 5G Networks",
      "abstract": "Federated learning (FL) has recently been proposed as an emerging paradigm to build machine learning models using distributed training datasets that are locally stored and maintained on different devices in 5G networks while providing privacy preservation for participants. In FL, the central aggregator accumulates local updates uploaded by participants to update a global model. However, there are two critical security threats: poisoning and membership inference attacks. These attacks may be carried out by malicious or unreliable participants, resulting in the construction failure of global models or privacy leakage of FL models. Therefore, it is crucial for FL to develop security means of defense. In this article, we propose a blockchain-based secure FL framework to create smart contracts and prevent malicious or unreliable participants from being involved in FL. In doing so, the central aggregator recognizes malicious and unreliable participants by automatically executing smart contracts to defend against poisoning attacks. Further, we use local differential privacy techniques to prevent membership inference attacks. Numerical results suggest that the proposed framework can effectively deter poisoning and membership inference attacks, thereby improving the security of FL in 5G networks.",
      "year": 2020,
      "venue": "IEEE wireless communications",
      "authors": [
        "Yi Liu",
        "Jia-Jie Peng",
        "Jiawen Kang",
        "Abdullah M. Iliyasu",
        "D. Niyato",
        "A. El-latif"
      ],
      "citation_count": 224,
      "url": "https://www.semanticscholar.org/paper/6646dc916aedee24cf1f76a5e69c8b70db8d3cac",
      "pdf_url": "https://arxiv.org/pdf/2005.05752",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05aaa8a1102601b0c58021443ed7047b927830d0",
      "title": "DifGuard: a privacy protection mechanism for neural network classifiers",
      "abstract": "Machine learning classifier may leak sensitive information from data providers. Attackers can use an algorithm called membership inference attack (MIA) to infer whether samples have been used as training data for the classifier. To prevent MIAs, researchers have proposed various defense methods. However, these methods often struggle to balance three constraints: the trade-off between utility and privacy, the impact on prediction accuracy, and effectiveness against both NN and Metric-based MIAs. Therefore, we propose a defense strategy called DifGuard, which is based on differential privacy and uses different privacy budgets depending on data membership during the model inference phase. To validate the effectiveness of DifGuard, we conducted extensive experiments on multiple publicly available datasets. Experimental results demonstrate that, compared to the state-of-the-art defense strategies, Dif- Guard can more effectively mitigate various risks of MIA without compromising classification accuracy.",
      "year": 2024,
      "venue": "International Conference on Mobile Ad-hoc and Sensor Networks",
      "authors": [
        "Zhao Jiang",
        "Ping Li",
        "Mingwei Liang",
        "Jingjing Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/05aaa8a1102601b0c58021443ed7047b927830d0",
      "pdf_url": "",
      "publication_date": "2024-12-20",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "364728abfc73ac805ff27eeb9d7acf2d125f2dbe",
      "title": "FedPP: Privacy-Enhanced Federated Learning for Parameter Aggregation in Heterogeneous Intelligent Connected Vehicles",
      "abstract": "With the popularization of intelligent connected vehicles (ICVs), traffic information sources are becoming ubiquitous and diverse. Given the inherent conflict between data value extraction and privacy protection, federated learning (FL) has emerged as a powerful tool for developing application models with certain generalization capability. Although FL ensures that data remains local, the parameters used for aggregation are still vulnerable to attacks, such as reverse engineering or membership inference. Methods based on homomorphic encryption or differential privacy can alleviate this issue to some extent; however, they also lead to a reduction in training performance. Furthermore, since the data collected by ICVs generally exhibit non-independent and identically distributed (non-IID) characteristics, ensuring model reliability becomes quite challenging. This paper presents a private-parameter-based federated learning method, FedPP, which integrates a Gaussian mechanism with multi-key homomorphic encryption to prevent parameter leakage while eliminating noise disturbance. By sorting and selecting the parameters to be aggregated, this approach not only demonstrates improved generalization capability under heterogeneous conditions but also effectively resists poisoning attacks. To evaluate the model, we constructed two non-IID traffic datasets using the Dirichlet distribution, which comprises a traffic sign dataset and a vehicle image dataset generated through the DALL-E model. Theoretical analysis and experimental results demonstrate that FedPP not only meets provable security under collaborative attacks but also exhibits higher model accuracy in heterogeneous vehicular network environments.",
      "year": 2025,
      "venue": "IEEE Transactions on Network and Service Management",
      "authors": [
        "Bo Mi",
        "Hangcheng Zou",
        "Darong Huang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/364728abfc73ac805ff27eeb9d7acf2d125f2dbe",
      "pdf_url": "",
      "publication_date": "2025-12-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "058a0a1e5fc792048fb8e3d730f887805cbb7bed",
      "title": "Bayesian Perspective on Memorization and Reconstruction",
      "abstract": "We introduce a new Bayesian perspective on the concept of data reconstruction, and leverage this viewpoint to propose a new security definition that, in certain settings, provably prevents reconstruction attacks. We use our paradigm to shed new light on one of the most notorious attacks in the privacy and memorization literature - fingerprinting code attacks (FPC). We argue that these attacks are really a form of membership inference attacks, rather than reconstruction attacks. Furthermore, we show that if the goal is solely to prevent reconstruction (but not membership inference), then in some cases the impossibility results derived from FPC no longer apply.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haim Kaplan",
        "Yishay Mansour",
        "Kobbi Nissim",
        "Uri Stemmer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/058a0a1e5fc792048fb8e3d730f887805cbb7bed",
      "pdf_url": "",
      "publication_date": "2025-05-29",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2b5cfeb3962ecd72dde710a562aadf20a17c9b0",
      "title": "Secure Partial Aggregation: Making Federated Learning More Robust for Industry 4.0 Applications",
      "abstract": "Big data, due to its promotion for industrial intelligence, has become the cornerstone of the Industry 4.0 era. Federated learning, proposed by Google, can effectively integrate data from different devices and different domains to train models under the premise of privacy preservation. Unfortunately, this new training paradigm faces security risks both on the client side and server side. This article proposes a new federated learning scheme to defend from client-side malicious uploads (e.g., backdoor attacks). In addition, we use cryptography techniques to prevent server-side privacy attacks (e.g., membership inference). The secure partial aggregation protocol we designed improves the privacy and robustness of federated learning. The experiments show that models can achieve high accuracy of over 90% with a proper upload proportion, while the accuracy of the backdoor attack decreased from 99.5% to 0% with the best result. Meanwhile, we prove that our protocol can disable privacy attacks.",
      "year": 2022,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Jiqiang Gao",
        "Baolei Zhang",
        "Xiaojie Guo",
        "T. Baker",
        "Min Li",
        "Zheli Liu"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/b2b5cfeb3962ecd72dde710a562aadf20a17c9b0",
      "pdf_url": "",
      "publication_date": "2022-09-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9e5e4a6bd4cc70ed89b534e427646e8f6b54403",
      "title": "FREDY: Federated Resilience Enhanced with Differential Privacy",
      "abstract": "Federated Learning is identified as a reliable technique for distributed training of ML models. Specifically, a set of dispersed nodes may collaborate through a federation in producing a jointly trained ML model without disclosing their data to each other. Each node performs local model training and then shares its trained model weights with a server node, usually called Aggregator in federated learning, as it aggregates the trained weights and then sends them back to its clients for another round of local training. Despite the data protection and security that FL provides to each client, there are still well-studied attacks such as membership inference attacks that can detect potential vulnerabilities of the FL system and thus expose sensitive data. In this paper, in order to prevent this kind of attack and address private data leakage, we introduce FREDY, a differential private federated learning framework that enables knowledge transfer from private data. Particularly, our approach has a teachers\u2013student scheme. Each teacher model is trained on sensitive, disjoint data in a federated manner, and the student model is trained on the most voted predictions of the teachers on public unlabeled data which are noisy aggregated in order to guarantee the privacy of each teacher\u2019s sensitive data. Only the student model is publicly accessible as the teacher models contain sensitive information. We show that our proposed approach guarantees the privacy of sensitive data against model inference attacks while it combines the federated learning settings for the model training procedures.",
      "year": 2023,
      "venue": "Future Internet",
      "authors": [
        "Zacharias Anastasakis",
        "T. Velivassaki",
        "Artemis C. Voulkidis",
        "S. Bourou",
        "Konstantinos Psychogyios",
        "Dimitrios Skias",
        "T. Zahariadis"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f9e5e4a6bd4cc70ed89b534e427646e8f6b54403",
      "pdf_url": "https://www.mdpi.com/1999-5903/15/9/296/pdf?version=1693550204",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b8e6133fba2aba18fecbc4f90216f9864c3522d4",
      "title": "FL-PATE: Differentially Private Federated Learning with Knowledge Transfer",
      "abstract": "Federated learning provides a solution for data privacy protection, while enabling training over the local data samples, without exchanging them. However, it is far from practical and secure because data privacy is still vulnerable due to the well-studied attacks, e.g., membership inference attacks and model inversion attacks. In this paper, to further prevent data leakage against these attacks, we propose FL-PATE, a differentially private federated learning framework with knowledge transfer. Specifically, participants with sensitive data are grouped to train teacher models under federated learning settings, and the knowledge of teacher models is transferred to a publicly accessible student model for prediction via aggregating teacher models' outputs of public datasets. A modified client-level differential privacy mechanism is used to guarantee each participant's data privacy during the corresponding teacher model's training process. The proposed framework preserves participant's privacy against membership inference attacks and the differential privacy cost is fixed. The privacy analysis and experiments demonstrate that trained teacher and student models have an excellent performance in accuracy and robustness theoretically and empirically.",
      "year": 2021,
      "venue": "Global Communications Conference",
      "authors": [
        "Yanghe Pan",
        "Jianbing Ni",
        "Z. Su"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/b8e6133fba2aba18fecbc4f90216f9864c3522d4",
      "pdf_url": "",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "36c1ee5b04350e33fb12181281139d2c003f4cdc",
      "title": "Hide-and-Seek Privacy Challenge",
      "abstract": "The clinical time-series setting poses a unique combination of challenges to data modeling and sharing. Due to the high dimensionality of clinical time series, adequate de-identification to preserve privacy while retaining data utility is difficult to achieve using common de-identification techniques. An innovative approach to this problem is synthetic data generation. From a technical perspective, a good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between high-dimensional variables across time. From the privacy perspective, the model should prevent patient re-identification by limiting vulnerability to membership inference attacks. The NeurIPS 2020 Hide-and-Seek Privacy Challenge is a novel two-tracked competition to simultaneously accelerate progress in tackling both problems. In our head-to-head format, participants in the synthetic data generation track (i.e. \"hiders\") and the patient re-identification track (i.e. \"seekers\") are directly pitted against each other by way of a new, high-quality intensive care time-series dataset: the AmsterdamUMCdb dataset. Ultimately, we seek to advance generative techniques for dense and high-dimensional temporal data streams that are (1) clinically meaningful in terms of fidelity and predictivity, as well as (2) capable of minimizing membership privacy risks in terms of the concrete notion of patient re-identification.",
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "James Jordon",
        "Daniel Jarrett",
        "Jinsung Yoon",
        "Tavian Barnes",
        "P. Elbers",
        "P. Thoral",
        "A. Ercole",
        "Cheng Zhang",
        "D. Belgrave",
        "M. Schaar"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/36c1ee5b04350e33fb12181281139d2c003f4cdc",
      "pdf_url": "",
      "publication_date": "2020-07-23",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1aa9c50a52537bdac9b1edf9f9399155ec16eee0",
      "title": "Students Parrot Their Teachers: Membership Inference on Model Distillation",
      "abstract": "Model distillation is frequently proposed as a technique to reduce the privacy leakage of machine learning. These empirical privacy defenses rely on the intuition that distilled ``student'' models protect the privacy of training data, as they only interact with this data indirectly through a ``teacher'' model. In this work, we design membership inference attacks to systematically study the privacy provided by knowledge distillation to both the teacher and student training sets. Our new attacks show that distillation alone provides only limited privacy across a number of domains. We explain the success of our attacks on distillation by showing that membership inference attacks on a private dataset can succeed even if the target model is *never* queried on any actual training points, but only on inputs whose predictions are highly influenced by training data. Finally, we show that our attacks are strongest when student and teacher sets are similar, or when the attacker can poison the teacher set.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Matthew Jagielski",
        "Milad Nasr",
        "Christopher A. Choquette-Choo",
        "Katherine Lee",
        "Nicholas Carlini"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/1aa9c50a52537bdac9b1edf9f9399155ec16eee0",
      "pdf_url": "http://arxiv.org/pdf/2303.03446",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c1f778c4cc37e555430b002f9cd7b4d8a1b6842a",
      "title": "Deep Neural Network Quantization Framework for Effective Defense against Membership Inference Attacks",
      "abstract": "Machine learning deployment on edge devices has faced challenges such as computational costs and privacy issues. Membership inference attack (MIA) refers to the attack where the adversary aims to infer whether a data sample belongs to the training set. In other words, user data privacy might be compromised by MIA from a well-trained model. Therefore, it is vital to have defense mechanisms in place to protect training data, especially in privacy-sensitive applications such as healthcare. This paper exploits the implications of quantization on privacy leakage and proposes a novel quantization method that enhances the resistance of a neural network against MIA. Recent studies have shown that model quantization leads to resistance against membership inference attacks. Existing quantization approaches primarily prioritize performance and energy efficiency; we propose a quantization framework with the main objective of boosting the resistance against membership inference attacks. Unlike conventional quantization methods whose primary objectives are compression or increased speed, our proposed quantization aims to provide defense against MIA. We evaluate the effectiveness of our methods on various popular benchmark datasets and model architectures. All popular evaluation metrics, including precision, recall, and F1-score, show improvement when compared to the full bitwidth model. For example, for ResNet on Cifar10, our experimental results show that our algorithm can reduce the attack accuracy of MIA by 14%, the true positive rate by 37%, and F1-score of members by 39% compared to the full bitwidth network. Here, reduction in true positive rate means the attacker will not be able to identify the training dataset members, which is the main goal of the MIA.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Azadeh Famili",
        "Yingjie Lao"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c1f778c4cc37e555430b002f9cd7b4d8a1b6842a",
      "pdf_url": "https://www.mdpi.com/1424-8220/23/18/7722/pdf?version=1694164508",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95745beec42dddd0ca2b53d3f09dffca9ff24eb3",
      "title": "Multi-level membership inference attacks in federated Learning based on active GAN",
      "abstract": null,
      "year": 2023,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "Hao Sui",
        "Xiaobing Sun",
        "Jiale Zhang",
        "Bing Chen",
        "Wenjuan Li"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/95745beec42dddd0ca2b53d3f09dffca9ff24eb3",
      "pdf_url": "",
      "publication_date": "2023-04-20",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4146b312ec7ddfd10d21c08a0aff6339fb12f542",
      "title": "Membership Inference Vulnerabilities in Peer-to-Peer Federated Learning",
      "abstract": "Federated learning is emerging as an efficient approach to exploit data silos that form due to regulations about data sharing and usage, thereby leveraging distributed resources to improve the learning of ML models. It is a fitting technology for cyber physical systems in applications like connected autonomous vehicles, smart farming, IoT surveillance etc. By design, every participant in federated learning has access to the latest ML model. In such a scenario, it becomes all the more important to protect the model\u2019s knowledge, and to keep the training data and its properties private. In this paper, we survey the literature of ML attacks to assess the risks that apply in a peer-to-peer (P2P) federated learning setup. We perform membership inference attacks specifically in a P2P federated learning setting with colluding adversaries to evaluate the privacy-accuracy trade offs in a deep neural network thus demonstrating the extent of data leakage possible.",
      "year": 2023,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "Alka Luqman",
        "A. Chattopadhyay",
        "Kwok-Yan Lam"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/4146b312ec7ddfd10d21c08a0aff6339fb12f542",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/173390/3/3591197.3593638.pdf",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d75b26b835292750aa199230c4a88ffee339a28",
      "title": "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models",
      "abstract": "Code pre-trained language models (CPLMs) have received great attention since they can benefit various tasks that facilitate software development and maintenance. However, CPLMs are trained on massive open-source code, raising concerns about potential data infringement. This paper launches the study of detecting unauthorized code use in CPLMs, i.e., Code Membership Inference (CMI) task. We design a framework Buzzer for different settings of CMI. Buzzer deploys several inference techniques, including signal extraction from pre-training tasks, hard-to-learn sample calibration and weighted inference, to identify code membership status accurately. Extensive experiments show that CMI can be achieved with high accuracy using Buzzer. Hence, Buzzer can serve as a CMI tool and help protect intellectual property rights.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Sheng Zhang",
        "Hui Li"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/7d75b26b835292750aa199230c4a88ffee339a28",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8544b570c20a1a9476c7e77b2d8840e26f27e6cb",
      "title": "Your Model Trains on My Data? Protecting Intellectual Property of Training Data via Membership Fingerprint Authentication",
      "abstract": "In recent years, data has become the new oil that fuels various machine learning (ML) applications. Just as the oil refining, providing data to an ML model is a product of massive costs and expertise efforts. However, how to protect the intellectual property (IP) of the training data in ML remains largely open. In this paper, we present MeFA, a novel framework for detecting training data IP embezzlement via Membership Fingerprint Authentication, which is able to determine whether a suspect ML model is trained on the to be protected target data or not. The key observation is that a part of data has a similar influence on the prediction behavior of different ML models. On this basis, MeFA leverages membership inference techniques to extract these data as the fingerprints of the target data and constructs an authentication model to verify the data\u2019s ownership by identifying the obtained membership fingerprints. MeFA has several salient features. It does not assume any knowledge of the suspect model except for its black-box prediction API, through which we can merely get the prediction output of a given input, and also does not require any modification to the dataset or the training process, since it takes advantage of the inherent membership property of the data. As a by-product, MeFA can also serve as a post-protection to verify the ownership of ML models, without modifying the training process of the model. Extensive experiments on three realistic datasets and seven types of ML models validate the effectiveness of MeFA, and demonstrate that it is also robust to scenarios when the training data is partially used or preprocessed with representative membership inference defenses.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Gaoyang Liu",
        "Tianlong Xu",
        "Xiaoqian Ma",
        "Chen Wang"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/8544b570c20a1a9476c7e77b2d8840e26f27e6cb",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "baf3b64df9dd05385abcfd11d964f15c9684cc4d",
      "title": "Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification",
      "abstract": "The risk posed by Membership Inference Attack (MIA) to deep learning models for Computer Vision (CV) tasks is well known, but MIA has not been addressed or explored fully in the Natural Language Processing (NLP) domain. In this work, we analyze the security risk posed by MIA to NLP models. We show that NLP models are at great risk to MIA, in some cases even more so than models trained on Computer Vision (CV) datasets. This includes an 8.04% increase in attack success rate on average for NLP models (as compared to CV models and datasets). We determine that there are some unique issues in NLP classification tasks in terms of model overfitting, model complexity, and data diversity that make the privacy leakage severe and very different from CV classification tasks. Based on these findings, we propose a novel defense algorithm - Gap score Regularization Integrated Pruning (GRIP), which can protect NLP models against MIA and achieve competitive testing accuracy. Our experimental results show that GRIP can decrease the MIA success rate by as much as 31.25% when compared to the undefended model. In addition, when compared to differential privacy, GRIP offers 7.81% more robustness to MIA and 13.24% higher testing accuracy. Overall our experimental results span four NLP and two CV datasets, and are tested with a total of five different model architectures.",
      "year": 2022,
      "venue": "2022 IEEE International Conference on Big Data (Big Data)",
      "authors": [
        "Yijue Wang",
        "Nuo Xu",
        "Shaoyi Huang",
        "Kaleel Mahmood",
        "Danyi Guo",
        "Caiwen Ding",
        "Wujie Wen",
        "S. Rajasekaran"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/baf3b64df9dd05385abcfd11d964f15c9684cc4d",
      "pdf_url": "",
      "publication_date": "2022-12-17",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c77aeccac9ef8fb821dc729605d9660a79eb4a6",
      "title": "Membership Inference Attacks on Aggregated Time Series with Linear Programming",
      "abstract": ": Aggregating data is a widely used technique to protect privacy. Membership inference attacks on aggregated data aim to infer whether a specific target belongs to a given aggregate. We propose to study how aggregated time series data can be susceptible to simple membership inference privacy attacks in the presence of adversarial background knowledge. We design a linear programming attack that strongly benefits from the number of data points published in the series and show on multiple public datasets how vulnerable the published data can be if the size of the aggregated data is not carefully balanced with the published time series length. We perform an extensive experimental evaluation of the attack on multiple publicly available datasets. We show the vulnerability of aggregates made of thousands of time series when the aggregate length is not carefully balanced with the published length of the time series.",
      "year": 2022,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Antonin Voyez",
        "T. Allard",
        "G. Avoine",
        "P. Cauchois",
        "\u00c9lisa Fromont",
        "Matthieu Simonin"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/6c77aeccac9ef8fb821dc729605d9660a79eb4a6",
      "pdf_url": "https://doi.org/10.5220/0011276100003283",
      "publication_date": null,
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca6b7058d79f845d8abfdc70b76f0bb0363cd91a",
      "title": "Robust and Lossless Fingerprinting of Deep Neural Networks via Pooled Membership Inference",
      "abstract": "Deep neural networks (DNNs) have already achieved great success in a lot of application areas and brought profound changes to our society. However, it also raises new security problems, among which how to protect the intellectual property (IP) of DNNs against infringement is one of the most important yet very challenging topics. To deal with this problem, recent studies focus on the IP protection of DNNs by applying digital watermarking, which embeds source information and/or authentication data into DNN models by tuning network parameters directly or indirectly. However, tuning network parameters inevitably distorts the DNN and therefore surely impairs the performance of the DNN model on its original task regardless of the degree of the performance degradation. It has motivated the authors in this paper to propose a novel technique called pooled membership inference (PMI) so as to protect the IP of the DNN models. The proposed PMI neither alters the network parameters of the given DNN model nor fine-tunes the DNN model with a sequence of carefully crafted trigger samples. Instead, it leaves the original DNN model unchanged, but can determine the ownership of the DNN model by inferring which mini-dataset among multiple mini-datasets was once used to train the target DNN model, which differs from previous arts and has remarkable potential in practice. Experiments also have demonstrated the superiority and applicability of this work.",
      "year": 2022,
      "venue": "2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",
      "authors": [
        "Hanzhou Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/ca6b7058d79f845d8abfdc70b76f0bb0363cd91a",
      "pdf_url": "https://arxiv.org/pdf/2209.04113",
      "publication_date": "2022-09-09",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a03342d346655722befae957c4daf50c2249d610",
      "title": "Identifying Harmful Media in End-to-End Encrypted Communication: Efficient Private Membership Computation",
      "abstract": null,
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Anunay Kulshrestha",
        "Jonathan R. Mayer"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/a03342d346655722befae957c4daf50c2249d610",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2540b1c8b60d44f01fc887fe0224466696c54820",
      "title": "A Dynamic Membership Data Aggregation (DMDA) Protocol for Smart Grid",
      "abstract": "In order to protect the privacy of individual data, meantime guaranteeing the utility of big data, the privacy preserving data aggregation is widely researched, which is a feasible solution since it not only preserves the statistical feature of the original data, but also masks single user's data. With smart meter owning the capability of connecting to Internet, the aggregation area extends to the virtual area rather than a traditional physical area. However, in a virtual aggregation area, the users\u2019 membership maybe frequently changes, if while executing the aggregation protocol for the traditional area, the overhead is not ignorable. In this paper, the homomorphic encryption and ID-based signature are employed to design a dynamic membership data aggregation (DMDA) scheme, which reduces the complexity on a new user's joining and an old user's quitting. In addition, the operation center obtains the sum of the data in the virtual aggregation area, meantime knows nothing about single user's data. Comparing with traditional privacy-preserving data aggregation scheme, DMDA is more suitable for next-generation smart grid and other Internet of Things environments.",
      "year": 2020,
      "venue": "IEEE Systems Journal",
      "authors": [
        "Jingcheng Song",
        "Yining Liu",
        "Jun Shao",
        "Chunming Tang"
      ],
      "citation_count": 95,
      "url": "https://www.semanticscholar.org/paper/2540b1c8b60d44f01fc887fe0224466696c54820",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/4267003/9020228/08709688.pdf",
      "publication_date": "2020-03-01",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "56800ea46f23427ef5b57448d00b15dc9833af4c",
      "title": "Subject Membership Inference Attacks in Federated Learning",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Anshuman Suri",
        "Pallika H. Kanani",
        "Virendra J. Marathe",
        "Daniel W. Peterson"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/56800ea46f23427ef5b57448d00b15dc9833af4c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "590287d411af3a4e937da8114930dbcdd4e98fde",
      "title": "The Influence of Dropout on Membership Inference in Differentially Private Models",
      "abstract": "Differentially private models seek to protect the privacy of data the model is trained on, making it an important component of model security and privacy. At the same time, data scientists and machine learning engineers seek to use uncertainty quantification methods to ensure models are as useful and actionable as possible. We explore the tension between uncertainty quantification via dropout and privacy by conducting membership inference attacks against models with and without differential privacy. We find that models with large dropout slightly increases a model's risk to succumbing to membership inference attacks in all cases including in differentially private models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Erick Galinkin"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/590287d411af3a4e937da8114930dbcdd4e98fde",
      "pdf_url": "",
      "publication_date": "2021-03-16",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "60379eaf20c6be845415a23f9b5932b8097a8ea6",
      "title": "Towards a Trusted and Privacy Preserving Membership Service in Distributed Ledger Using Intel Software Guard Extensions",
      "abstract": null,
      "year": 2017,
      "venue": "International Conference on Information, Communications and Signal Processing",
      "authors": [
        "Xueping Liang",
        "S. Shetty",
        "Deepak K. Tosh",
        "Peter B. Foytik",
        "Lingchen Zhang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/60379eaf20c6be845415a23f9b5932b8097a8ea6",
      "pdf_url": "",
      "publication_date": "2017-12-06",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "570a341a8fd511cf0e05687110f053aaac646010",
      "title": "Towards Unbounded Machine Unlearning",
      "abstract": "Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Meghdad Kurmanji",
        "P. Triantafillou",
        "Eleni Triantafillou"
      ],
      "citation_count": 210,
      "url": "https://www.semanticscholar.org/paper/570a341a8fd511cf0e05687110f053aaac646010",
      "pdf_url": "https://arxiv.org/pdf/2302.09880",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1fb9e71e8daa050b0276916aae362144f0fa5c4c",
      "title": "Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs",
      "abstract": "Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yepeng Liu",
        "Xuandong Zhao",
        "D. Song",
        "Yuheng Bu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/1fb9e71e8daa050b0276916aae362144f0fa5c4c",
      "pdf_url": "",
      "publication_date": "2025-02-15",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "19810573c9f9709afdd379386b5ef6a3c6f9fc91",
      "title": "An Optimized Stacking Ensemble Model for Phishing Websites Detection",
      "abstract": "Security attacks on legitimate websites to steal users\u2019 information, known as phishing attacks, have been increasing. This kind of attack does not just affect individuals\u2019 or organisations\u2019 websites. Although several detection methods for phishing websites have been proposed using machine learning, deep learning, and other approaches, their detection accuracy still needs to be enhanced. This paper proposes an optimized stacking ensemble method for phishing website detection. The optimisation was carried out using a genetic algorithm (GA) to tune the parameters of several ensemble machine learning methods, including random forests, AdaBoost, XGBoost, Bagging, GradientBoost, and LightGBM. The optimized classifiers were then ranked, and the best three models were chosen as base classifiers of a stacking ensemble method. The experiments were conducted on three phishing website datasets that consisted of both phishing websites and legitimate websites\u2014the Phishing Websites Data Set from UCI (Dataset 1); Phishing Dataset for Machine Learning from Mendeley (Dataset 2, and Datasets for Phishing Websites Detection from Mendeley (Dataset 3). The experimental results showed an improvement using the optimized stacking ensemble method, where the detection accuracy reached 97.16%, 98.58%, and 97.39% for Dataset 1, Dataset 2, and Dataset 3, respectively.",
      "year": 2021,
      "venue": "Electronics",
      "authors": [
        "Mohammed Al-Sarem",
        "Faisal Saeed",
        "Zeyad Ghaleb Al-Mekhlafi",
        "B. Mohammed",
        "Tawfik Al-Hadhrami",
        "Mohammad T. Alshammari",
        "Abdulrahman Alreshidi",
        "Talal Sarheed Alshammari"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/19810573c9f9709afdd379386b5ef6a3c6f9fc91",
      "pdf_url": "https://www.mdpi.com/2079-9292/10/11/1285/pdf?version=1622191375",
      "publication_date": "2021-05-28",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8ab04b65df7c48efe2763b72213cce23e9fc51f1",
      "title": "Robust Ensemble Machine Learning Model for Filtering Phishing URLs: Expandable Random Gradient Stacked Voting Classifier (ERG-SVC)",
      "abstract": "As cyber-attacks grow fast and complicated, the cybersecurity industry faces challenges to utilize state-of-the-art technology and strategies to battle the consistently present malicious threats. Phishing is a sort of social engineering attack produced technically and classified as identity theft and complicated attack vectors to steal information of internet users. In this perspective, our main objective of this study is to propose a unique, robust ensemble machine learning model architecture that provides the highest prediction accuracy with a low error rate while proposing few other robust machine learning models. Both <italic>supervised</italic> and <italic>unsupervised</italic> techniques were used for the detection process. For our experiments, seven classification algorithms, one clustering algorithm, two ensemble techniques, and two large standard legitimate datasets with 73,575 URLs and 100,000 URLs were used. Two test modes (percentage split, K-Fold cross-validation) were utilized for conducting experiments and final predictions. Mechanisms were developed to (I) identify the best <inline-formula> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula>, which is the optimal heuristic-based threshold value for splitting words into subwords for each classifier, (II) tune hyperparameters for each classifier to specify the best parameter combination, (III) select prominent features using various feature selection techniques, (IV) propose a robust ensemble model (classifier) called the <italic>Expandable Random Gradient Stacked Voting Classifier</italic> (<italic>ERG-SVC</italic>) utilizing a voting classifier along with a model architecture, (V) analyze possible clusters of the dataset using k-means clustering, (VI) thoroughly analyze the <italic>gradient boost</italic> classifier (<italic>GB</italic>) with respect to utilizing the \u201ccriterion\u201d parameter with the Mean Absolute Error (<italic>MAE</italic>), Mean Squared Error (<italic>MSE</italic>), and <italic>Friendman_MSE</italic>, and(VII) propose a lightweight preprocessor to reduce computational cost and preprocessing time. Initial experiments were carried out with 46 features; the number of features was reduced to 22 after the experiments. The results show that the <italic>GB</italic> classifier outperformed with the least number of <italic>NLP</italic> based features by achieving a 98.118% prediction accuracy. Furthermore, our stacking ensemble model and proposed voting ensemble model (<italic>ERG-SVC</italic>) outperformed other tested approaches and yielded reliable prediction accuracy results in detecting malicious URLs at rates of 98.23% and 98.27%, respectively.",
      "year": 2021,
      "venue": "IEEE Access",
      "authors": [
        "Pubudu L. Indrasiri",
        "Malka N. Halgamuge",
        "Azeem Mohammad"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/8ab04b65df7c48efe2763b72213cce23e9fc51f1",
      "pdf_url": "https://doi.org/10.1109/access.2021.3124628",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9313ada269360c9faa74385d966122e5a20e69a",
      "title": "The Secret Sharer: Measuring Unintended Neural Network Memorization & Extracting Secrets",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Nicholas Carlini",
        "Chang Liu",
        "Jernej Kos",
        "\u00da. Erlingsson",
        "D. Song"
      ],
      "citation_count": 197,
      "url": "https://www.semanticscholar.org/paper/f9313ada269360c9faa74385d966122e5a20e69a",
      "pdf_url": "",
      "publication_date": "2018-02-22",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b0a6db399dd7f430ebad6d68dce61ec51a99fd9d",
      "title": "Explainable Fake News Detection with Large Language Model via Defense Among Competing Wisdom",
      "abstract": "Most fake news detection methods learn latent feature representations based on neural networks, which makes them black boxes to classify a piece of news without giving any justification. Existing explainable systems generate veracity justifications from investigative journalism, which suffer from debunking delayed and low efficiency. Recent studies simply assume that the justification is equivalent to the majority opinions expressed in the wisdom of crowds. However, the opinions typically contain some inaccurate or biased information since the wisdom of crowds is uncensored. To detect fake news from a sea of diverse, crowded and even competing narratives, in this paper, we propose a novel defense-based explainable fake news detection framework. Specifically, we first propose an evidence extraction module to split the wisdom of crowds into two competing parties and respectively detect salient evidences. To gain concise insights from evidences, we then design a prompt-based module that utilizes a large language model to generate justifications by inferring reasons towards two possible veracities. Finally, we propose a defense-based inference module to determine veracity via modeling the defense among these justifications. Extensive experiments conducted on two real-world benchmarks demonstrate that our proposed method outperforms state-of-the-art baselines in terms of fake news detection and provides high-quality justifications.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Bo Wang",
        "Jing Ma",
        "Hongzhan Lin",
        "Zhiwei Yang",
        "Ruichao Yang",
        "Yuan Tian",
        "Yi Chang"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/b0a6db399dd7f430ebad6d68dce61ec51a99fd9d",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "336e81d1074c54b91bc0ef1cac420e584e66de91",
      "title": "A Big Data Provenance Model for Data Security Supervision Based on PROV-DM Model",
      "abstract": "Nowadays, big data has become a hot research topic. It gives fresh impetus to the economic and social development. However, the huge value of big data also makes it the focus of attacks. Big data security incidents occur frequently in recent years. The security supervision capacities for big data do not match its important role. Data provenance which describes the origins of data and the process by which it arrived the current state, is an effective approach for data supervision. For the full use of provenance in big data supervision, a provenance model which defines the concepts used to represent the provenance types and relations is required to be built in advance, but current provenance models do not adapt to big data scenarios well. In this paper, we comprehensively consider the characteristics of big data and the requirements of data security supervision, extend the widely used provenance model PROV-DM by subtyping and new relation definition, and propose a big data provenance model (BDPM) for data supervision. BDPM model supports the provenance representation of various data types and diverse data processing modes to represent the entire data transformation process through different components in the big data system, and defines new relations to enrich provenance analysis functions. Based on BDPM model, we introduce the constraints that should be satisfied in the construction of valid provenance graph and present the data security supervision methods via provenance graph analysis. Finally, we evaluated the satisfiability of BDPM model through a case study.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Yuanzhao Gao",
        "Xing-yuan Chen",
        "Xuehui Du"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/336e81d1074c54b91bc0ef1cac420e584e66de91",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007438.pdf",
      "publication_date": "2020-02-24",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c0fa3d95e5ffdd58fb62b22ec6c7e91147e4d33a",
      "title": "A Provenance Model for the European Union General Data Protection Regulation",
      "abstract": null,
      "year": 2018,
      "venue": "International Provenance and Annotation Workshop",
      "authors": [
        "Benjamin E. Ujcich",
        "Adam Bates",
        "W. Sanders"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/c0fa3d95e5ffdd58fb62b22ec6c7e91147e4d33a",
      "pdf_url": "",
      "publication_date": "2018-07-09",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f167736904a7bff24f471493b04725474ea89741",
      "title": "A Provenance Model for Quantified Self Data",
      "abstract": null,
      "year": 2016,
      "venue": "Interacci\u00f3n",
      "authors": [
        "A. Schreiber"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/f167736904a7bff24f471493b04725474ea89741",
      "pdf_url": "",
      "publication_date": "2016-07-17",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b787a01e583df8438389de08b6c4f80332e3363c",
      "title": "DF-MIA: A Distribution-Free Membership Inference Attack on Fine-Tuned Large Language Models",
      "abstract": "Membership Inference Attack (MIA) aims to determine if a specific sample is present in the training dataset of a target machine learning model. \nPrevious MIAs against fine-tuned Large Language Models (LLMs) either fail to address the unique challenges in the fine-tuned setting or rely on strong assumption of the training data distribution.\nThis paper proposes a distribution-free MIA framework tailored for fine-tuned LLMs, named DF-MIA. \nWe recognize that samples await to test can serve as a valuable reference dataset for fine-tuning reference models. \nBy enhancing the signals of non-member samples within this reference dataset, we can achieve a more reliable and practical calibration of probabilities, improving the differentiation between members and non-members.\nLeveraging these insights, we have developed a two-stage framework that employs specially designed data augmentation and perturbation techniques to prioritize the significance of non-members and mitigate the influence of potential members within the reference dataset.\nWe evaluate our method on three representative LLM models ranging from 1B to 8B on three datasets. The results demonstrate that the DF-MIA significantly enhances the performance of MIA.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Zhiheng Huang",
        "Yannan Liu",
        "Daojing He",
        "Yu Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b787a01e583df8438389de08b6c4f80332e3363c",
      "pdf_url": "https://doi.org/10.1609/aaai.v39i1.32012",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "78d7fba363597eadb425e8a477262eaa98a0fd91",
      "title": "Prompt-based Unifying Inference Attack on Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) provide important prospective insights in applications such as social behavior analysis and financial risk analysis based on their powerful learning capabilities on graph data. Nevertheless, GNNs' predictive performance relies on the quality of task-specific node labels, so it is common practice to improve the model's generalization ability in the downstream execution of decision-making tasks through pre-training. Graph prompting is a prudent choice but risky without taking measures to prevent data leakage. In other words, in high-risk decision scenarios, prompt learning can infer private information by accessing model parameters trained on private data (publishing model parameters in pre-training, i.e., without directly leaking the raw data, is a tacitly accepted trend). However, myriad graph inference attacks necessitate tailored module design and processing to enhance inference capabilities due to variations in supervision signals. In this paper, we propose a novel Prompt-based unifying Inference Attack framework on GNNs, named ProIA. Specifically, ProIA retains the crucial topological information of the graph during pre-training, enhancing the background knowledge of the inference attack model. \nIt then utilizes a unified prompt and introduces additional disentanglement factors in downstream attacks to adapt to task-relevant knowledge. Finally, extensive experiments show that ProIA enhances attack capabilities and demonstrates remarkable adaptability to various inference attacks.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yuecen Wei",
        "Xingcheng Fu",
        "Lingyun Liu",
        "Qingyun Sun",
        "Hao Peng",
        "Chunming Hu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/78d7fba363597eadb425e8a477262eaa98a0fd91",
      "pdf_url": "",
      "publication_date": "2024-12-20",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9013c23f1f9aaae58480b6ef77266c21b5eec91",
      "title": "Anti-Attack Intrusion Detection Model Based on MPNN and Traffic Spatiotemporal Characteristics",
      "abstract": null,
      "year": 2023,
      "venue": "Journal of Grid Computing",
      "authors": [
        "Jiazhong Lu",
        "Jin Lan",
        "Yuanyuan Huang",
        "Maojia Song",
        "Xiaolei Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/f9013c23f1f9aaae58480b6ef77266c21b5eec91",
      "pdf_url": "",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "db5b9b624a4595ba9dfdb1d0e144c5167371a608",
      "title": "On the Privacy Risk of In-context Learning",
      "abstract": "Large language models (LLMs) are excellent few-shot learners. They can perform a wide variety of tasks purely based on natural language prompts provided to them. These prompts contain data of a specific downstream task -- often the private dataset of a party, e.g., a company that wants to leverage the LLM for their purposes. We show that deploying prompted models presents a significant privacy risk for the data used within the prompt by instantiating a highly effective membership inference attack. We also observe that the privacy risk of prompted models exceeds fine-tuned models at the same utility levels. After identifying the model's sensitivity to their prompts -- in the form of a significantly higher prediction confidence on the prompted data -- as a cause for the increased risk, we propose ensembling as a mitigation strategy. By aggregating over multiple different versions of a prompted model, membership inference risk can be decreased.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Haonan Duan",
        "Adam Dziedzic",
        "Mohammad Yaghini",
        "Nicolas Papernot",
        "Franziska Boenisch"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/db5b9b624a4595ba9dfdb1d0e144c5167371a608",
      "pdf_url": "",
      "publication_date": "2024-11-15",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0a58c2665ea05a75939a870131389fc193617ac",
      "title": "A Secure Object Detection Technique for Intelligent Transportation Systems",
      "abstract": "Federated Learning is a decentralized machine learning technique that creates a global model by aggregating local models from multiple edge devices without a need to access the local data. However, due to the distributed nature of federated learning, there is a larger attack surface, making cyber-attack detection and defense challenging. Although prior works developed various defense strategies to address security issues in federated learning settings, most approaches fail to mitigate cyber-attacks due to the diverse characteristics of the attack, edge devices, and data distribution. To address this issue, this paper develops a hybrid privacy-preserving algorithm to safeguard federated learning methods against malicious attacks in Intelligent Transportation Systems, considering object detection as a downstream machine learning task. This algorithm involves the edge devices (e.g., autonomous vehicles) and road side units to collaboratively train their model while maintaining the privacy of their respective data. Furthermore, this hybrid algorithm provides robust security against data poisoning-based model replacement and inference attacks throughout the training phase. We evaluated our model using the CIFAR10 and LISA traffic light dataset, demonstrating its ability to mitigate malicious attacks with minimal impact on the performance of main tasks.",
      "year": 2024,
      "venue": "IEEE Open Journal of Intelligent Transportation Systems",
      "authors": [
        "Jueal Mia",
        "M. H. Amini"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/d0a58c2665ea05a75939a870131389fc193617ac",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca744e2e6ee50788695948c3a3058146de4d2cb9",
      "title": "M-Door: Joint Attack of Backdoor Injection and Membership Inference in Federated Learning",
      "abstract": "Federated learning (FL) collaboratively trains global models while preserving private data locally, making it an ideal privacy-preserving learning technique. However, recent studies have shown that FL poses risks of security attacks and privacy leaks during model parameter transfer. Existing research suggests that backdoor attacks cannot assist with membership inference attacks in machine learning. This paper proposes a joint attack of backdoor injection and membership inference in FL, M-Door, which can connect two independent work lines to ensure the security and privacy of FL. In M-Door, an attacker hidden within the client can not only perform backdoor attacks on the global model, but also perform membership inference attacks on the global model by analyzing the transmitted model parameters. This attack method can improve the success rate of backdoor attacks, and simultaneously increase the success rate of membership inference attacks. We conduct extensive experiments on three image classification tasks to evaluate the effectiveness of M-Door. Compared with the other two attack methods, the experimental results show that M-Door exhibits significant advantages in backdoor and membership inference attacks under both IID and Non-IID data settings.",
      "year": 2024,
      "venue": "Global Communications Conference",
      "authors": [
        "Ye Liu",
        "Shan Chang",
        "Denghui Li",
        "Minghui Dai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ca744e2e6ee50788695948c3a3058146de4d2cb9",
      "pdf_url": "",
      "publication_date": "2024-12-08",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "125f707757abc332a2b225914d5f6e91cce0c875",
      "title": "GNN4IFA: Interest Flooding Attack Detection With Graph Neural Networks",
      "abstract": "In the context of Information-Centric Networking, Interest Flooding Attacks (IFAs) represent a new and dangerous sort of distributed denial of service. Since existing proposals targeting IFAs mainly focus on local information, in this paper we propose GNN4IFA as the first mechanism exploiting complex non-local knowledge for IFA detection by leveraging Graph Neural Networks (GNNs) handling the overall network topology.In order to test GNN4IFA, we collect SPOTIFAI, a novel dataset filling the current lack of available IFA datasets by covering a variety of IFA setups, including ~40 heterogeneous scenarios over three network topologies. We show that GNN4IFA performs well on all tested topologies and setups, reaching over 99% detection rate along with a negligible false positive rate and small computational costs. Overall, GNN4IFA overcomes state-of-the-art detection mechanisms both in terms of raw detection and flexibility, and \u2013 unlike all previous solutions in the literature \u2013 also enables the transfer of its detection on network topologies different from the one used in its design phase.",
      "year": 2023,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Andrea Agiollo",
        "Enkeleda Bardhi",
        "M. Conti",
        "R. Lazzeretti",
        "E. Losiouk",
        "A. Omicini"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/125f707757abc332a2b225914d5f6e91cce0c875",
      "pdf_url": "https://iris.uniroma1.it/bitstream/11573/1686129/1/Agiollo_GNN4IFA_2023.pdf",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "24b8c37adcd20f5490a76003ce5146a80ee4711a",
      "title": "SplitAUM: Auxiliary Model-Based Label Inference Attack Against Split Learning",
      "abstract": "Split learning has emerged as a practical and efficient privacy-preserving distributed machine learning paradigm. Understanding the privacy risks of split learning is critical for its application in privacy-sensitive scenarios. However, previous attacks against split learning generally depended on unduly strong assumptions or non-standard settings advantageous to the attacker. This paper proposes a novel auxiliary model-based label inference attack framework against learning, named SplitAUM. SplitAUM first builds an auxiliary model on the client side using intermediate representations of the cut layer and a small number of dummy labels. Then, the learning regularization objective is carefully designed to train the auxiliary model and transfer the knowledge of the server model to the client. Finally, SplitAUM uses the auxiliary model output on local data to infer the server\u2019s privacy label. In addition, to further improve the attack effect, we use semi-supervised clustering to initialize the dummy labels of the auxiliary model. Since SplitAUM relies only on auxiliary models, it is highly scalable. We conduct extensive experiments on three different categories of datasets, comparing four typical attacks. Experimental results demonstrate that SplitAUM can effectively infer privacy labels and outperform existing attack frameworks in challenging yet practical scenarios. We hope our work paves the way for future analyses of the security of split learning.",
      "year": 2025,
      "venue": "IEEE Transactions on Network and Service Management",
      "authors": [
        "Kai Zhao",
        "Xiaowei Chuo",
        "Fangchao Yu",
        "Bo Zeng",
        "Zhi Pang",
        "Lina Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/24b8c37adcd20f5490a76003ce5146a80ee4711a",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95d4cdaec450c0e26e45f13e231c075c1809ac7c",
      "title": "You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods",
      "abstract": null,
      "year": 2020,
      "venue": "SafeAI@AAAI",
      "authors": [
        "B. Dimanov",
        "Umang Bhatt",
        "M. Jamnik",
        "Adrian Weller"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/95d4cdaec450c0e26e45f13e231c075c1809ac7c",
      "pdf_url": "",
      "publication_date": "2020-01-14",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "949a5c5dff29940d8b5cab356620aaa8cd2cda88",
      "title": "PASS: A Parameter Audit-Based Secure and Fair Federated Learning Scheme Against Free-Rider Attack",
      "abstract": "Federated learning (FL) as a secure distributed learning framework gains interests in Internet of Things (IoT) due to its capability of protecting the privacy of participant data. However, traditional FL systems are vulnerable to free-rider (FR) attacks, which causes unfairness, privacy leakage and inferior performance to FL systems. The prior defense mechanisms against FR attacks assumed that malicious clients (namely, adversaries) declare less than 50% of the total amount of clients. Moreover, they aimed for anonymous FR (AFR) attacks and lost effectiveness in resisting selfish FR (SFR) attacks. In this article, we propose a parameter audit-based secure and fair FL scheme (PASS) against FR attack. PASS has the following key features: 1) prevent from privacy leakage with less accuracy loss; 2) be effective in countering both AFR and SFR attacks; and 3) work well no matter whether AFR and SFR adversaries occupy the majority of clients or not. Extensive experimental results validate that PASS: 1) has the same level as the state-of-the-art method in mean square error against privacy leakage; 2) defends against AFR and SFR attacks in terms of a higher defense success rate, lower false positive rate, and higher F1-score; and 3) is still effective where adversaries exceed 50%, with F1-score 89% against AFR attack and F1-score 87% against SFR attack. Note that PASS produces no negative effect on FL accuracy when there is no FR adversary.",
      "year": 2022,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Jianhua Wang",
        "Xiaolin Chang",
        "J. Misic",
        "V. Mi\u0161i\u0107",
        "Yixiang Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/949a5c5dff29940d8b5cab356620aaa8cd2cda88",
      "pdf_url": "https://arxiv.org/pdf/2207.07292",
      "publication_date": "2022-07-15",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d90d93931c6ccc15d7f9cda3b514d8f5f663c1cd",
      "title": "An Analysis of Insider Attack Detection Using Machine Learning Algorithms",
      "abstract": "Among the greatest obstacles in cybersecurity is insider threat, which is a well-known massive issue. This anomaly shows that the vulnerability calls for specialized detection techniques, and resources that can help with the accurate and quick detection of an insider who is harmful. Numerous studies on identifying insider threats and related topics were also conducted to tackle this problem are proposed. Various researches sought to improve the conceptual perception of insider risks. Furthermore, there are numerous drawbacks, including a dearth of actual cases, unfairness in drawing decisions, a lack of self-optimization in learning, which would be a huge concern and is still vague, and the absence of an investigation that focuses on the conceptual, technological, and numerical facets concerning insider threats and identifying insider threats from a wide range of perspectives. The intention of the paper is to afford a thorough exploration of the categories, levels, and methodologies of modern insiders based on machine learning techniques. Further, the approach and evaluation metrics for predictive models based on machine learning are discussed. The paper concludes by outlining the difficulties encountered and offering some suggestions for efficient threat identification using machine learning.",
      "year": 2022,
      "venue": "2022 IEEE 2nd International Conference on Mobile Networks and Wireless Communications (ICMNWC)",
      "authors": [
        "B. Nagabhushana Babu",
        "M. Gunasekaran"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d90d93931c6ccc15d7f9cda3b514d8f5f663c1cd",
      "pdf_url": "",
      "publication_date": "2022-12-02",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b6ba866d5b3c8e960a0df35f21d77fe11fff3a39",
      "title": "Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation",
      "abstract": "Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Yi Sun",
        "Abel N. Valente",
        "Sijia Liu",
        "Dakuo Wang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b6ba866d5b3c8e960a0df35f21d77fe11fff3a39",
      "pdf_url": "",
      "publication_date": "2021-03-25",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abb71ad6f49479aa1d0f2b99838f45dda12d4536",
      "title": "EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning",
      "abstract": "Federated Learning (FL) is a technique that allows multiple parties to train a shared model collaboratively without disclosing their private data. It has become increasingly popular due to its distinct privacy advantages. However, FL models can suffer from biases against certain demographic groups (e.g., racial and gender groups) due to the heterogeneity of data and party selection. Researchers have proposed various strategies for characterizing the group fairness of FL algorithms to address this issue. However, the effectiveness of these strategies in the face of deliberate adversarial attacks has not been fully explored. Although existing studies have revealed various threats (e.g., model poisoning attacks) against FL systems caused by malicious participants, their primary aim is to decrease model accuracy, while the potential of leveraging poisonous model updates to exacerbate model unfairness remains unexplored. In this paper, we propose a new type of model poisoning attack, EAB-FL, with a focus on exacerbating group unfairness while maintaining a good level of model utility. Extensive experiments on three datasets demonstrate the effectiveness and efficiency of our attack, even with state-of-the-art fairness optimization algorithms and secure aggregation rules employed. We hope this work will help the community fully understand the attack surfaces of current FL systems and facilitate corresponding mitigation to improve their resilience.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Syed Irfan Ali Meerza",
        "Jian Liu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/abb71ad6f49479aa1d0f2b99838f45dda12d4536",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0ba1a71f65c609cd2709d818cfdbd9d912f7e17a",
      "title": "Towards Fair Federated Learning via Unbiased Feature Aggregation",
      "abstract": "Federated learning (FL) is a distributed machine learning framework that enables multiple clients to collaboratively train models without raw data exchange. Prior studies on FL mainly focus on optimizing learning performance, enhancing privacy preservation, and improving attack resilience. However, little work studies how to mitigate the unfairness of federated trained models while unfair models would make discriminatory decisions toward certain groups or populations (e.g., favoring males over females), leading to serious ethical concerns. Thus, it is crucial to mitigate model unfairness in FL, yet challenging as this requires centralized access to each data point's fairness-sensitive information (e.g., race, gender), which is prohibited in FL. In this work, we propose a novel fair FL framework FedUFA, where the server can aggregate clients\u2019 learned knowledge in an unbiased manner, to obtain fair and high-usability federated trained models. Specifically, to unearth the bias in clients\u2019 local data and account for potentially heterogeneous local models, we propose a knowledge distillation-based FL scheme, where clients\u2019 knowledge of learned features on a public dataset is amalgamated to the server for aggregation. We train an unbiased feature mapper at the server to remove fairness-sensitive latent features and extract fair representations from clients\u2019 submitted raw features. In particular, we design an adversarial training method to train the mapper, which involves a predictor aiming to maximize the prediction accuracy on the FL task and a discriminator intending to help identify fairness-sensitive features. Extensive experiments on real-world datasets demonstrate the effectiveness of FedUFA.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zeqing He",
        "Zhibo Wang",
        "Xiaowei Dong",
        "Peng Sun",
        "Ju Ren",
        "Kui Ren"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0ba1a71f65c609cd2709d818cfdbd9d912f7e17a",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb70086f96a21495572ae0e4a175b1f1441314e7",
      "title": "Interpreting Memorization in Deep Learning from Data Distribution",
      "abstract": "A deep learning model can be vulnerable to a membership inference attack (MIA) which allows an attacker to determine if a specific data record was used for its training. In this paper, we investigate the unfairness of disparate vulnerability to MIA across different subgroups in terms of their data distributions. We propose three practical methods to characterize the distribution of complex training data for deep learning models, which are validated to be effective in identifying the vulnerable data records. We then provide a theoretical definition for MIA vulnerability. Experimental results demonstrate the impact of data distribution on disparate vulnerability, where the out-of-distribution outliers are much more easily attacked than normal data records. Even if the accuracy of MIA looks no better than random guessing over the whole population, there are certain groups of \"outliers\" can be significantly more vulnerable than others. For example, the attack accuracy on examples with the largest 10% outlierness is 15% higher than that on in-distribution examples.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Likun Zhang",
        "Jingwei Sun",
        "Shoukun Guo",
        "Fenghua Li",
        "Jin Cao",
        "Ben Niu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/eb70086f96a21495572ae0e4a175b1f1441314e7",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42e9e3930b2bdc11bce5b37e52f4400001afb1a4",
      "title": "Understanding Disparate Effects of Membership Inference Attacks and their Countermeasures",
      "abstract": "Machine learning algorithms, when applied to sensitive data, can pose severe threats to privacy. A growing body of prior work has demonstrated that membership inference attack (MIA) can disclose whether specific private data samples are present in the training data to an attacker. However, most existing studies on MIA focus on aggregated privacy leakage for an entire population, while leaving privacy leakage across different demographic subgroups (e.g., females and males) in the population largely unexplored. This raises two important issues: (1) privacy unfairness (i.e., if some subgroups are more vulnerable to MIAs than the others); and (2) defense unfairness (i.e., if the defense mechanisms provide more protection to some particular subgroups than the others). In this paper, we investigate both privacy unfairness and defense fairness.We formalize a new notation of privacy-leakage disparity (PLD), which quantifies the disparate privacy leakage of machine learning models to MIA across different subgroups. In terms of privacy unfairness, our empirical analysis of PLD on real-world datasets shows that privacy unfairness exists. The minority subgroups (i.e., the less represented subgroups) tend to have higher privacy leakage. We analyze how subgroup size and subgroup data distribution impact PLD through the lens of model memorization. In terms of defense unfairness, our empirical evaluation shows the existence of unfairness of three state-of-the-art defenses, namely differential privacy, L2-regularizer, and Dropout, against MIA. However, defense unfairness mitigates privacy unfairness as the minority subgroups receive stronger protection than the others. We analyze how the three defense mechanisms affect subgroup data distribution disparately and thus leads to defense unfairness.",
      "year": 2022,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Da Zhong",
        "Haipei Sun",
        "Jun Xu",
        "N. Gong",
        "Wendy Hui Wang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/42e9e3930b2bdc11bce5b37e52f4400001afb1a4",
      "pdf_url": "",
      "publication_date": "2022-05-30",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d6a67bd32ee9d4a712d4d289ab4e2aa5258ace5",
      "title": "Attacks on fairness in Federated Learning",
      "abstract": "Federated Learning is an important emerging distributed training paradigm that keeps data private on clients. It is now well understood that by controlling only a small subset of FL clients, it is possible to introduce a backdoor to a federated learning model, in the presence of certain attributes. In this paper, we present a new type of attack that compromises the fairness of the trained model. Fairness is understood to be the attribute-level performance distribution of a trained model. It is particularly salient in domains where, for example, skewed accuracy discrimination between subpopulations could have disastrous consequences. We find that by employing a threat model similar to that of a backdoor attack, an attacker is able to influence the aggregated model to have an unfair performance distribution between any given set of attributes. Furthermore, we find that this attack is possible by controlling only a single client. While combating naturally induced unfairness in FL has previously been discussed in depth, its artificially induced kind has been neglected. We show that defending against attacks on fairness should be a critical consideration in any situation where unfairness in a trained model could benefit a user who participated in its training.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Joseph Rance",
        "Filip Svoboda"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8d6a67bd32ee9d4a712d4d289ab4e2aa5258ace5",
      "pdf_url": "",
      "publication_date": "2023-11-21",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70249be9eb8eb2ee5f5955980e3ae63b32150bdc",
      "title": "Research on MAC security based on D-S evidence theory in wireless sensor networks",
      "abstract": null,
      "year": 2009,
      "venue": "",
      "authors": [
        "Yang Ting"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/70249be9eb8eb2ee5f5955980e3ae63b32150bdc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f7aa171a55347ab8c61eceb8a922b54f0e04d4eb",
      "title": "Understanding Unfairness via Training Concept Influence",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanshun Yao",
        "Yang Liu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f7aa171a55347ab8c61eceb8a922b54f0e04d4eb",
      "pdf_url": "http://arxiv.org/pdf/2306.17828",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b9d6757b0170523916353062f96ee075ac915c4d",
      "title": "Demographic bias of expert-level vision-language foundation models in medical imaging",
      "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, disadvantaging historically marginalized groups such as females or Black patients. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest x-ray diagnosis across five globally sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups such as Black female patients. Such biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its substantial encoding of demographic information. Deploying medical AI systems with biases can intensify preexisting care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical applications.",
      "year": 2024,
      "venue": "Science Advances",
      "authors": [
        "Yuzhe Yang",
        "Yujia Liu",
        "Xin Liu",
        "Avanti V Gulhane",
        "Domenico Mastrodicasa",
        "Wei Wu",
        "E. J. Wang",
        "Dushyant W. Sahani",
        "Shwetak N. Patel"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/b9d6757b0170523916353062f96ee075ac915c4d",
      "pdf_url": "http://arxiv.org/pdf/2402.14815",
      "publication_date": "2024-02-22",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0af997e992db316b5f6842c81217b059d4e5c641",
      "title": "Metrics for Dataset Demographic Bias: A Case Study on Facial Expression Recognition",
      "abstract": "Demographic biases in source datasets have been shown as one of the causes of unfairness and discrimination in the predictions of Machine Learning models. One of the most prominent types of demographic bias are statistical imbalances in the representation of demographic groups in the datasets. In this article, we study the measurement of these biases by reviewing the existing metrics, including those that can be borrowed from other disciplines. We develop a taxonomy for the classification of these metrics, providing a practical guide for the selection of appropriate metrics. To illustrate the utility of our framework, and to further understand the practical characteristics of the metrics, we conduct a case study of 20 datasets used in Facial Emotion Recognition (FER), analyzing the biases present in them. Our experimental results show that many metrics are redundant and that a reduced subset of metrics may be sufficient to measure the amount of demographic bias. The article provides valuable insights for researchers in AI and related fields to mitigate dataset bias and improve the fairness and accuracy of AI models.",
      "year": 2023,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Iris Dominguez-Catena",
        "D. Paternain",
        "M. Galar"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/0af997e992db316b5f6842c81217b059d4e5c641",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10420507.pdf",
      "publication_date": "2023-03-28",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "363d00f60bb43206b40948b95b258dfa6a87b636",
      "title": "Deconstructing demographic bias in speech-based machine learning models for digital health",
      "abstract": "Introduction Machine learning (ML) algorithms have been heralded as promising solutions to the realization of assistive systems in digital healthcare, due to their ability to detect fine-grain patterns that are not easily perceived by humans. Yet, ML algorithms have also been critiqued for treating individuals differently based on their demography, thus propagating existing disparities. This paper explores gender and race bias in speech-based ML algorithms that detect behavioral and mental health outcomes. Methods This paper examines potential sources of bias in the data used to train the ML, encompassing acoustic features extracted from speech signals and associated labels, as well as in the ML decisions. The paper further examines approaches to reduce existing bias via using the features that are the least informative of one\u2019s demographic information as the ML input, and transforming the feature space in an adversarial manner to diminish the evidence of the demographic information while retaining information about the focal behavioral and mental health state. Results Results are presented in two domains, the first pertaining to gender and race bias when estimating levels of anxiety, and the second pertaining to gender bias in detecting depression. Findings indicate the presence of statistically significant differences in both acoustic features and labels among demographic groups, as well as differential ML performance among groups. The statistically significant differences present in the label space are partially preserved in the ML decisions. Although variations in ML performance across demographic groups were noted, results are mixed regarding the models\u2019 ability to accurately estimate healthcare outcomes for the sensitive groups. Discussion These findings underscore the necessity for careful and thoughtful design in developing ML models that are capable of maintaining crucial aspects of the data and perform effectively across all populations in digital healthcare applications.",
      "year": 2024,
      "venue": "Frontiers Digit. Health",
      "authors": [
        "Michael Yang",
        "Abd-Allah El-Attar",
        "Theodora Chaspari"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/363d00f60bb43206b40948b95b258dfa6a87b636",
      "pdf_url": "https://doi.org/10.3389/fdgth.2024.1351637",
      "publication_date": "2024-07-25",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "58b8c80923a7e5694a66b7db7f128b79031c879a",
      "title": "Mitigating demographic bias of machine learning models on social media",
      "abstract": "Social media posts have been used to predict different user behaviors and attitudes, including mental health condition, political affiliation, and vaccine hesitancy. Unfortunately, while social media platforms make APIs available for collecting user data, they also make it challenging to collect well structured demographic features about individuals who post on their platforms. This makes it difficult for researchers to assess the fairness of models they develop using these data. Researchers have begun considering approaches for determining fairness of machine learning models built using social media data. In this paper, we consider both the case when the sensitive demographic feature is available to the researcher and when it is not. After framing our specific problem and discussing the challenges, we focus on the scenario when the training data does not explicitly contain a sensitive demographic feature, but instead contains a hidden sensitive feature that can be approximated using a sensitive feature proxy. In this case, we propose an approach for determining whether a sensitive feature proxy exists in the training data and apply a fixing method to reduce the correlation between the sensitive feature proxy and the sensitive feature. To demonstrate our approach, we present two case studies using micro-linked Twitter/X data and show biases resulting from sensitive feature proxies that are present in the training data and are highly correlated to hidden sensitive features. We then show that a standard fixing approach can effectively reduce bias even if the sensitive attribute needs to be inferred by the researcher using existing reliable inference models. This is an important step toward understanding approaches for improving fairness on social media.",
      "year": 2023,
      "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "authors": [
        "Yanchen Wang",
        "Lisa Singh"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/58b8c80923a7e5694a66b7db7f128b79031c879a",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3617694.3623244",
      "publication_date": "2023-10-30",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e1d53567b15d50cc1f58729083cbff0460cd395b",
      "title": "Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation",
      "abstract": "Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",
      "year": 2022,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "authors": [
        "Yu Yang",
        "Aayush Gupta",
        "Jianfeng Feng",
        "Prateek Singhal",
        "Vivek Yadav",
        "Yue Wu",
        "P. Natarajan",
        "Varsha Hedau",
        "Jungseock Joo"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/e1d53567b15d50cc1f58729083cbff0460cd395b",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3514094.3534153",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4b1ce0b40a8efcaafbae4bafac2023923cf9c796",
      "title": "Behind the Mask: Demographic bias in name detection for PII masking",
      "abstract": "Many datasets contain personally identifiable information, or PII, which poses privacy risks to individuals. PII masking is commonly used to redact personal information such as names, addresses, and phone numbers from text data. Most modern PII masking pipelines involve machine learning algorithms. However, these systems may vary in performance, such that individuals from particular demographic groups bear a higher risk for having their personal information exposed. In this paper, we evaluate the performance of three off-the-shelf PII masking systems on name detection and redaction. We generate data using names and templates from the customer service domain. We find that an open-source RoBERTa-based system shows fewer disparities than the commercial models we test. However, all systems demonstrate significant differences in error rate based on demographics. In particular, the highest error rates occurred for names associated with Black and Asian/Pacific Islander individuals.",
      "year": 2022,
      "venue": "LTEDI",
      "authors": [
        "Courtney Mansfield",
        "Amandalynne Paullada",
        "Kristen Howell"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/4b1ce0b40a8efcaafbae4bafac2023923cf9c796",
      "pdf_url": "http://arxiv.org/pdf/2205.04505",
      "publication_date": "2022-05-09",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1ce9f6d1a00e6ef3f8a473e5fd25b719248f3dc",
      "title": "Demographic Bias in Biometrics: A Survey on an Emerging Challenge",
      "abstract": "Systems incorporating biometric technologies have become ubiquitous in personal, commercial, and governmental identity management applications. Both cooperative (e.g., access control) and noncooperative (e.g., surveillance and forensics) systems have benefited from biometrics. Such systems rely on the uniqueness of certain biological or behavioral characteristics of human beings, which enable for individuals to be reliably recognized using automated algorithms. Recently, however, there has been a wave of public and academic concerns regarding the existence of systemic bias in automated decision systems (including biometrics). Most prominently, face recognition algorithms have often been labeled as \u201cracist\u201d or \u201cbiased\u201d by the media, nongovernmental organizations, and researchers alike. The main contributions of this article are: 1) an overview of the topic of algorithmic bias in the context of biometrics; 2) a comprehensive survey of the existing literature on biometric bias estimation and mitigation; 3) a discussion of the pertinent technical and social matters; and 4) an outline of the remaining challenges and future work items, both from technological and social points of view.",
      "year": 2020,
      "venue": "IEEE Transactions on Technology and Society",
      "authors": [
        "P. Drozdowski",
        "C. Rathgeb",
        "A. Dantcheva",
        "Naser Damer",
        "C. Busch"
      ],
      "citation_count": 229,
      "url": "https://www.semanticscholar.org/paper/f1ce9f6d1a00e6ef3f8a473e5fd25b719248f3dc",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8566059/9107511/09086771.pdf",
      "publication_date": "2020-03-05",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2685f321330fac6d038f6758158b29cff34c8ee3",
      "title": "FairFL: A Fair Federated Learning Approach to Reducing Demographic Bias in Privacy-Sensitive Classification Models",
      "abstract": "The recent advance of the federated learning (FL) has brought new opportunities for privacy-aware distributed machine learning (ML) applications to train a powerful ML model without accessing the private training data of the participants. In this paper, we focus on addressing a novel fair classification problem in FL where the model trained by FL displays discriminatory bias towards particular demographic groups. Addressing the fairness issue in a FL framework posts three critical challenges: fairness and performance trade-offs, restricted information, and constrained coordination. To address these challenges, we develop FairFL, a fair federated learning framework dedicated to reducing the bias in privacy-sensitive ML applications. It consists of a principled deep multi-agent reinforcement learning framework and a secure information aggregation protocol that optimizes both the accuracy and the fairness of the learned model while respecting the strict privacy constraints of the clients. Evaluation results on real-world applications showed that FairFL can achieve significant performance gains in both fairness and accuracy of the learned model compared to state-of-the-art baselines.",
      "year": 2020,
      "venue": "2020 IEEE International Conference on Big Data (Big Data)",
      "authors": [
        "D. Zhang",
        "Ziyi Kou",
        "Dong Wang"
      ],
      "citation_count": 93,
      "url": "https://www.semanticscholar.org/paper/2685f321330fac6d038f6758158b29cff34c8ee3",
      "pdf_url": "",
      "publication_date": "2020-12-10",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b8c20196d48b3c52ce033a82e0107c6048278dd9",
      "title": "Towards Explaining Demographic Bias through the Eyes of Face Recognition Models",
      "abstract": "Biases inherent in both data and algorithms make the fairness of widespread machine learning (ML)-based decision-making systems less than optimal. To improve the trustfulness of such ML decision systems, it is crucial to be aware of the inherent biases in these solutions and to make them more transparent to the public and developers. In this work, we aim at providing a set of explainability tool that analyse the difference in the face recognition models' behaviors when processing different demographic groups. We do that by leveraging higher-order statistical information based on activation maps to build explainability tools that link the FR models' behavior differences to certain facial regions. The experimental results on two datasets and two face recognition models pointed out certain areas of the face where the FR models react differently for certain demographic groups compared to reference groups. The outcome of these analyses interestingly aligns well with the results of studies that analyzed the anthropometric differences and the human judgment differences on the faces of different demographic groups. This is thus the first study that specifically tries to explain the biased behavior of FR models on different demographic groups and link it directly to the spatial facial features. The code is publicly available here11https://github.com/fbiying87/Demographic-Bias-Visualization.git.",
      "year": 2022,
      "venue": "2022 IEEE International Joint Conference on Biometrics (IJCB)",
      "authors": [
        "Biying Fu",
        "Naser Damer"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/b8c20196d48b3c52ce033a82e0107c6048278dd9",
      "pdf_url": "http://arxiv.org/pdf/2208.13400",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4d3664ab2d6e1f97ff1bae51dd26ccb8906fdd56",
      "title": "Demographic Bias in Presentation Attack Detection of Iris Recognition Systems",
      "abstract": "With the widespread use of biometric systems, the demographic bias problem raises more attention. Although many studies addressed bias issues in biometric verification, there are no works that analyze the bias in presentation attack detection (PAD) decisions. Hence, we investigate and analyze the demographic bias in iris PAD algorithms in this paper. To enable a clear discussion, we adapt the notions of differential performance and differential outcome to the PAD problem. We study the bias in iris PAD using three baselines (hand-crafted, transfer-learning, and training from scratch) using the NDCLD-2013 [18] database. The experimental results point out that female users will be significantly less protected by the PAD, in comparison to males.",
      "year": 2020,
      "venue": "European Signal Processing Conference",
      "authors": [
        "Meiling Fang",
        "Naser Damer",
        "Florian Kirchbuchner",
        "Arjan Kuijper"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4d3664ab2d6e1f97ff1bae51dd26ccb8906fdd56",
      "pdf_url": "https://arxiv.org/pdf/2003.03151",
      "publication_date": "2020-03-06",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50fafee882fd1461429b3fbd76c40b706dfd5f83",
      "title": "Risk of Training Diagnostic Algorithms on Data with Demographic Bias",
      "abstract": "One of the critical challenges in machine learning applications is to have fair predictions. There are numerous recent examples in various domains that convincingly show that algorithms trained with biased datasets can easily lead to erroneous or discriminatory conclusions. This is even more crucial in clinical applications where the predictive algorithms are designed mainly based on a limited or given set of medical images and demographic variables such as age, sex and race are not taken into account. In this work, we conduct a survey of the MICCAI 2018 proceedings to investigate the common practice in medical image analysis applications. Surprisingly, we found that papers focusing on diagnosis rarely describe the demographics of the datasets used, and the diagnosis is purely based on images. In order to highlight the importance of considering the demographics in diagnosis tasks, we used a publicly available dataset of skin lesions. We then demonstrate that a classifier with an overall area under the curve (AUC) of 0.83 has variable performance between 0.76 and 0.91 on subgroups based on age and sex, even though the training set was relatively balanced. Moreover, we show that it is possible to learn unbiased features by explicitly using demographic variables in an adversarial training setup, which leads to balanced scores per subgroups. Finally, we discuss the implications of these results and provide recommendations for further research.",
      "year": 2020,
      "venue": "iMIMIC/MIL3iD/LABELS@MICCAI",
      "authors": [
        "Samaneh Abbasi-Sureshjani",
        "Ralf Raumanns",
        "B. Michels",
        "Gerard Schouten",
        "V. Cheplygina"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/50fafee882fd1461429b3fbd76c40b706dfd5f83",
      "pdf_url": "",
      "publication_date": "2020-05-20",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
      "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
      "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
      "year": 2022,
      "venue": "AACL",
      "authors": [
        "Aparna Garimella",
        "Rada Mihalcea",
        "Akhash Amarnath"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "30461b178ef1f68427789d00ad351b6e94b0ae53",
      "title": "Identifying and Measuring Annotator Bias Based on Annotators\u2019 Demographic Characteristics",
      "abstract": "Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.",
      "year": 2020,
      "venue": "Workshop on Abusive Language Online",
      "authors": [
        "Hala Al Kuwatly",
        "Maximilian Wich",
        "Georg Groh"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/30461b178ef1f68427789d00ad351b6e94b0ae53",
      "pdf_url": "https://www.aclweb.org/anthology/2020.alw-1.21.pdf",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1473110f6c33b483251ade10b79416d3efee2da4",
      "title": "Ridge Regression: Biased Estimation for Nonorthogonal Problems",
      "abstract": null,
      "year": 2000,
      "venue": "Technometrics",
      "authors": [
        "A. E. Hoerl",
        "R. Kennard"
      ],
      "citation_count": 11669,
      "url": "https://www.semanticscholar.org/paper/1473110f6c33b483251ade10b79416d3efee2da4",
      "pdf_url": "",
      "publication_date": "2000-02-01",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1986be3bd1c14748fa7390ec87142d91ce4a6b05",
      "title": "BiasRV: uncovering biased sentiment predictions at runtime",
      "abstract": "Sentiment analysis (SA) systems, though widely applied in many domains, have been demonstrated to produce biased results. Some research works have been done in automatically generating test cases to reveal unfairness in SA systems, but the community still lacks tools that can monitor and uncover biased predictions at runtime. This paper fills this gap by proposing BiasRV, the first tool to raise an alarm when a deployed SA system makes a biased prediction on a given input text. To implement this feature, BiasRV dynamically extracts a template from an input text and generates gender-discriminatory mutants (semantically-equivalent texts that only differ in gender information) from the template. Based on popular metrics used to evaluate the overall fairness of an SA system, we define the distributional fairness property for an individual prediction of an SA system. This property specifies a requirement that for one piece of text, mutants from different gender classes should be treated similarly. Verifying the distributional fairness property causes much overhead to the running system. To run more efficiently, BiasRV adopts a two-step heuristic: (1) sampling several mutants from each gender and checking if the system predicts them as of the same sentiment, (2) checking distributional fairness only when sampled mutants have conflicting results. Experiments show that when compared to directly checking the distributional fairness property for each input text, our two-step heuristic can decrease the overhead used for analyzing mutants by 73.81% while only resulting in 6.7% of biased predictions being missed. Besides, BiasRV can be used conveniently without knowing the implementation of SA systems. Future researchers can easily extend BiasRV to detect more types of bias, e.g., race and occupation. The demo video for BiasRV can be viewed at https://youtu.be/WPe4Ml77d3U and the source code can be found at https://github.com/soarsmu/BiasRV.",
      "year": 2021,
      "venue": "ESEC/SIGSOFT FSE",
      "authors": [
        "Zhou Yang",
        "Muhammad Hilmi Asyrofi",
        "D. Lo"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/1986be3bd1c14748fa7390ec87142d91ce4a6b05",
      "pdf_url": "https://arxiv.org/pdf/2105.14874",
      "publication_date": "2021-05-31",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c7df5eaaac2921dd2b25b2da3e2ce3f34a3a5092",
      "title": "An algorithm for removing sensitive information: Application to race-independent recidivism prediction",
      "abstract": "Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing or augmenting human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived \"neutrality\" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it if the training data were generated by a process that is itself biased. In this paper, we provide a probabilistic notion of algorithmic bias. We propose a method to eliminate bias from predictive models by removing all information regarding protected variables from the data to which the models will ultimately be trained. Unlike previous work in this area, our framework is general enough to accommodate data on any measurement scale. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and parole, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce \"race-neutral\" predictions of re-arrest. In the process, we demonstrate that a common approach to creating \"race-neutral\" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy.",
      "year": 2017,
      "venue": "Annals of Applied Statistics",
      "authors": [
        "J. Johndrow",
        "K. Lum"
      ],
      "citation_count": 173,
      "url": "https://www.semanticscholar.org/paper/c7df5eaaac2921dd2b25b2da3e2ce3f34a3a5092",
      "pdf_url": "https://projecteuclid.org/journals/annals-of-applied-statistics/volume-13/issue-1/An-algorithm-for-removing-sensitive-information--Application-to-race/10.1214/18-AOAS1201.pdf",
      "publication_date": "2017-03-15",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2a501b074261e81b9126e80a0a308cfa5e76f8c1",
      "title": "Linguistic Models for Analyzing and Detecting Biased Language",
      "abstract": null,
      "year": 2013,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Marta Recasens",
        "Cristian Danescu-Niculescu-Mizil",
        "Dan Jurafsky"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2a501b074261e81b9126e80a0a308cfa5e76f8c1",
      "pdf_url": "",
      "publication_date": "2013-08-01",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "962c625f06e020ad1484323512c40675f9e9211f",
      "title": "Operationalizing the Search for Less Discriminatory Alternatives in Fair Lending",
      "abstract": "The Less Discriminatory Alternative is a key provision of the disparate impact doctrine in the United States. In fair lending, this provision mandates that lenders must adopt models that reduce discrimination when they do not compromise their business interests. In this paper, we develop practical methods to audit for less discriminatory alternatives. Our approach is designed to verify the existence of less discriminatory machine learning models \u2013 by returning an alternative model that can reduce discrimination without compromising performance (discovery) or by certifying that an alternative model does not exist (refutation). We develop a method to fit the least discriminatory linear classification model in a specific lending task \u2013 by minimizing an exact measure of disparity (e.g., the maximum gap in group FNR) and enforcing hard performance constraints for business necessity (e.g., on FNR and FPR). We apply our method to study the prevalence of less discriminatory alternatives on real-world datasets from consumer finance applications. Our results highlight how models may inadvertently lead to unnecessary discrimination across common deployment regimes, and demonstrate how our approach can support lenders, regulators, and plaintiffs by reliably detecting less discriminatory alternatives in such instances.",
      "year": 2024,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Talia B. Gillis",
        "Vitaly Meursault",
        "Berk Ustun"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/962c625f06e020ad1484323512c40675f9e9211f",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658912",
      "publication_date": "2024-06-03",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8be9cd757087e2a9b9a8a9b54e1b5cc114544286",
      "title": "The Legal Duty to Search for Less Discriminatory Algorithms",
      "abstract": "Work in computer science has established that, contrary to conventional wisdom, for a given prediction problem there are almost always multiple possible models with equivalent performance--a phenomenon often termed model multiplicity. Critically, different models of equivalent performance can produce different predictions for the same individual, and, in aggregate, exhibit different levels of impacts across demographic groups. Thus, when an algorithmic system displays a disparate impact, model multiplicity suggests that developers could discover an alternative model that performs equally well, but has less discriminatory impact. Indeed, the promise of model multiplicity is that an equally accurate, but less discriminatory algorithm (LDA) almost always exists. But without dedicated exploration, it is unlikely developers will discover potential LDAs. Model multiplicity and the availability of LDAs have significant ramifications for the legal response to discriminatory algorithms, in particular for disparate impact doctrine, which has long taken into account the availability of alternatives with less disparate effect when assessing liability. A close reading of legal authorities over the decades reveals that the law has on numerous occasions recognized that the existence of a less discriminatory alternative is sometimes relevant to a defendant's burden of justification at the second step of disparate impact analysis. Indeed, under disparate impact doctrine, it makes little sense to say that a given algorithmic system used by an employer, creditor, or housing provider is\"necessary\"if an equally accurate model that exhibits less disparate effect is available and possible to discover with reasonable effort. As a result, we argue that the law should place a duty of a reasonable search for LDAs on entities that develop and deploy predictive models in covered civil rights domains.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Emily Black",
        "Logan Koepke",
        "Pauline Kim",
        "Solon Barocas",
        "Mingwei Hsu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/8be9cd757087e2a9b9a8a9b54e1b5cc114544286",
      "pdf_url": "",
      "publication_date": "2024-06-10",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd674e10770eb72e66a20e1c752c62dc7c12c0a4",
      "title": "Why Is My Classifier Discriminatory?",
      "abstract": "Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.",
      "year": 2018,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "I. Chen",
        "Fredrik D. Johansson",
        "D. Sontag"
      ],
      "citation_count": 426,
      "url": "https://www.semanticscholar.org/paper/fd674e10770eb72e66a20e1c752c62dc7c12c0a4",
      "pdf_url": "",
      "publication_date": "2018-05-30",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09402ede90a34b0adf284a6b839cf690383ab448",
      "title": "Full Protection and Security, Arbitrary or Discriminatory Treatment and the Invisible EU Model BIT",
      "abstract": null,
      "year": 2014,
      "venue": "",
      "authors": [
        "C. Titi"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/09402ede90a34b0adf284a6b839cf690383ab448",
      "pdf_url": "",
      "publication_date": "2014-07-28",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40517ac4306199f390aaa322779c9e2a1def6fac",
      "title": "Detecting discriminatory risk through data annotation based on Bayesian inferences",
      "abstract": "Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.",
      "year": 2021,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "E. Beretta",
        "A. Vetr\u00f2",
        "B. Lepri",
        "Juan Carlos De Martin"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/40517ac4306199f390aaa322779c9e2a1def6fac",
      "pdf_url": "https://arxiv.org/pdf/2101.11358",
      "publication_date": "2021-01-27",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5361ad5c096f21d50f20a655b95205c20767afcd",
      "title": "Assessing Discriminatory Performance of a Binary Logistic Model: ROC Curves",
      "abstract": null,
      "year": 2010,
      "venue": "",
      "authors": [
        "D. Kleinbaum",
        "M. Klein"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/5361ad5c096f21d50f20a655b95205c20767afcd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ef195d65b9b97c86b4af9fcd2e577fb7de9022f",
      "title": "Hunting for Discriminatory Proxies in Linear Regression Models",
      "abstract": "A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.",
      "year": 2018,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Samuel Yeom",
        "Anupam Datta",
        "Matt Fredrikson"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/6ef195d65b9b97c86b4af9fcd2e577fb7de9022f",
      "pdf_url": "",
      "publication_date": "2018-10-16",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13454480d778c1d777af39c30d29875f170e8ff8",
      "title": "Towards model-based bias mitigation in machine learning",
      "abstract": "Models produced by machine learning are not guaranteed to be free from bias, particularly when trained and tested with data produced in discriminatory environments. The bias can be unethical, mainly when the data contains sensitive attributes, such as sex, race, age, etc. Some approaches have contributed to mitigating such biases by providing bias metrics and mitigation algorithms. The challenge is users have to implement their code in general/statistical programming languages, which can be demanding for users with little programming and fairness in machine learning experience. We present FairML, a model-based approach to facilitate bias measurement and mitigation with reduced software development effort. Our evaluation shows that FairML requires fewer lines of code to produce comparable measurement values to the ones produced by the baseline code.",
      "year": 2022,
      "venue": "ACM/IEEE International Conference on Model Driven Engineering Languages and Systems",
      "authors": [
        "Alfa Yohannis",
        "D. Kolovos"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/13454480d778c1d777af39c30d29875f170e8ff8",
      "pdf_url": "",
      "publication_date": "2022-10-23",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0b1c6faf0a463a3dcf477e21b481b83dd082651",
      "title": "A scoping review of fair machine learning techniques when using real-world data",
      "abstract": "Objective: The integration of artificial intelligence (AI) and machine learning (ML) in health care to aid clinical decisions is widespread. However, as AI and ML take important roles in health care, there are concerns about AI and ML associated fairness and bias. That is, an AI tool may have a disparate impact, with its benefits and drawbacks unevenly distributed across societal strata and subpopulations, potentially exacerbating existing health inequities. Thus, the objectives of this scoping review were to summarize existing literature and identify gaps in the topic of tackling algorithmic bias and optimizing fairness in AI/ML models using real-world data (RWD) in health care domains. Methods: We conducted a thorough review of techniques for assessing and optimizing AI/ML model fairness in health care when using RWD in health care domains. The focus lies on appraising different quantification metrics for accessing fairness, publicly accessible datasets for ML fairness research, and bias mitigation approaches. Results: We identified 11 papers that are focused on optimizing model fairness in health care applications. The current research on mitigating bias issues in RWD is limited, both in terms of disease variety and health care applications, as well as the accessibility of public datasets for ML fairness research. Existing studies often indicate positive outcomes when using pre-processing techniques to address algorithmic bias. There remain unresolved questions within the field that require further research, which includes pinpointing the root causes of bias in ML models, broadening fairness research in AI/ML with the use of RWD and exploring its implications in healthcare settings, and evaluating and addressing bias in multi-modal data. Conclusion: This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD.",
      "year": 2024,
      "venue": "medRxiv",
      "authors": [
        "Yu Huang",
        "Jingchuan Guo",
        "Wei-Han Chen",
        "Hsin-Yueh Lin",
        "Huilin Tang",
        "Fei Wang",
        "Hua Xu",
        "Jiang Bian"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/d0b1c6faf0a463a3dcf477e21b481b83dd082651",
      "pdf_url": "https://doi.org/10.1016/j.jbi.2024.104622",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d2459ca6704c4354d1c52c2d7e64954dddd571f6",
      "title": "Not So Fair: The Impact of Presumably Fair Machine Learning Models",
      "abstract": "When bias mitigation methods are applied to make fairer machine learning models in fairness-related classification settings, there is an assumption that the disadvantaged group should be better off than if no mitigation method was applied. However, this is a potentially dangerous assumption because a \u201cfair\u201d model outcome does not automatically imply a positive impact for a disadvantaged individual\u2014they could still be negatively impacted. Modeling and accounting for those impacts is key to ensure that mitigated models are not unintentionally harming individuals; we investigate if mitigated models can still negatively impact disadvantaged individuals and what conditions affect those impacts in a loan repayment example. Our results show that most mitigated models negatively impact disadvantaged group members in comparison to the unmitigated models. The domain-dependent impacts of model outcomes should help drive future bias mitigation method development.",
      "year": 2023,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "authors": [
        "Mackenzie Jorgensen",
        "Hannah Richert",
        "Elizabeth Black",
        "N. Criado",
        "J. Such"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/d2459ca6704c4354d1c52c2d7e64954dddd571f6",
      "pdf_url": "https://kclpure.kcl.ac.uk/portal/files/222548535/Jorgensen_AIES23.pdf",
      "publication_date": "2023-08-08",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "060477c57d503b30982741b1cf13129ad60f0c32",
      "title": "Insights From Insurance for Fair Machine Learning",
      "abstract": "We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.",
      "year": 2023,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Christiane Fr\u00f6hlich",
        "R. C. Williamson"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/060477c57d503b30982741b1cf13129ad60f0c32",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658914",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2563d796b24e4fe1cc3ceed99bb9b2deb23edf46",
      "title": "Fair Machine Learning in Healthcare: A Survey",
      "abstract": "The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problems is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in ML and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a ML standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. The article concludes with a discussion on the pressing challenges that remain unaddressed in ensuring fairness in healthcare ML and proposes several new research directions that hold promise for developing ethical and equitable ML applications in healthcare.",
      "year": 2025,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Qizhang Feng",
        "Mengnan Du",
        "Na Zou",
        "Xia Hu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/2563d796b24e4fe1cc3ceed99bb9b2deb23edf46",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8da819090dcb5e49aa4b35bbbcda2e5706f2217a",
      "title": "Strategic Best Response Fairness in Fair Machine Learning",
      "abstract": "While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called \"strategic best-response fairness\" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.",
      "year": 2022,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "authors": [
        "Hajime Shimao",
        "Warut Khern-am-nuai",
        "Karthik N. Kannan",
        "Maxime C. Cohen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8da819090dcb5e49aa4b35bbbcda2e5706f2217a",
      "pdf_url": "",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2dd9b62adda563f03005a164fe2eddb0f9fb9062",
      "title": "Residual Unfairness in Fair Machine Learning from Prejudiced Data",
      "abstract": "Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a \"bias in, bias out\" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.",
      "year": 2018,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nathan Kallus",
        "Angela Zhou"
      ],
      "citation_count": 140,
      "url": "https://www.semanticscholar.org/paper/2dd9b62adda563f03005a164fe2eddb0f9fb9062",
      "pdf_url": "",
      "publication_date": "2018-06-07",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "64e24c2168b8a44a93f6d56a271f5684795ff271",
      "title": "Non-empirical problems in fair machine learning",
      "abstract": "The problem of fair machine learning has drawn much attention over the last few years and the bulk of offered solutions are, in principle, empirical. However, algorithmic fairness also raises important conceptual issues that would fail to be addressed if one relies entirely on empirical considerations. Herein, I will argue that the current debate has developed an empirical framework that has brought important contributions to the development of algorithmic decision-making, such as new techniques to discover and prevent discrimination, additional assessment criteria, and analyses of the interaction between fairness and predictive accuracy. However, the same framework has also suggested higher-order issues regarding the translation of fairness into metrics and quantifiable trade-offs. Although the (empirical) tools which have been developed so far are essential to address discrimination encoded in data and algorithms, their integration into society elicits key (conceptual) questions such as: What kind of assumptions and decisions underlies the empirical framework? How do the results of the empirical approach penetrate public debate? What kind of reflection and deliberation should stakeholders have over available fairness metrics? I will outline the empirical approach to fair machine learning, i.e. how the problem is framed and addressed, and suggest that there are important non-empirical issues that should be tackled. While this work will focus on the problem of algorithmic fairness, the lesson can extend to other conceptual problems in the analysis of algorithmic decision-making such as privacy and explainability.",
      "year": 2021,
      "venue": "Ethics and Information Technology",
      "authors": [
        "Teresa Scantamburlo"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/64e24c2168b8a44a93f6d56a271f5684795ff271",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10676-021-09608-9.pdf",
      "publication_date": "2021-08-05",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "21207fbe0431c6ca971af74d5fc7fe7fdaddeef4",
      "title": "Slice Tuner: A Selective Data Acquisition Framework for Accurate and Fair Machine Learning Models",
      "abstract": "As machine learning becomes democratized in the era of Software 2.0, a serious bottleneck is acquiring enough data to ensure accurate and fair models. Recent techniques including crowdsourcing provide cost-effective ways to gather such data. However, simply acquiring data as much as possible is not necessarily an effective strategy for optimizing accuracy and fairness. For example, if an online app store has enough training data for certain slices of data (say American customers), but not for others, obtaining more American customer data will only bias the model training. Instead, we contend that one needs to selectively acquire data and propose Slice Tuner, which acquires possibly-different amounts of data per slice such that the model accuracy and fairness on all slices are optimized. This problem is different than labeling existing data (as in active learning or weak supervision) because the goal is obtaining the right amounts of new data. At its core, Slice Tuner maintains learning curves of slices that estimate the model accuracies given more data and uses convex optimization to find the best data acquisition strategy. The key challenges of estimating learning curves are that they may be inaccurate if there is not enough data, and there may be dependencies among slices where acquiring data for one slice influences the learning curves of others. We solve these issues by iteratively and efficiently updating the learning curves as more data is acquired. We evaluate Slice Tuner on real datasets using crowdsourcing for data acquisition and show that Slice Tuner significantly outperforms baselines in terms of model accuracy and fairness, even when the learning curves cannot be reliably estimated.",
      "year": 2020,
      "venue": "SIGMOD Conference",
      "authors": [
        "Ki Hyun Tae",
        "Steven Euijong Whang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/21207fbe0431c6ca971af74d5fc7fe7fdaddeef4",
      "pdf_url": "",
      "publication_date": "2020-03-10",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "822cb0f83c6e080758ed00a1958c7baa969dd88f",
      "title": "Model Projection: Theory and Applications to Fair Machine Learning",
      "abstract": "We study the problem of finding the element within a convex set of conditional distributions with the smallest f-divergence to a reference distribution. Motivated by applications in machine learning, we refer to this problem as model projection since any probabilistic classification model can be viewed as a conditional distribution. We provide conditions under which the existence and uniqueness of the optimal model can be guaranteed and establish strong duality results. Strong duality, in turn, allows the model projection problem to be reduced to a tractable finite-dimensional optimization. Our application of interest is fair machine learning: the model projection formulation can be directly used to design fair models according to different group fairness metrics. Moreover, this information-theoretic formulation generalizes existing approaches within the fair machine learning literature. We give explicit formulas for the optimal fair model and a systematic procedure for computing it.",
      "year": 2020,
      "venue": "International Symposium on Information Theory",
      "authors": [
        "Wael Alghamdi",
        "S. Asoodeh",
        "Hao Wang",
        "F. Calmon",
        "Dennis Wei",
        "K. Ramamurthy"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/822cb0f83c6e080758ed00a1958c7baa969dd88f",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a1eb6dd5bf57162f365f98717b9fc3882fe94025",
      "title": "Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",
      "abstract": null,
      "year": 2018,
      "venue": "Human and Machine Learning",
      "authors": [
        "B. Abdollahi",
        "O. Nasraoui"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/a1eb6dd5bf57162f365f98717b9fc3882fe94025",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eef65bfd4ad4db06c814d433789376521b4cbb54",
      "title": "Breaking Taboos in Fair Machine Learning: An Experimental Study",
      "abstract": "Many scholars, engineers, and policymakers believe that algorithmic fairness requires disregarding information about certain characteristics of individuals, such as their race or gender. Often, the mandate to \u201cblind\u201d algorithms in this way is conveyed as an unconditional ethical imperative\u2014a minimal requirement of fair treatment\u2014and any contrary practice is assumed to be morally and politically untenable. However, in some circumstances, prohibiting algorithms from considering information about race or gender can in fact lead to worse outcomes for racial minorities and women, complicating the rationale for blinding. In this paper, we conduct a series of randomized studies to investigate attitudes toward blinding algorithms, both among the general public as well as among computer scientists and professional lawyers. We find, first, that people are generally averse to the use of race and gender in algorithmic determinations of \u201cpretrial risk\u201d\u2014the risk that criminal defendants pose to the public if released while awaiting trial. We find, however, that this preference for blinding shifts in response to a relatively mild intervention. In particular, we show that support for the use of race and gender in algorithmic decision-making increases substantially after respondents read a short passage about the possibility that blinding could lead to higher detention rates for Black and female defendants, respectively. Similar effect sizes are observed among the general public, computer scientists, and professional lawyers. These findings suggest that, while many respondents attest that they prefer blind algorithms, their preference is not based on an absolute principle. Rather, blinding is perceived as a way to ensure better outcomes for members of marginalized groups. Accordingly, in circumstances where blinding serves to disadvantage marginalized groups, respondents no longer view the exclusion of protected characteristics as a moral imperative, and the use of such information may become politically viable.",
      "year": 2021,
      "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",
      "authors": [
        "Julian Nyarko",
        "Sharad Goel",
        "R. Sommers"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/eef65bfd4ad4db06c814d433789376521b4cbb54",
      "pdf_url": "",
      "publication_date": "2021-10-05",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4ca79008fa03f6cd2cbc0bc29d98169ebd085448",
      "title": "Investigating Oversampling Techniques for Fair Machine Learning Models",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Decision Support Systems",
      "authors": [
        "Sanja Rancic",
        "S. Radovanovi\u0107",
        "Boris Delibasic"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/4ca79008fa03f6cd2cbc0bc29d98169ebd085448",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85722396188e8671137a137a6a5e3a566f42e11c",
      "title": "A Distributed Fair Machine Learning Framework with Private Demographic Data Protection",
      "abstract": "Fair machine learning has become a significant research topic with broad societal impact. However, most fair learning methods require direct access to personal demographic data, which is increasingly restricted to use for protecting user privacy (e.g. by the EU General Data Protection Regulation). In this paper, we propose a distributed fair learning framework for protecting the privacy of demographic data. We assume this data is privately held by a third party, which can communicate with the data center (responsible for model development) without revealing the demographic information. We propose a principled approach to design fair learning methods under this framework, exemplify four methods and show they consistently outperform their existing counterparts in both fairness and accuracy across two real-world data sets. We theoretically analyze the framework, and prove it can learn models with high fairness or high accuracy, with their trade-offs balanced by a threshold variable.",
      "year": 2019,
      "venue": "Industrial Conference on Data Mining",
      "authors": [
        "Hui Hu",
        "Yijun Liu",
        "Zhen Wang",
        "Chao Lan"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/85722396188e8671137a137a6a5e3a566f42e11c",
      "pdf_url": "https://arxiv.org/pdf/1909.08081",
      "publication_date": "2019-09-17",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "63a00ba2eb537b049cb8c88882a306351500e407",
      "title": "Fair Machine Learning",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "J.A.R. Michorius"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/63a00ba2eb537b049cb8c88882a306351500e407",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "93756d1993b8403d715babfdffc73c04d968736d",
      "title": "A systematic review of fairness in machine learning",
      "abstract": null,
      "year": 2024,
      "venue": "AI and Ethics",
      "authors": [
        "Ricardo Trainotti Rabonato",
        "Lilian Berton"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/93756d1993b8403d715babfdffc73c04d968736d",
      "pdf_url": "",
      "publication_date": "2024-09-19",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8d824c89604d4df7820b5351c61936c7bbaf678",
      "title": "An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning",
      "abstract": "We address an inherent difficulty in welfare-theoretic fair machine learning by proposing an equivalently axiomatically-justified alternative and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of one or more groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair machine learning methods to produce satisfactory solutions that consider the diverse needs of multiple groups. Unfortunately, many machine-learning problems are more naturally cast as loss minimization tasks, rather than utility maximization, which complicates direct application of welfare-centric methods to fair machine learning. In this work, we define a complementary measure, termed malfare, measuring overall societal harm (rather than wellbeing), with axiomatic justification via the standard axioms of cardinal welfare. We then cast fair machine learning as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss. Building upon these concepts, we define fair-PAC (FPAC) learning, where an FPAC learner is an algorithm that learns an $\\varepsilon$-$\\delta$ malfare-optimal model with bounded sample complexity, for any data distribution, and for any (axiomatically justified) malfare concept. Finally, we show broad conditions under which, with appropriate modifications, standard PAC-learners may be converted to FPAC learners. This places FPAC learning on firm theoretical ground, as it yields statistical and computational efficiency guarantees for many well-studied machine-learning models, and is also practically relevant, as it democratizes fair ML by providing concrete training algorithms and rigorous generalization guarantees for these models",
      "year": 2021,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Cyrus Cousins"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/a8d824c89604d4df7820b5351c61936c7bbaf678",
      "pdf_url": "",
      "publication_date": "2021-04-29",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35af5472a483b8d4e87ea4177fc12d07b1563777",
      "title": "Enhancing Network Attack Detection with Interpretable Machine Learning Models",
      "abstract": "An Intrusion Detection System (IDS) can be used to monitor network and identify the occurrence of attacks that may compromise the integrity of the system. In spite of its popularity, IDS tend to have a large false positive rate. This paper deals with the presentation of IDS that employs various machine learning approaches (SVM, MLP, KNN, Random Forest) in categorizing the traffic into five classes: Normal, Denial of Service (DoS), Remote-to- Local (R2L), User-to-Root (U2R), and Probe attacks. The system is accurate to the extent of 99 percent. Random Forest has an F1-score of 99.7 percent in DoS whereas MLP has an F1-score of 83.4 percent in U2R. In case of Probe and R2L attacks, Random Forest sustains F1-scores of 99.1 percent and 95.6 percent, respectively. One of the characteristics of this line is a user friendly interface where the administrator is able to manually enter fields of traffic and watch the output of predictions. Such user-exploratory style of interpretability allows comprehending and trusting the model even without automated explanations. These results were validated on the NSL-KDD dataset, which proves the practical usage of the system in the field of cybersecurity.",
      "year": 2025,
      "venue": "OPTIMA",
      "authors": [
        "Amina Rachad",
        "Nora El Amrani",
        "Soumia Ziti"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/35af5472a483b8d4e87ea4177fc12d07b1563777",
      "pdf_url": "",
      "publication_date": "2025-10-16",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c43d40dff6664d17ff101a131a6fccf3b5de97e9",
      "title": "USE OF EXPERT SYSTEMS TO PREDICT ATTACKS ON WEB-BASED SERVERS",
      "abstract": "Cyber-attacks are on the rise, and various types of threats can compromise data confidentiality, integrity, and availability. Reports from the National Cyber and Crypto Agency (BSSN) and research by Check Point indicate a significant increase in cyber-attacks. These attacks often occur due to a lack of understanding and security testing of systems. In this context, the fundamental rules of the CIA (Confidentiality, Integrity, and Availability) become a crucial foundation for system security. Self-testing through penetration testing methods emerges as a solution to identify security vulnerabilities. Therefore, this research aims to develop an expert system using the OWASP Zap penetration testing tool to predict attacks on web-based servers. Utilizing a rule-based algorithm, the output of this expert system will provide results containing the type of attack, CIA classification, score, solutions, and more. In this study, testing and evaluation of the expert system are conducted on domains within the State University of Malang as the target. The test results indicate a satisfactory expert system performance with an accuracy rate of 91.62 percent. This evaluation is expected to provide a comprehensive insight into the expert system's performance in securing the system, enabling developers or campus administrators to address any issues promptly..",
      "year": 2024,
      "venue": "Jurnal Inovasi Teknologi dan Edukasi Teknik",
      "authors": [
        "M. Z. Ariffin",
        "Hildhan Fauzul Hakim"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c43d40dff6664d17ff101a131a6fccf3b5de97e9",
      "pdf_url": "",
      "publication_date": "2024-02-28",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2fc70d5a8c6b95f7e41111a51d633310fd2feed6",
      "title": "Fuzzy Logic-Driven Approaches to Enhancing Hardware Security in IoT Devices",
      "abstract": "The rapid growth of Internet of Things (IoT) devices has revolutionized various industries but has simultaneously amplified vulnerabilities in hardware security. This paper proposes a fuzzy logic-driven approach to enhance hardware security in IoT devices by addressing critical challenges such as tampering, side-channel attacks, and unauthorized access. The methodology defines key input parameters, including power consumption, temperature variations, and authentication anomalies, while producing adaptive output variables such as security breach likelihood and response intensity. By leveraging the uncertainty-handling capabilities of fuzzy logic, the framework dynamically assesses threats and applies real-time responses to improve security without compromising device performance. The proposed system offers significant improvements in detection accuracy, reduces false positives, and ensures computational efficiency, making it suitable for resource-constrained IoT environments. Validation through simulations highlights the system's ability to balance security and performance, providing a scalable and reliable solution for safeguarding IoT ecosystems.",
      "year": 2024,
      "venue": "International Conference on Communication and Electronics Systems",
      "authors": [
        "Rahib Imamguluyev"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/2fc70d5a8c6b95f7e41111a51d633310fd2feed6",
      "pdf_url": "",
      "publication_date": "2024-12-16",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2a0711c067c45545d54bcccffaa2f49d35dff64d",
      "title": "One-Shot Backdoor Removal for Federated Learning",
      "abstract": "Federated learning is a distributed machine learning approach that enables multiple participants to collaboratively train a model without sharing their data, thus preserving privacy. However, the decentralized nature of federated learning also makes it susceptible to backdoor attacks, where malicious participants can embed hidden vulnerabilities within the model. Addressing these threats efficiently and effectively is crucial, especially given the impracticality of iterative and resource-intensive detection methods in federated learning environments. This article presents a novel framework for one-shot backdoor removal in federated learning. Our approach integrates advanced anomaly detection techniques with a unique model update aggregation strategy, allowing for the identification and neutralization of backdoor influences in a single update cycle without the need for extensive data access or communication between participants. Extensive experiments across various federated architectures and data distributions demonstrate that our method effectively mitigates backdoor threats while maintaining model performance and scalability. This work not only enhances the security of federated models but also contributes to the broader applicability of federated learning in sensitive and critical domains.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Zijie Pan",
        "Zuobin Ying",
        "Yajie Wang",
        "Chuan Zhang",
        "Chunhai Li",
        "Liehuang Zhu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2a0711c067c45545d54bcccffaa2f49d35dff64d",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}