{
  "updated": "2026-01-06",
  "total": 22,
  "owasp_id": "ML04",
  "owasp_name": "Membership Inference Attack",
  "description": "Attacks that determine whether specific data points were used to train a\n        machine learning model. This includes membership inference attacks, privacy\n        attacks that reveal training set membership, and techniques to distinguish\n        training data from non-training data based on model behavior.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "title": "TSQP: Safeguarding Real-Time Inference for Quantization Neural Networks on Edge Devices",
      "abstract": "Quantization Neural Networks (QNNs) has been widely adopted in resource-constrained edge devices due to their real-time capabilities and low resource requirement. However, concerns have arisen regarding that deployed models are white-box available to model thefts. To address this issue, TEE-shielded secure inference has been introduced as a secure and efficient solution. Nevertheless, existing methods neglect the compatibility with 8-bit quantized computation, which leads to severe integer overflow issue during inference. This issue could result a disastrous degradation in QNNs (to random guessing level), completely destroying model utility. Moreover, the model confidentiality and inference integrity also face a substantial threat due to the limited data representation space. To safeguard accurate and efficient inference for QNNs, TEE-Shielded QNN Partition (TSQP) are proposed, which presents three key insights: Firstly, Quantization Manager is designed to convert white-box inference to black-box by shielding critical scales in TEE. Additionally, overflow concerns are effectively addressed using reduced-range approaches. Secondly, by leveraging the Information Bottleneck theory to enhance model training, we introduce Parameter De-Similarity to defend against powerful Model Stealing attacks that existing methods are vulnerable to. Thirdly, the Integrity Monitor is suggested to detect inference integrity breaches in an oblivious manner. In contrast, existing method can be bypassed due to the lack of obliviousness. Experimental results demonstrate that proposed TSQP maintains high accuracy and achieves accurate integrity breaches detection. Our method achieves more than $8\\times$ speedup compared to full TEE inference, while reducing Model Stealing attacks accuracy from $3.99\\times$ to $1.29\\times$. To our best knowledge, proposed method is the first TEE-shielded secure inference solution that achieves model confidentiality, inference integrity and model utility on QNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yu Sun",
        "Gaojian Xiong",
        "Jianhua Liu",
        "Zheng Liu",
        "Jian Cui"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "title": "Unraveling Attacks to Machine-Learning-Based IoT Systems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This article embarks on a comprehensive exploration of the security threats arising from ML\u2019s integration into various facets of IoT, spanning various attack types, including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria, such as adversary models, attack targets, and key security attributes (confidentiality, integrity, and availability). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this article seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
      "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrey V. Galichin",
        "Mikhail Aleksandrovich Pautov",
        "Alexey Zhavoronkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
      "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "pdf_url": "",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "title": "Defending Against Label-Only Attacks via Meta-Reinforcement Learning",
      "abstract": "Machine learning models are susceptible to a range of adversarial activities. These attacks are designed to either infer private information from the target model or deceive it. For instance, an attacker may attempt to discern if a given data example is from the model\u2019s training set (membership inference attacks) or create adversarial examples to mislead the model to make incorrect predictions (adversarial example attacks). Numerous defense methods have been proposed to counter these attacks. However, these methods typically share two common limitations. Firstly, most are not designed to address label-only attacks, which is a newly emerged kind of attacks that rely solely on the hard labels predicted by the target model. Secondly, they are often developed to mitigate specific attacks rather than universally various attacks. To address these limitations, this paper proposes a novel defense method that focuses on the most challenging attacks, i.e., label-only attacks, and can handle various types of label-only attacks. The key idea is to strategically modify the target model\u2019s predicted labels using a meta-reinforcement learning technique. This ensures that attackers receive incorrect labels while benign users continue to receive correct labels. Notably, the defender, i.e., the owner of the target model, can make effective decisions without knowledge of the attacker\u2019s behavior. The experimental results demonstrate that our proposed method is an effective defense against a range of attacks, including label-only model stealing, label-only membership inference, label-only model inversion, and label-only adversarial example attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Kun Gao",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "title": "MTL-Leak: Privacy Risk Assessment in Multi-Task Learning",
      "abstract": "Multi-task learning (MTL) supports simultaneous training over multiple related tasks and learns the shared representation. While improving the generalization ability of training on a single task, MTL has higher privacy risk than traditional single-task learning because more sensitive information is extracted and learned in a correlated manner. Unfortunately, very few works have attempted to address the privacy risks posed by MTL. In this article, we first investigate such risk by designing model extraction attack (MEA) and membership inference attack (MIA) in MTL. Then we evaluate the privacy risks on six MTL model architectures and two popular MTL datasets, whose results show that both the number of tasks and the complexity of training data play an important role in the attack performance. Our investigation shows that MTL is more vulnerable than traditional single-task learning under both attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongyang Yan",
        "Anli Yan",
        "Li Hu",
        "Jiaming Liang",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "title": "No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML",
      "abstract": "On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) against model weights and membership inference attack (MIA) against training data privacy. Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming of TEEs is the sharply increased latency (up to 50\u00d7). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading1 the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. However, the community lacks an in-depth understanding of the seemingly encouraging privacy guarantees offered by existing TSDP solutions during DNN inference. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding the optimal DNN partition configurations, which vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. Unlike existing approaches, TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the \"upper-bound\" security guarantees) with over 10\u00d7less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss. We make the code and artifacts publicly available on the Internet.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ziqi Zhang",
        "Chen Gong",
        "Yifeng Cai",
        "Yuanyuan Yuan",
        "Bingyan Liu",
        "Ding Li",
        "Yao Guo",
        "Xiangqun Chen"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "pdf_url": "https://arxiv.org/pdf/2310.07152",
      "publication_date": "2023-10-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "title": "Membership Inference Attacks Against Sequential Recommender Systems",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user\u2019s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user\u2019s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model\u2019s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model\u2019s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users\u2019 membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
      "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wen",
        "Tianhao Wang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bf87c3c380802df24628cab8f6dff90b42304f77",
      "title": "Split HE: Fast Secure Inference Combining Split Learning and Homomorphic Encryption",
      "abstract": "This work presents a novel protocol for fast secure inference of neural networks applied to computer vision applications. It focuses on improving the overall performance of the online execution by deploying a subset of the model weights in plaintext on the client's machine, in the fashion of SplitNNs. We evaluate our protocol on benchmark neural networks trained on the CIFAR-10 dataset using SEAL via TenSEAL and discuss runtime and security performances. Empirical security evaluation using Membership Inference and Model Extraction attacks showed that the protocol was more resilient under the same attacks than a similar approach also based on SplitNN. When compared to related work, we demonstrate improvements of 2.5x-10x for the inference time and 14x-290x in communication costs.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "George-Liviu Pereteanu",
        "A. Alansary",
        "Jonathan Passerat-Palmbach"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/bf87c3c380802df24628cab8f6dff90b42304f77",
      "pdf_url": "",
      "publication_date": "2022-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e8572d9722e992a770b7c00a8419dda17297a9da",
      "title": "Side-Channel Fuzzy Analysis-Based AI Model Extraction Attack With Information-Theoretic Perspective in Intelligent IoT",
      "abstract": "Accessibility to smart devices provides opportunities for side-channel attacks (SCAs) on artificial intelligent (AI) models in the intelligent Internet of Things (IoT). However, the existing literature exposes some shortcomings: 1) incapability of quantifying and analyzing the leaked information through side channels of the intelligent IoT and 2) inability to devise efficient and accurate SCA algorithms. To address these challenges, we propose a side-channel fuzzy analysis-empowered AI model extraction attack in the intelligent IoT. First, the integrated AI model extraction framework is proposed, including power trace-based structure, execution time-based metaparameters, and hierarchical weight extractions. Then, we develop the information theory-based analysis for the AI model extraction via SCA. We derive a mutual information-enabled quantification method, theoretical lower/upper bounds of information leakage, and the minimum number of attack queries to obtain accurate weights. Furthermore, a fuzzy gray correlation-based multiple-microspace parallel SCA algorithm is proposed to extract model weights in the intelligent IoT. Based on the established information-theoretic analysis model, the proposed fuzzy gray correlation-based SCA algorithm obtains high-precision AI weights. Experimental results, consisting of simulation and real-world experiments, verify that the developed analysis method with the information-theoretic perspective is feasible and demonstrate that the designed fuzzy gray correlation-based SCA algorithm is effective for AI model extraction.",
      "year": 2022,
      "venue": "IEEE transactions on fuzzy systems",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "A. Bashir",
        "Jianhua Li",
        "Jie Wu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/e8572d9722e992a770b7c00a8419dda17297a9da",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "title": "On the Effectiveness of Dataset Watermarking",
      "abstract": "In a data-driven world, datasets constitute a significant economic value. Dataset owners who spend time and money to collect and curate the data are incentivized to ensure that their datasets are not used in ways that they did not authorize. When such misuse occurs, dataset owners need technical mechanisms for demonstrating their ownership of the dataset in question. Dataset watermarking provides one approach for ownership demonstration which can, in turn, deter unauthorized use. In this paper, we investigate a recently proposed data provenance method, radioactive data, to assess if it can be used to demonstrate ownership of (image) datasets used to train machine learning (ML) models. The original paper radioactive reported that radioactive data is effective in white-box settings. We show that while this is true for large datasets with many classes, it is not as effective for datasets where the number of classes is low (\u0142eq 30) or the number of samples per class is low (\u0142eq 500). We also show that, counter-intuitively, the black-box verification technique described in radioactive is effective for all datasets used in this paper, even when white-box verification in radioactive is not. Given this observation, we show that the confidence in white-box verification can be improved by using watermarked samples directly during the verification process. We also highlight the need to assess the robustness of radioactive data if it were to be used for ownership demonstration since it is an adversarial setting unlike provenance identification. Compared to dataset watermarking, ML model watermarking has been explored more extensively in recent literature. However, most of the state-of-the-art model watermarking techniques can be defeated via model extraction robustness. We show that radioactive data can effectively survive model extraction attacks, which raises the possibility that it can be used for ML model ownership verification robust against model extraction.",
      "year": 2022,
      "venue": "IWSPA@CODASPY",
      "authors": [
        "Buse Gul",
        "Atli Tekgul",
        "N. Asokan"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3510548.3519376",
      "publication_date": "2022-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "title": "MExMI: Pool-based Active Model Extraction Crossover Membership Inference",
      "abstract": null,
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Haibo Hu",
        "Huadi Zheng",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14df6adb35c1e13d69d9f8c61f12c07bd7294ecf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b778d465e9223b3a54601033000f61c985b00849",
      "title": "SEEK: model extraction attack against hybrid secure inference protocols",
      "abstract": "Security concerns about a machine learning model used in a prediction-as-a-service include the privacy of the model, the query and the result. Secure inference solutions based on homomorphic encryption (HE) and/or multiparty computation (MPC) have been developed to protect all the sensitive information. One of the most efficient type of solution utilizes HE for linear layers, and MPC for non-linear layers. However, for such hybrid protocols with semi-honest security, an adversary can malleate the intermediate features in the inference process, and extract model information more effectively than methods against inference service in plaintext. In this paper, we propose SEEK, a general extraction method for hybrid secure inference services outputing only class labels. This method can extract each layer of the target model independently, and is not affected by the depth of the model. For ResNet-18, SEEK can extract a parameter with less than 50 queries on average, with average error less than $0.03\\%$.",
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Si-Quan Chen",
        "Junfeng Fan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b778d465e9223b3a54601033000f61c985b00849",
      "pdf_url": "http://arxiv.org/pdf/2209.06373",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "title": "Holistic risk assessment of inference attacks in machine learning",
      "abstract": "As machine learning expanding application, there are more and more unignorable privacy and safety issues. Especially inference attacks against Machine Learning models allow adversaries to infer sensitive information about the target model, such as training data, model parameters, etc. Inference attacks can lead to serious consequences, including violating individuals privacy, compromising the intellectual property of the owner of the machine learning model. As far as concerned, researchers have studied and analyzed in depth several types of inference attacks, albeit in isolation, but there is still a lack of a holistic rick assessment of inference attacks against machine learning models, such as their application in different scenarios, the common factors affecting the performance of these attacks and the relationship among the attacks. As a result, this paper performs a holistic risk assessment of different inference attacks against Machine Learning models. This paper focuses on three kinds of representative attacks: membership inference attack, attribute inference attack and model stealing attack. And a threat model taxonomy is established. A total of 12 target models using three model architectures, including AlexNet, ResNet18 and Simple CNN, are trained on four datasets, namely CelebA, UTKFace, STL10 and FMNIST.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yang Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "pdf_url": "http://arxiv.org/pdf/2212.10628",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network",
      "abstract": "We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons. These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Amit Daniely",
        "Elad Granot"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "pdf_url": "",
      "publication_date": "2021-05-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "title": "Securing Machine Learning Architectures and Systems",
      "abstract": "Machine learning (ML), and deep learning in particular, have become a critical workload as they are becoming increasingly applied at the core of a wide range of application spaces. Computer systems, from the architecture up, have been impacted by ML in two primary directions: (1) ML is an increasingly important computing workload, with new accelerators and systems targeted to support both training and inference at scale; and (2) ML supporting computer system decisions, both during design and run times, with new machine learning based algorithms controlling systems to optimize their performance, reliability and robustness. In this paper, we will explore the intersection of security, ML and computing systems, identifying both security challenges and opportunities. Machine learning systems are vulnerable to new attacks including adversarial attacks crafted to fool a classifier to the attacker's advantage, membership inference attacks attempting to compromise the privacy of the training data, and model extraction attacks seeking to recover the hyperparameters of a (secret) model. Architecture can be a target of these attacks when supporting ML (or is supported by ML), but also provides an opportunity to develop defenses against them, which we will illustrate with three examples from our recent work. First, we show how ML based hardware malware detectors can be attacked with adversarial perturbations to the Malware and how we can develop detectors that resist these attacks. Second, we show an example of microarchitectural side channel attacks that can be used to extract the secret parameters of a neural network and potential defenses against it. Finally, we discuss how hardware and systems can be used to make ML more robust against adversarial and other attacks.",
      "year": 2020,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Shirin Haji Amin Shirazi",
        "Hoda Naghibijouybari",
        "N. Abu-Ghazaleh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "pdf_url": "",
      "publication_date": "2020-09-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "title": "Monitoring-based Differential Privacy Mechanism Against Query-Flooding Parameter Duplication Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this paper, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "title": "Mitigating Query-Flooding Parameter Duplication Attack on Regression Models with High-Dimensional Gaussian Mechanism",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Differential privacy (DP) has been considered a promising technique to mitigate this attack. However, we find that the vulnerability persists when regression models are being protected by current DP solutions. We show that the adversary can launch a query-flooding parameter duplication (QPD) attack to infer the model information by repeated queries. \nTo defend against the QPD attack on logistic and linear regression models, we propose a novel High-Dimensional Gaussian (HDG) mechanism to prevent unauthorized information disclosure without interrupting the intended services. In contrast to prior work, the proposed HDG mechanism will dynamically generate the privacy budget and random noise for different queries and their results to enhance the obfuscation. Besides, for the first time, HDG enables an optimal privacy budget allocation that automatically determines the minimum amount of noise to be added per user-desired privacy level on each dimension. We comprehensively evaluate the performance of HDG using real-world datasets and shows that HDG effectively mitigates the QPD attack while satisfying the privacy requirements. We also prepare to open-source the relevant codes to the community for further research.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Xiaoguang Li",
        "Hui Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "544e7fe88816924a81962eb79fbb549571ec2217",
      "title": "Killing Four Birds with one Gaussian Process: The Relation between different Test-Time Attacks",
      "abstract": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. Consequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process Classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a different norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all information when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g. model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other.",
      "year": 2018,
      "venue": "International Conference on Pattern Recognition",
      "authors": [
        "Kathrin Grosse",
        "M. Smith",
        "M. Backes"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/544e7fe88816924a81962eb79fbb549571ec2217",
      "pdf_url": "https://arxiv.org/pdf/1806.02032",
      "publication_date": "2018-06-06",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}