{
  "owasp_id": "ML04",
  "owasp_name": "Membership Inference Attack",
  "total": 57,
  "updated": "2026-01-28",
  "papers": [
    {
      "paper_id": "seed_a056458d",
      "title": "Sharpness-Aware Initialization: Improving Differentially Private Machine Learning from First Principles",
      "abstract": "Artificial intelligence research is ushering in a new era of sophisticated, mass-market transportation technology. While computers can already fly a passenger jet better than a trained human pilot, people are still faced with the dangerous yet tedious task of driving automobiles. Intelligent Transportation Systems (ITS) is the field that focuses on integrating information technology with vehicles and transportation infrastructure to make transportation safer, cheaper, and more efficient. Recent advances in ITS point to a future in which vehicles themselves handle the vast majority of the driving task. Once autonomous vehicles become popular, autonomous interactions amongst multiple vehicles will be possible. Current methods of vehicle coordination, which are all designed to work with human drivers, will be outdated. The bottleneck for roadway efficiency will no longer be the drivers, but rather the mechanism by which those drivers' actions are coordinated. While open-road driving is a well-studied and more-or-less-solved problem, urban traffic scenarios, especially intersections, are much more challenging. We believe current methods for controlling traffic, specifically at intersections, will not be able to take advantage of the increased sensitivity and precision of autonomous vehicles as compared to human drivers. In this article, we suggest an alternative mechanism for coordinating the movement of autonomous vehicles through intersections. Drivers and intersections in this mechanism are treated as autonomous agents in a multiagent system. In this multiagent system, intersections use a new reservation-based approach built around a detailed communication protocol, which we also present. We demonstrate in simulation that our new mechanism has the potential to significantly outperform current intersection control technology -- traffic lights and stop signs. Because our mechanism can emulate a traffic light or stop sign, it subsumes the most popular current methods of intersection control. This article also presents two extensions to the mechanism. The first extension allows the system to control human-driven vehicles in addition to autonomous vehicles. The second gives priority to emergency vehicles without significant cost to civilian vehicles. The mechanism, including both extensions, is implemented and tested in simulation, and we present experimental results that strongly attest to the efficacy of this approach.",
      "year": 2008,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Kurt M. Dresner",
        "P. Stone"
      ],
      "author_details": [
        {
          "name": "Kurt M. Dresner",
          "h_index": 11,
          "citation_count": 2888,
          "affiliations": []
        },
        {
          "name": "P. Stone",
          "h_index": 93,
          "citation_count": 40144,
          "affiliations": []
        }
      ],
      "max_h_index": 93,
      "url": "https://openalex.org/W2137514195",
      "pdf_url": "https://jair.org/index.php/jair/article/download/10542/25241",
      "doi": "https://doi.org/10.1613/jair.2502",
      "citation_count": 1300,
      "influential_citation_count": 117,
      "reference_count": 53,
      "is_open_access": true,
      "tldr": "This article suggests an alternative mechanism for coordinating the movement of autonomous vehicles through intersections and demonstrates in simulation that this new mechanism has the potential to significantly outperform current intersection control technology--traffic lights and stop signs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "differential-privacy",
        "initialization"
      ],
      "open_access_pdf": "https://jair.org/index.php/jair/article/download/10542/25241"
    },
    {
      "paper_id": "1912.03817",
      "title": "Machine Unlearning",
      "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Christopher A. Choquette-Choo",
        "Hengrui Jia",
        "Adelin Travers",
        "Baiwu Zhang",
        "D. Lie",
        "Nicolas Papernot"
      ],
      "author_details": [
        {
          "name": "Lucas Bourtoule",
          "h_index": 1,
          "citation_count": 1203,
          "affiliations": []
        },
        {
          "name": "Varun Chandrasekaran",
          "h_index": 15,
          "citation_count": 6121,
          "affiliations": []
        },
        {
          "name": "Christopher A. Choquette-Choo",
          "h_index": 24,
          "citation_count": 9873,
          "affiliations": [
            "Google DeepMind"
          ]
        },
        {
          "name": "Hengrui Jia",
          "h_index": 11,
          "citation_count": 2087,
          "affiliations": []
        },
        {
          "name": "Adelin Travers",
          "h_index": 3,
          "citation_count": 1226,
          "affiliations": []
        },
        {
          "name": "Baiwu Zhang",
          "h_index": 5,
          "citation_count": 1300,
          "affiliations": []
        },
        {
          "name": "D. Lie",
          "h_index": 26,
          "citation_count": 5578,
          "affiliations": []
        },
        {
          "name": "Nicolas Papernot",
          "h_index": 30,
          "citation_count": 5329,
          "affiliations": []
        }
      ],
      "max_h_index": 30,
      "url": "https://arxiv.org/abs/1912.03817",
      "citation_count": 1204,
      "influential_citation_count": 174,
      "reference_count": 69,
      "is_open_access": false,
      "publication_date": "2019-12-09",
      "tldr": "This work introduces SISA training, a framework that decreases the number of model parameters affected by an unlearning request and caches intermediate outputs of the training algorithm to limit thenumber of model updates that need to be computed to have these parameters unlearn.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "machine-unlearning",
        "SISA",
        "right-to-be-forgotten"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_6f528159",
      "title": "Enhanced Label-Only Membership Inference Attacks with Fewer Queries",
      "abstract": "A comprehensive review of an area of machine learning that deals with the use of unlabeled data in classification problems: state-of-the-art algorithms, a taxonomy of the field, applications, benchmark experiments, and directions for future research. In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction.",
      "year": 2006,
      "venue": "The MIT Press eBooks",
      "authors": [
        "O. Chapelle",
        "Bernhard Schlkopf",
        "A. Zien"
      ],
      "author_details": [
        {
          "name": "O. Chapelle",
          "h_index": 64,
          "citation_count": 24272,
          "affiliations": []
        },
        {
          "name": "Bernhard Schlkopf",
          "h_index": 1,
          "citation_count": 1190,
          "affiliations": []
        },
        {
          "name": "A. Zien",
          "h_index": 35,
          "citation_count": 8922,
          "affiliations": []
        }
      ],
      "max_h_index": 64,
      "url": "https://openalex.org/W1479807131",
      "pdf_url": "http://hdl.handle.net/11858/00-001M-0000-0013-D029-E",
      "doi": "https://doi.org/10.7551/mitpress/9780262033589.001.0001",
      "citation_count": 1188,
      "influential_citation_count": 39,
      "reference_count": 369,
      "is_open_access": true,
      "publication_date": "2006-11-01",
      "tldr": "This first comprehensive overview of semi-supervised learning presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Review"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "label-only",
        "query-efficient"
      ],
      "open_access_pdf": "https://www.molgen.mpg.de/3659531/MITPress--SemiSupervised-Learning.pdf"
    },
    {
      "paper_id": "seed_ea780f94",
      "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
      "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. Next, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to identify samples with high privacy risks and perform attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score and demonstrate that the distribution of privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risks, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Liwei Song",
        "Prateek Mittal"
      ],
      "author_details": [
        {
          "name": "Liwei Song",
          "h_index": 12,
          "citation_count": 1376,
          "affiliations": []
        },
        {
          "name": "Prateek Mittal",
          "h_index": 59,
          "citation_count": 21589,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W3013068160",
      "pdf_url": "https://arxiv.org/pdf/2003.10595",
      "doi": "https://doi.org/10.48550/arxiv.2003.10595",
      "citation_count": 457,
      "influential_citation_count": 79,
      "reference_count": 60,
      "is_open_access": false,
      "publication_date": "2020-03-24",
      "tldr": "This paper proposes to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy, and introduces a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "privacy-risk",
        "systematic-evaluation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2111.09679",
      "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
      "abstract": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensive \\textit{hypothesis testing framework} that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explain \\textit{why} different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform a \\textit{differential analysis} between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of the \\textit{Privacy Meter} software tool.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiayuan Ye",
        "Aadyaa Maddi",
        "S. K. Murakonda",
        "R. Shokri"
      ],
      "author_details": [
        {
          "name": "Jiayuan Ye",
          "h_index": 7,
          "citation_count": 547,
          "affiliations": []
        },
        {
          "name": "Aadyaa Maddi",
          "h_index": 2,
          "citation_count": 334,
          "affiliations": []
        },
        {
          "name": "S. K. Murakonda",
          "h_index": 5,
          "citation_count": 534,
          "affiliations": []
        },
        {
          "name": "R. Shokri",
          "h_index": 45,
          "citation_count": 16637,
          "affiliations": []
        }
      ],
      "max_h_index": 45,
      "url": "https://arxiv.org/abs/2111.09679",
      "citation_count": 331,
      "influential_citation_count": 42,
      "reference_count": 63,
      "is_open_access": true,
      "publication_date": "2021-11-18",
      "tldr": "This paper presents a comprehensive hypothesis testing framework that enables not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power for any (false positive rate) error.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "hypothesis-testing",
        "enhanced-attack"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560675"
    },
    {
      "paper_id": "seed_cb751896",
      "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
      "abstract": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\u03b5$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\u03b5$ that are sometimes used in practice (e.g., $\u03b5=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Klas Leino",
        "Matt Fredrikson"
      ],
      "author_details": [
        {
          "name": "Klas Leino",
          "h_index": 10,
          "citation_count": 743,
          "affiliations": []
        },
        {
          "name": "Matt Fredrikson",
          "h_index": 34,
          "citation_count": 15038,
          "affiliations": []
        }
      ],
      "max_h_index": 34,
      "url": "https://openalex.org/W2956128647",
      "pdf_url": "https://arxiv.org/pdf/1906.11798",
      "doi": "https://doi.org/10.48550/arxiv.1906.11798",
      "citation_count": 308,
      "influential_citation_count": 24,
      "reference_count": 50,
      "is_open_access": false,
      "publication_date": "2019-06-27",
      "tldr": "This work shows how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrates that this attack outperforms prior black- box methods.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "white-box",
        "memorization",
        "calibrated"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2005.02205",
      "title": "When Machine Unlearning Jeopardizes Privacy",
      "abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "M. Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Min Chen",
          "h_index": 9,
          "citation_count": 641,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "Tianhao Wang",
          "h_index": 29,
          "citation_count": 3900,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Mathias Humbert",
          "h_index": 25,
          "citation_count": 3331,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://arxiv.org/abs/2005.02205",
      "citation_count": 288,
      "influential_citation_count": 27,
      "reference_count": 94,
      "is_open_access": true,
      "publication_date": "2020-05-05",
      "tldr": "This paper proposes a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of theTraining set of a corresponding unlearned model.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "unlearning-privacy",
        "membership-inference"
      ],
      "open_access_pdf": "https://serval.unil.ch/resource/serval:BIB_E901AD8FB769.P001/REF.pdf"
    },
    {
      "paper_id": "seed_bd37232d",
      "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
      "abstract": "Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. In this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model ({\\UGAN}) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "A. Salem",
        "Apratim Bhattacharyya",
        "M. Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "A. Salem",
          "h_index": 17,
          "citation_count": 3182,
          "affiliations": []
        },
        {
          "name": "Apratim Bhattacharyya",
          "h_index": 13,
          "citation_count": 1120,
          "affiliations": [
            "University of T\u00fcbingen"
          ]
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Mario Fritz",
          "h_index": 69,
          "citation_count": 22326,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://openalex.org/W2926319231",
      "pdf_url": "https://arxiv.org/pdf/1904.01067",
      "doi": "https://doi.org/10.60882/cispa.24613164",
      "citation_count": 283,
      "influential_citation_count": 14,
      "reference_count": 59,
      "is_open_access": false,
      "publication_date": "2019-04-01",
      "tldr": "This paper investigates whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set, and proposes four attacks following an encoder-decoder formulation that allows inferring diverse information ofThe updating set.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "online-learning",
        "dataset-inference",
        "reconstruction"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_c6706fde",
      "title": "Machine Unlearning of Features and Labels",
      "abstract": "Removing information from a machine learning model is a non-trivial task that requires to partially revert the training process.This task is unavoidable when sensitive data, such as credit card numbers or passwords, accidentally enter the model and need to be removed afterwards.Recently, different concepts for machine unlearning have been proposed to address this problem.While these approaches are effective in removing individual data points, they do not scale to scenarios where larger groups of features and labels need to be reverted.In this paper, we propose the first method for unlearning features and labels.Our approach builds on the concept of influence functions and realizes unlearning through closed-form updates of model parameters.It enables to adapt the influence of training data on a learning model retrospectively, thereby correcting data leaks and privacy issues.For learning models with strongly convex loss functions, our method provides certified unlearning with theoretical guarantees.For models with non-convex losses, we empirically show that the unlearning of features and labels is effective and significantly faster than other strategies.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Alexander Warnecke",
        "Lukas Pirch",
        "Christian Wressnegger",
        "Konrad Rieck"
      ],
      "author_details": [
        {
          "name": "Alexander Warnecke",
          "h_index": 9,
          "citation_count": 620,
          "affiliations": []
        },
        {
          "name": "Lukas Pirch",
          "h_index": 6,
          "citation_count": 320,
          "affiliations": []
        },
        {
          "name": "Christian Wressnegger",
          "h_index": 19,
          "citation_count": 1625,
          "affiliations": []
        },
        {
          "name": "Konrad Rieck",
          "h_index": 42,
          "citation_count": 11256,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://openalex.org/W3193974325",
      "doi": "https://doi.org/10.14722/ndss.2023.23087",
      "citation_count": 268,
      "influential_citation_count": 31,
      "reference_count": 81,
      "is_open_access": true,
      "publication_date": "2021-08-26",
      "tldr": "This approach builds on the concept of influence functions and realizes unlearning through closed-form updates of model parameters and enables to adapt the influence of training data on a learning model retrospectively, thereby correcting data leaks and privacy issues.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "feature-unlearning",
        "label-unlearning"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2023.23087"
    },
    {
      "paper_id": "seed_0e016306",
      "title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
      "abstract": "Machine unlearning, i.e. having a model forget about some of its training data, has become increasingly more important as privacy legislation promotes variants of the right-to-be-forgotten. In the context of deep learning, approaches for machine unlearning are broadly categorized into two classes: exact unlearning methods, where an entity has formally removed the data point's impact on the model by retraining the model from scratch, and approximate unlearning, where an entity approximates the model parameters one would obtain by exact unlearning to save on compute costs. In this paper, we first show that the definition that underlies approximate unlearning, which seeks to prove the approximately unlearned model is close to an exactly retrained model, is incorrect because one can obtain the same model using different datasets. Thus one could unlearn without modifying the model at all. We then turn to exact unlearning approaches and ask how to verify their claims of unlearning. Our results show that even for a given training trajectory one cannot formally prove the absence of certain data points used during training. We thus conclude that unlearning is only well-defined at the algorithmic level, where an entity's only possible auditable claim to unlearning is that they used a particular algorithm designed to allow for external scrutiny during an audit.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Anvith Thudi",
        "Hengrui Jia",
        "Ilia Shumailov",
        "Nicolas Papernot"
      ],
      "author_details": [
        {
          "name": "Anvith Thudi",
          "h_index": 9,
          "citation_count": 728,
          "affiliations": []
        },
        {
          "name": "Hengrui Jia",
          "h_index": 11,
          "citation_count": 2087,
          "affiliations": []
        },
        {
          "name": "Ilia Shumailov",
          "h_index": 25,
          "citation_count": 7624,
          "affiliations": [
            "Google Deepmind"
          ]
        },
        {
          "name": "Nicolas Papernot",
          "h_index": 30,
          "citation_count": 5329,
          "affiliations": []
        }
      ],
      "max_h_index": 30,
      "url": "https://openalex.org/W3208439705",
      "pdf_url": "https://arxiv.org/pdf/2110.11891",
      "doi": "https://doi.org/10.48550/arxiv.2110.11891",
      "citation_count": 201,
      "influential_citation_count": 25,
      "reference_count": 26,
      "is_open_access": false,
      "publication_date": "2021-10-22",
      "tldr": "It is shown that the definition that underlies approximate unlearning, which seeks to prove the approximately unlearned model is close to an exactly retrained model, is incorrect because one can obtain the same model using different datasets, and therefore one could unlearn without modifying the model at all.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "unlearning-definitions",
        "auditability"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2103.14991",
      "title": "Graph Unlearning",
      "abstract": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to $62.5\\%$ higher F1 score and our proposed learning-based aggregation method achieves up to $112\\%$ higher F1 score.\\footnote{Our code is available at \\url{https://github.com/MinChen00/Graph-Unlearning}.}",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "M. Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Min Chen",
          "h_index": 9,
          "citation_count": 641,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "Tianhao Wang",
          "h_index": 29,
          "citation_count": 3900,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Mathias Humbert",
          "h_index": 25,
          "citation_count": 3331,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://arxiv.org/abs/2103.14991",
      "citation_count": 189,
      "influential_citation_count": 27,
      "reference_count": 103,
      "is_open_access": true,
      "publication_date": "2021-03-27",
      "tldr": "GraphEraser is proposed, a novel machine unlearning framework tailored to graph data whose contributions include two novel graph partition algorithms and a learning-based aggregation method.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "graph-unlearning",
        "partition"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2103.14991"
    },
    {
      "paper_id": "seed_c76b43b1",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "A. Salem",
        "Zhikun Zhang",
        "M. Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Yugeng Liu",
          "h_index": 9,
          "citation_count": 608,
          "affiliations": []
        },
        {
          "name": "Rui Wen",
          "h_index": 10,
          "citation_count": 804,
          "affiliations": []
        },
        {
          "name": "Xinlei He",
          "h_index": 20,
          "citation_count": 1612,
          "affiliations": []
        },
        {
          "name": "A. Salem",
          "h_index": 17,
          "citation_count": 3182,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Emiliano De Cristofaro",
          "h_index": 55,
          "citation_count": 11460,
          "affiliations": [
            "University College London"
          ]
        },
        {
          "name": "Mario Fritz",
          "h_index": 69,
          "citation_count": 22326,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://openalex.org/W3126152116",
      "pdf_url": "https://arxiv.org/pdf/2102.02551",
      "doi": "https://doi.org/10.48550/arxiv.2102.02551",
      "citation_count": 155,
      "influential_citation_count": 24,
      "reference_count": 70,
      "is_open_access": false,
      "publication_date": "2021-02-04",
      "tldr": "This paper presents a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models, concentrating on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establishes a threat model taxonomy.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "holistic-assessment",
        "inference-attacks"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2006.05535",
      "title": "Locally Private Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in learning node representations for various graph inference tasks. However, learning over graph data can raise privacy concerns when nodes represent people or human-related variables that involve sensitive or personal information. While numerous techniques have been proposed for privacy-preserving deep learning over non-relational data, there is less work addressing the privacy issues pertained to applying deep learning algorithms on graphs. In this paper, we study the problem of node data privacy, where graph nodes have potentially sensitive data that is kept private, but they could be beneficial for a central server for training a GNN over the graph. To address this problem, we develop a privacy-preserving, architecture-agnostic GNN learning algorithm with formal privacy guarantees based on Local Differential Privacy (LDP). Specifically, we propose an LDP encoder and an unbiased rectifier, by which the server can communicate with the graph nodes to privately collect their data and approximate the GNN's first layer. To further reduce the effect of the injected noise, we propose to prepend a simple graph convolution layer, called KProp, which is based on the multi-hop aggregation of the nodes' features acting as a denoising mechanism. Finally, we propose a robust training framework, in which we benefit from KProp's denoising capability to increase the accuracy of inference in the presence of noisy labels. Extensive experiments conducted over real-world datasets demonstrate that our method can maintain a satisfying level of accuracy with low privacy loss.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Sina Sajadmanesh",
        "D. G\u00e1tica-P\u00e9rez"
      ],
      "author_details": [
        {
          "name": "Sina Sajadmanesh",
          "h_index": 7,
          "citation_count": 653,
          "affiliations": [
            "Idiap Research Institute, EPFL"
          ]
        },
        {
          "name": "D. G\u00e1tica-P\u00e9rez",
          "h_index": 61,
          "citation_count": 15132,
          "affiliations": []
        }
      ],
      "max_h_index": 61,
      "url": "https://arxiv.org/abs/2006.05535",
      "citation_count": 151,
      "influential_citation_count": 24,
      "reference_count": 104,
      "is_open_access": true,
      "publication_date": "2020-06-09",
      "tldr": "This paper proposes a privacy-preserving, architecture-agnostic GNN learning framework with formal privacy guarantees based on Local Differential Privacy (LDP), and develops a locally private mechanism to perturb and compress node features, which the server can efficiently collect to approximate the GNN's neighborhood aggregation step.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "local-DP",
        "GNN-privacy"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2006.05535"
    },
    {
      "paper_id": "2208.14933",
      "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
      "abstract": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called \\system, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely \\emph{distilled loss trajectory}, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6$\\times$ higher true-positive rate at a low false-positive rate of 0.1\\% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yiyong Liu",
        "Zhengyu Zhao",
        "M. Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Yiyong Liu",
          "h_index": 3,
          "citation_count": 203,
          "affiliations": []
        },
        {
          "name": "Zhengyu Zhao",
          "h_index": 18,
          "citation_count": 1134,
          "affiliations": [
            "Xi'an Jiaotong University"
          ]
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://arxiv.org/abs/2208.14933",
      "citation_count": 147,
      "influential_citation_count": 17,
      "reference_count": 54,
      "is_open_access": true,
      "publication_date": "2022-08-31",
      "tldr": "A new attack method is proposed, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance, and leverage knowledge distillation to mount the attack in the common black-box setting.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "loss-trajectory",
        "training-dynamics"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2208.14933"
    },
    {
      "paper_id": "2204.00032",
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.   Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8x more precise inference on all other users' otherwise-private data points.   Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Florian Tram\u00e8r",
        "R. Shokri",
        "Ayrton San Joaquin",
        "Hoang M. Le",
        "Matthew Jagielski",
        "Sanghyun Hong",
        "Nicholas Carlini"
      ],
      "author_details": [
        {
          "name": "Florian Tram\u00e8r",
          "h_index": 51,
          "citation_count": 33525,
          "affiliations": [
            "ETH Z\u00fcrich"
          ]
        },
        {
          "name": "R. Shokri",
          "h_index": 45,
          "citation_count": 16637,
          "affiliations": []
        },
        {
          "name": "Ayrton San Joaquin",
          "h_index": 3,
          "citation_count": 148,
          "affiliations": []
        },
        {
          "name": "Hoang M. Le",
          "h_index": 2,
          "citation_count": 159,
          "affiliations": []
        },
        {
          "name": "Matthew Jagielski",
          "h_index": 36,
          "citation_count": 11099,
          "affiliations": []
        },
        {
          "name": "Sanghyun Hong",
          "h_index": 14,
          "citation_count": 1372,
          "affiliations": []
        },
        {
          "name": "Nicholas Carlini",
          "h_index": 35,
          "citation_count": 10308,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2204.00032",
      "citation_count": 137,
      "influential_citation_count": 15,
      "reference_count": 81,
      "is_open_access": true,
      "publication_date": "2022-03-31",
      "tldr": "It is shown that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties, casting doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "privacy-poisoning",
        "membership-inference",
        "active-attack"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560554"
    },
    {
      "paper_id": "seed_bb7e186a",
      "title": "Analyzing Information Leakage of Updates to Natural Language Models",
      "abstract": "To continuously improve quality and reflect changes in data, machine learning\\napplications have to regularly retrain and update their core models. We show\\nthat a differential analysis of language model snapshots before and after an\\nupdate can reveal a surprising amount of detailed information about changes in\\nthe training data. We propose two new metrics---\\\\emph{differential score} and\\n\\\\emph{differential rank}---for analyzing the leakage due to updates of natural\\nlanguage models. We perform leakage analysis using these metrics across models\\ntrained on several different datasets using different methods and\\nconfigurations. We discuss the privacy implications of our findings, propose\\nmitigation strategies and evaluate their effect.\\n",
      "year": 2020,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Santiago Zanella-B\u00e9guelin",
        "Lukas Wutschitz",
        "Shruti Tople",
        "Victor R\u00fchle",
        "Andrew J. Paverd",
        "O. Ohrimenko",
        "Boris K\u00f6pf",
        "Marc Brockschmidt"
      ],
      "author_details": [
        {
          "name": "Santiago Zanella-B\u00e9guelin",
          "h_index": 5,
          "citation_count": 230,
          "affiliations": []
        },
        {
          "name": "Lukas Wutschitz",
          "h_index": 7,
          "citation_count": 576,
          "affiliations": []
        },
        {
          "name": "Shruti Tople",
          "h_index": 20,
          "citation_count": 2828,
          "affiliations": []
        },
        {
          "name": "Victor R\u00fchle",
          "h_index": 6,
          "citation_count": 382,
          "affiliations": []
        },
        {
          "name": "Andrew J. Paverd",
          "h_index": 21,
          "citation_count": 1989,
          "affiliations": []
        },
        {
          "name": "O. Ohrimenko",
          "h_index": 5,
          "citation_count": 499,
          "affiliations": []
        },
        {
          "name": "Boris K\u00f6pf",
          "h_index": 24,
          "citation_count": 2952,
          "affiliations": []
        },
        {
          "name": "Marc Brockschmidt",
          "h_index": 35,
          "citation_count": 11291,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://openalex.org/W3027379683",
      "pdf_url": "https://arxiv.org/pdf/1912.07942",
      "doi": "https://doi.org/10.1145/3372297.3417880",
      "citation_count": 135,
      "influential_citation_count": 5,
      "reference_count": 32,
      "is_open_access": true,
      "publication_date": "2019-12-17",
      "tldr": "It is shown that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "model-updates",
        "information-leakage"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/1912.07942"
    },
    {
      "paper_id": "2101.01341",
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "abstract": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information.   In this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa.   BlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
      "year": 2021,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Bo Hui",
        "Yuchen Yang",
        "Haolin Yuan",
        "P. Burlina",
        "N. Gong",
        "Yinzhi Cao"
      ],
      "author_details": [
        {
          "name": "Bo Hui",
          "h_index": 7,
          "citation_count": 501,
          "affiliations": []
        },
        {
          "name": "Yuchen Yang",
          "h_index": 11,
          "citation_count": 415,
          "affiliations": []
        },
        {
          "name": "Haolin Yuan",
          "h_index": 9,
          "citation_count": 583,
          "affiliations": []
        },
        {
          "name": "P. Burlina",
          "h_index": 32,
          "citation_count": 4798,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        },
        {
          "name": "Yinzhi Cao",
          "h_index": 27,
          "citation_count": 4673,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://arxiv.org/abs/2101.01341",
      "citation_count": 133,
      "influential_citation_count": 10,
      "reference_count": 56,
      "is_open_access": true,
      "publication_date": "2021-01-05",
      "tldr": "An MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison, which improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "blind-attack",
        "differential-comparison"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2021.24293"
    },
    {
      "paper_id": "seed_e8af6e93",
      "title": "Inference Attacks Against Graph Neural Networks",
      "abstract": "Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinlei He",
        "Rui Wen",
        "Yixin Wu",
        "M. Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Xinlei He",
          "h_index": 20,
          "citation_count": 1612,
          "affiliations": []
        },
        {
          "name": "Rui Wen",
          "h_index": 10,
          "citation_count": 804,
          "affiliations": []
        },
        {
          "name": "Yixin Wu",
          "h_index": 8,
          "citation_count": 337,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 16,
          "citation_count": 1497,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://openalex.org/W3126787694",
      "pdf_url": "https://arxiv.org/pdf/2102.05429",
      "doi": "https://doi.org/10.48550/arxiv.2102.05429",
      "citation_count": 116,
      "influential_citation_count": 12,
      "reference_count": 55,
      "is_open_access": false,
      "publication_date": "2021-02-10",
      "tldr": "This paper systematically defines the threat models and proposes three node-level membership inference attacks based on an adversary's background knowledge against graph neural networks, showing that GNNs are vulnerable to node- level membership inference even when the adversary has minimal background knowledge.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "GNN-inference",
        "membership-inference"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5570446a",
      "title": "Membership Inference Attacks Against Recommender Systems",
      "abstract": "Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Minxing Zhang",
        "Z. Ren",
        "Zihan Wang",
        "Pengjie Ren",
        "Zhumin Chen",
        "Pengfei Hu",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Minxing Zhang",
          "h_index": 5,
          "citation_count": 176,
          "affiliations": []
        },
        {
          "name": "Z. Ren",
          "h_index": 48,
          "citation_count": 7939,
          "affiliations": []
        },
        {
          "name": "Zihan Wang",
          "h_index": 9,
          "citation_count": 446,
          "affiliations": []
        },
        {
          "name": "Pengjie Ren",
          "h_index": 40,
          "citation_count": 6557,
          "affiliations": []
        },
        {
          "name": "Zhumin Chen",
          "h_index": 39,
          "citation_count": 6251,
          "affiliations": []
        },
        {
          "name": "Pengfei Hu",
          "h_index": 23,
          "citation_count": 3021,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 48,
      "url": "https://openalex.org/W3211930400",
      "doi": "https://doi.org/10.1145/3460120.3484770",
      "citation_count": 115,
      "influential_citation_count": 9,
      "reference_count": 55,
      "is_open_access": true,
      "publication_date": "2021-09-16",
      "tldr": "This paper makes the first attempt on quantifying the privacy leakage of recommender systems through the lens of membership inference, and proposes a novel method by representing users from relevant items.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "recommender-systems",
        "membership-inference"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2109.08045"
    },
    {
      "paper_id": "2108.11023",
      "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
      "abstract": "Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Wenjie Qu",
        "N. Gong"
      ],
      "author_details": [
        {
          "name": "Hongbin Liu",
          "h_index": 11,
          "citation_count": 494,
          "affiliations": []
        },
        {
          "name": "Jinyuan Jia",
          "h_index": 21,
          "citation_count": 3203,
          "affiliations": []
        },
        {
          "name": "Wenjie Qu",
          "h_index": 6,
          "citation_count": 379,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://arxiv.org/abs/2108.11023",
      "citation_count": 111,
      "influential_citation_count": 9,
      "reference_count": 51,
      "is_open_access": true,
      "publication_date": "2021-08-25",
      "tldr": "This work proposes EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning, which exploits the overfitting of the image encoder towards its training data.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "encoder-attack",
        "contrastive-learning"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484749"
    },
    {
      "paper_id": "1906.09679",
      "title": "The Value of Collaboration in Convex Machine Learning with Differential Privacy",
      "abstract": "In this paper, we apply machine learning to distributed private data owned by multiple data owners, entities with access to non-overlapping training datasets. We use noisy, differentially-private gradients to minimize the fitness cost of the machine learning model using stochastic gradient descent. We quantify the quality of the trained model, using the fitness cost, as a function of privacy budget and size of the distributed datasets to capture the trade-off between privacy and utility in machine learning. This way, we can predict the outcome of collaboration among privacy-aware data owners prior to executing potentially computationally-expensive machine learning algorithms. Particularly, we show that the difference between the fitness of the trained machine learning model using differentially-private gradient queries and the fitness of the trained machine model in the absence of any privacy concerns is inversely proportional to the size of the training datasets squared and the privacy budget squared. We successfully validate the performance prediction with the actual performance of the proposed privacy-aware learning algorithms, applied to: financial datasets for determining interest rates of loans using regression; and detecting credit card frauds using support vector machines.",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nan Wu",
        "Farhad Farokhi",
        "David B. Smith",
        "M. K\u00e2afar"
      ],
      "author_details": [
        {
          "name": "Nan Wu",
          "h_index": 5,
          "citation_count": 206,
          "affiliations": []
        },
        {
          "name": "Farhad Farokhi",
          "h_index": 18,
          "citation_count": 1169,
          "affiliations": []
        },
        {
          "name": "David B. Smith",
          "h_index": 7,
          "citation_count": 597,
          "affiliations": []
        },
        {
          "name": "M. K\u00e2afar",
          "h_index": 39,
          "citation_count": 5752,
          "affiliations": []
        }
      ],
      "max_h_index": 39,
      "url": "https://arxiv.org/abs/1906.09679",
      "citation_count": 109,
      "influential_citation_count": 11,
      "reference_count": 36,
      "is_open_access": true,
      "publication_date": "2019-06-24",
      "tldr": "This paper uses noisy, differentially-private gradients to minimize the fitness cost of the machine learning model using stochastic gradient descent, and quantifies the quality of the trained model, as a function of privacy budget and size of the distributed datasets to capture the trade-off between privacy and utility in machine learning.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "differential-privacy",
        "multi-party-ML"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152691.pdf"
    },
    {
      "paper_id": "seed_ae64a5dd",
      "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
      "abstract": "Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyu Tang",
        "Saeed Mahloujifar",
        "Liwei Song",
        "Virat Shejwalkar",
        "Milad Nasr",
        "Amir Houmansadr",
        "Prateek Mittal"
      ],
      "author_details": [
        {
          "name": "Xinyu Tang",
          "h_index": 14,
          "citation_count": 462,
          "affiliations": []
        },
        {
          "name": "Saeed Mahloujifar",
          "h_index": 14,
          "citation_count": 972,
          "affiliations": []
        },
        {
          "name": "Liwei Song",
          "h_index": 12,
          "citation_count": 1376,
          "affiliations": []
        },
        {
          "name": "Virat Shejwalkar",
          "h_index": 13,
          "citation_count": 3112,
          "affiliations": [
            "University of Massachusetts Amherst"
          ]
        },
        {
          "name": "Milad Nasr",
          "h_index": 31,
          "citation_count": 11780,
          "affiliations": []
        },
        {
          "name": "Amir Houmansadr",
          "h_index": 42,
          "citation_count": 7508,
          "affiliations": []
        },
        {
          "name": "Prateek Mittal",
          "h_index": 59,
          "citation_count": 21589,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W3205533264",
      "pdf_url": "https://arxiv.org/pdf/2110.08324",
      "doi": "https://doi.org/10.48550/arxiv.2110.08324",
      "citation_count": 107,
      "influential_citation_count": 19,
      "reference_count": 59,
      "is_open_access": false,
      "publication_date": "2021-10-15",
      "tldr": "This work proposes a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks and shows that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "self-distillation",
        "ensemble",
        "MI-defense"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2103.11109",
      "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation",
      "abstract": "Recent success of deep neural networks (DNNs) hinges on the availability of large-scale dataset; however, training on such dataset often poses privacy risks for sensitive training information. In this paper, we aim to explore the power of generative models and gradient sparsity, and propose a scalable privacy-preserving generative model DATALENS. Comparing with the standard PATE privacy-preserving framework which allows teachers to vote on one-dimensional predictions, voting on the high dimensional gradient vectors is challenging in terms of privacy preservation. As dimension reduction techniques are required, we need to navigate a delicate tradeoff space between (1) the improvement of privacy preservation and (2) the slowdown of SGD convergence. To tackle this, we take advantage of communication efficient learning and propose a novel noise compression and aggregation approach TOPAGG by combining top-k compression for dimension reduction with a corresponding noise injection mechanism. We theoretically prove that the DATALENS framework guarantees differential privacy for its generated data, and provide analysis on its convergence. To demonstrate the practical usage of DATALENS, we conduct extensive experiments on diverse datasets including MNIST, Fashion-MNIST, and high dimensional CelebA, and we show that, DATALENS significantly outperforms other baseline DP generative models. In addition, we adapt the proposed TOPAGG approach, which is one of the key building blocks in DATALENS, to DP SGD training, and show that it is able to achieve higher utility than the state-of-the-art DP SGD approach in most cases. Our code is publicly available at https://github.com/AI-secure/DataLens.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Boxin Wang",
        "Fan Wu",
        "Yunhui Long",
        "Luka Rimanic",
        "Ce Zhang",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Boxin Wang",
          "h_index": 8,
          "citation_count": 554,
          "affiliations": []
        },
        {
          "name": "Fan Wu",
          "h_index": 11,
          "citation_count": 355,
          "affiliations": []
        },
        {
          "name": "Yunhui Long",
          "h_index": 11,
          "citation_count": 1237,
          "affiliations": []
        },
        {
          "name": "Luka Rimanic",
          "h_index": 10,
          "citation_count": 401,
          "affiliations": []
        },
        {
          "name": "Ce Zhang",
          "h_index": 14,
          "citation_count": 692,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 69,
          "citation_count": 27430,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://arxiv.org/abs/2103.11109",
      "citation_count": 74,
      "influential_citation_count": 13,
      "reference_count": 65,
      "is_open_access": true,
      "publication_date": "2021-03-20",
      "tldr": "This paper proposes a scalable privacy-preserving generative model DataLens, which is able to generate synthetic data in a differentially private (DP) way given sensitive input data, and theoretically proves that the DataLens framework guarantees differential privacy for its generated data.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "gan"
      ],
      "tags": [
        "privacy-preserving",
        "gradient-compression"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484579"
    },
    {
      "paper_id": "seed_3132e6a6",
      "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
      "abstract": "Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well. In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinlei He",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Xinlei He",
          "h_index": 20,
          "citation_count": 1612,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 40,
      "url": "https://openalex.org/W3211574353",
      "doi": "https://doi.org/10.1145/3460120.3484571",
      "citation_count": 59,
      "influential_citation_count": 3,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2021-02-08",
      "tldr": "The first privacy analysis of contrastive learning through the lens of membership inference and attribute inference is performed, and it is shown that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "contrastive-learning",
        "privacy-risk"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2102.04140"
    },
    {
      "paper_id": "2212.10986",
      "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
      "abstract": "Deploying machine learning models in production may allow adversaries to infer sensitive information about training data. There is a vast literature analyzing different types of inference risks, ranging from membership inference to reconstruction attacks. Inspired by the success of games (i.e., probabilistic experiments) to study security properties in cryptography, some authors describe privacy inference risks in machine learning using a similar game-based style. However, adversary capabilities and goals are often stated in subtly different ways from one presentation to the other, which makes it hard to relate and compose results. In this paper, we present a game-based framework to systematize the body of knowledge on privacy inference risks in machine learning. We use this framework to (1) provide a unifying structure for definitions of inference risks, (2) formally establish known relations among definitions, and (3) to uncover hitherto unknown relations that would have been difficult to spot otherwise.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. Salem",
        "Giovanni Cherubin",
        "David Evans",
        "Boris Kopf",
        "Andrew J. Paverd",
        "Anshuman Suri",
        "Shruti Tople",
        "Santiago Zanella-B'eguelin"
      ],
      "author_details": [
        {
          "name": "A. Salem",
          "h_index": 17,
          "citation_count": 3182,
          "affiliations": []
        },
        {
          "name": "Giovanni Cherubin",
          "h_index": 14,
          "citation_count": 884,
          "affiliations": []
        },
        {
          "name": "David Evans",
          "h_index": 6,
          "citation_count": 255,
          "affiliations": []
        },
        {
          "name": "Boris Kopf",
          "h_index": 3,
          "citation_count": 120,
          "affiliations": []
        },
        {
          "name": "Andrew J. Paverd",
          "h_index": 21,
          "citation_count": 1989,
          "affiliations": []
        },
        {
          "name": "Anshuman Suri",
          "h_index": 11,
          "citation_count": 563,
          "affiliations": [
            "Northeastern University"
          ]
        },
        {
          "name": "Shruti Tople",
          "h_index": 20,
          "citation_count": 2828,
          "affiliations": []
        },
        {
          "name": "Santiago Zanella-B'eguelin",
          "h_index": 6,
          "citation_count": 471,
          "affiliations": []
        }
      ],
      "max_h_index": 21,
      "url": "https://arxiv.org/abs/2212.10986",
      "citation_count": 56,
      "influential_citation_count": 4,
      "reference_count": 89,
      "is_open_access": false,
      "publication_date": "2022-12-21",
      "tldr": "A game-based framework is presented to systematize the body of knowledge on privacy inference risks in machine learning to provide a unifying structure for definitions of inference risks, formally establish known relations among definitions, and to uncover hitherto unknown relations that would have been difficult to spot otherwise.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "survey",
      "domains": [],
      "model_types": [],
      "tags": [
        "SoK",
        "privacy-games",
        "inference-privacy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_171a82d9",
      "title": "Membership Inference Attacks and Defenses in Neural Network Pruning",
      "abstract": "Neural network pruning has been an essential technique to reduce the computation and memory requirements for using deep neural networks for resource-constrained devices. Most existing research focuses primarily on balancing the sparsity and accuracy of a pruned neural network by strategically removing insignificant parameters and retraining the pruned model. Such efforts on reusing training samples pose serious privacy risks due to increased memorization, which, however, has not been investigated yet. In this paper, we conduct the first analysis of privacy risks in neural network pruning. Specifically, we investigate the impacts of neural network pruning on training data privacy, i.e., membership inference attacks. We first explore the impact of neural network pruning on prediction divergence, where the pruning process disproportionately affects the pruned model's behavior for members and non-members. Meanwhile, the influence of divergence even varies among different classes in a fine-grained manner. Enlighten by such divergence, we proposed a self-attention membership inference attack against the pruned neural networks. Extensive experiments are conducted to rigorously evaluate the privacy impacts of different pruning approaches, sparsity levels, and adversary knowledge. The proposed attack shows the higher attack performance on the pruned models when compared with eight existing membership inference attacks. In addition, we propose a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance, whose effectiveness has been experimentally demonstrated to effectively mitigate the privacy risks while maintaining the sparsity and accuracy of the pruned models.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xiaoyong Yuan",
        "Lan Zhang"
      ],
      "author_details": [
        {
          "name": "Xiaoyong Yuan",
          "h_index": 14,
          "citation_count": 745,
          "affiliations": []
        },
        {
          "name": "Lan Zhang",
          "h_index": 12,
          "citation_count": 398,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://openalex.org/W4221154610",
      "pdf_url": "https://arxiv.org/pdf/2202.03335",
      "doi": "https://doi.org/10.48550/arxiv.2202.03335",
      "citation_count": 55,
      "influential_citation_count": 7,
      "reference_count": 62,
      "is_open_access": false,
      "publication_date": "2022-02-07",
      "tldr": "This paper investigates the impacts of neural network pruning on training data privacy, i.e., membership inference attacks, and proposes a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "pruning",
        "membership-inference"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "1909.03935",
      "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
      "abstract": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Dingfan Chen",
        "Ning Yu",
        "Yang Zhang",
        "Mario Fritz"
      ],
      "author_details": [
        {
          "name": "Dingfan Chen",
          "h_index": 12,
          "citation_count": 1479,
          "affiliations": []
        },
        {
          "name": "Ning Yu",
          "h_index": 23,
          "citation_count": 2582,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        },
        {
          "name": "Mario Fritz",
          "h_index": 69,
          "citation_count": 22326,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://arxiv.org/abs/1909.03935",
      "citation_count": 52,
      "influential_citation_count": 8,
      "reference_count": 91,
      "is_open_access": false,
      "publication_date": "2019-09-09",
      "tldr": "This paper presents the first taxonomy of membership inference attacks against GANs, which encompasses not only existing attacks but also the novel ones, and proposes the first generic attack model that can be instantiated in various settings according to adversary's knowledge about the victim model.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "generative"
      ],
      "model_types": [
        "gan"
      ],
      "tags": [
        "taxonomy",
        "GAN-privacy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2307.02106",
      "title": "SoK: Privacy-Preserving Data Synthesis",
      "abstract": "As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuzheng Hu",
        "Fan Wu",
        "Q. Li",
        "Yunhui Long",
        "Gonzalo Munilla Garrido",
        "Chang Ge",
        "Bolin Ding",
        "D. Forsyth",
        "Bo Li",
        "D. Song"
      ],
      "author_details": [
        {
          "name": "Yuzheng Hu",
          "h_index": 2,
          "citation_count": 62,
          "affiliations": []
        },
        {
          "name": "Fan Wu",
          "h_index": 10,
          "citation_count": 447,
          "affiliations": [
            "University of Illinois at Urbana-Champaign"
          ]
        },
        {
          "name": "Q. Li",
          "h_index": 15,
          "citation_count": 3443,
          "affiliations": []
        },
        {
          "name": "Yunhui Long",
          "h_index": 11,
          "citation_count": 1237,
          "affiliations": []
        },
        {
          "name": "Gonzalo Munilla Garrido",
          "h_index": 9,
          "citation_count": 347,
          "affiliations": []
        },
        {
          "name": "Chang Ge",
          "h_index": 6,
          "citation_count": 234,
          "affiliations": []
        },
        {
          "name": "Bolin Ding",
          "h_index": 54,
          "citation_count": 12223,
          "affiliations": []
        },
        {
          "name": "D. Forsyth",
          "h_index": 1,
          "citation_count": 50,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 11,
          "citation_count": 1089,
          "affiliations": []
        },
        {
          "name": "D. Song",
          "h_index": 136,
          "citation_count": 105349,
          "affiliations": []
        }
      ],
      "max_h_index": 136,
      "url": "https://arxiv.org/abs/2307.02106",
      "citation_count": 50,
      "influential_citation_count": 5,
      "reference_count": 254,
      "is_open_access": true,
      "publication_date": "2023-07-05",
      "tldr": "This paper puts forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods and concludes that DP-MERF is an all-purpose approach.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "survey",
      "domains": [],
      "model_types": [],
      "tags": [
        "SoK",
        "data-synthesis",
        "privacy-preserving"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2307.02106"
    },
    {
      "paper_id": "2208.11180",
      "title": "Auditing Membership Leakages of Multi-Exit Networks",
      "abstract": "Relying on the fact that not all inputs require the same amount of computation to yield a confident prediction, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing to obtain predictions at intermediate layers of the model and thus save computation time and/or energy. However, current various designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.   In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zheng Li",
        "Yiyong Liu",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Zheng Li",
          "h_index": 15,
          "citation_count": 1128,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        },
        {
          "name": "Yiyong Liu",
          "h_index": 3,
          "citation_count": 203,
          "affiliations": []
        },
        {
          "name": "Xinlei He",
          "h_index": 20,
          "citation_count": 1612,
          "affiliations": []
        },
        {
          "name": "Ning Yu",
          "h_index": 16,
          "citation_count": 1149,
          "affiliations": [
            "School of Information Science, Indiana University",
            "Leidos",
            "LivingSocial"
          ]
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 82,
          "citation_count": 35162,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://arxiv.org/abs/2208.11180",
      "citation_count": 46,
      "influential_citation_count": 3,
      "reference_count": 90,
      "is_open_access": true,
      "publication_date": "2022-08-23",
      "tldr": "This paper presents a defense mechanism called TimeGuard specifically for multi-exit networks and shows that TimeGuard mitigates the newly proposed attacks perfectly and clearly demonstrates that the corresponding risks are much more severe than shown by existing membership inference attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "multi-exit",
        "membership-leakage"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2208.11180"
    },
    {
      "paper_id": "2311.02324",
      "title": "Bounded and Unbiased Composite Differential Privacy",
      "abstract": "The objective of differential privacy (DP) is to protect privacy by producing an output distribution that is indistinguishable between any two neighboring databases. However, traditional differentially private mechanisms tend to produce unbounded outputs in order to achieve maximum disturbance range, which is not always in line with real-world applications. Existing solutions attempt to address this issue by employing post-processing or truncation techniques to restrict the output results, but at the cost of introducing bias issues. In this paper, we propose a novel differentially private mechanism which uses a composite probability density function to generate bounded and unbiased outputs for any numerical input data. The composition consists of an activation function and a base function, providing users with the flexibility to define the functions according to the DP constraints. We also develop an optimization algorithm that enables the iterative search for the optimal hyper-parameter setting without the need for repeated experiments, which prevents additional privacy overhead. Furthermore, we evaluate the utility of the proposed mechanism by assessing the variance of the composite probability density function and introducing two alternative metrics that are simpler to compute than variance estimation. Our extensive evaluation on three benchmark datasets demonstrates consistent and significant improvement over the traditional Laplace and Gaussian mechanisms. The proposed bounded and unbiased composite differentially private mechanism will underpin the broader DP arsenal and foster future privacy-preserving studies.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Kai Zhang",
        "Yanjun Zhang",
        "Ruoxi Sun",
        "Pei-Wei Tsai",
        "M. Hassan",
        "Xingliang Yuan",
        "Minhui Xue",
        "Jinjun Chen"
      ],
      "author_details": [
        {
          "name": "Kai Zhang",
          "h_index": 4,
          "citation_count": 109,
          "affiliations": []
        },
        {
          "name": "Yanjun Zhang",
          "h_index": 10,
          "citation_count": 283,
          "affiliations": []
        },
        {
          "name": "Ruoxi Sun",
          "h_index": 14,
          "citation_count": 836,
          "affiliations": []
        },
        {
          "name": "Pei-Wei Tsai",
          "h_index": 3,
          "citation_count": 92,
          "affiliations": []
        },
        {
          "name": "M. Hassan",
          "h_index": 10,
          "citation_count": 1164,
          "affiliations": []
        },
        {
          "name": "Xingliang Yuan",
          "h_index": 3,
          "citation_count": 57,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 6,
          "citation_count": 144,
          "affiliations": []
        },
        {
          "name": "Jinjun Chen",
          "h_index": 2,
          "citation_count": 46,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://arxiv.org/abs/2311.02324",
      "citation_count": 43,
      "influential_citation_count": 1,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2023-11-04",
      "tldr": "This paper proposes a novel differentially private mechanism which uses a composite probability density function to generate bounded and unbiased outputs for any numerical input data and develops an optimization algorithm that enables the iterative search for the optimal hyper-parameter setting without the need for repeated experiments.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "composite-DP",
        "bounded",
        "unbiased"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2311.02324"
    },
    {
      "paper_id": "2309.08230",
      "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services",
      "abstract": "The right to be forgotten requires the removal or \"unlearning\" of a user's data from machine learning models. However, in the context of Machine Learning as a Service (MLaaS), retraining a model from scratch to fulfill the unlearning request is impractical due to the lack of training data on the service provider's side (the server). Furthermore, approximate unlearning further embraces a complex trade-off between utility (model performance) and privacy (unlearning performance). In this paper, we try to explore the potential threats posed by unlearning services in MLaaS, specifically over-unlearning, where more information is unlearned than expected. We propose two strategies that leverage over-unlearning to measure the impact on the trade-off balancing, under black-box access settings, in which the existing machine unlearning attacks are not applicable. The effectiveness of these strategies is evaluated through extensive experiments on benchmark datasets, across various model architectures and representative unlearning approaches. Results indicate significant potential for both strategies to undermine model efficacy in unlearning scenarios. This study uncovers an underexplored gap between unlearning and contemporary MLaaS, highlighting the need for careful considerations in balancing data unlearning, model utility, and security.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Hongsheng Hu",
        "Shuo Wang",
        "Jiamin Chang",
        "Haonan Zhong",
        "Ruoxi Sun",
        "Shuang Hao",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "author_details": [
        {
          "name": "Hongsheng Hu",
          "h_index": 4,
          "citation_count": 108,
          "affiliations": []
        },
        {
          "name": "Shuo Wang",
          "h_index": 57,
          "citation_count": 13830,
          "affiliations": []
        },
        {
          "name": "Jiamin Chang",
          "h_index": 2,
          "citation_count": 97,
          "affiliations": []
        },
        {
          "name": "Haonan Zhong",
          "h_index": 1,
          "citation_count": 42,
          "affiliations": []
        },
        {
          "name": "Ruoxi Sun",
          "h_index": 14,
          "citation_count": 836,
          "affiliations": []
        },
        {
          "name": "Shuang Hao",
          "h_index": 2,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Haojin Zhu",
          "h_index": 5,
          "citation_count": 121,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 6,
          "citation_count": 144,
          "affiliations": []
        }
      ],
      "max_h_index": 57,
      "url": "https://arxiv.org/abs/2309.08230",
      "citation_count": 42,
      "influential_citation_count": 3,
      "reference_count": 59,
      "is_open_access": true,
      "publication_date": "2023-09-15",
      "tldr": "This study uncovers an underexplored gap between unlearning and contemporary MLaaS, highlighting the need for careful considerations in balancing data unlearning, model utility, and security.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "MLaaS-unlearning",
        "vulnerabilities"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2309.08230"
    },
    {
      "paper_id": "seed_794e5960",
      "title": "Federated Boosted Decision Trees with Differential Privacy",
      "abstract": "There is great demand for scalable, secure, and effcient privacypreserving machine learning models that can be trained over distributed data. While deep learning models typically achieve the best results in a centralized non-secure setting, different models can excel when privacy and communication constraints are imposed. Instead, tree-based approaches such as XGBoost have attracted much attention for their high performance and ease of use; in particular, they often achieve state-of-the-art results on tabular data. Consequently, several recent works have focused on translating Gradient Boosted Decision Tree (GBDT) models like XGBoost into federated settings, via cryptographic mechanisms such as Homomorphic Encryption (HE) and Secure Multi-Party Computation (MPC). However, these do not always provide formal privacy guarantees, or consider the full range of hyperparameters and implementation settings. In this work, we implement the GBDT model under Differential Privacy (DP). We propose a general framework that captures and extends existing approaches for differentially private decision trees. Our framework of methods is tailored to the federated setting, and we show that with a careful choice of techniques it is possible to achieve very high utility while maintaining strong levels of privacy.&#13;\\n&#13;\\n",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Samuel Maddock",
        "Graham Cormode",
        "Tianhao Wang",
        "C. Maple",
        "S. Jha"
      ],
      "author_details": [
        {
          "name": "Samuel Maddock",
          "h_index": 4,
          "citation_count": 146,
          "affiliations": []
        },
        {
          "name": "Graham Cormode",
          "h_index": 67,
          "citation_count": 25761,
          "affiliations": [
            "University of Oxford"
          ]
        },
        {
          "name": "Tianhao Wang",
          "h_index": 29,
          "citation_count": 3900,
          "affiliations": []
        },
        {
          "name": "C. Maple",
          "h_index": 14,
          "citation_count": 729,
          "affiliations": []
        },
        {
          "name": "S. Jha",
          "h_index": 86,
          "citation_count": 41377,
          "affiliations": []
        }
      ],
      "max_h_index": 86,
      "url": "https://openalex.org/W4308643126",
      "doi": "https://doi.org/10.1145/3548606.3560687",
      "citation_count": 42,
      "influential_citation_count": 4,
      "reference_count": 66,
      "is_open_access": true,
      "publication_date": "2022-10-06",
      "tldr": "This work implements the GBDT model under Differential Privacy (DP), and proposes a general framework that captures and extends existing approaches for differentially private decision trees, and shows that with a careful choice of techniques it is possible to achieve very high utility while maintaining strong levels of privacy.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "federated-learning"
      ],
      "model_types": [
        "tree"
      ],
      "tags": [
        "differential-privacy",
        "boosted-trees"
      ],
      "open_access_pdf": "http://wrap.warwick.ac.uk/169990/1/WRAP-federated-boosted-decision-trees-differential-privacy-2022.pdf"
    },
    {
      "paper_id": "2404.17399",
      "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
      "abstract": "Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Michael Aerni",
        "Jie Zhang",
        "Florian Tram\u00e8r"
      ],
      "author_details": [
        {
          "name": "Michael Aerni",
          "h_index": 5,
          "citation_count": 97,
          "affiliations": []
        },
        {
          "name": "Jie Zhang",
          "h_index": 7,
          "citation_count": 272,
          "affiliations": []
        },
        {
          "name": "Florian Tram\u00e8r",
          "h_index": 51,
          "citation_count": 33525,
          "affiliations": [
            "ETH Z\u00fcrich"
          ]
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2404.17399",
      "citation_count": 41,
      "influential_citation_count": 4,
      "reference_count": 68,
      "is_open_access": true,
      "publication_date": "2024-04-26",
      "tldr": "This work identifies severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions and shows that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "privacy-defense-evaluation",
        "misleading-metrics"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690194"
    },
    {
      "paper_id": "2307.01610",
      "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
      "abstract": "Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Zitao Chen",
        "K. Pattabiraman"
      ],
      "author_details": [
        {
          "name": "Zitao Chen",
          "h_index": 10,
          "citation_count": 476,
          "affiliations": []
        },
        {
          "name": "K. Pattabiraman",
          "h_index": 42,
          "citation_count": 5943,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://arxiv.org/abs/2307.01610",
      "citation_count": 32,
      "influential_citation_count": 7,
      "reference_count": 59,
      "is_open_access": true,
      "publication_date": "2023-07-04",
      "tldr": "HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy, which effectively obscures the differences between the prediction on members and non-members.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "overconfidence",
        "MI-defense",
        "calibration"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2307.01610"
    },
    {
      "paper_id": "2209.08615",
      "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
      "abstract": "Membership inference (MI) attacks highlight a privacy weakness in present stochastic training methods for neural networks. It is not well understood, however, why they arise. Are they a natural consequence of imperfect generalization only? Which underlying causes should we address during training to mitigate these attacks? Towards answering such questions, we propose the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning. We offer causal graphs that quantitatively explain the observed MI attack performance achieved for $6$ attack variants. We refute several prior non-quantitative hypotheses that over-simplify or over-estimate the influence of underlying causes, thereby failing to capture the complex interplay between several factors. Our causal models also show a new connection between generalization and MI attacks via their shared causal factors. Our causal models have high predictive power ($0.90$), i.e., their analytical predictions match with observations in unseen experiments often, which makes analysis via them a pragmatic alternative.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Teodora Baluta",
        "Shiqi Shen",
        "S. Hitarth",
        "Shruti Tople",
        "Prateek Saxena"
      ],
      "author_details": [
        {
          "name": "Teodora Baluta",
          "h_index": 8,
          "citation_count": 314,
          "affiliations": []
        },
        {
          "name": "Shiqi Shen",
          "h_index": 7,
          "citation_count": 787,
          "affiliations": []
        },
        {
          "name": "S. Hitarth",
          "h_index": 1,
          "citation_count": 29,
          "affiliations": []
        },
        {
          "name": "Shruti Tople",
          "h_index": 20,
          "citation_count": 2828,
          "affiliations": []
        },
        {
          "name": "Prateek Saxena",
          "h_index": 11,
          "citation_count": 346,
          "affiliations": []
        }
      ],
      "max_h_index": 20,
      "url": "https://arxiv.org/abs/2209.08615",
      "citation_count": 28,
      "influential_citation_count": 1,
      "reference_count": 72,
      "is_open_access": true,
      "publication_date": "2022-09-18",
      "tldr": "This work proposes the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning, and offers causal graphs that quantitatively explain the observed MI attack performance achieved for 6 attack variants.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "causal-analysis",
        "generalization"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560694"
    },
    {
      "paper_id": "2311.16136",
      "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
      "abstract": "Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ERASER framework for machinE unleaRning in MLaAS via an inferencE seRving-aware approach. ERASER strategically choose appropriate unlearning execution timing to address the inference service obsolescence issue. A novel inference consistency certification mechanism is proposed to avoid the violation of RTBF principle caused by postponed unlearning executions, thereby mitigating the undesirable exposure vulnerability. ERASER offers three groups of design choices to allow for tailor-made variants that best suit the specific environments and preferences of various MLaaS systems. Extensive empirical evaluations across various settings confirm ERASER's effectiveness, e.g., it can effectively save up to 99% of inference latency and 31% of computation overhead over the inference-oblivion baseline.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yuke Hu",
        "Jian Lou",
        "Jiaqi Liu",
        "Wangze Ni",
        "Feng Lin",
        "Zhan Qin",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Yuke Hu",
          "h_index": 6,
          "citation_count": 150,
          "affiliations": []
        },
        {
          "name": "Jian Lou",
          "h_index": 8,
          "citation_count": 233,
          "affiliations": []
        },
        {
          "name": "Jiaqi Liu",
          "h_index": 2,
          "citation_count": 55,
          "affiliations": []
        },
        {
          "name": "Wangze Ni",
          "h_index": 3,
          "citation_count": 33,
          "affiliations": []
        },
        {
          "name": "Feng Lin",
          "h_index": 3,
          "citation_count": 37,
          "affiliations": []
        },
        {
          "name": "Zhan Qin",
          "h_index": 5,
          "citation_count": 164,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 19,
          "citation_count": 1201,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://arxiv.org/abs/2311.16136",
      "citation_count": 26,
      "influential_citation_count": 2,
      "reference_count": 78,
      "is_open_access": true,
      "publication_date": "2023-11-03",
      "tldr": "ERASER strategically choose appropriate unlearning execution timing to address the inference service obsolescence issue, and a novel inference consistency certification mechanism is proposed to avoid the violation of RTBF principle caused by postponed unlearning executions, thereby mitigating the undesirable exposure vulnerability.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "MLaaS-unlearning",
        "inference-serving"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670398"
    },
    {
      "paper_id": "2304.00129",
      "title": "Scalable and Privacy-Preserving Federated Principal Component Analysis",
      "abstract": "Principal component analysis (PCA) is an essential algorithm for dimensionality reduction in many data science domains. We address the problem of performing a federated PCA on private data distributed among multiple data providers while ensuring data confidentiality. Our solution, SF-PCA, is an end-to-end secure system that preserves the confidentiality of both the original data and all intermediate results in a passive-adversary model with up to all-but-one colluding parties. SF-PCA jointly leverages multiparty homomorphic encryption, interactive protocols, and edge computing to efficiently interleave computations on local cleartext data with operations on collectively encrypted data. SF-PCA obtains results as accurate as non-secure centralized solutions, independently of the data distribution among the parties. It scales linearly or better with the dataset dimensions and with the number of data providers. SF-PCA is more precise than existing approaches that approximate the solution by combining local analysis results, and between 3x and 250x faster than privacy-preserving alternatives based solely on secure multiparty computation or homomorphic encryption. Our work demonstrates the practical applicability of secure and federated PCA on private distributed datasets.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "David Froelicher",
        "Hyunghoon Cho",
        "Manaswitha Edupalli",
        "Jo\u00e3o S\u00e1 Sousa",
        "Jean-Philippe Bossuat",
        "Apostolos Pyrgelis",
        "J. Troncoso-Pastoriza",
        "Bonnie Berger",
        "Jean-Pierre Hubaux"
      ],
      "author_details": [
        {
          "name": "David Froelicher",
          "h_index": 10,
          "citation_count": 706,
          "affiliations": []
        },
        {
          "name": "Hyunghoon Cho",
          "h_index": 20,
          "citation_count": 2061,
          "affiliations": []
        },
        {
          "name": "Manaswitha Edupalli",
          "h_index": 3,
          "citation_count": 47,
          "affiliations": []
        },
        {
          "name": "Jo\u00e3o S\u00e1 Sousa",
          "h_index": 9,
          "citation_count": 695,
          "affiliations": []
        },
        {
          "name": "Jean-Philippe Bossuat",
          "h_index": 10,
          "citation_count": 950,
          "affiliations": []
        },
        {
          "name": "Apostolos Pyrgelis",
          "h_index": 7,
          "citation_count": 150,
          "affiliations": []
        },
        {
          "name": "J. Troncoso-Pastoriza",
          "h_index": 28,
          "citation_count": 2928,
          "affiliations": []
        },
        {
          "name": "Bonnie Berger",
          "h_index": 4,
          "citation_count": 84,
          "affiliations": []
        },
        {
          "name": "Jean-Pierre Hubaux",
          "h_index": 19,
          "citation_count": 3059,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://arxiv.org/abs/2304.00129",
      "citation_count": 22,
      "influential_citation_count": 3,
      "reference_count": 111,
      "is_open_access": true,
      "publication_date": "2023-03-31",
      "tldr": "The solution, SF-PCA, is an end-to-end secure system that preserves the confidentiality of both the original data and all intermediate results in a passive-adversary model with up to all-but-one colluding parties.",
      "fields_of_study": [
        "Computer Science",
        "Medicine"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "federated-PCA",
        "privacy-preserving"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2304.00129"
    },
    {
      "paper_id": "seed_cb5663e4",
      "title": "Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs",
      "abstract": "Vertical Federated Learning (VFL) is a privacy-preserving distributed learning paradigm where different parties collaboratively learn models using partitioned features of shared samples, without leaking private data. Recent research has shown promising results addressing various challenges in VFL, highlighting its potential for practical applications in cross-domain collaboration. However, the corresponding research is scattered and lacks organization. To advance VFL research, this survey offers a systematic overview of recent developments. First, we provide a history and background introduction, along with a summary of the general training protocol of VFL. We then revisit the taxonomy in recent reviews and analyze limitations in-depth. For a comprehensive and structured discussion, we synthesize recent research from three fundamental perspectives: effectiveness, security, and applicability. Finally, we discuss several critical future research directions in VFL, which will facilitate the developments in this field. We provide a collection of research lists and periodically update them at https://github.com/shentt67/VFL_Survey.",
      "year": 2024,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Mang Ye",
        "Wei Shen",
        "Bo Du",
        "E. Snezhko",
        "Vassili Kovalev",
        "PongChi Yuen"
      ],
      "author_details": [
        {
          "name": "Mang Ye",
          "h_index": 12,
          "citation_count": 602,
          "affiliations": []
        },
        {
          "name": "Wei Shen",
          "h_index": 4,
          "citation_count": 83,
          "affiliations": []
        },
        {
          "name": "Bo Du",
          "h_index": 12,
          "citation_count": 636,
          "affiliations": []
        },
        {
          "name": "E. Snezhko",
          "h_index": 5,
          "citation_count": 344,
          "affiliations": []
        },
        {
          "name": "Vassili Kovalev",
          "h_index": 7,
          "citation_count": 129,
          "affiliations": []
        },
        {
          "name": "PongChi Yuen",
          "h_index": 13,
          "citation_count": 1697,
          "affiliations": []
        }
      ],
      "max_h_index": 13,
      "url": "https://openalex.org/W4399151237",
      "pdf_url": "https://arxiv.org/pdf/2405.17495",
      "doi": "https://doi.org/10.48550/arxiv.2405.17495",
      "citation_count": 22,
      "influential_citation_count": 1,
      "reference_count": 217,
      "is_open_access": true,
      "publication_date": "2024-05-25",
      "tldr": "This survey provides a history and background introduction of VFL, along with a summary of the general training protocol, and synthesizes recent research from three fundamental perspectives: effectiveness, security, and applicability.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "differential-privacy",
        "zeroth-order",
        "fine-tuning"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2405.17495"
    },
    {
      "paper_id": "2208.08662",
      "title": "Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy",
      "abstract": "Secure multi-party computation-based machine learning, referred to as MPL, has become an important technology to utilize data from multiple parties with privacy preservation. While MPL provides rigorous security guarantees for the computation process, the models trained by MPL are still vulnerable to attacks that solely depend on access to the models. Differential privacy could help to defend against such attacks. However, the accuracy loss brought by differential privacy and the huge communication overhead of secure multi-party computation protocols make it highly challenging to balance the 3-way trade-off between privacy, efficiency, and accuracy.   In this paper, we are motivated to resolve the above issue by proposing a solution, referred to as PEA (Private, Efficient, Accurate), which consists of a secure DPSGD protocol and two optimization methods. First, we propose a secure DPSGD protocol to enforce DPSGD in secret sharing-based MPL frameworks. Second, to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, we propose two optimization methods for the training process of MPL: (1) the data-independent feature extraction method, which aims to simplify the trained model structure; (2) the local data-based global model initialization method, which aims to speed up the convergence of the model training. We implement PEA in two open-source MPL frameworks: TF-Encrypted and Queqiao. The experimental results on various datasets demonstrate the efficiency and effectiveness of PEA. E.g. when $\u03b5$ = 2, we can train a differentially private classification model with an accuracy of 88% for CIFAR-10 within 7 minutes under the LAN setting. This result significantly outperforms the one from CryptGPU, one SOTA MPL framework: it costs more than 16 hours to train a non-private deep neural network model on CIFAR-10 with the same accuracy.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Wenqiang Ruan",
        "Ming Xu",
        "Wenjing Fang",
        "Li Wang",
        "Lei Wang",
        "Wei Han"
      ],
      "author_details": [
        {
          "name": "Wenqiang Ruan",
          "h_index": 9,
          "citation_count": 212,
          "affiliations": []
        },
        {
          "name": "Ming Xu",
          "h_index": 8,
          "citation_count": 206,
          "affiliations": []
        },
        {
          "name": "Wenjing Fang",
          "h_index": 10,
          "citation_count": 381,
          "affiliations": []
        },
        {
          "name": "Li Wang",
          "h_index": 5,
          "citation_count": 263,
          "affiliations": []
        },
        {
          "name": "Lei Wang",
          "h_index": 7,
          "citation_count": 373,
          "affiliations": []
        },
        {
          "name": "Wei Han",
          "h_index": 42,
          "citation_count": 7810,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://arxiv.org/abs/2208.08662",
      "citation_count": 22,
      "influential_citation_count": 5,
      "reference_count": 92,
      "is_open_access": true,
      "publication_date": "2022-08-18",
      "tldr": "A solution to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, referred to as PEA (Private, Efficient, Accurate), which consists of a secure differentially private stochastic gradient descent (DPSGD for short) protocol and two optimization methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "MPL",
        "differential-privacy"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2208.08662"
    },
    {
      "paper_id": "seed_cbc0385f",
      "title": "Membership Inference Attacks Against Vision-Language Models",
      "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\u00e9nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhan Li",
        "Yongtao Wu",
        "Yihang Chen",
        "Francesco Tonin",
        "El\u00edas Abad-Rocamora",
        "V. Cevher"
      ],
      "author_details": [
        {
          "name": "Zhan Li",
          "h_index": 1,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Yongtao Wu",
          "h_index": 7,
          "citation_count": 116,
          "affiliations": []
        },
        {
          "name": "Yihang Chen",
          "h_index": 3,
          "citation_count": 29,
          "affiliations": []
        },
        {
          "name": "Francesco Tonin",
          "h_index": 2,
          "citation_count": 36,
          "affiliations": []
        },
        {
          "name": "El\u00edas Abad-Rocamora",
          "h_index": 4,
          "citation_count": 45,
          "affiliations": []
        },
        {
          "name": "V. Cevher",
          "h_index": 60,
          "citation_count": 15585,
          "affiliations": []
        }
      ],
      "max_h_index": 60,
      "url": "https://openalex.org/W4404395433",
      "pdf_url": "https://arxiv.org/pdf/2411.02902",
      "doi": "https://doi.org/10.48550/arxiv.2411.02902",
      "citation_count": 21,
      "influential_citation_count": 5,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2024-11-05",
      "tldr": "This study introduces the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection and proposes a novel MIA pipeline specifically designed for token-level image detection.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "VLM",
        "membership-inference"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2107.13190",
      "title": "TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing",
      "abstract": "Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator.   Our experimental evaluations on TableGAN-MCA have five main findings. First, TableGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Aoting Hu",
        "Renjie Xie",
        "Zhigang Lu",
        "Aiqun Hu",
        "Minhui Xue"
      ],
      "author_details": [
        {
          "name": "Aoting Hu",
          "h_index": 3,
          "citation_count": 37,
          "affiliations": []
        },
        {
          "name": "Renjie Xie",
          "h_index": 11,
          "citation_count": 475,
          "affiliations": []
        },
        {
          "name": "Zhigang Lu",
          "h_index": 5,
          "citation_count": 86,
          "affiliations": []
        },
        {
          "name": "Aiqun Hu",
          "h_index": 3,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 32,
          "citation_count": 4765,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://arxiv.org/abs/2107.13190",
      "citation_count": 18,
      "influential_citation_count": 0,
      "reference_count": 52,
      "is_open_access": true,
      "publication_date": "2021-07-28",
      "tldr": "This paper proposes a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "tabular",
        "generative"
      ],
      "model_types": [
        "gan"
      ],
      "tags": [
        "membership-collision",
        "tabular-GAN"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2107.13190"
    },
    {
      "paper_id": "seed_dba7327d",
      "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
      "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\\ie, \\textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \\textbf{PETAL}: a label-only membership inference attack based on \\textbf{PE}r-\\textbf{T}oken sem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yu He",
        "Boheng Li",
        "Liu Liu",
        "Zhongjie Ba",
        "Wei Dong",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren",
        "Chun Chen"
      ],
      "author_details": [
        {
          "name": "Yu He",
          "h_index": 2,
          "citation_count": 38,
          "affiliations": []
        },
        {
          "name": "Boheng Li",
          "h_index": 5,
          "citation_count": 95,
          "affiliations": []
        },
        {
          "name": "Liu Liu",
          "h_index": 4,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Zhongjie Ba",
          "h_index": 18,
          "citation_count": 1419,
          "affiliations": []
        },
        {
          "name": "Wei Dong",
          "h_index": 4,
          "citation_count": 60,
          "affiliations": []
        },
        {
          "name": "Yiming Li",
          "h_index": 7,
          "citation_count": 182,
          "affiliations": []
        },
        {
          "name": "Zhan Qin",
          "h_index": 6,
          "citation_count": 153,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 19,
          "citation_count": 1201,
          "affiliations": []
        },
        {
          "name": "Chun Chen",
          "h_index": 4,
          "citation_count": 60,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://openalex.org/W4416043568",
      "pdf_url": "https://arxiv.org/pdf/2502.18943",
      "doi": "https://doi.org/10.48550/arxiv.2502.18943",
      "citation_count": 18,
      "influential_citation_count": 1,
      "reference_count": 74,
      "is_open_access": false,
      "publication_date": "2025-02-26",
      "tldr": "A label-only membership inference attack based on token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity, which performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "label-only",
        "LLM-privacy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_88d094a2",
      "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data",
      "abstract": "Large language models (LLMs) have demonstrated significant success in various domain-specific tasks, with their performance often improving substantially after fine-tuning. However, fine-tuning with real-world data introduces privacy risks. To mitigate these risks, developers increasingly rely on synthetic data generation as an alternative to using real data, as data generated by traditional models is believed to be different from real-world data. However, with the advanced capabilities of LLMs, the distinction between real data and data generated by these models has become nearly indistinguishable. This convergence introduces similar privacy risks for generated data to those associated with real data. Our study investigates whether fine-tuning with LLM-generated data truly enhances privacy or introduces additional privacy risks by examining the structural characteristics of data generated by LLMs, focusing on two primary fine-tuning approaches: supervised fine-tuning (SFT) with unstructured (plain-text) generated data and self-instruct tuning. In the scenario of SFT, the data is put into a particular instruction tuning format used by previous studies. We use Personal Information Identifier (PII) leakage and Membership Inference Attacks (MIAs) on the Pythia Model Suite and Open Pre-trained Transformer (OPT) to measure privacy risks. Notably, after fine-tuning with unstructured generated data, the rate of successful PII extractions for Pythia increased by over 20%, highlighting the potential privacy implications of such approaches. Furthermore, the ROC-AUC score of MIAs for Pythia-6.9b, the second biggest model of the suite, increases over 40% after self-instruct tuning. Our results indicate the potential privacy risks associated with fine-tuning LLMs using generated data, underscoring the need for careful consideration of privacy safeguards in such approaches.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Atilla Akkus",
        "Mingjie Li",
        "Junjie Chu",
        "Michael Backes",
        "Yang Zhang",
        "Sinem Sav"
      ],
      "author_details": [
        {
          "name": "Atilla Akkus",
          "h_index": 1,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Mingjie Li",
          "h_index": 4,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Junjie Chu",
          "h_index": 4,
          "citation_count": 172,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 16,
          "citation_count": 877,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 3,
          "citation_count": 28,
          "affiliations": []
        },
        {
          "name": "Sinem Sav",
          "h_index": 7,
          "citation_count": 345,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://openalex.org/W4403709727",
      "pdf_url": "https://arxiv.org/pdf/2409.11423",
      "doi": "https://doi.org/10.48550/arxiv.2409.11423",
      "citation_count": 14,
      "influential_citation_count": 0,
      "reference_count": 57,
      "is_open_access": false,
      "publication_date": "2024-09-12",
      "tldr": "This study investigates whether fine-tuning with LLM-generated data truly enhances privacy or introduces additional privacy risks by examining the structural characteristics of data generated by LLMs, focusing on two primary fine-tuning approaches: supervised fine-tuning with unstructured generated data and self-instruct tuning.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "synthetic-data",
        "privacy-leakage",
        "fine-tuning"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2309.07983",
      "title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems",
      "abstract": "Membership inference attacks allow adversaries to determine whether a particular example was contained in the model's training dataset. While previous works have confirmed the feasibility of such attacks in various applications, none has focused on speaker recognition (SR), a promising voice-based biometric recognition technique. In this work, we propose SLMIA-SR, the first membership inference attack tailored to SR. In contrast to conventional example-level attack, our attack features speaker-level membership inference, i.e., determining if any voices of a given speaker, either the same as or different from the given inference voices, have been involved in the training of a model. It is particularly useful and practical since the training and inference voices are usually distinct, and it is also meaningful considering the open-set nature of SR, namely, the recognition speakers were often not present in the training data. We utilize intra-similarity and inter-dissimilarity, two training objectives of SR, to characterize the differences between training and non-training speakers and quantify them with two groups of features driven by carefully-established feature engineering to mount the attack. To improve the generalizability of our attack, we propose a novel mixing ratio training strategy to train attack models. To enhance the attack performance, we introduce voice chunk splitting to cope with the limited number of inference voices and propose to train attack models dependent on the number of inference voices. Our attack is versatile and can work in both white-box and black-box scenarios. Additionally, we propose two novel techniques to reduce the number of black-box queries while maintaining the attack performance. Extensive experiments demonstrate the effectiveness of SLMIA-SR.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Guangke Chen",
        "Yedi Zhang",
        "Fu Song"
      ],
      "author_details": [
        {
          "name": "Guangke Chen",
          "h_index": 10,
          "citation_count": 460,
          "affiliations": [
            "ShanghaiTech University"
          ]
        },
        {
          "name": "Yedi Zhang",
          "h_index": 8,
          "citation_count": 144,
          "affiliations": []
        },
        {
          "name": "Fu Song",
          "h_index": 16,
          "citation_count": 965,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://arxiv.org/abs/2309.07983",
      "citation_count": 13,
      "influential_citation_count": 1,
      "reference_count": 64,
      "is_open_access": true,
      "publication_date": "2023-09-14",
      "tldr": "This work proposes SLMIA-SR, the first membership inference attack tailored to speaker recognition, and utilizes intra-similarity and inter-dissimilarity, two training objectives of SR, to characterize the differences between training and non-training speakers and quantify them with two groups of features driven by carefully-established feature engineering to mount the attack.",
      "fields_of_study": [
        "Computer Science",
        "Engineering"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [],
      "tags": [
        "speaker-recognition",
        "speaker-level"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.241323"
    },
    {
      "paper_id": "2309.03081",
      "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
      "abstract": "Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "L. Du",
        "Min Chen",
        "Mingyang Sun",
        "Shouling Ji",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "author_details": [
        {
          "name": "L. Du",
          "h_index": 8,
          "citation_count": 252,
          "affiliations": []
        },
        {
          "name": "Min Chen",
          "h_index": 3,
          "citation_count": 40,
          "affiliations": []
        },
        {
          "name": "Mingyang Sun",
          "h_index": 2,
          "citation_count": 105,
          "affiliations": []
        },
        {
          "name": "Shouling Ji",
          "h_index": 3,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Peng Cheng",
          "h_index": 7,
          "citation_count": 204,
          "affiliations": []
        },
        {
          "name": "Jiming Chen",
          "h_index": 4,
          "citation_count": 136,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 4,
          "citation_count": 47,
          "affiliations": []
        }
      ],
      "max_h_index": 8,
      "url": "https://arxiv.org/abs/2309.03081",
      "citation_count": 12,
      "influential_citation_count": 1,
      "reference_count": 73,
      "is_open_access": true,
      "publication_date": "2023-09-06",
      "tldr": "This paper proposes ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios, and advocates a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "dataset-auditing",
        "offline-DRL"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.23184"
    },
    {
      "paper_id": "seed_0c4e3f3c",
      "title": "Locally Differentially Private Frequency Estimation Based on Convolution Framework",
      "abstract": "Local differential privacy (LDP) collects user data while protecting user privacy and eliminating the need for a trusted data collector. Several LDP protocols have been proposed and deployed in real-world applications. Frequency estimation is a fundamental task in the LDP protocols, which enables more advanced tasks in data analytics. However, the existing LDP protocols amplify the added noise in estimating the frequencies and therefore do not achieve optimal performance in accuracy. This paper introduces a convolution framework to analyze and optimize the estimated frequencies of LDP protocols. The convolution framework can equivalently transform the original frequency estimation problem into a deconvolution problem with noise. We thus add the Wiener filter-based deconvolution algorithms to LDP protocols to estimate the frequency while suppressing the added noise. Experimental results on different real-world datasets demonstrate that our proposed algorithms can lead to significantly better accuracy for state-of-the-art LDP protocols by orders of magnitude for the smooth dataset. And these algorithms also work on non-smooth datasets, but only to a limited extent. Our code is available at https://github.com/SEUNICK/LDP.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Huiyu Fang",
        "Liquan Chen",
        "Yali Liu",
        "Yuan Gao"
      ],
      "author_details": [
        {
          "name": "Huiyu Fang",
          "h_index": 4,
          "citation_count": 62,
          "affiliations": []
        },
        {
          "name": "Liquan Chen",
          "h_index": 14,
          "citation_count": 632,
          "affiliations": []
        },
        {
          "name": "Yali Liu",
          "h_index": 5,
          "citation_count": 90,
          "affiliations": []
        },
        {
          "name": "Yuan Gao",
          "h_index": 6,
          "citation_count": 84,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://openalex.org/W4385679702",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179389",
      "citation_count": 10,
      "influential_citation_count": 2,
      "reference_count": 35,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "LDP",
        "frequency-estimation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2209.01688",
      "title": "On the Privacy Risks of Cell-Based NAS Architectures",
      "abstract": "Existing studies on neural architecture search (NAS) mainly focus on efficiently and effectively searching for network architectures with better performance. Little progress has been made to systematically understand if the NAS-searched architectures are robust to privacy attacks while abundant work has already shown that human-designed architectures are prone to privacy attacks. In this paper, we fill this gap and systematically measure the privacy risks of NAS architectures. Leveraging the insights from our measurement study, we further explore the cell patterns of cell-based NAS architectures and evaluate how the cell patterns affect the privacy risks of NAS-searched architectures. Through extensive experiments, we shed light on how to design robust NAS architectures against privacy attacks, and also offer a general methodology to understand the hidden correlation between the NAS-searched architectures and other privacy risks.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Haiping Huang",
        "Zhikun Zhang",
        "Yun Shen",
        "M. Backes",
        "Qi Li",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Haiping Huang",
          "h_index": 8,
          "citation_count": 275,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 16,
          "citation_count": 1497,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Qi Li",
          "h_index": 10,
          "citation_count": 400,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 82,
          "citation_count": 35162,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://arxiv.org/abs/2209.01688",
      "citation_count": 8,
      "influential_citation_count": 0,
      "reference_count": 90,
      "is_open_access": true,
      "publication_date": "2022-09-04",
      "tldr": "Through extensive experiments, this paper sheds light on how to design robust NAS architectures against privacy attacks, and also offers a general methodology to understand the hidden correlation between the NAS-searched architectures and other privacy risks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "NAS",
        "privacy-risk"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2209.01688"
    },
    {
      "paper_id": "seed_1e6f5476",
      "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
      "abstract": "Large language models (LLMs) have achieved remarkable success and are widely adopted for diverse applications. However, fine-tuning these models often involves private or sensitive information, raising critical privacy concerns. In this work, we conduct the first comprehensive study evaluating the vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our empirical analysis demonstrates that MIAs exploit the loss reduction during fine-tuning, making them highly effective in revealing membership information. These findings motivate the development of our defense. We propose SOFT (\\textbf{S}elective data \\textbf{O}bfuscation in LLM \\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates privacy leakage by leveraging influential data selection with an adjustable parameter to balance utility preservation and privacy protection. Our extensive experiments span six diverse domains and multiple LLM architectures and scales. Results show that SOFT effectively reduces privacy risks while maintaining competitive model performance, offering a practical and scalable solution to safeguard sensitive information in fine-tuned LLMs.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Kaiyuan Zhang",
        "Siyuan Cheng",
        "Hanxi Guo",
        "Yuetian Chen",
        "Zian Su",
        "Shengwei An",
        "Yuntao Du",
        "Charles Fleming",
        "Ashish Kundu",
        "Xiangyu Zhang",
        "Ninghui Li"
      ],
      "author_details": [
        {
          "name": "Kaiyuan Zhang",
          "h_index": 11,
          "citation_count": 477,
          "affiliations": []
        },
        {
          "name": "Siyuan Cheng",
          "h_index": 21,
          "citation_count": 1806,
          "affiliations": []
        },
        {
          "name": "Hanxi Guo",
          "h_index": 5,
          "citation_count": 138,
          "affiliations": []
        },
        {
          "name": "Yuetian Chen",
          "h_index": 2,
          "citation_count": 10,
          "affiliations": []
        },
        {
          "name": "Zian Su",
          "h_index": 10,
          "citation_count": 216,
          "affiliations": []
        },
        {
          "name": "Shengwei An",
          "h_index": 17,
          "citation_count": 1032,
          "affiliations": []
        },
        {
          "name": "Yuntao Du",
          "h_index": 2,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Charles Fleming",
          "h_index": 3,
          "citation_count": 20,
          "affiliations": []
        },
        {
          "name": "Ashish Kundu",
          "h_index": 5,
          "citation_count": 54,
          "affiliations": []
        },
        {
          "name": "Xiangyu Zhang",
          "h_index": 3,
          "citation_count": 28,
          "affiliations": []
        },
        {
          "name": "Ninghui Li",
          "h_index": 2,
          "citation_count": 20,
          "affiliations": []
        }
      ],
      "max_h_index": 21,
      "url": "https://openalex.org/W4417351543",
      "pdf_url": "https://arxiv.org/pdf/2506.10424",
      "doi": "https://doi.org/10.48550/arxiv.2506.10424",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 99,
      "is_open_access": false,
      "publication_date": "2025-06-12",
      "tldr": "SOFT is proposed, a novel defense technique that mitigates privacy leakage by leveraging influential data selection with an adjustable parameter to balance utility preservation and privacy protection in fine-tuned LLMs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "data-obfuscation",
        "fine-tuning-privacy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2308.10422",
      "title": "Split Unlearning",
      "abstract": "We introduce Split Unlearning, a novel machine unlearning technology designed for Split Learning (SL), enabling the first-ever implementation of Sharded, Isolated, Sliced, and Aggregated (SISA) unlearning in SL frameworks. Particularly, the tight coupling between clients and the server in existing SL frameworks results in frequent bidirectional data flows and iterative training across all clients, violating the \"Isolated\" principle and making them struggle to implement SISA for independent and efficient unlearning. To address this, we propose SplitWiper with a new one-way-one-off propagation scheme, which leverages the inherently \"Sharded\" structure of SL and decouples neural signal propagation between clients and the server, enabling effective SISA unlearning even in scenarios with absent clients. We further design SplitWiper+ to enhance client label privacy, which integrates differential privacy and label expansion strategy to defend the privacy of client labels against the server and other potential adversaries. Experiments across diverse data distributions and tasks demonstrate that SplitWiper achieves 0% accuracy for unlearned labels, and 8% better accuracy for retained labels than non-SISA unlearning in SL. Moreover, the one-way-one-off propagation maintains constant overhead, reducing computational and communication costs by 99%. SplitWiper+ preserves 90% of label privacy when sharing masked labels with the server.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yanna Jiang",
        "Guangsheng Yu",
        "Qin Wang",
        "Xu Wang",
        "Baihe Ma",
        "Caijun Sun",
        "Wei Ni",
        "Renping Liu"
      ],
      "author_details": [
        {
          "name": "Yanna Jiang",
          "h_index": 4,
          "citation_count": 77,
          "affiliations": []
        },
        {
          "name": "Guangsheng Yu",
          "h_index": 17,
          "citation_count": 1208,
          "affiliations": []
        },
        {
          "name": "Qin Wang",
          "h_index": 6,
          "citation_count": 115,
          "affiliations": []
        },
        {
          "name": "Xu Wang",
          "h_index": 14,
          "citation_count": 763,
          "affiliations": []
        },
        {
          "name": "Baihe Ma",
          "h_index": 6,
          "citation_count": 137,
          "affiliations": [
            "Global Big Data Technologies center, University of Technology Sydney"
          ]
        },
        {
          "name": "Caijun Sun",
          "h_index": 9,
          "citation_count": 225,
          "affiliations": []
        },
        {
          "name": "Wei Ni",
          "h_index": 5,
          "citation_count": 95,
          "affiliations": []
        },
        {
          "name": "Renping Liu",
          "h_index": 6,
          "citation_count": 149,
          "affiliations": []
        }
      ],
      "max_h_index": 17,
      "url": "https://arxiv.org/abs/2308.10422",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 66,
      "is_open_access": false,
      "publication_date": "2023-08-21",
      "tldr": "This work proposes SplitWiper with a new one-way-one-off propagation scheme, which leverages the inherently ''Sharded'' structure of SL and decouples neural signal propagation between clients and the server, enabling effective SISA unlearning even in scenarios with absent clients.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "split-learning",
        "SISA-unlearning"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2301.08517",
      "title": "Cohere: Managing Differential Privacy in Large Scale Systems",
      "abstract": "The need for a privacy management layer in today's systems started to manifest with the emergence of new systems for privacy-preserving analytics and privacy compliance. As a result, many independent efforts have emerged that try to provide system support for privacy. Recently, the scope of privacy solutions used in systems has expanded to encompass more complex techniques such as Differential Privacy (DP). The use of these solutions in large-scale systems imposes new challenges and requirements. Careful planning and coordination are necessary to ensure that privacy guarantees are maintained across a wide range of heterogeneous applications and data systems. This requires new solutions for managing and allocating scarce and non-replenishable privacy resources. In this paper, we introduce Cohere, a new system that simplifies the use of DP in large-scale systems. Cohere implements a unified interface that allows heterogeneous applications to operate on a unified view of users' data. In this work, we further address two pressing system challenges that arise in the context of real-world deployments: ensuring the continuity of privacy-based applications (i.e., preventing privacy budget depletion) and effectively allocating scarce shared privacy resources (i.e., budget) under complex preferences. Our experiments show that Cohere achieves a 6.4--28x improvement in utility compared to the state-of-the-art across a range of complex workloads.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nicolas K\u00fcchler",
        "Emanuel Opel",
        "Hidde Lycklama",
        "Alexander Viand",
        "Anwar Hithnawi"
      ],
      "author_details": [
        {
          "name": "Nicolas K\u00fcchler",
          "h_index": 5,
          "citation_count": 207,
          "affiliations": []
        },
        {
          "name": "Emanuel Opel",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Hidde Lycklama",
          "h_index": 4,
          "citation_count": 176,
          "affiliations": []
        },
        {
          "name": "Alexander Viand",
          "h_index": 11,
          "citation_count": 577,
          "affiliations": []
        },
        {
          "name": "Anwar Hithnawi",
          "h_index": 18,
          "citation_count": 1444,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://arxiv.org/abs/2301.08517",
      "citation_count": 7,
      "influential_citation_count": 2,
      "reference_count": 104,
      "is_open_access": true,
      "publication_date": "2023-01-20",
      "tldr": "This work introduces Cohere, a new system that simplifies the use of DP in large-scale systems and addresses two pressing system challenges that arise in the context of real-world deployments: ensuring the continuity of privacy-based applications and effectively allocating scarce shared privacy resources under complex preferences.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "tool",
      "domains": [],
      "model_types": [],
      "tags": [
        "DP-management",
        "large-scale"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2301.08517"
    },
    {
      "paper_id": "seed_1dec002a",
      "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness",
      "abstract": "By adopting a more flexible definition of unlearning and adjusting the model distribution to simulate training without the targeted data, approximate machine unlearning provides a less resource-demanding alternative to the more laborious exact unlearning methods. Yet, the unlearning completeness of target samples-even when the approximate algorithms are executed faithfully without external threats-remains largely unexamined, raising questions about those approximate algorithms' ability to fulfill their commitment of unlearning during the lifecycle. In this paper, we introduce the task of Lifecycle Unlearning Commitment Management (LUCM) for approximate unlearning and outline its primary challenges. We propose an efficient metric designed to assess the sample-level unlearning completeness. Our empirical results demonstrate its superiority over membership inference techniques in two key areas: the strong correlation of its measurements with unlearning completeness across various unlearning tasks, and its computational efficiency, making it suitable for real-time applications. Additionally, we show that this metric is able to serve as a tool for monitoring unlearning anomalies throughout the unlearning lifecycle, including both under-unlearning and over-unlearning. We apply this metric to evaluate the unlearning commitments of current approximate algorithms. Our analysis, conducted across multiple unlearning benchmarks, reveals that these algorithms inconsistently fulfill their unlearning commitments due to two main issues: 1) unlearning new data can significantly affect the unlearning utility of previously requested data, and 2) approximate algorithms fail to ensure equitable unlearning utility across different groups. These insights emphasize the crucial importance of LUCM throughout the unlearning lifecycle. We will soon open-source our newly developed benchmark.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Cheng-Long Wang",
        "Qi Li",
        "Zihang Xiang",
        "Yinzhi Cao",
        "Di Wang"
      ],
      "author_details": [
        {
          "name": "Cheng-Long Wang",
          "h_index": 7,
          "citation_count": 297,
          "affiliations": []
        },
        {
          "name": "Qi Li",
          "h_index": 3,
          "citation_count": 16,
          "affiliations": []
        },
        {
          "name": "Zihang Xiang",
          "h_index": 6,
          "citation_count": 80,
          "affiliations": []
        },
        {
          "name": "Yinzhi Cao",
          "h_index": 2,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Di Wang",
          "h_index": 3,
          "citation_count": 46,
          "affiliations": []
        }
      ],
      "max_h_index": 7,
      "url": "https://openalex.org/W4393027608",
      "pdf_url": "https://arxiv.org/pdf/2403.12830",
      "doi": "https://doi.org/10.48550/arxiv.2403.12830",
      "citation_count": 6,
      "influential_citation_count": 0,
      "reference_count": 44,
      "is_open_access": false,
      "publication_date": "2024-03-19",
      "tldr": "An efficient metric designed to assess the sample-level unlearning completeness is proposed and demonstrated its superiority over membership inference techniques in two key areas: the strong correlation of its measurements with unlearning completeness across various unlearning tasks, and its computational efficiency, making it suitable for real-time applications.",
      "fields_of_study": [
        "Computer Science"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "unlearning-completeness",
        "lifecycle"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2409.06280",
      "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
      "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.   This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models. We view data auditing through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.   MembershipTracker only requires the users to mark a small fraction of data (0.005% to 0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zitao Chen",
        "K. Pattabiraman"
      ],
      "author_details": [
        {
          "name": "Zitao Chen",
          "h_index": 10,
          "citation_count": 476,
          "affiliations": []
        },
        {
          "name": "K. Pattabiraman",
          "h_index": 42,
          "citation_count": 5943,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://arxiv.org/abs/2409.06280",
      "citation_count": 3,
      "influential_citation_count": 1,
      "reference_count": 80,
      "is_open_access": false,
      "publication_date": "2024-09-10",
      "tldr": "This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models and shows that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "data-auditing",
        "privacy",
        "facial-images"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2506.13972",
      "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
      "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhiqi Wang",
        "Chengyu Zhang",
        "Yuetian Chen",
        "Nathalie Baracaldo",
        "S. Kadhe",
        "Lei Yu"
      ],
      "author_details": [
        {
          "name": "Zhiqi Wang",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Chengyu Zhang",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Yuetian Chen",
          "h_index": 2,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Nathalie Baracaldo",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "S. Kadhe",
          "h_index": 21,
          "citation_count": 1467,
          "affiliations": []
        },
        {
          "name": "Lei Yu",
          "h_index": 2,
          "citation_count": 6,
          "affiliations": []
        }
      ],
      "max_h_index": 21,
      "url": "https://arxiv.org/abs/2506.13972",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 54,
      "is_open_access": false,
      "publication_date": "2025-06-16",
      "tldr": "An ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities is proposed, which enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "privacy-auditing",
        "reliability",
        "ensemble"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_ef429cfe",
      "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
      "abstract": "Machine unlearning focuses on efficiently removing specific data from trained models, addressing privacy and compliance concerns with reasonable costs. Although exact unlearning ensures complete data removal equivalent to retraining, it is impractical for large-scale models, leading to growing interest in inexact unlearning methods. However, the lack of formal guarantees in these methods necessitates the need for robust evaluation frameworks to assess their privacy and effectiveness. In this work, we first identify several key pitfalls of the existing unlearning evaluation frameworks, e.g., focusing on average-case evaluation or targeting random samples for evaluation, incomplete comparisons with the retraining baseline. Then, we propose RULI (Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel framework to address critical gaps in the evaluation of inexact unlearning methods. RULI introduces a dual-objective attack to measure both unlearning efficacy and privacy risks at a per-sample granularity. Our findings reveal significant vulnerabilities in state-of-the-art unlearning methods, where RULI achieves higher attack success rates, exposing privacy risks underestimated by existing methods. Built on a game-based foundation and validated through empirical evaluations on both image and text data (spanning tasks from classification to generation), RULI provides a rigorous, scalable, and fine-grained methodology for evaluating unlearning techniques.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Nima Naderloui",
        "Shenao Yan",
        "Binghui Wang",
        "Jie Fu",
        "Wendy Hui Wang",
        "Weiran Liu",
        "Yuan Hong"
      ],
      "author_details": [
        {
          "name": "Nima Naderloui",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Shenao Yan",
          "h_index": 3,
          "citation_count": 62,
          "affiliations": []
        },
        {
          "name": "Binghui Wang",
          "h_index": 5,
          "citation_count": 92,
          "affiliations": []
        },
        {
          "name": "Jie Fu",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Wendy Hui Wang",
          "h_index": 2,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Weiran Liu",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Yuan Hong",
          "h_index": 2,
          "citation_count": 35,
          "affiliations": []
        }
      ],
      "max_h_index": 5,
      "url": "https://openalex.org/W4415125319",
      "pdf_url": "https://arxiv.org/pdf/2506.13009",
      "doi": "https://doi.org/10.48550/arxiv.2506.13009",
      "citation_count": 2,
      "influential_citation_count": 1,
      "reference_count": 82,
      "is_open_access": false,
      "publication_date": "2025-06-16",
      "tldr": "This work proposes RULI (Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel framework to address critical gaps in the evaluation of inexact unlearning methods, and introduces a dual-objective attack to measure both unlearning efficacy and privacy risks at a per-sample granularity.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "unlearning-evaluation",
        "inference-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2506.02761",
      "title": "Rethinking Machine Unlearning in Image Generation Models",
      "abstract": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Renyang Liu",
        "Wenjie Feng",
        "Tianwei Zhang",
        "Wei Zhou",
        "Xueqi Cheng",
        "See-Kiong Ng"
      ],
      "author_details": [
        {
          "name": "Renyang Liu",
          "h_index": 6,
          "citation_count": 103,
          "affiliations": []
        },
        {
          "name": "Wenjie Feng",
          "h_index": 4,
          "citation_count": 59,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 1,
          "citation_count": 22,
          "affiliations": []
        },
        {
          "name": "Wei Zhou",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Xueqi Cheng",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "See-Kiong Ng",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://arxiv.org/abs/2506.02761",
      "citation_count": 1,
      "influential_citation_count": 1,
      "reference_count": 68,
      "is_open_access": false,
      "publication_date": "2025-06-03",
      "tldr": "This work designs CatIGMU, a novel hierarchical task categorization framework, and introduces EvalIGMU, a comprehensive evaluation framework, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "generative-unlearning",
        "image-generation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_4093b576",
      "title": "Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Gaoyang Liu",
        "Xijie Wang",
        "Zixiong Wang",
        "Chen Wang",
        "A. M. Abdelmoniem",
        "Desheng Wang"
      ],
      "author_details": [
        {
          "name": "Gaoyang Liu",
          "h_index": 18,
          "citation_count": 1236,
          "affiliations": []
        },
        {
          "name": "Xijie Wang",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Zixiong Wang",
          "h_index": 2,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "Chen Wang",
          "h_index": 24,
          "citation_count": 2374,
          "affiliations": [
            "Huazhong University of Science and Technology"
          ]
        },
        {
          "name": "A. M. Abdelmoniem",
          "h_index": 21,
          "citation_count": 1533,
          "affiliations": []
        },
        {
          "name": "Desheng Wang",
          "h_index": 2,
          "citation_count": 27,
          "affiliations": []
        }
      ],
      "max_h_index": 24,
      "url": "https://ccs25files.zoolab.org/main/ccsfa/sV74FPQe/3719027.3744827.pdf",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 43,
      "is_open_access": false,
      "publication_date": "2025-11-19",
      "tldr": "This paper introduces Naive Prototype Surgery (Naive PS), a fast and simplified method that uses a closed-form solution to approximate unlearning effect by directly adjusting the prototype associated with the unlearned data, and Prototype Surgery (PS), which incorporates soft label information to fine-tune the prototypes of all classes, to achieve a more effective unlearning.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "prototype-unlearning",
        "efficient"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_55d8825e",
      "title": "Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications",
      "abstract": "Large language models (LLMs) have facilitated the generation of high-quality, cost-effective synthetic data for developing downstream models and conducting statistical analyses in various domains. However, the increased reliance on synthetic data may pose potential negative impacts. Numerous studies have demonstrated that LLM-generated synthetic data can perpetuate and even amplify societal biases and stereotypes, and produce erroneous outputs known as ``hallucinations'' that deviate from factual knowledge. In this paper, we aim to audit artifacts, such as classifiers, generators, or statistical plots, to identify those trained on or derived from synthetic data and raise user awareness, thereby reducing unexpected consequences and risks in downstream applications. To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data. We then propose an auditing framework with three methods including metric-based auditing, tuning-based auditing, and classification-based auditing. These methods operate without requiring the artifact owner to disclose proprietary training details. We evaluate our auditing framework on three text classification tasks, two text summarization tasks, and two data visualization tasks across three training scenarios. Our evaluation demonstrates the effectiveness of all proposed auditing methods across all these tasks. For instance, black-box metric-based auditing can achieve an average accuracy of $0.868 \\pm 0.071$ for auditing classifiers and $0.880 \\pm 0.052$ for auditing generators using only 200 random queries across three scenarios. We hope our research will enhance model transparency and regulatory compliance, ensuring the ethical and responsible use of synthetic data.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yixin Wu",
        "Ziqing Yang",
        "Yun Shen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Yixin Wu",
          "h_index": 8,
          "citation_count": 337,
          "affiliations": []
        },
        {
          "name": "Ziqing Yang",
          "h_index": 4,
          "citation_count": 161,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 7,
          "citation_count": 304,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 16,
          "citation_count": 877,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 12,
          "citation_count": 598,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://openalex.org/W4407124110",
      "pdf_url": "https://arxiv.org/pdf/2502.00808",
      "doi": "https://doi.org/10.48550/arxiv.2502.00808",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 87,
      "is_open_access": false,
      "publication_date": "2025-02-02",
      "tldr": "This paper takes the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data, and proposes an auditing framework with three methods including metric-based auditing, tuning-based auditing, and classification-based auditing.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "tool",
      "domains": [
        "llm"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "synthetic-data",
        "auditing",
        "tracing"
      ],
      "open_access_pdf": ""
    }
  ]
}