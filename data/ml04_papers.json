{
  "updated": "2026-01-06",
  "total": 199,
  "owasp_id": "ML04",
  "owasp_name": "Membership Inference Attack",
  "description": "Detecting if data was used to train an ML model",
  "keywords": [
    "model stealing",
    "model extraction",
    "model theft",
    "steal model",
    "stealing model",
    "extract model",
    "model stealing attack",
    "model extraction attack",
    "neural network extraction attack",
    "knockoff nets",
    "knockoff net",
    "copycat CNN",
    "copycat model",
    "imitation attack",
    "clone model",
    "cloning attack",
    "stealing machine learning",
    "steal ML model",
    "steal ML models",
    "steal neural network",
    "DNN model stealing",
    "DNN extraction",
    "stealing deep learning",
    "LLM stealing",
    "LLM extraction",
    "stealing language model",
    "stealing functionality",
    "functionality stealing",
    "black-box model stealing",
    "blackbox model extraction",
    "model stealing defense",
    "model extraction defense",
    "prevent model stealing",
    "protect model extraction",
    "side-channel model extraction",
    "side-channel neural network",
    "timing attack neural network",
    "cache attack DNN",
    "power analysis neural network",
    "electromagnetic neural network",
    "DNN weights leakage",
    "neural network weight extraction",
    "reverse engineer neural network",
    "reverse engineering DNN",
    "cryptanalytic extraction neural",
    "API model extraction",
    "query-based model stealing",
    "prediction API stealing"
  ],
  "note": "Filtered for Membership Inference Attack",
  "papers": [
    {
      "paper_id": "37ef78b60cb09e136e13df735fc58ade8348d671",
      "title": "Finding the PISTE: Towards Understanding Privacy Leaks in Vertical Federated Learning Systems",
      "abstract": "Vertical Federated Learning (VFL) is a collaborative learning paradigm where participants share the same sample space while splitting the feature space. In VFL, local participants host their bottom models for feature extraction and collaboratively train a classifier by exchanging intermediate results with the server owning the labels. Both local training data and bottom models contain privacy-sensitive information and are considered the intellectual property of each participant, and thus should be protected by the design of VFL. Our study exposes the fundamental susceptibility of VFL systems to privacy leaks, which arise from the collaboration between the server and clients during both training and testing. Based on our findings, we propose PISTE, a model-agnostic framework of privacy stealing attacks against VFL. PISTE delivers three privacy inference attacks, i.e., model stealing, data reconstruction, and property inference attacks on five benchmark datasets and four different model architectures. We further discuss four potential countermeasures. Experimental results show that all of them cannot prevent all three privacy stealing attacks in PISTE. In summary, our study demonstrates the inherent yet rarely uncovered vulnerability of VFL on leaking data and model privacy.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiangru Xu",
        "Wei Wang",
        "Zheng Chen",
        "Bin Wang",
        "Chao Li",
        "Li Duan",
        "Zhen Han",
        "Yufei Han"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/37ef78b60cb09e136e13df735fc58ade8348d671",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7044d075fba8c188f716a25618d522c808a67a96",
      "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
      "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7044d075fba8c188f716a25618d522c808a67a96",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "633f03f1eea4387f02e826e4768841ef10190446",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nianfeng Tzeng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/633f03f1eea4387f02e826e4768841ef10190446",
      "pdf_url": "",
      "publication_date": "2025-01-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "title": "Adversarial Autoencoder based Model Extraction Attacks for Collaborative DNN Inference at Edge",
      "abstract": "Deep neural networks (DNNs) are influencing a wide range of applications from safety-critical to security-sensitive use cases. In many such use cases, the DNN inference process relies on distributed systems involving IoT devices and edge/cloud severs as participants where a pre-trained DNN model is partitioned/split onto multiple parts and the participants collaboratively execute them. However, often such collaboration requires dynamic DNN partitioning information to be exchanged among the participants over unsecured network or via relays/hops which can lead to novel privacy vulnerabilities. In this paper, we propose a DNN model extraction attack that exploits such vulnerabilities to not only extract the original input data, but also reconstruct the entire victim DNN model. Specifically, the proposed attack model utilizes extracted/leaked data and adversarial autoencoders to generate and train a shadow model that closely mimics the behavior of the original victim model. The proposed attack is query-free and does not require the attacker to have any prior information about the victim model and input data. Using an IoT-edge hardware testbed running collaborative DNN inference, we demonstrate the effectiveness of the proposed attack model in extracting the victim model with high levels of certainty across many realistic scenarios.",
      "year": 2025,
      "venue": "IEEE/IFIP Network Operations and Management Symposium",
      "authors": [
        "Manal Zneit",
        "Xiaojie Zhang",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
      "abstract": "Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Shang Wang",
        "Bo Liu",
        "Leo Yu Zhang",
        "Wanlei Zhou",
        "Yang Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
      "abstract": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ganghua Wang",
        "Yuhong Yang",
        "Jie Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "90360529f01cf996d62369fd0c47af3b1823c7f4",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attack poses a significant risk to Machine-Learning-as-a-Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying Black-Box (without internal insight) APIs. Despite FL\u2019s privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of the FL-based victim model to two types of model extraction attacks. For various federated clients built under NVFlare platform, we implemented ME attack across two deep-learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for various FL clients, the accuracy and fidelity of the extraction model are closely related to the size of the attack query set. Additionally, we explore a transfer learning-based approach where pre-trained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pre-trained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/90360529f01cf996d62369fd0c47af3b1823c7f4",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6678e884da10e0d6cd763811069a813700b685a6",
      "title": "Exploring Query Efficient Data Generation Towards Data-Free Model Stealing in Hard Label Setting",
      "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. Instead, the adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (QEDG). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real MLaaS scenario and five datasets.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Gaozheng Pei",
        "Shaojie Lyu",
        "Ke Ma",
        "Pinci Yang",
        "Qianqian Xu",
        "Yingfei Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6678e884da10e0d6cd763811069a813700b685a6",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "abstract": "On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yujin Huang",
        "Zhi Zhang",
        "Qingchuan Zhao",
        "Xingliang Yuan",
        "Chunyang Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "title": "Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel",
      "abstract": "Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-world physical devices. User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive. Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning. However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level. In this paper, we introduce a generic physical side-channel attack, ScaAR, that extracts user interactions with neural networks by leveraging electromagnetic (EM) emissions of physical devices. Our proposed attack is implementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA). Experimental results demonstrate that, through the EM side channel, ScaAR can effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuoran Liu",
        "Senna van Hoek",
        "P'eter Horv'ath",
        "Dirk Lauret",
        "Xiaoyun Xu",
        "L. Batina"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cb626a77f1e0c634d557ca88af22547f21f16afa",
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and interactions between entities, is growing in scale and complexity. As a result, training state-of-the-art graph machine learning (GML) models have become increasingly resource-intensive, turning these models and data into invaluable Intellectual Property (IP). To address the resource-intensive nature of model training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an efficient solution by leveraging third-party cloud services for model development and management. However, deploying such models in GMLaaS also exposes them to potential threats from attackers. Specifically, while the APIs within a GMLaaS system provide interfaces for users to query the model and receive outputs, they also allow attackers to exploit and steal model functionalities or sensitive training data, posing severe threats to the safety of these GML models and the underlying graph data. To address these challenges, this survey systematically introduces the first taxonomy of threats and defenses at the level of both GML model and graph-structured data. Such a tailored taxonomy facilitates an in-depth understanding of GML IP protection. Furthermore, we present a systematic evaluation framework to assess the effectiveness of IP protection methods, introduce a curated set of benchmark datasets across various domains, and discuss their application scopes and future challenges. Finally, we establish an open-sourced versatile library named PyGIP, which evaluates various attack and defense techniques in GMLaaS scenarios and facilitates the implementation of existing benchmark methods. The library resource can be accessed at: https://labrai.github.io/PyGIP. We believe this survey will play a fundamental role in intellectual property protection for GML and provide practical recipes for the GML community.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cb626a77f1e0c634d557ca88af22547f21f16afa",
      "pdf_url": "",
      "publication_date": "2025-08-27",
      "keywords_matched": [
        "steal model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
      "abstract": "To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35\u00d7 (avg. 5.85\u00d7) faster than the state-of-the-art work, without accuracy loss.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tong Sun",
        "Bowen Jiang",
        "Hailong Lin",
        "Borui Li",
        "Yixiao Teng",
        "Yi Gao",
        "Wei Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "title": "Towards Functional Safety of Neural Network Hardware Accelerators: Concurrent Out-of-Distribution Detection in Hardware Using Power Side-Channel Analysis",
      "abstract": "For AI hardware, functional safety is crucial, especially for neural network (NN) accelerators used in safety-critical systems. A key requirement for maintaining this safety is the precise detection of out-of-distribution (OOD) instances, which are inputs significantly distinct from the training data. Neglecting to integrate robust OOD detection may result in possible safety hazards, diminished performance, and inaccurate decision-making within NN applications. Existing methods for OOD detection have been explored for full-precision models. However, the evaluation of methods on quantized neural network (QNN), which are often deployed on hardware accelerators such as FPGAs, and on-device hardware realization of concurrent OOD detection (COD) is missing in literature. In this paper, we provide a novel approach to OOD detection for NN FPGA accelerators using power measurements. Utilizing the power side-channel through digital voltage sensors allows on-device OOD detection in a non-intrusive and concurrent manner, without relying on explicit labels or modifications to the underlying NN. Furthermore, our method allows OOD detection before the inference finishes. Additionally to the evaluation, we provide an efficient hardware implementation of COD on an actual FPGA.",
      "year": 2025,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Mahboobe Sadeghipourrudsari",
        "Dennis R. E. Gnad",
        "Mehdi B. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "pdf_url": "",
      "publication_date": "2025-01-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-21"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "LLM stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "title": "Inside the Mind of an Attacker: Review Sistematika Tujuan Pencurian Machine Learning Model",
      "abstract": "Abstrak - Pencurian model (model stealing) menjadi salah satu ancaman serius dalam penerapan machine learning modern, terutama pada layanan berbasis API dan cloud. Artikel ini mengulas secara sistematik berbagai tujuan di balik serangan pencurian model untuk memahami motif penyerang dan implikasinya bagi pengembang sistem. Metode penulisan berupa kajian literatur terkini yang mengklasifikasikan tujuan pencurian ke dalam delapan kategori utama: (1) pencurian properti internal seperti arsitektur, bobot, dan hyperparameter; (2) peniruan perilaku model untuk menghasilkan efektivitas setara dan konsistensi prediksi pada data normal maupun adversarial; (3) transfer pengetahuan untuk distillation dan deployment ringan; (4) serangan privasi berupa membership inference dan model inversion; (5) monetisasi dengan menjual model bajakan atau menyediakan layanan API ilegal; (6) pencurian kemampuan pertahanan adversarial untuk meningkatkan efektivitas serangan; (7) spionase industri untuk reverse engineering model pesaing; serta (8) penghindaran regulasi dengan mencuri model yang sudah tersertifikasi. Review ini menegaskan bahwa ancaman pencurian model tidak hanya merugikan secara teknis, tetapi juga membuka peluang eksploitasi ekonomi ilegal, kebocoran data sensitif, dan persaingan usaha tidak sehat. Pemahaman yang detail atas ragam tujuan ini diharapkan mendorong perancang sistem untuk mengembangkan strategi pertahanan yang lebih cermat dan menyeluruh.Kata kunci: Machine Learning; Model Stealing; API; Adversarial; Transfer Pengetahuan;\u00a0Abstract - Model stealing has become one of the most serious threats in modern machine learning applications, especially in API- and cloud-based services. This article systematically reviews the various objectives behind model stealing attacks to understand the attackers\u2019 motivations and their implications for system developers. The writing method is a current literature review that classifies model stealing objectives into eight main categories: (1) theft of internal properties such as architecture, weights, and hyperparameters; (2) imitation of model behavior to achieve comparable effectiveness and prediction consistency on both normal and adversarial data; (3) knowledge transfer for distillation and lightweight deployment; (4) privacy attacks through membership inference and model inversion; (5) monetization by selling stolen models or offering illegal API services; (6) stealing adversarial robustness to improve attack effectiveness; (7) industrial espionage for reverse engineering competitor models; and (8) regulatory evasion by stealing pre-certified models. This review emphasizes that model stealing threats are not merely technical issues but also open opportunities for illegal economic exploitation, leakage of sensitive data, and unfair business competition. A detailed understanding of these diverse objectives is expected to encourage system designers to develop more careful and comprehensive defense strategies.Keywords: Machine Learning; Model Stealing; API; Adversarial; Knowledge Transfer;",
      "year": 2025,
      "venue": "Jurnal Nasional Komputasi dan Teknologi Informasi (JNKTI)",
      "authors": [
        "Mulkan Fadhli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "pdf_url": "",
      "publication_date": "2025-07-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "title": "Model Rake: A Defense Against Stealing Attacks in Split Learning",
      "abstract": "Split learning is a prominent framework for vertical federated learning, where multiple clients collaborate with a central server for model training by exchanging intermediate embeddings. Recently, it is shown that an adversarial server can exploit the intermediate embeddings to train surrogate models to replace the bottom models on the clients (i.e., model stealing). The surrogate models can also be used to reconstruct private training data of the clients (i.e., data stealing).\n\nTo defend against these stealing attacks, we propose Model Rake (i.e., Rake), which runs two bottom models on each client and differentiates their output spaces to make the two models distinct. Rake hinders the stealing attacks because it is difficult for a surrogate model to approximate two distinct bottom models. We prove that, under some assumptions, the surrogate model converges to the average of the two bottom models and thus will be inaccurate. Extensive experiments show that Rake is much more effective than existing methods in defending against both model and data stealing attacks, and the accuracy of normal model training is not affected.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yanfeng Zhao",
        "Fangcheng Fu",
        "Quanqing Xu",
        "Yukai Ding",
        "Xiaokai Zhou",
        "Chuang Hu",
        "Jiawei Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "056036a3ce3a21daf80167dee622b6b515f9490e",
      "title": "Black-box model functionality stealing for Vietnamese sentiment analysis",
      "abstract": "Black-box deep learning models often keep critical components such as model architecture, hyperparameters, and training data confidential, allowing users to observe only the inputs and outputs without understanding their internal workings. Consequently, there is growing interested in developing \"knockoff\" models that replicate the behavior of these black-box models without direct access to internal details. We have conducted extensive studies on function extraction attacks targeting English text sentiment analysis models. By employing random or adaptive sampling methods, we have successfully reconstructed knockoff models that achieve functionality equivalent to the original models with high similarity. In this study, we extend our investigation to sentiment analysis datasets in Vietnamese. Experimental results demonstrate that for black-box models in Vietnamese text sentiment analysis, our method remains effective, successfully constructing models with equivalent functionality.",
      "year": 2025,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Viet-Binh Do",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/056036a3ce3a21daf80167dee622b6b515f9490e",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "title": "Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks",
      "abstract": "The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model's functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model's training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at https://github.com/rajankita/QueryWise.",
      "year": 2025,
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "authors": [
        "Ankita Raj",
        "Harsh Swaika",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "title": "Middleware Architecture for the Management and Mitigation of OWASP ML05: Model Theft in IoT Machine Learning Networks",
      "abstract": "The increasing integration of machine learning (ML) models into Internet of Things (IoT) applications has led to notable advancements in automation and decision-making. However, these models are vulnerable to modern attack vectors recognized by the OWASP Top 10 for Large Language Model Applications, specifically ML05: Model Theft, where adversaries gain unauthorized access to model parameters and training data, compromising intellectual property and sensitive information. Such threats are particularly concerning in IoT environments due to their distributed nature and resource limitations. This paper proposes a middleware architecture for the management and mitigation of model theft risks by incorporating encryption, access control, obfuscation, watermarking, continuous monitoring, and service assurance programmability. By strengthening the security management framework of ML models deployed in IoT, the proposed architecture aims to protect against theft, ensure data confidentiality, and maintain network resilience. The approach includes detailed mathematical models and an evaluation of existing security measures, demonstrating the architecture's effectiveness in diverse IoT deployments, such as telemedicine and smart cities.",
      "year": 2025,
      "venue": "Global",
      "authors": [
        "Yair Enrique Rivera Julio",
        "\u00c1ngel Pinto",
        "Nelson A. P\u00e9rez-Garc\u00eda",
        "M\u00f3nica-Karel Huerta",
        "C\u00e9sar Viloria-N\u00fa\u00f1ez",
        "Marvin Luis P\u00e9rez Cabrera",
        "Frank Ibarra Hern\u00e1ndez",
        "Juan Manuel Torres Tovio",
        "Erwin J. Sacoto-Cabrera"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "86c0661fdb12bcde0709e91416510fda72f10f13",
      "title": "A Comprehensive Survey of Model Extraction Attacks: Current Trends, Defenses, and Future Directions",
      "abstract": "Model extraction attacks pose a significant threat to Machine Learning (ML) systems, especially in cloud-based services like Machine Learning as a Service (MLaaS). Attacks aim to steal proprietary models by replicating their functionality or extracting their internal parameters. This paper reviews model extraction attack types, examining existing defensive techniques and weaknesses in current defenses. Promising defense mechanisms are discussed, including adaptive privacy budgets, hybrid defense strategies, combining multiple methods, and hardwarebased security solutions. Emerging attack models like collaborative attacks in federated learning environments are explored. Future research focuses on adaptive defenses and Artificial Intelligence (AI)-driven detection methods to improve model robustness and contribute to more resilient machine learning systems.",
      "year": 2025,
      "venue": "2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)",
      "authors": [
        "Quazi Rian Hasnaine",
        "Yaodan Hu",
        "Mohamed I. Ibrahem",
        "Mostafa M. Fouda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/86c0661fdb12bcde0709e91416510fda72f10f13",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68f0633a0be1a7009c4caf765529d898b2450044",
      "title": "SONNI: Secure Oblivious Neural Network Inference",
      "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS) model, the client encrypts data using homomorphic encryption and uploads it to a server for computation. The result is then sent back to the client for decryption. It has become more and more common for the computation to be outsourced to third-party servers. In this paper we identify a weakness in this protocol that enables a completely undetectable novel model-stealing attack that we call the Silver Platter attack. This attack works even under multikey encryption that prevents a simple collusion attack to steal model parameters. We also propose a mitigation that protects privacy even in the presence of a malicious server and malicious client or model provider (majority dishonest). When compared to a state-of-the-art but small encrypted model with 32k parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while batching capability is reduced by 0.2%. Our approach uses a novel results-checking protocol that ensures the computation was performed correctly without violating honest clients' data privacy. Even with collusion between the client and the server, they are unable to steal model parameters. Additionally, the model provider cannot learn any client data if maliciously working with the server.",
      "year": 2025,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Luke Sperling",
        "Sandeep S. Kulkarni"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68f0633a0be1a7009c4caf765529d898b2450044",
      "pdf_url": "",
      "publication_date": "2025-04-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "title": "Detecting Generative Model Inversion Attacks for Protecting Intellectual Property of Deep Neural Networks",
      "abstract": "Recently, protecting the Intellectual Property (IP) of deep neural networks (DNNs) has attracted attention from researchers. This is because training DNN models can be costly especially when acquiring and labeling training data require domain expertise. DNN watermarking and fingerprinting are two techniques proposed to prevent DNN IP infringement. Although these two techniques achieve high performance on defending against previously proposed DNN stealing attacks, researchers recently show that both of them are ineffective against generative model inversion attacks. Specifically, an adversary inverts training data from well-trained DNNs and uses the inverted data to train DNNs from scratch such that DNN watermarking and fingerprinting are both bypassed. This novel model stealing strategy shows that data inverted from victim models can be effectively exploited by adversaries, which poses a new threat to the IP protection of DNNs. To combat this new threat, one potential solution is to enable defenders to prove ownership on data inverted from models being protected. If the training data of a suspected model, which can be disclosed via the judicial process, are proven to be data inverted from victim models, then IP infringement is detected. This research direction is currently underexplored. In this paper, we fill the gap in the literature to investigate countermeasures against this emerging threat. We propose a simple but effective method, called InverseDataInspector (IDI), to detect whether data are inverted from victim models. Specifically, our method first extracts features from both the inverted data and victim models. These features are then combined and used for training classifiers. Experimental results demonstrate that our method achieves high performance on detecting inverted data and also generalizes to new generative model inversion methods that are not seen when training classifiers.",
      "year": 2025,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Yiding Yu",
        "W. Zong",
        "Wenjing Su",
        "Yang-Wai Chow",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "pdf_url": "",
      "publication_date": "2025-10-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "title": "Too Clever by Half: Detecting Sampling-based Model Stealing Attacks by Their Own Cleverness",
      "abstract": "Machine learning as a service (MLaaS) has gained significant popularity and market traction in recent years, driven by advancements in Artificial Intelligence particularly Generative AI (GAI). However, MLaaS faces severe challenges from sampling-based model stealing attacks (MSAs), where attackers strategically query the targeted ML models provided by MLaaS providers to minimize the query burden while closely replicating the model\u2019s functionality. Such MSAs pose severe consequences, including intellectual property (IP) theft and potential leakage of private training data. Unfortunately, existing defenses either sacrifice model utility or fail to generalize across diverse MSAs.In this paper, we propose DIARY, an innovative detection method specifically tailored to sampling-based MSAs by exploiting their inherent sophistication. Our key insight is that \u2018clever\u2019 malicious queries tend to extract more information from the targeted (victim) model than typical benign queries, as these attacks iteratively refine their queries by examining and analyzing prior queries and the corresponding responses. Hence we design DIARY to extract timing dependence within a query sequence and incorporate contrastive learning for properly characterizing such dependency that holds for different sampling-based MSAs. Comprehensive evaluations using five different sampling-based MSAs and two state-of-the-art defense baselines across four popular datasets consistently validate DIARY\u2019s superior performance.",
      "year": 2025,
      "venue": "IEEE International Conference on Distributed Computing Systems",
      "authors": [
        "Xin Yao",
        "Chenyang Wang",
        "Yimin Chen",
        "Kecheng Huang",
        "Jiawei Guo",
        "Ming Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "title": "Security Challenges and Mitigation Strategies in Generative AI Systems",
      "abstract": "This article examines the critical security challenges and mitigation strategies in generative AI systems. The article explores how these systems have transformed various sectors, particularly in financial markets and critical infrastructure, while introducing significant security concerns. The article analyzes various types of adversarial attacks, including input perturbation and backdoor attacks, and their impact on AI model performance. Additionally, it investigates model stealing threats and data privacy concerns in AI deployments. The article presents comprehensive mitigation strategies, including advanced defense mechanisms, enhanced protection frameworks, and secure access control implementations. The article findings demonstrate the effectiveness of integrated security approaches in protecting AI systems while maintaining operational efficiency. This article contributes to the growing body of knowledge on AI security by providing evidence-based strategies for protecting generative AI systems across different application domains.",
      "year": 2025,
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "authors": [
        "Satya Naga Mallika Pothukuchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "pdf_url": "https://doi.org/10.32628/cseit25112377",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
      "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical machine learning, primarily to solve classification, regression and generative tasks. However, its rapid development raises critical security challenges in the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines adversarial threats unique to QML systems, focusing on vulnerabilities in cloud-based deployments, hybrid architectures, and quantum generative models. Key attack vectors include model stealing via transpilation or output extraction, data poisoning through quantum-specific perturbations, reverse engineering of proprietary variational quantum circuits, and backdoor attacks. Adversaries exploit noise-prone quantum hardware and insufficiently secured QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership, and functionality. Defense mechanisms leverage quantum properties to counter these threats. Noise signatures from training hardware act as non-invasive watermarks, while hardware-aware obfuscation techniques and ensemble strategies disrupt cloning attempts. Emerging solutions also adapt classical adversarial training and differential privacy to quantum settings, addressing vulnerabilities in quantum neural networks and generative architectures. However, securing QML requires addressing open challenges such as balancing noise levels for reliability and security, mitigating cross-platform attacks, and developing quantum-classical trust frameworks. This chapter summarizes recent advances in attacks and defenses, offering a roadmap for researchers and practitioners to build robust, trustworthy QML systems resilient to evolving adversarial landscapes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Archisman Ghosh",
        "Satwik Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "title": "Model Extraction Attack and Its Countermeasure for Denoising Diffusion Implicit Models",
      "abstract": "Recently, the threat of cyber attacks against machine learning models has been increasing. Typical examples include Model Extraction Attack (MEA), which steals the functionality of a victim model by creating its clone model that has almost the same functionality. Thus, the literature has studied MEA and its defense methods, mainly focusing on image recognition models. However, no existing studies evaluate the risk of MEA on diffusion-based image generation models, despite the recent advances and widespread use of image generation AI services powered by diffusion models. In this paper, we first demonstrate the feasibility of MEA on DDIM, one of the most common diffusion-based image generation models. Then, as a countermeasure, we propose a defense method that detects clone models of DDIM. In the proposed method, we add a small number of out-of-distribution images, referred to as \u201cmarking images\u201d, to the training dataset of a victim DDIM model. This technique provides the property of occasionally generating marking images for the victim model. This property works as a watermark and is inherited by the clone models, being used as a clue for detecting them. In the results of our experiments conducted on face, fruit, and church image datasets, the proposed defense method can correctly detect all clone models without seriously degrading the usability of victim DDIM models.",
      "year": 2025,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Hayato Shoji",
        "Kazuaki Nakamura"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
      "abstract": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuochen Yang",
        "Fok Kar Wai",
        "V. Thing"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d66b4910d4c16463e1859035cca94bbefbd76f92",
      "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
      "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhengchunmin Dai",
        "Jiaxiong Tang",
        "Peng Sun",
        "Honglong Chen",
        "Liantao Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d66b4910d4c16463e1859035cca94bbefbd76f92",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8cf823eb4c9309221953480a9b98c7700136c546",
      "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment",
      "abstract": "With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tushar Nayan",
        "Ziqi Zhang",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8cf823eb4c9309221953480a9b98c7700136c546",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms",
      "abstract": "Online collaborative medical prediction platforms offer convenience and real-time feedback by leveraging massive electronic health records. However, growing concerns about privacy and low prediction quality can deter patient participation and doctor cooperation. In this paper, we first clarify the privacy attacks, namely attribute attacks targeting patients and model extraction attacks targeting doctors, and specify the corresponding privacy principles. We then propose a privacy-preserving mechanism and integrate it into a novel one-shot distributed learning framework, aiming to simultaneously meet both privacy requirements and prediction performance objectives. Within the framework of statistical learning theory, we theoretically demonstrate that the proposed distributed learning framework can achieve the optimal prediction performance under specific privacy requirements. We further validate the developed privacy-preserving collaborative medical prediction platform through both toy simulations and real-world data experiments.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shao-Bo Lin",
        "Xiaotong Liu",
        "Yao Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "pdf_url": "",
      "publication_date": "2025-07-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9d0f0fb82a339debb181382f725baa278bb202d7",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d0f0fb82a339debb181382f725baa278bb202d7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "title": "On the Interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction",
      "abstract": "Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.",
      "year": 2025,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine",
        "Rinad Akel",
        "Ihab Sbeity",
        "Silvia Giordano",
        "Marc Langheinrich",
        "Omran Ayoub"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "pdf_url": "",
      "publication_date": "2025-05-13",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "title": "Explore the vulnerability of black-box models via diffusion models",
      "abstract": "Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Huajie Shao",
        "Ashley Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
      "abstract": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Sohely Jahan",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-12"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "title": "Unraveling Attacks to Machine-Learning-Based IoT Systems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This article embarks on a comprehensive exploration of the security threats arising from ML\u2019s integration into various facets of IoT, spanning various attack types, including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria, such as adversary models, attack targets, and key security attributes (confidentiality, integrity, and availability). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this article seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "model stealing",
        "stealing language model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
      "abstract": "The past few years have seen an intense research interest in the practical needs of the \u201cright to be forgotten\u201d, which has motivated researchers to develop machine unlearning methods to unlearn a fraction of training data and its lineage. While existing machine unlearning methods prioritize the protection of individuals\u2019 private data, they over-look investigating the unlearned models\u2019 susceptibility to adversarial attacks and security breaches. In this work, we uncover a novel security vulnerability of machine unlearning based on the insight that adversarial vulnerabilities can be bol-stered, especially for adversarially robust models. To exploit this observed vulnerability, we pro-pose a novel attack called Adv ersarial U nlearning A ttack (AdvUA), which aims to generate a small fraction of malicious unlearning requests during the unlearning process. AdvUA causes a significant reduction of adversarial robustness in the unlearned model compared to the original model, providing an entirely new capability for adversaries that is infeasible in conventional machine learning pipelines. Notably, we also show that AdvUA can effectively enhance model stealing attacks by extracting additional decision boundary information, further emphasizing the breadth and significance of our research. We also conduct both theoretical analysis and computational complexity of AdvUA. Extensive numerical studies are performed to demonstrate the effectiveness and efficiency of the proposed attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chenxu Zhao",
        "Wei Qian",
        "Yangyi Li",
        "Aobo Chen",
        "Mengdi Huai"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
      "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrey V. Galichin",
        "Mikhail Aleksandrovich Pautov",
        "Alexey Zhavoronkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16b5db7894dde1b49960f03e68b280ac895d86fc",
      "title": "Securing Data From Side-Channel Attacks: A Graph Neural Network-Based Approach for Smartphone-Based Side Channel Attack Detection",
      "abstract": "The widespread use of smartphones has brought convenience and connectivity to the fingertips of the masses. As a result, this has paved the way for potential security vulnerabilities concerning sensitive data, particularly by exploiting side-channel attacks. When typing on a smartphone\u2019s keyboard, its vibrations can be misused to discern the entered characters, thus facilitating side-channel attacks. These smartphone hardware sensors can capture such information while users input sensitive data like personal details, names, email addresses, age, bank details and passwords. This study presents a novel Graph Neural Network (GNN) approach to predict side-channel attacks on smartphone keyboards; different GNN architectures were used, including GNN, DeepGraphNet, Gradient Boosting (GB)+DeepGraphNet, Extreme Gradient Boosting (XGB)+DeepGraphNet and K-Nearest Neighbor (KNN)+DeepGraphNet. The proposed approach detects the side channel attack using vibrations produced while typing on the smartphone soft keyboard. The data was collected from three smartphone sensors, an accelerometer, gyroscope, and magnetometer, and evaluated this data using common evaluation measures such as accuracy, precision, recall, F1-score, ROC curves, confusion matrix and accuracy and loss curves. This study demonstrated that GNN architectures can effectively capture complex relationships in data, making them well-suited for analyzing patterns in smartphone sensor data. Likewise, this research aims to fill a crucial gap by enhancing data privacy in the information entered through smartphone keyboards, shielding it from side-channel attacks by providing an accuracy of 98.26%. Subsequently, the primary objective of this study is to assess the effectiveness of GNN architectures in this precise context. Similarly, the GNN model exhibits compelling performance, achieving accuracy, precision, recall, and f1 score metrics that showcase the model\u2019s effectiveness, with the highest values of 0.98, 0.98, 0.98, and 0.98, respectively. Significantly, the metrics mentioned in the study outperform those documented in the previous literature. Overall, the study contributes to the detection of side-channel smartphone attacks, which advances secure data practices.INDEX TERMS Graph neural networks (GNN), keystroke inference, motion sensors, machine learning, smartphone security, side-channel attacks.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Sidra Abbas",
        "Stephn Ojo",
        "Imen Bouazzi",
        "Gabriel Avelino Sampedro",
        "Abdullah Al Hejaili",
        "Ahmad S. Almadhor",
        "Rastislav Kulh\u00e1nek"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/16b5db7894dde1b49960f03e68b280ac895d86fc",
      "pdf_url": "https://doi.org/10.1109/access.2024.3465662",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
      "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "pdf_url": "",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "title": "Privacy Implications of Explainable AI in Data-Driven Systems",
      "abstract": "Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.",
      "year": 2024,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "pdf_url": "",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0207b4706683924d5b0e1399bd08b49fd19c394f",
      "title": "Stealing Training Graphs from Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.",
      "year": 2024,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Min Lin",
        "Enyan Dai",
        "Junjie Xu",
        "Jinyuan Jia",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0207b4706683924d5b0e1399bd08b49fd19c394f",
      "pdf_url": "",
      "publication_date": "2024-11-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "389f1fad962f58799327a0112526452d2da5157d",
      "title": "TEXTKNOCKOFF: KNOCKOFF NETS FOR STEALING FUNCTIONALITY OF TEXT SENTIMENT MODELS",
      "abstract": "Most commercial machine learning models today are designed to require significant amounts of time, money, and human effort. Therefore, intrinsic information about the model (such as architecture, hyperparameters, and training data) needs to be kept confidential. These models are referred to as black boxes, and there is an increasing amount of research focused on both attacking and protecting them. Recent publications have often concentrated on the field of computer vision; in contrast, there is still relatively little research on methods for attacking black box models with textual data. This article introduces a research method for extracting the functionality of a black box model in the task of text sentiment analysis. The method has been effectively tested based on random sampling techniques to reconstruct a new model with equivalent functionality to the original model, achieving high accuracy (94.46% compared to 94.92%) and high similarity (96.82%).",
      "year": 2024,
      "venue": "Journal of Science and Technique",
      "authors": [
        "X. Pham"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/389f1fad962f58799327a0112526452d2da5157d",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "title": "Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights",
      "abstract": "Applications driven by neural networks (NNs) have been advancing various work flows in industries and everyday life. FPGA accelerators are a popular low latency solution for NN inference in the cloud, edge devices and critical systems, offering efficiency and availability. Additionally, cloud FPGAs enable maximizing resource utilization by sharing one device with multiple users in a multi-tenant scenario. However, due to the high energy costs, hardware requirements and time consumption for training an NN, using machine learning services or acquiring pre-trained models has become increasingly popular. This creates a trust issue that potentially puts the privacy of the user at risk. Specifically, malicious mechanisms may be hidden in the weights of the NN. We show that by manipulating the training process of an NN, the power consumption and resulting leakage can be manipulated to correlate strongly with the networks output, allowing the reliable recovery of the classification results through remote power side-channel analysis. In comparison to power traces from a benign model, which leak less information, our trained-in Trojan Side-Channel enhances the credibility and reliability of the stolen outputs, making them more usable and valuable for malicious intent.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "title": "Revisiting Black-box Ownership Verification for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data, enabling applications in various domains. Yet, GNNs are vulnerable to model extraction attacks, imposing risks to intellectual property. To mitigate model extraction attacks, model ownership verification is considered an effective method. However, throughout a series of empirical studies, we found that the existing GNN ownership verification methods either mandate unrealistic conditions or present unsatisfactory accuracy under the most practical settings\u2014the black-box setting where the verifier only requires access to the final output (e.g., posterior probability) of the target model and the suspect model.Inspired by the studies, we propose a new, black-box GNN ownership verification method that involves local independent models and shadow surrogate models to train a classifier for performing ownership verification. Our method boosts the verification accuracy by exploiting two insights: (1) We consider the overall behaviors of the target model for decision-making, better utilizing its holistic fingerprinting; (2) We enrich the fingerprinting of the target model by masking a subset of features of its training data, injecting extra information to facilitate ownership verification.To assess the effectiveness of our proposed method, we perform an intensive series of evaluations with 5 popular datasets, 5 mainstream GNN architectures, and 16 different settings. Our method achieves nearly perfect accuracy with a marginal impact on the target model in all cases, significantly outperforming the existing methods and enlarging their practicality. We also demonstrate that our method maintains robustness against adversarial attempts to evade the verification.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ruikai Zhou",
        "Kang Yang",
        "Xiuling Wang",
        "Wendy Hui Wang",
        "Jun Xu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "title": "Protecting Confidential Virtual Machines from Hardware Performance Counter Side Channels",
      "abstract": "In modern cloud platforms, it is becoming more important to preserve the privacy of guest virtual machines (VMs) from the untrusted host. To this end, Secure Encrypted Virtualization (SEV) is developed as a hardware extension to protect VMs by encrypting their memory pages and register states. Unfortunately, such confidential VMs are still vulnerable to micro-architectural side channels, and Hardware Performance Counters (HPCs) are a prominent information leakage source. To make matters worse, currently there is no systematic defense against the HPC side channels. We introduce Aegis, a unified framework for demystifying the inherent relations between the instruction execution and HPC event statistics, and defending VMs against HPC side channels with provable privacy guarantee and minimal performance overhead. Aegis consists of three modules. Application Profiler profiles the application offline and adopts information theory to quantitatively estimate the vulnerability of HPC events. Event Fuzzer leverages the fuzzing technique to automatically generate interesting inputs, i.e., instruction sequences, that can effectively alter the HPC observations. Event Obfuscator injects noisy instructions into the protected VM based on the differential privacy mechanisms for high efficiency and privacy. We present three case studies to demonstrate that Aegis can defeat different types of HPC side-channel attacks (i.e., website fingerprinting, DNN model extraction, keystroke sniffing). Evaluations show that Aegis can effectively decrease the attack accuracy from 90% to 2%, with only 3% overhead on the application execution time and 7% overhead on the CPU usage.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Xiaoxuan Lou",
        "Kangjie Chen",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "title": "Exploring Zero-Day Attacks on Machine Learning and Deep Learning Algorithms",
      "abstract": "In the rapidly evolving field of artificial intelligence, machine learning (ML) and deep learning (DL) algorithms have emerged as powerful tools for solving complex problems in various domains, including cyber security. However, as these algorithms become increasingly prevalent, they also face new security challenges. One of the most significant of these challenges is the threat of zero-day attacks, which exploit unknown and unpredictable vulnerabilities in the algorithms or the data they process. \nThis paper provides a comprehensive overview of zero-day attacks on ML/DL algorithms, exploring their types, causes, effects, and potential countermeasures. The paper begins by introducing the concept and definition of zero-day attacks, providing a clear understanding of this emerging threat. It then reviews the existing research on zero-day attacks on ML/DL algorithms, focusing on three main categories: data poisoning attacks, adversarial input attacks, and model stealing attacks. Each of these attack types poses unique challenges and requires specific countermeasures. \nThe paper also discusses the potential impacts and risks of these attacks on various application domains. For instance, in facial expression recognition, an adversarial input attack could lead to misclassification of emotions, with serious implications for user experience and system integrity. In object classification, a data poisoning attack could cause the algorithm to misidentify critical objects, potentially endangering human lives in applications like autonomous driving. In satellite intersection recognition, a model stealing attack could compromise national security by revealing sensitive information. \nFinally, the paper presents some possible protection methods against zero-day attacks on ML/DL algorithms. These include anomaly detection techniques to identify unusual patterns in the data or the algorithm\u2019s behaviour, model verification and validation methods to ensure the algorithm\u2019s correctness and robustness, federated learning approaches to protect the privacy of the training data, and differential privacy techniques to add noise to the data or the algorithm\u2019s outputs to prevent information leakage. \nThe paper concludes by highlighting some open issues and future directions for research in this area, emphasizing the need for ongoing efforts to secure ML/DL algorithms against zero-day attacks.",
      "year": 2024,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "Marie Kov\u00e1\u0159ov\u00e1"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "pdf_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/2310/2134",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b3744d928fcd84b7e2296d5983ba199a300955e0",
      "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
      "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Younghan Lee",
        "Sohee Jun",
        "Yungi Cho",
        "Woorim Han",
        "Hyungon Moon",
        "Y. Paek"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b3744d928fcd84b7e2296d5983ba199a300955e0",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "title": "Secure AI Systems: Emerging Threats and Defense Mechanisms",
      "abstract": "The capability of artificial intelligence (AI), increasingly embedded in critical domains, faces a complex array of security threats. It has motivated researchers to explore the security vulnerability of AI solutions and propose effective countermeasures. This article offers a comprehensive exploration of diverse attacks on AI models, including backdoors (Trojans), adversarial, fault injection, data poisoning, model inversion, model extraction, membership inference attacks, etc. These security vulnerabilities are classified into two broad categories, namely, Supply Chain Attacks and Runtime Attacks. We highlight threat models, attack strategies, and defenses to secure AI systems against these attacks. The work also underscores the significance of developing secure and robust AI models and their implementation to safeguard sensitive data and embedded systems. We present some emerging research directions on secure AI systems.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "S. Bhunia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "befa5df856721b10c50d034594fbc7194d96e386",
      "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
      "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hanbo Huang",
        "Yihan Li",
        "Bowen Jiang",
        "Bowen Jiang",
        "Lin Liu",
        "Ruoyu Sun",
        "Zhuotao Liu",
        "Shiyu Liang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/befa5df856721b10c50d034594fbc7194d96e386",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "title": "Bio-Rollup: a new privacy protection solution for biometrics based on two-layer scalability-focused blockchain",
      "abstract": "The increased use of artificial intelligence generated content (AIGC) among vast user populations has heightened the risk of private data leaks. Effective auditing and regulation remain challenging, further compounding the risks associated with the leaks involving model parameters and user data. Blockchain technology, renowned for its decentralized consensus mechanism and tamper-resistant properties, is emerging as an ideal tool for documenting, auditing, and analyzing the behaviors of all stakeholders in machine learning as a service (MLaaS). This study centers on biometric recognition systems, addressing pressing privacy and security concerns through innovative endeavors. We conducted experiments to analyze six distinct deep neural networks, leveraging a dataset quality metric grounded in the query output space to quantify the value of the transfer datasets. This analysis revealed the impact of imbalanced datasets on training accuracy, thereby bolstering the system\u2019s capacity to detect model data thefts. Furthermore, we designed and implemented a novel Bio-Rollup scheme, seamlessly integrating technologies such as certificate authority, blockchain layer two scaling, and zero-knowledge proofs. This innovative scheme facilitates lightweight auditing through Merkle proofs, enhancing efficiency while minimizing blockchain storage requirements. Compared to the baseline approach, Bio-Rollup restores the integrity of the biometric system and simplifies deployment procedures. It effectively prevents unauthorized use through certificate authorization and zero-knowledge proofs, thus safeguarding user privacy and offering a passive defense against model stealing attacks.",
      "year": 2024,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Jian Yun",
        "Yusheng Lu",
        "Xinyang Liu",
        "Jingdan Guan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "pdf_url": "https://doi.org/10.7717/peerj-cs.2268",
      "publication_date": "2024-09-09",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "title": "EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning",
      "abstract": "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA\".",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jingtao Li",
        "Xing Chen",
        "Li Yang",
        "A. S. Rakin",
        "Deliang Fan",
        "Chaitali Chakrabarti"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/29258/30374",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "title": "Defending Against Label-Only Attacks via Meta-Reinforcement Learning",
      "abstract": "Machine learning models are susceptible to a range of adversarial activities. These attacks are designed to either infer private information from the target model or deceive it. For instance, an attacker may attempt to discern if a given data example is from the model\u2019s training set (membership inference attacks) or create adversarial examples to mislead the model to make incorrect predictions (adversarial example attacks). Numerous defense methods have been proposed to counter these attacks. However, these methods typically share two common limitations. Firstly, most are not designed to address label-only attacks, which is a newly emerged kind of attacks that rely solely on the hard labels predicted by the target model. Secondly, they are often developed to mitigate specific attacks rather than universally various attacks. To address these limitations, this paper proposes a novel defense method that focuses on the most challenging attacks, i.e., label-only attacks, and can handle various types of label-only attacks. The key idea is to strategically modify the target model\u2019s predicted labels using a meta-reinforcement learning technique. This ensures that attackers receive incorrect labels while benign users continue to receive correct labels. Notably, the defender, i.e., the owner of the target model, can make effective decisions without knowledge of the attacker\u2019s behavior. The experimental results demonstrate that our proposed method is an effective defense against a range of attacks, including label-only model stealing, label-only membership inference, label-only model inversion, and label-only adversarial example attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Kun Gao",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "20993724635d425f696e1c7e38c9e8da025ff757",
      "title": "FedMCT: A Federated Framework for Intellectual Property Protection and Malicious Client Tracking",
      "abstract": "In the era of big data, federated learning (FL) emerges as a solution to train models collectively without exposing individual data, maintaining similar accuracy to models trained on shared datasets. However, challenges arise with the advent of privacy inference attacks and model theft, posing significant threats to the privacy of FL models, especially regarding intellectual property (IP) protection. This paper introduces FedMCT (Federated Malicious Client Tracking), a novel framework addressing these challenges in the FL context. The FedMCT framework is a new approach to protect IP rights of FL clients and track cheaters, which can improve efficiency in resource-heterogeneous environments. By embedding unique watermarks or fingerprints in Deep Neural Network (DNN) models, we can protect model IP. We employ a configuration round before watermark embedding, segmenting clients based on performance for tiered model watermarking. We also propose a tiered watermarking and traitor tracking mechanism, which reduces the tracking time and ensures high traitor tracking efficiency. Extensive experiments validate our solution\u2019s efficacy in maintaining original model performance, watermark privacy, and detectability, robust against various attacks, demonstrating superior traitor tracing efficiency compared to existing frameworks.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Qianyi Chen",
        "Peijia Zheng",
        "Yusong Du",
        "Weiqi Luo",
        "Hongmei Liu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/20993724635d425f696e1c7e38c9e8da025ff757",
      "pdf_url": "",
      "publication_date": "2024-02-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "55414dc21b091006bf868b28008c9fc30fa38dca",
      "title": "Model Extraction Attack against On-device Deep Learning with Power Side Channel",
      "abstract": "The proliferation of on-device deep learning models in resource-constrained environments has led to significant advancements in privacy-preserving machine learning. However, the deployment of these models also introduces new security challenges, one of which is the vulnerability to model extraction attacks. In this paper, we investigate a novel attack with power side channel to extract on-device deep learning model deployed, which poses a substantial threat to on-device deep learning systems. By carefully monitoring power consumption during inference, an adversary can gain insights into the model\u2019s internal behavior, potentially compromising the model\u2019s intellectual property and sensitive data. Through experiments on a real-world embedded device (Jetson Nano) and various types of deep learning models, we demonstrate that the proposed attack can extract models with high fidelity. Based on experiments, we find that the power side channel-assisted model extraction attack can achieve high attacking success rate, up to 96.7% and 87.5% under close world and open world settings. This research sheds light on the evolving landscape of security threats in the context of on-device DL and provides valuable insights into safeguarding these models from potential adversaries.",
      "year": 2024,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Jiali Liu",
        "Han Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/55414dc21b091006bf868b28008c9fc30fa38dca",
      "pdf_url": "",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "title": "MTL-Leak: Privacy Risk Assessment in Multi-Task Learning",
      "abstract": "Multi-task learning (MTL) supports simultaneous training over multiple related tasks and learns the shared representation. While improving the generalization ability of training on a single task, MTL has higher privacy risk than traditional single-task learning because more sensitive information is extracted and learned in a correlated manner. Unfortunately, very few works have attempted to address the privacy risks posed by MTL. In this article, we first investigate such risk by designing model extraction attack (MEA) and membership inference attack (MIA) in MTL. Then we evaluate the privacy risks on six MTL model architectures and two popular MTL datasets, whose results show that both the number of tasks and the complexity of training data play an important role in the attack performance. Our investigation shows that MTL is more vulnerable than traditional single-task learning under both attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongyang Yan",
        "Anli Yan",
        "Li Hu",
        "Jiaming Liang",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "title": "MCD: Defense Against Query-Based Black-Box Surrogate Attacks",
      "abstract": "Deep neural networks (DNNs) is susceptible to surrogate attacks, where adversaries use surrogate data and corresponding outputs from the target model to build their own stolen model. Model stealing attacks jeopardize model privacy and model owners' commercial benefits. To address this issue, this paper proposes a hybrid protection approach-Maximize the confidence differences between benign samples and adversarial samples (MCD), to protect models from theft. Firstly, the LogitNorm approach is used to overcome the overconfidence problem in adversary query classification. Then, samples are divided into four groups according to ES and RS. Different groups are poisoned by different degrees. In addition to enhancing defensive performance and accounting for model integrity, the MCD uses a trigger to confirm the cloned model's owner. Experimental results show that the MCD defends against a variety of original models and attack techniques well. Against KnockoffNets and DFME attacks, the MCD yields an average defense performance of 54.58 % on five datasets, which is a great improvement over other defenses. Compared to other poisoning techniques, the Strong Poisoning (SP) module reduces the adversary's accuracy by 48.23 % on average. Additionally, the MCD overcomes the issue of OOD overconfidence while safeguarding the model accuracy in OOD detection and reduces the misclassification rate of ID samples for multiple OOD datasets.",
      "year": 2024,
      "venue": "IEEE International Conference on Systems, Man and Cybernetics",
      "authors": [
        "Yiwen Zou",
        "Wing W. Y. Ng",
        "Xueli Zhang",
        "Brick Loo",
        "Xingfu Yan",
        "Ran Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "pdf_url": "",
      "publication_date": "2024-10-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8c34055674248b2b3c2de461eab414e0704f9782",
      "title": "Model theft attack against a tinyML application running on an Ultra-Low-Power Open-Source SoC",
      "abstract": "With the advent of tinyML, IoT devices have expanded their range of operations from simple data gathering and transmission to full-fledged inference. This expansion has been further enabled by the rise in popularity of open-source hardware, with the RISC-V architecture being the most prominent example. TinyML's decentralization can solve the current privacy and security issues of IoT infrastructures. However, it also shifts the burden of security on already resource-constrained devices. Ultra-low-power devices, in particular, often sacrifice security features for energy and area efficiency. This work aims at showing that, in the context of edge computing based on open-source hardware, neglecting hardware security features for the sake of efficiency is not an acceptable trade-off with respect to AI security.",
      "year": 2024,
      "venue": "ACM International Conference on Computing Frontiers",
      "authors": [
        "A. Porsia",
        "A. Ruospo",
        "Ernesto S\u00e1nchez"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8c34055674248b2b3c2de461eab414e0704f9782",
      "pdf_url": "",
      "publication_date": "2024-05-07",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "title": "MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction",
      "abstract": "The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes\"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Gurve",
        "S. Behera",
        "Satyadev Ahlawat",
        "Yamuna Prasad"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "model extraction",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "title": "Enhancing Data-Free Model Stealing Attack on Robust Models",
      "abstract": "Machine Learning Model Deployment as a Service (MLaaS) has surged in popularity, offering substantial business value. However, the significant resources and costs required to train models have raised concerns about Model Stealing Attacks (MSAs), where attackers create a clone model to replicate the knowledge of a victim model without access to its parameters. In data-free MSA, attackers also lack access to the training data for the victim model. In this setting, existing MSA methods rely on Generative Adversarial Networks (GANs) to generate images to query the victim model. However, GANs are known to suffer from model collapse, resulting in limited diversity in generated images. The lack of diversity in generated images will significantly impact the accuracy of the clone model, especially in stealing robust models trained with adversarial training. Recent studies have demonstrated that Denoising Diffusion Probabilistic Models (DDPMs) outperform GANs in generating images with greater diversity. In our data-free MSA framework, using DDPM as the generator to steal robust models significantly increases the effectiveness, improving the accuracy of the clone model from 21.34% to 60.23% compared to the GANs-based approach DFME, and requires fewer queries. We further use denoise diffusion GANs to address the problem of low sampling speed of DDPM, while retaining the advantage of its high sample diversity and obtaining better results.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jianping He",
        "Haichang Gao",
        "Yunyi Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
      "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jinyin Chen",
        "Haonan Ma",
        "Haibin Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "title": "Stealing Brains: From English to Czech Language Model",
      "abstract": ": We present a simple approach for efficiently adapting pre-trained English language models to generate text in lower-resource language, specifically Czech. We propose a vocabulary swap method that leverages parallel corpora to map tokens between languages, allowing the model to retain much of its learned capabilities. Experiments conducted on a Czech translation of the TinyStories dataset demonstrate that our approach significantly outperforms baseline methods, especially when using small amounts of training data. With only 10% of the data, our method achieves a perplexity of 17.89, compared to 34.19 for the next best baseline. We aim to contribute to work in the field of cross-lingual transfer in natural language processing and we propose a simple to implement, computationally efficient method tested in a controlled environment.",
      "year": 2024,
      "venue": "International Joint Conference on Computational Intelligence",
      "authors": [
        "Petr Hyner",
        "Petr Marek",
        "D. Adamczyk",
        "Jan Hula",
        "Jan \u0160ediv\u00fd"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "title": "On the Security Vulnerabilities of MRAM-Based in-Memory Computing Architectures Against Model Extraction Attacks",
      "abstract": "This paper studies the security vulnerabilities of embedded nonvolatile memory (eNVM)-based in-memory computing (IMC) architectures to model extraction attacks (MEAs). These attacks allow the reconstruction of private training data from trained model parameters thereby leaking sensitive user information. The presence of analog noise in eNVM-based IMC computation suggests that they may be intrinsically robust to MEA. However, we show that this conjecture is false. Specifically, we consider the scenario where an attacker aims to retrieve model parameters via input-output query access, and propose three attacks that exploit the statistics of the IMC computation. We demonstrate the efficacy of these attacks in extracting the model parameters of the last layer of a ResNet-20 network from the bitcell array of an MRAM-based IMC prototype in 22 nm process. Employing the proposed MEAs, the attacker obtains a CIFAR-10 accuracy within 0.1 % of that of a $N=64$ dimensional, $7 \\mathrm{b} \\times 4 \\mathrm{b}$ fixed-point digital baseline. To the best of our knowledge, this is the first work to demonstrate MEAs for eNVM-based IMC on a real-life IC prototype. Our results indicate the critical importance of investigating the security vulnerabilities of IMCs in general, and eNVM-based IMCs, in particular.",
      "year": 2024,
      "venue": "International Conference on Computer Aided Design",
      "authors": [
        "Saion K. Roy",
        "Naresh R. Shanbhag"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "pdf_url": "",
      "publication_date": "2024-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "title": "CaBaGe: Data-Free Model Extraction using ClAss BAlanced Generator Ensemble",
      "abstract": "Machine Learning as a Service (MLaaS) is often provided as a pay-per-query, black-box system to clients. Such a black-box approach not only hinders open replication, validation, and interpretation of model results, but also makes it harder for white-hat researchers to identify vulnerabilities in the MLaaS systems. Model extraction is a promising technique to address these challenges by reverse-engineering black-box models. Since training data is typically unavailable for MLaaS models, this paper focuses on the realistic version of it: data-free model extraction. We propose a data-free model extraction approach, CaBaGe, to achieve higher model extraction accuracy with a small number of queries. Our innovations include (1) a novel experience replay for focusing on difficult training samples; (2) an ensemble of generators for steadily producing diverse synthetic data; and (3) a selective filtering process for querying the victim model with harder, more balanced samples. In addition, we create a more realistic setting, for the first time, where the attacker has no knowledge of the number of classes in the victim training data, and create a solution to learn the number of classes on the fly. Our evaluation shows that CaBaGe outperforms existing techniques on seven datasets -- MNIST, FMNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet-subset, and Tiny ImageNet -- with an accuracy improvement of the extracted models by up to 43.13%. Furthermore, the number of queries required to extract a clone model matching the final accuracy of prior work is reduced by up to 75.7%.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jonathan Rosenthal",
        "Shanchao Liang",
        "Kevin Zhang",
        "Lin Tan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "pdf_url": "",
      "publication_date": "2024-09-16",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "title": "FP-OCS: A Fingerprint Based Ownership Detection System for Insulator Fault Detection Model",
      "abstract": "In smart grids, the robustness and reliability of the transmission system depend on the operational integrity of the insulators. The success of deep learning has facilitated the development of advanced fault detection algorithms for classifying and identifying insulator states. However, these machine learning based detection systems rely on high-quality datasets, making them potential targets for intellectual property theft, and model extraction attacks pose risks of privacy breaches and unauthorized exploitation. To address the challenge of protecting neural network ownership in this situation, we introduce a fingerprint based ownership detection system for insulator fault detection model FP-OCS. FP-OCS uses model extraction attack to generate a series of piracy models, and uses white-box access victim models to generate similarity models and universal adversarial perturbation. The system\u2019s fingerprint generation module augments the original dataset to craft distinctive model fingerprints. Subsequently, FP-OCS\u2019s encoder training module extends the fingerprint dataset using K-means methods and uses contrast learning to train the encoder network and the projection network. Upon finalization of training, FP-OCS evaluates a model\u2019s authenticity by matching its derived fingerprint against the victim model. We evaluated the effectiveness of the system using data-enhanced InsPLAD datasets. Our findings prove that FP-OCS can achieve 100% accuracy in Ownership Detection tasks with 50% similarity dividing line.",
      "year": 2024,
      "venue": "International Conference on Innovative Computing and Cloud Computing",
      "authors": [
        "Wenqian Xu",
        "Fazhong Liu",
        "Ximing Zhang",
        "Yixin Jiang",
        "Tian Dong",
        "Zhihong Liang",
        "Yiwei Yang",
        "Yan Meng",
        "Haojin Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
      "abstract": "Pre-trained encoders available online have been widely adopted to build downstream machine learning (ML) services, but various attacks against these encoders also post security and privacy threats toward such a downstream ML service paradigm. We unveil a new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which can extract sensitive encoder information from a targeted downstream ML service that can then be used to promote other ML attacks against the targeted service. By only providing API accesses to a targeted downstream service and a set of candidate encoders, the PEI attack can successfully infer which encoder is secretly used by the targeted service based on candidate ones. Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat. We empirically verify the effectiveness of the PEI attack on vision encoders. we first conduct PEI attacks against two downstream services (i.e., image classification and multimodal generation), and then show how PEI attacks can facilitate other ML attacks (i.e., model stealing attacks vs. image classification models and adversarial attacks vs. multimodal generative models). Our results call for new security and privacy considerations when deploying encoders in downstream services. The code is available at https://github.com/fshp971/encoder-inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shaopeng Fu",
        "Xuexue Sun",
        "Ke Qing",
        "Tianhang Zheng",
        "Di Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "pdf_url": "",
      "publication_date": "2024-08-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7727e2a4116a63759d0a942b31732c2014007029",
      "title": "LDPKiT: Superimposing Remote Queries for Privacy-Preserving Local Model Training",
      "abstract": "Users of modern Machine Learning (ML) cloud services face a privacy conundrum -- on one hand, they may have concerns about sending private data to the service for inference, but on the other hand, for specialized models, there may be no alternative but to use the proprietary model of the ML service. In this work, we present LDPKiT, a framework for non-adversarial, privacy-preserving model extraction that leverages a user's private in-distribution data while bounding privacy leakage. LDPKiT introduces a novel superimposition technique that generates approximately in-distribution samples, enabling effective knowledge transfer under local differential privacy (LDP). Experiments on Fashion-MNIST, SVHN, and PathMNIST demonstrate that LDPKiT consistently improves utility while maintaining privacy, with benefits that become more pronounced at stronger noise levels. For example, on SVHN, LDPKiT achieves nearly the same inference accuracy at $\\epsilon=1.25$ as at $\\epsilon=2.0$, yielding stronger privacy guarantees with less than a 2% accuracy reduction. We further conduct sensitivity analyses to examine the effect of dataset size on performance and provide a systematic analysis of latent space representations, offering theoretical insights into the accuracy gains of LDPKiT.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Kexin Li",
        "Aastha Mehta",
        "David Lie"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7727e2a4116a63759d0a942b31732c2014007029",
      "pdf_url": "",
      "publication_date": "2024-05-25",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d032a269b465df9116f080ff9c56049bc581acb4",
      "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
      "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Shuai Wang",
        "Cuiyun Gao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d032a269b465df9116f080ff9c56049bc581acb4",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "title": "No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML",
      "abstract": "On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) against model weights and membership inference attack (MIA) against training data privacy. Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming of TEEs is the sharply increased latency (up to 50\u00d7). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading1 the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. However, the community lacks an in-depth understanding of the seemingly encouraging privacy guarantees offered by existing TSDP solutions during DNN inference. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding the optimal DNN partition configurations, which vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. Unlike existing approaches, TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the \"upper-bound\" security guarantees) with over 10\u00d7less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss. We make the code and artifacts publicly available on the Internet.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ziqi Zhang",
        "Chen Gong",
        "Yifeng Cai",
        "Yuanyuan Yuan",
        "Bingyan Liu",
        "Ding Li",
        "Yao Guo",
        "Xiangqun Chen"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "pdf_url": "https://arxiv.org/pdf/2310.07152",
      "publication_date": "2023-10-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68b560859078171978f2c040b1522f4e7668c38e",
      "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
      "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \u201ccode synthesis\u201d and \u201ccode translation:\u2019 We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Pingchuan Ma",
        "Chaowei Liu",
        "Shuai Wang",
        "Daoyuan Wu",
        "Cuiyun Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/68b560859078171978f2c040b1522f4e7668c38e",
      "pdf_url": "https://arxiv.org/pdf/2303.03012",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "81cd9575100643a3463465ec19e90ee78e122f93",
      "title": "SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap",
      "abstract": "A crucial module of the widely applied machine learning (ML) model is the model training phase, which involves large-scale training data, often including sensitive private data. ML models trained on these sensitive data suffer from significant privacy concerns since ML models can intentionally or unintendedly leak information about training data. Adversaries can exploit this information to perform privacy attacks, including model extraction, membership inference, and model inversion. While a model extraction attack steals and replicates a trained model functionality, and membership inference infers the data sample's inclusiveness to the training set, a model inversion attack has the goal of inferring the training data sample's sensitive attribute value or reconstructing the training sample (i.e., image/audio/text). Distinct and inconsistent characteristics of model inversion attack make this attack even more challenging and consequential, opening up model inversion attack as a more prominent and increasingly expanding research paradigm. Thereby, to flourish research in this relatively underexplored model inversion domain, we conduct the first-ever systematic literature review of the model inversion attack landscape. We characterize model inversion attacks and provide a comprehensive taxonomy based on different dimensions. We illustrate foundational perspectives emphasizing methodologies and key principles of the existing attacks and defense techniques. Finally, we discuss challenges and open issues in the existing model inversion attacks, focusing on the roadmap for future research directions.",
      "year": 2023,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "S. Dibbo"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/81cd9575100643a3463465ec19e90ee78e122f93",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "title": "Membership Inference Attacks Against Sequential Recommender Systems",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user\u2019s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user\u2019s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model\u2019s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model\u2019s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users\u2019 membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3d2efd249eaffbf3675957444271ca97330156a1",
      "title": "Protecting Regression Models With Personalized Local Differential Privacy",
      "abstract": "The equation-solving model extraction attack is an intuitively simple but devastating attack to steal confidential information of regression models through a sufficient number of queries. Complete mitigation is difficult. Thus, the development of countermeasures is focused on degrading the attack effectiveness as much as possible without losing the model utilities. We investigate a novel personalized local differential privacy mechanism to defend against the attack. We obfuscate the model by adding high-dimensional Gaussian noise on model coefficients. Our solution can adaptively produce the noise to protect the model on the fly. We thoroughly evaluate the performance of our mechanisms using real-world datasets. The experiment shows that the proposed scheme outperforms the existing differential-privacy-enabled solution, i.e., 4 times more queries are required to achieve the same attack result. We also plan to publish the relevant codes to the community for further research.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiaoguang Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/3d2efd249eaffbf3675957444271ca97330156a1",
      "pdf_url": "",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
      "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wen",
        "Tianhao Wang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "title": "B3: Backdoor Attacks against Black-box Machine Learning Models",
      "abstract": "Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the trigger. The reason for backdoor attacks is that resource-limited users usually download sophisticated models from model zoos or query the models from MLaaS rather than training a model from scratch, thus a malicious third party has a chance to provide a backdoored model. In general, the more precious the model provided (i.e., models trained on rare datasets), the more popular it is with users. In this article, from a malicious model provider perspective, we propose a black-box backdoor attack, named B3, where neither the rare victim model (including the model architecture, parameters, and hyperparameters) nor the training data is available to the adversary. To facilitate backdoor attacks in the black-box scenario, we design a cost-effective model extraction method that leverages a carefully constructed query dataset to steal the functionality of the victim model with a limited budget. As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted misclassification label through the neuron with the highest impact on the targeted label. Extensive experiments have been conducted on various simulated deep learning models and the commercial API of Alibaba Cloud Compute Service. We demonstrate that B3 has a high attack success rate and maintains high prediction accuracy for benign inputs. It is also shown that B3 is robust against state-of-the-art defense strategies against backdoor attacks, such as model pruning and NC.",
      "year": 2023,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Huayang Huang",
        "Qian Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "pdf_url": "",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "title": "Detection of Crucial Power Side Channel Data Leakage in Neural Networks",
      "abstract": "Neural network (NN) accelerators are now extensively utilized in a range of applications that need a high degree of security, such as driverless cars, NLP, and image recognition. Due to privacy issues and the high cost, hardware implementations contained within NN Propagators were often not accessible for general populace. Additionally with power and time data, accelerators also disclose critical data by electro-magnetic (EM) sided channels. Within this study, we demonstrate a side-channel information-based attack that can successfully steal models from large-scale NN accelerators deployed on real-world hardware. The use of these accelerators is widespread. The proposed method of attack consists of two distinct phases: 1) Using EM side-channel data to estimate networking's underlying architecture; 2) Using margin-dependent, attackers learning actively in estimating parameters, notably weights. Deducing the underlying network structure from EM sidechannel data. Inferring the underlying network structure from EM sidechannel data. Experimental findings demonstrate that the disclosed attack technique can be used to precisely retrieve the large-scale NN via the use of EM side-channel information leaking. Overall, our attack shows how critical it is to conceal electromagnetic (EM) traces for massive NN accelerators in practical settings.",
      "year": 2023,
      "venue": "International Telecommunication Networks and Applications Conference",
      "authors": [
        "A. A. Ahmed",
        "Mohammad Kamrul Hasan",
        "Nurhizam Safie Mohd Satar",
        "N. Nafi",
        "A. Aman",
        "S. Islam",
        "Saif Aamer Fadhil"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "pdf_url": "",
      "publication_date": "2023-11-29",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
      "abstract": "Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanmin Tang",
        "Jing Yu",
        "Keke Gai",
        "Xiangyang Qu",
        "Yue Hu",
        "Gang Xiong",
        "Qi Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b885e291f660df34d9f22777dc0678bdf8e0860d",
      "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data",
      "abstract": "We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\\sim 60-95\\%$ of true model's accuracy and uses $\\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\\sim 2-4\\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to $84-96\\%$ accuracy under membership inference attacks. Experimental results validate that Marich is query-efficient, and capable of performing task-accurate, high-fidelity, and informative model extraction.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pratik Karmakar",
        "D. Basu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b885e291f660df34d9f22777dc0678bdf8e0860d",
      "pdf_url": "http://arxiv.org/pdf/2302.08466",
      "publication_date": "2023-02-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c103408ff825aad19d715edc01025a8c3fccdb4",
      "title": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles",
      "abstract": "Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to pruned models, and when applied across GPUs. We find that EZClone correctly predicts DNN architectures for the entire set of PyTorch vision architectures with 100% accuracy. No other work has shown this degree of architecture prediction accuracy with the same adversarial constraints or using aggregate side-channel information. Prior work has shown that, once a DNN has been successfully cloned, further attacks such as model evasion or model inversion can be accelerated significantly.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jonah O'Brien Weiss",
        "Tiago A. O. Alves",
        "S. Kundu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3c103408ff825aad19d715edc01025a8c3fccdb4",
      "pdf_url": "http://arxiv.org/pdf/2304.03388",
      "publication_date": "2023-04-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "468a78d431be0d6290bb3007e6920e15761b751e",
      "title": "SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models",
      "abstract": "While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community. and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Zheng Li",
        "Ziqing Yang",
        "Xinlei He",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/468a78d431be0d6290bb3007e6920e15761b751e",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "title": "AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the decision-making processes of AI models. However, the data used for such explanations can pose security and privacy risks. Existing literature identifies attacks on machine learning models, including membership inference, model inversion, and model extraction attacks. These attacks target either the model or the training data, depending on the settings and parties involved. XAI tools can increase the vulnerability of model extraction attacks, which is a concern when model owners prefer black-box access, thereby keeping model parameters and architecture private. To exploit this risk, we propose AUTOLYCUS, a novel retraining (learning) based model extraction attack framework against interpretable models under black-box settings. As XAI tools, we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley values (SHAP) to infer decision boundaries and create surrogate models that replicate the functionality of the target model. LIME and SHAP are mainly chosen for their realistic yet information-rich explanations, coupled with their extensive adoption, simplicity, and usability. We evaluate AUTOLYCUS on six machine learning datasets, measuring the accuracy and similarity of the surrogate model to the target model. The results show that AUTOLYCUS is highly effective, requiring significantly fewer queries compared to state-of-the-art attacks, while maintaining comparable accuracy and similarity. We validate its performance and transferability on multiple interpretable ML models, including decision trees, logistic regression, naive bayes, and k-nearest neighbor. Additionally, we show the resilience of AUTOLYCUS against proposed countermeasures.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "pdf_url": "",
      "publication_date": "2023-02-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eacb66ac30b489f704dedae7abc6e98429f95c88",
      "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
      "abstract": "As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Guangjing Wang",
        "Ce Zhou",
        "Yuanda Wang",
        "Bocheng Chen",
        "Hanqing Guo",
        "Qiben Yan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eacb66ac30b489f704dedae7abc6e98429f95c88",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "761cb683cf1bf94b878f6d7527600c2c62aee796",
      "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
      "abstract": "While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yi Xie",
        "Jie Zhang",
        "Shiqian Zhao",
        "Tianwei Zhang",
        "Xiaofeng Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/761cb683cf1bf94b878f6d7527600c2c62aee796",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "title": "ShrewdAttack: Low Cost High Accuracy Model Extraction",
      "abstract": "Machine learning as a service (MLaaS) plays an essential role in the current ecosystem. Enterprises do not need to train models by themselves separately. Instead, they can use well-trained models provided by MLaaS to support business activities. However, such an ecosystem could be threatened by model extraction attacks\u2014an attacker steals the functionality of a trained model provided by MLaaS and builds a substitute model locally. In this paper, we proposed a model extraction method with low query costs and high accuracy. In particular, we use pre-trained models and task-relevant data to decrease the size of query data. We use instance selection to reduce query samples. In addition, we divided query data into two categories, namely low-confidence data and high-confidence data, to reduce the budget and improve accuracy. We then conducted attacks on two models provided by Microsoft Azure as our experiments. The results show that our scheme achieves high accuracy at low cost, with the substitution models achieving 96.10% and 95.24% substitution while querying only 7.32% and 5.30% of their training data on the two models, respectively. This new attack approach creates additional security challenges for models deployed on cloud platforms. It raises the need for novel mitigation strategies to secure the models. In future work, generative adversarial networks and model inversion attacks can be used to generate more diverse data to be applied to the attacks.",
      "year": 2023,
      "venue": "Entropy",
      "authors": [
        "Yang Liu",
        "Ji Luo",
        "Yi Yang",
        "Xuan Wang",
        "M. Gheisari",
        "Feng Luo"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "pdf_url": "https://www.mdpi.com/1099-4300/25/2/282/pdf?version=1675337976",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks",
      "abstract": "Artificial Intelligence (AI) hardware accelerators have been widely adopted to enhance the efficiency of deep learning applications. However, they also raise security concerns regarding their vulnerability to power side-channel attacks (SCA). In these attacks, the adversary exploits unintended communication channels to infer sensitive information processed by the accelerator, posing significant privacy and copyright risks to the models. Advanced machine learning algorithms are further employed to facilitate the side-channel analysis and exacerbate the privacy issue of AI accelerators. Traditional defense strategies naively inject execution noise to the runtime of AI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguard FPGA-based AI accelerators and mitigate model extraction threats via power-based SCAs. The key insight of AIAShield is to leverage the prominent adversarial attack technique from the machine learning community to craft delicate noise, which can significantly obfuscate the adversary's side-channel observation while incurring minimal overhead to the execution of the protected model. At the hardware level, we design a new module based on ring oscillators to achieve fine-grained noise generation. At the algorithm level, we repurpose Neural Architecture Search to worsen the adversary's extraction results. Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA) demonstrate that AIAShield outperforms existing solutions with excellent transferability.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xiaobei Yan",
        "Chip Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "34bed407d65517ed2c8b98bab3a33da175677c59",
      "title": "A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots",
      "abstract": "Building advanced machine learning (ML) models requires expert knowledge and many trials to discover the best architecture and hyperparameter settings. Previous work demonstrates that model information can be leveraged to assist other attacks, such as membership inference, generating adversarial examples. Therefore, such information, e.g., hyperparameters, should be kept confidential. It is well known that an adversary can leverage a target ML model's output to steal the model's information. In this paper, we discover a new side channel for model information stealing attacks, i.e., models' scientific plots which are extensively used to demonstrate model performance and are easily accessible. Our attack is simple and straightforward. We leverage the shadow model training techniques to generate training data for the attack model which is essentially an image classifier. Extensive evaluation on three benchmark datasets shows that our proposed attack can effectively infer the architecture/hyperparameters of image classifiers based on convolutional neural network (CNN) given the scientific plot generated from it. We also reveal that the attack's success is mainly caused by the shape of the scientific plots, and further demonstrate that the attacks are robust in various scenarios. Given the simplicity and effectiveness of the attack method, our study indicates scientific plots indeed constitute a valid side channel for model information stealing attacks. To mitigate the attacks, we propose several defense mechanisms that can reduce the original attacks' accuracy while maintaining the plot utility. However, such defenses can still be bypassed by adaptive attacks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Xinlei He",
        "Yun Shen",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/34bed407d65517ed2c8b98bab3a33da175677c59",
      "pdf_url": "http://arxiv.org/pdf/2302.11982",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
      "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model\u2019s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd\u2019s wisdom. Based on the ensemble\u2019s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Akshit Jindal",
        "Vikram Goyal",
        "Saket Anand",
        "Chetan Arora"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "pdf_url": "https://arxiv.org/pdf/2311.04588",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model theft",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "title": "Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation",
      "abstract": "Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing.",
      "year": 2023,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Vlad Hondru",
        "R. Ionescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "pdf_url": "https://arxiv.org/pdf/2309.01786",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "title": "Stolen Risks of Models with Security Properties",
      "abstract": "Verifiable robust machine learning, as a new trend of ML security defense, enforces security properties (e.g., Lipschitzness, Monotonicity) on machine learning models and achieves satisfying accuracy-security trade-off. Such security properties identify a series of evasion strategies of ML security attackers and specify logical constraints on their effects on a classifier (e.g., the classifier is monotonically increasing along some feature dimensions). However, little has been done so far to understand the side effect of those security properties on the model privacy. In this paper, we aim at better understanding the privacy impacts on security properties of robust ML models. Particularly, we report the first measurement study to investigate the model stolen risks of robust models satisfying four security properties (i.e., LocalInvariance, Lipschitzness, SmallNeighborhood, and Monotonicity). Our findings bring to light the factors that influence model stealing attacks and defense performance on models trained with security properties. In addition, to train an ML model satisfying goals in accuracy, security, and privacy, we propose a novel technique, called BoundaryFuzz, which introduces a privacy property into verifiable robust training frameworks to defend against model stealing attacks on robust models. Experimental results demonstrate the defense effectiveness of BoundaryFuzz.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yue Qin",
        "Zhuoqun Fu",
        "Chuyun Deng",
        "Xiaojing Liao",
        "Jia Zhang",
        "Haixin Duan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8796b36ed1a74a1c9fdafe2409c981102983653f",
      "title": "On Function-Coupled Watermarks for Deep Neural Networks",
      "abstract": "Well-performed deep neural networks (DNNs) generally require massive labeled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers can claim IP ownership by retrieving their embedded watermarks. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning, model pruning, and model extraction. In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model\u2019s performance on normal inputs. Specifically, on one hand, we sample inputs from the original training dataset and fuse them as watermark images. On the other hand, we randomly mask model weights during training to distribute the watermark information in the network. Our method can successfully defend against common watermark removal attacks, watermark ambiguity attacks, and existing widely used backdoor detection methods, outperforming existing solutions as demonstrated by evaluation results on various benchmarks. Our code is available at: https://github.com/cure-lab/Function-Coupled-Watermark.",
      "year": 2023,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Xiangyu Wen",
        "Yu Li",
        "Weizhen Jiang",
        "Qian-Lan Xu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8796b36ed1a74a1c9fdafe2409c981102983653f",
      "pdf_url": "https://doi.org/10.1109/jetcas.2024.3476386",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d79a93189a8acc8306504ed20a48fec9934897d9",
      "title": "Electromagnetic Field Model of Tubular Permanent Magnet Synchronous Linear Motor Based on Deep Transfer Neural Network",
      "abstract": "During high-speed operation, tubular permanent magnet linear motors with slotted stators often experience large fluctuations in thrust, overheating of the coils and irreversible demagnetization of the permanent magnets. For the first two cases, this paper proposes a deep transfer neural network (DTNN)-based electromagnetic field model for TPMLM, the specific implementation steps of which include: (1) Using different geometric parameters of TPMLM as input and average thrust, thrust fluctuation and coil copper loss as output, finite element analysis (FEA) and analytical method (AM) are used to provide the electromagnetic parameter dataset of the motor respectively, and these two datasets are used as the source and target domains for transfer learning; (2) Based on the features of the sample dataset, the source tasks corresponding to the source domain are pre-trained using DTNN, and the target tasks corresponding to the target domain are fitted by fine-tuning the model parameters. Then, in order to verify the accuracy of the proposed model, this paper compares the output prediction results with some non-parametric models, such as Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN), by dividing the target domain dataset with different scales. The results show that the best prediction results are obtained from the DTNN model, which fully combines the accuracy of FEA and the efficiency of AM, and shows better generalization ability in the case of insufficient real training data.",
      "year": 2023,
      "venue": "ACM Cloud and Autonomic Computing Conference",
      "authors": [
        "Kai Zhu",
        "Tao Wu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d79a93189a8acc8306504ed20a48fec9934897d9",
      "pdf_url": "",
      "publication_date": "2023-11-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "title": "Interesting Near-boundary Data: Inferring Model Ownership for DNNs",
      "abstract": "Deep neural networks (DNNs) require expensive training, which is why the protection of model intellectual property (IP) is becoming more critical. Recently, model stealing has emerged frequently, and many researchers design model watermarking and fingerprinting for verifying model ownership. However, attacks such as ambiguity statements have been used to break the current defense, which poses a challenge to model ownership verification. Therefore, this paper proposes an interesting near-boundary data as evidence for obtaining model ownership and innovatively proposes to infer model ownership instead of verifying model ownership. In this paper, we propose to generate the initial near-boundary data using an algorithm that adds slight noise to generate adversarial examples. We design a generator to privatize the near-boundary data. Our main observation is that the near-boundary data exhibit results close to the classification boundary in both the source model and its derived stolen model. At the end of this work, we design many experiments to verify the effectiveness of the proposed method. The experimental results demonstrate that model ownership can be inferred with high confidence. Noting that our method does not require the training data to be private, and it is extremely costly for model stealers to reuse our method.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhe Sun",
        "Zongwen Yang",
        "Zhongyu Huang",
        "Yu Zhang",
        "Jianzhong Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "title": "Power Side-Channel Attacks and Defenses for Neural Network Accelerators",
      "abstract": "Neural networks are becoming increasingly utilized in a range of real-world applications, often involving privacy-sensitive or safety-critical tasks like medical image analysis or autonomous driving. Despite their usefulness, designing and training neural networks (NNs) can be costly, both in terms of financial and energy expenses [4]. Gathering and labeling training data, actual training, and fine-tuning require considerable resources. The network models themselves are also considered confidential intellectual property (IP). Additionally, the carbon footprint of model training and development has a significant impact on the environment [5].",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "title": "Image Translation-Based Deniable Encryption against Model Extraction Attack",
      "abstract": "In cloud storage applications, data owners\u2019 original images are usually encrypted before being outsourced to the cloud for preserving data owners\u2019 privacy. However, in deep learning model-based image encryption methods, an adversary can conduct the model extraction attack to reveal the model parameters and thus restore the privacy information by obtaining numerous encrypted images. In this paper, we propose an image translation-based deniable encryption (ITDE) scheme to achieve encryption deniability and defend against model extraction attacks. Differing from traditional encryption methods in which encrypted images are visually meaningless, ITDE applies image translation to generate encrypted images in the form of human faces. Moreover, ITDE provides deniability for data owners to keep the encryption parameters private. To defend against model extraction attacks, the defense mechanism is introduced in our proposed ITDE to preserve deep learning models. Experimental results demonstrate the superiority of our proposed methods in terms of encryption deniability and privacy preservation.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Yiling Chen",
        "Yuanzhi Yao",
        "Nenghai Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "pdf_url": "",
      "publication_date": "2023-10-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "title": "Safety or Not? A Comparative Study for Deep Learning Apps on Smartphones",
      "abstract": "Recent years have witnessed an astonishing explosion in the evolution of mobile applications powered by deep learning (DL) technologies. Considering that inference of DL models in the cloud requires transferring user data to server, which is prone to the risk of user privacy leakage, many developers choose to deploy the models on local devices for executing the inference process. However, this also raises a number of other security issues, such as adversarial attacks, model stealing attacks, etc. To explore the security issues that exist in deep learning applications (DL apps), we conducted the first comprehensive comparative study of the top 200 apps in each category on Google Play. We built DLApplnspector, a vulnerability detection tool that combines dynamic and static analysis methods for dissecting apps, which helped us automate our empirical study on real-world mobile DL apps. First, we identify DL apps and extract their models by using DL Checker. Subsequently, Static Scoper is provided to detect encryption and reusability of DL models. Finally, within Dynamic Scoper, we use reverse engineering techniques on the network traffic to parse out the packets and collect side-channel information during application runtime. Our research shows that the majority of developers prefer to use open-source models, with almost 92% of models successfully parsed. This suggests that most models are unprotected. DL apps are more likely to upload user behaviour and collect private data than Non-DL apps. We provide security recommendations for developers and users to address the issues discovered.",
      "year": 2023,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Jin Au-Yeung",
        "Shanshan Wang",
        "Yuchen Liu",
        "Zhenxiang Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "Machine-Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex Machine Learning models available for clients via, e.g., a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property such as sensitive training data, optimised hyperparameters, or learned model parameters. In some cases, adversaries can create a copy of the model with (almost) identical behaviour using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies that address isolated threats have been proposed. To arrive at a comprehensive understanding why these attacks are successful and how they could be holistically defended against, a thorough systematisation of the field of model stealing is necessary. We address this by categorising and comparing model stealing attacks, assessing their performance, and exploring corresponding defence techniques in different settings. We propose a taxonomy for attack and defence approaches and provide guidelines on how to select the right attack or defence strategy based on the goal and available resources. Finally, we analyse which defences are rendered less effective by current attack strategies.",
      "year": 2022,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "A. Rauber"
      ],
      "citation_count": 143,
      "url": "https://www.semanticscholar.org/paper/d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3595292",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "title": "Dataset Inference for Self-Supervised Models",
      "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Adam Dziedzic",
        "Haonan Duan",
        "Muhammad Ahmad Kaleem",
        "Nikita Dhawan",
        "Jonas Guan",
        "Yannis Cattan",
        "Franziska Boenisch",
        "Nicolas Papernot"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "pdf_url": "http://arxiv.org/pdf/2209.09024",
      "publication_date": "2022-09-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bf87c3c380802df24628cab8f6dff90b42304f77",
      "title": "Split HE: Fast Secure Inference Combining Split Learning and Homomorphic Encryption",
      "abstract": "This work presents a novel protocol for fast secure inference of neural networks applied to computer vision applications. It focuses on improving the overall performance of the online execution by deploying a subset of the model weights in plaintext on the client's machine, in the fashion of SplitNNs. We evaluate our protocol on benchmark neural networks trained on the CIFAR-10 dataset using SEAL via TenSEAL and discuss runtime and security performances. Empirical security evaluation using Membership Inference and Model Extraction attacks showed that the protocol was more resilient under the same attacks than a similar approach also based on SplitNN. When compared to related work, we demonstrate improvements of 2.5x-10x for the inference time and 14x-290x in communication costs.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "George-Liviu Pereteanu",
        "A. Alansary",
        "Jonathan Passerat-Palmbach"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/bf87c3c380802df24628cab8f6dff90b42304f77",
      "pdf_url": "",
      "publication_date": "2022-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "title": "A Neural Network-Based Hybrid Framework for Least-Squares Inversion of Transient Electromagnetic Data",
      "abstract": "Inversion of large-scale time-domain transient electromagnetic (TEM) surveys is computationally expensive and time-consuming. The calculation of partial derivatives for the Jacobian matrix is by far the most computationally intensive task, as this requires calculation of a significant number of forward responses. We propose to accelerate the inversion process by predicting partial derivatives using an artificial neural network. Network training data for resistivity models for a broad range of geological settings are generated by computing partial derivatives as symmetric differences between two forward responses. Given that certain applications have larger tolerances for modeling inaccuracy and varying degrees of flexibility throughout the different phases of interpretation, we present four inversion schemes that provide a tunable balance between computational time and inversion accuracy when modeling TEM datasets. We improve speed and maintain accuracy with a hybrid framework, where the neural network derivatives are used initially and switched to full numerical derivatives in the final iterations. We also present a full neural network solution where neural network forward and derivatives are used throughout the inversion. In a least-squares inversion framework, a speedup factor exceeding 70 is obtained on the calculation of derivatives, and the inversion process is expedited ~36 times when the full neural network solution is used. Field examples show that the full nonlinear inversion and the hybrid approach gives identical results, whereas the full neural network inversion results in higher deviation but provides a reasonable indication about the overall subsurface geology.",
      "year": 2022,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "M. Asif",
        "T. Bording",
        "P. Maurya",
        "Bo Zhang",
        "G. Fiandaca",
        "D. Grombacher",
        "A. Christiansen",
        "E. Auken",
        "J. Larsen"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "53cfd82a7f51f7d8fb279fc6813c9e3877e3be76",
      "title": "Fast Multi-Objective Optimization of Electromagnetic Devices Using Adaptive Neural Network Surrogate Model",
      "abstract": "This article presents a fast population-based multi-objective optimization of electromagnetic devices using an adaptive neural network (NN) surrogate model. The proposed method does not require any training data or construction of a surrogate model before the optimization phase. Instead, the NN surrogate model is built from the initial population in the optimization process, and then it is sequentially updated with high-ranking individuals. All individuals were evaluated using the surrogate model. Based on this evaluation, high-ranking individuals are reevaluated using high-fidelity electromagnetic field computation. The suppression of the execution of expensive field computations effectively reduces the computing costs. It is shown that the proposed method works two to four times faster, maintaining optimization performance than the original method that does not use surrogate models.",
      "year": 2022,
      "venue": "IEEE transactions on magnetics",
      "authors": [
        "Hayaho Sato",
        "H. Igarashi"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/53cfd82a7f51f7d8fb279fc6813c9e3877e3be76",
      "pdf_url": "https://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/87016/1/OnlineMethod_v8_submitforIEEE.pdf",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab3e38c43b08ef17694e1b2a3b334c6ce1bba476",
      "title": "A Hybrid Neural Network Electromagnetic Inversion Scheme (HNNEMIS) for Super-Resolution 3-D Microwave Human Brain Imaging",
      "abstract": "Super-resolution three-dimensional (3-D) electromagnetic (EM) inversion for microwave human brain imaging is a typical high contrast EM inverse problem and requires huge computational costs. This work proposes a hybrid neural network electromagnetic inversion scheme (HNNEMIS) which contains shallow and deep neural networks to alleviate the required huge computational costs and solve this high contrast inverse problem. In the proposed scheme, semi-join back propagation neural network (SJ-BPNN) is employed to nonlinearly map the measured scattered electric field to two output channels, namely the permittivity and conductivity of scatterers, respectively. Such a semi-join strategy decreases the computational burden in training and testing processes. Then, a deep learning technique, termed U-Net, is employed to further enhance the imaging quality of the output from SJ-BPNN. To decrease the training cost and make neural networks fast convergent for human brain inversion, a novel training dataset construction strategy which contains the characteristics of human brain is also proposed. Noise-free and noisy numerical examples demonstrate that HNNEMIS has superior super-resolution inversion capabilities for human brain imaging.",
      "year": 2022,
      "venue": "IEEE Transactions on Antennas and Propagation",
      "authors": [
        "Li-Ye Xiao",
        "Ronghan Hong",
        "Le Zhao",
        "Hao Hu",
        "Q. Liu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/ab3e38c43b08ef17694e1b2a3b334c6ce1bba476",
      "pdf_url": "",
      "publication_date": "2022-08-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "title": "Towards explainable model extraction attacks",
      "abstract": "One key factor able to boost the applications of artificial intelligence (AI) in security\u2010sensitive domains is to leverage them responsibly, which is engaged in providing explanations for AI. To date, a plethora of explainable artificial intelligence (XAI) has been proposed to help users interpret model decisions. However, given its data\u2010driven nature, the explanation itself is potentially susceptible to a high risk of exposing privacy. In this paper, we first show that the existing XAI is vulnerable to model extraction attacks and then present an XAI\u2010aware dual\u2010task model extraction attack (DTMEA). DTMEA can attack a target model with explanation services, that is, it can extract both the classification and explanation tasks of the target model. More specifically, the substitution model extracted by DTMEA is a multitask learning architecture, consisting of a sharing layer and two task\u2010specific layers for classification and explanation. To reveal which explanation technologies are more vulnerable to expose privacy information, we conduct an empirical evaluation of four major explanation types in the benchmark data set. Experimental results show that the attack accuracy of DTMEA outperforms the predicted\u2010only method with up to 1.25%, 1.53%, 9.25%, and 7.45% in MNIST, Fashion\u2010MNIST, CIFAR\u201010, and CIFAR\u2010100, respectively. By exposing the potential threats on explanation technologies, our research offers the insights to develop effective tools that are able to trade off security\u2010sensitive relationships.",
      "year": 2022,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Xiaozhang Liu",
        "Hongyang Yan",
        "Teng Huang",
        "Xianmin Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "pdf_url": "",
      "publication_date": "2022-09-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "title": "Stealthy Inference Attack on DNN via Cache-based Side-Channel Attacks",
      "abstract": "The advancement of deep neural networks (DNNs) motivates the deployment in various domains, including image classification, disease diagnoses, voice recognition, etc. Since some tasks that DNN undertakes are very sensitive, the label information is confidential and contains a commercial value or critical privacy. This paper demonstrates that DNNs also bring a new security threat, leading to the leakage of label information of input instances for the DNN models. In particular, we leverage the cache-based side-channel attack (SCA), i.e., Flush-Reload on the DNN (victim) models, to observe the execution of computation graphs, and create a database of them for building a classifier that the attacker can use to decide the label information of (unknown) input instances for victim models. Then we deploy the cache-based SCA on the same host machine with victim models and deduce the labels with the attacker's classification model to compromise the privacy and confidentiality of victim models. We explore different settings and classification techniques to achieve a high attack success rate of stealing label information from the victim models. Additionally, we consider two attacking scenarios: binary attacking identifies specific sensitive labels and others while multi-class attacking targets recognize all classes victim DNNs provide. Last, we implement the attack on both static DNN models with identical architectures for all inputs and dynamic DNN models with an adaptation of architectures for different inputs to demonstrate the vast existence of the proposed attack, including DenseNet 121, DenseNet 169, VGG 16, VGG 19, MobileNet v1, and MobileNet v2. Our experiment exhibits that MobileNet v1 is the most vulnerable one with 99% and 75.6% attacking success rates for binary and multi-class attacking scenarios, respectively.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Han Wang",
        "Syed Mahbub Hafiz",
        "Kartik Patwari",
        "Chen-Nee Chuah",
        "Zubair Shafiq",
        "H. Homayoun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
      "abstract": "Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Cato Pauling",
        "Michael Gimson",
        "Muhammed Qaid",
        "Ahmad Kida",
        "Basel Halak"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "pdf_url": "",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ec461790ee249f9df979ac014243291e3a40794f",
      "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
      "abstract": "A variety of explanation methods have been proposed in recent years to help users gain insights into the results returned by neural networks, which are otherwise complex and opaque black-boxes. However, explanations give rise to potential side-channels that can be leveraged by an adversary for mounting attacks on the system. In particular, post-hoc explanation methods that highlight input dimensions according to their importance or relevance to the result also leak information that weakens security and privacy. In this work, we perform the first systematic characterization of the privacy and security risks arising from various popular explanation techniques. First, we propose novel explanation-guided black-box evasion attacks that lead to 10 times reduction in query count for the same success rate. We show that the adversarial advantage from explanations can be quantified as a reduction in the total variance of the estimated gradient. Second, we revisit the membership information leaked by common explanations. Contrary to observations in prior studies, via our modified attacks we show significant leakage of membership information (above 100% improvement over prior results), even in a much stricter black-box setting. Finally, we study explanation-guided model extraction attacks and demonstrate adversarial gains through a large reduction in query count.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Pengrui Quan",
        "Supriyo Chakraborty",
        "J. Jeyakumar",
        "Mani Srivastava"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/ec461790ee249f9df979ac014243291e3a40794f",
      "pdf_url": "https://arxiv.org/pdf/2206.14004",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4908fdc1feff153269670f0f8aac837346042553",
      "title": "Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System",
      "abstract": "The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Zhendong Wang",
        "Xiaoming Zeng",
        "Xulong Tang",
        "Danfeng Zhang",
        "Xingbo Hu",
        "Yang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4908fdc1feff153269670f0f8aac837346042553",
      "pdf_url": "http://arxiv.org/pdf/2208.13720",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eb39dda2df56270599f2a28bc6433c84c1704949",
      "title": "Extracted BERT Model Leaks More Information than You Think!",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xuanli He",
        "Chen Chen",
        "L. Lyu",
        "Qiongkai Xu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eb39dda2df56270599f2a28bc6433c84c1704949",
      "pdf_url": "https://arxiv.org/pdf/2210.11735",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "title": "Security and Privacy Challenges for Intelligent Internet of Things Devices 2022 TADW: Traceable and Antidetection Dynamic Watermarking of Deep Neural Networks",
      "abstract": "Deep neural networks (DNN) with incomparably advanced performance have been extensively applied in diverse fields (e.g., image recognition, natural language processing, and speech recognition). Training a high-performance DNN model requires a lot of training data and intellectual and computing resources, which bring a high cost to the model owners. Therefore, illegal model abuse (model theft, derivation, resale or redistribution, etc.) seriously infringes model owners\u2019 legitimate rights and interests. Watermarking is considered the main topic of DNN ownership protection. However, almost all existing watermarking works apply solely to image data. They do not trace the unique infringing model, and the adversary easily detects these ownership verification samples (trigger set) simultaneously. This paper introduces TADW, a dynamic watermarking scheme with tracking and antidetection abilities in the deep learning (DL) textual domain. Specifically, we propose a new approach to construct trigger set samples for antidetection and innovatively design a mapping algorithm that assigns a unique serial number (SN) to every watermarked model. Furthermore, we implement and detailedly evaluate TADW on 2 benchmark datasets and 3 popular DNNs. Experiment results show that TADW can successfully verify the ownership of the target model at a less than 0.5% accuracy cost and identify unique infringing models. In addition, TADW is excellently robust against different model modifications and can serve numerous users.",
      "year": 2022,
      "venue": "Security and Communication Networks",
      "authors": [
        "Jinwei Dong",
        "He Wang",
        "Zhipeng He",
        "Jun Niu",
        "Xiaoyan Zhu",
        "Gaofei Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "pdf_url": "https://downloads.hindawi.com/journals/scn/2022/9505808.pdf",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c082d0fdc703fd06f99a93f4683416f987d335ff",
      "title": "Privacy-Preserving DNN Model Authorization against Model Theft and Feature Leakage",
      "abstract": "Today\u2019s intelligent services are built on well-trained deep neural network (DNN) models, which usually require large private datasets along with a high cost for model training. It consequently makes the model providers cherish the pre-trained DNN models and only distribute them to authorized users. However, malicious users can steal these valuable models for abuse, illegal copy and redistribution. Attackers can also extract private features from even authorized models to leak partial training datasets. They both violate privacy. Existing techniques from secure community attempt to avoid parameter leakage during model authorization but yet cannot solve privacy issues sufficiently. In this paper, we propose a privacy-preserving model authorization approach, AgAuth, to resist the aforementioned privacy threats. We devise a novel scheme called Information-Agnostic Conversion (IAC) for forwarding procedure to eliminate residual features in model parameters. Based on it, we then propose Inference-on-Ciphertext (CiFer) mechanism for DNN reasoning, which includes three stages in each forwarding. The Encrypt phase first converts the proprietary model parameters to demonstrate uniform distribution. The Forward stage per-forms forwarding function without decryption at authorized side. Specifically, this stage just computes over ciphertext. The Decrypt phase finally recovers the information-agnostic outputs to informative output tensor for real-world services. In addition, we implement a prototype and conduct extensive experiments to evaluate its performance. The qualitative and quantitative results demonstrate that our solution AgAuth is privacy-preserving to defend against model theft and feature leakage, without accuracy loss or notable performance decrease.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Qiushi Li",
        "Ju Ren",
        "Yuezhi Zhou",
        "Yaoxue Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c082d0fdc703fd06f99a93f4683416f987d335ff",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7cfae03e2dc7b411644acbe5d686b1a6384f4b5e",
      "title": "From copycat to copyright: intellectual property amendments and the development of Chinese online video industries",
      "abstract": "ABSTRACT Many commentators have in the past hailed the production in China of lower cost versions of famous Chinese and international cultural and media products, better known as a shanzhai (\u5c71\u5be8) form of production. Against that, this paper argues that there has been a significant move away from a copycat model in the Chinese creative industries, a trend which should be viewed within the context of China\u2019s obligations as a full member of the WTO. This paper argues that the way in which online video industries have developed and innovated over the last 14 years in China has changed in that online video industries are constantly mutating their business models in response to lawsuits for IP violations instead of simply aligning with existing regulations. By doing that, they are indirectly adapting their business models to local legislation relating to the protection of IP for domestic and international content.",
      "year": 2022,
      "venue": "The International Journal of Cultural Policy",
      "authors": [
        "Filippo Gilardi",
        "A. White",
        "Z. Chen",
        "Shuxin Cheng",
        "Wei Song",
        "Yifan Zhao"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7cfae03e2dc7b411644acbe5d686b1a6384f4b5e",
      "pdf_url": "https://www.tandfonline.com/doi/pdf/10.1080/10286632.2022.2040494?needAccess=true",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "title": "Transformer-based Extraction of Deep Image Models",
      "abstract": "Model extraction attacks pose a threat to the security of ML models and to the privacy of the data used for training. Previous research has shown that such attacks can be either monetarily motivated to gain an edge over competitors or maliciously in order to mount subsequent attacks on the extracted model. In this paper, recent advances in the field of transformers are exploited to propose an attack tailored to the task of image classification that allows stealing complex convolutional neural network models without any knowledge of their architecture. The attack was performed on a range of datasets and target architectures to evaluate the robustness of the proposed attack. With only 100k queries, we were able to recover up to 99.2% of the black-box target network's accuracy on the test set. We conclude that it is possible to effectively steal complex neural networks with relatively little expertise and conventional means \u2013 even without knowledge of the target's architecture. Recently proposed defences have also been examined for their effectiveness in preventing the attack proposed in this paper.",
      "year": 2022,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Verena Battis",
        "A. Penner"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b18ab575ec113d117bb2482243c412cceb544756",
      "title": "Data-free Defense of Black Box Models Against Adversarial Attacks",
      "abstract": "Several companies often safeguard their trained deep models (i.e. details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as \u2018black boxes' through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via a generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose \u2018wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our \u2018wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a \u2018regenerator' network with an objective to retrieve the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% against the state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender.",
      "year": 2022,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Gaurav Kumar Nayak",
        "Inder Khatri",
        "Shubham Randive",
        "Ruchit Rawal",
        "Anirban Chakraborty"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b18ab575ec113d117bb2482243c412cceb544756",
      "pdf_url": "https://arxiv.org/pdf/2211.01579",
      "publication_date": "2022-11-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b778d465e9223b3a54601033000f61c985b00849",
      "title": "SEEK: model extraction attack against hybrid secure inference protocols",
      "abstract": "Security concerns about a machine learning model used in a prediction-as-a-service include the privacy of the model, the query and the result. Secure inference solutions based on homomorphic encryption (HE) and/or multiparty computation (MPC) have been developed to protect all the sensitive information. One of the most efficient type of solution utilizes HE for linear layers, and MPC for non-linear layers. However, for such hybrid protocols with semi-honest security, an adversary can malleate the intermediate features in the inference process, and extract model information more effectively than methods against inference service in plaintext. In this paper, we propose SEEK, a general extraction method for hybrid secure inference services outputing only class labels. This method can extract each layer of the target model independently, and is not affected by the depth of the model. For ResNet-18, SEEK can extract a parameter with less than 50 queries on average, with average error less than $0.03\\%$.",
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Si-Quan Chen",
        "Junfeng Fan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b778d465e9223b3a54601033000f61c985b00849",
      "pdf_url": "http://arxiv.org/pdf/2209.06373",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "title": "Holistic risk assessment of inference attacks in machine learning",
      "abstract": "As machine learning expanding application, there are more and more unignorable privacy and safety issues. Especially inference attacks against Machine Learning models allow adversaries to infer sensitive information about the target model, such as training data, model parameters, etc. Inference attacks can lead to serious consequences, including violating individuals privacy, compromising the intellectual property of the owner of the machine learning model. As far as concerned, researchers have studied and analyzed in depth several types of inference attacks, albeit in isolation, but there is still a lack of a holistic rick assessment of inference attacks against machine learning models, such as their application in different scenarios, the common factors affecting the performance of these attacks and the relationship among the attacks. As a result, this paper performs a holistic risk assessment of different inference attacks against Machine Learning models. This paper focuses on three kinds of representative attacks: membership inference attack, attribute inference attack and model stealing attack. And a threat model taxonomy is established. A total of 12 target models using three model architectures, including AlexNet, ResNet18 and Simple CNN, are trained on four datasets, namely CelebA, UTKFace, STL10 and FMNIST.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yang Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "pdf_url": "http://arxiv.org/pdf/2212.10628",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "title": "HWGN2: Side-channel Protected Neural Networks through Secure and Private Function Evaluation",
      "abstract": "Recent work has highlighted the risks of intellectual property (IP) piracy of deep learning (DL) models from the side-channel leakage of DL hardware accelerators. In response, to provide side-channel leakage resiliency to DL hardware accelerators, several approaches have been proposed, mainly borrowed from the methodologies devised for cryptographic implementations. Therefore, as expected, the same challenges posed by the complex design of such countermeasures should be dealt with. This is despite the fact that fundamental cryptographic approaches, specifically secure and private function evaluation, could potentially improve the robustness against side-channel leakage. To examine this and weigh the costs and benefits, we introduce hardware garbled NN (HWGN2), a DL hardware accelerator implemented on FPGA. HWGN2 also provides NN designers with the flexibility to protect their IP in real-time applications, where hardware resources are heavily constrained, through a hardware-communication cost trade-off. Concretely, we apply garbled circuits, implemented using a MIPS architecture that achieves up to 62.5x fewer logical and 66x less memory utilization than the state-of-the-art approaches at the price of communication overhead. Further, the side-channel resiliency of HWGN2 is demonstrated by employing the test vector leakage assessment (TVLA) test against both power and electromagnetic side-channels. This is in addition to the inherent feature of HWGN2: it ensures the privacy of users' input, including the architecture of NNs. We also demonstrate a natural extension to the malicious security modeljust as a by-product of our implementation.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mohammad J. Hashemi",
        "Steffi Roy",
        "Domenic Forte",
        "F. Ganji"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "pdf_url": "http://arxiv.org/pdf/2208.03806",
      "publication_date": "2022-08-07",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9286db7b087e6ba805d0526aeeeccfccf540069c",
      "title": "TP-NET: Training Privacy-Preserving Deep Neural Networks under Side-Channel Power Attacks",
      "abstract": "Privacy in deep learning is receiving tremendous attention with its wide applications in industry and academics. Recent studies have shown the internal structure of a deep neural network is easily inferred via side-channel power attacks in the training process. To address this pressing privacy issue, we propose TP-NET, a novel solution for training privacy-preserving deep neural networks under side-channel power attacks. The key contribution of TP-NET is the introduction of randomness into the internal structure of a deep neural network and the training process. Specifically, the workflow of TP-NET includes three steps: First, Independent Sub-network Construction, which generates multiple independent sub-networks via randomly se-lecting nodes in each hidden layer. Second, Sub-network Random Training, which randomly trains multiple sub-networks such that power traces keep random in the temporal domain. Third, Prediction, which outputs the predictions made by the most accu-rate sub-network to achieve high classification performance. The performance of TP-NET is evaluated under side-channel power attacks. The experimental results on two benchmark datasets demonstrate that TP-NET decreases the inference accuracy on the number of hidden nodes by at least 38.07% while maintaining competitive classification accuracy compared with traditional deep neural networks. Finally, a theoretical analysis shows that the power consumption of TP-NET depends on the number of sub-networks, the structure of each sub-network, and atomic operations in the training process.",
      "year": 2022,
      "venue": "International Symposium on Smart Electronic Systems",
      "authors": [
        "Hui Hu",
        "Jessa Gegax-Randazzo",
        "Clay Carper",
        "Mike Borowczak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9286db7b087e6ba805d0526aeeeccfccf540069c",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "title": "Verify Deep Learning Models Ownership via Preset Embedding",
      "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Wenxuan Yin",
        "Hai-feng Qian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "title": "Enhancing Cybersecurity in Edge AI through Model Distillation and Quantization: A Robust and Efficient Approach",
      "abstract": "The rapid proliferation of Edge AI has introduced significant cybersecurity challenges, including adversarial attacks, model theft, and data privacy concerns. Traditional deep learning models deployed on edge devices often suffer from high computational complexity and memory requirements, making them vulnerable to exploitation. This paper explores the integration of model distillation and quantization techniques to enhance the security and efficiency of Edge AI systems. Model distillation reduces model complexity by transferring knowledge from a large, cumbersome model (teacher) to a compact, efficient one (student), thereby improving resilience against adversarial manipulations. \nQuantization further optimizes the student model by reducing bit precision, minimizing attack surfaces while maintaining performance. \nWe present a comprehensive analysis of how these techniques mitigate cybersecurity threats such as model inversion, membership inference, and evasion attacks. Additionally, we evaluate trade-offs between model accuracy, latency, and robustness in resource-constrained edge environments. Experimental results on benchmark datasets demonstrate that distilled and quantized models achieve comparable accuracy to their full-precision counterparts while significantly reducing vulnerability to cyber threats. Our findings highlight the potential of distillation and quantization as key enablers for secure, lightweight, and high-performance Edge AI deployments.",
      "year": 2022,
      "venue": "International Journal for Sciences and Technology",
      "authors": [
        "Mangesh Pujari",
        "Anshul Goel",
        "Ashwin Sharma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "pdf_url": "",
      "publication_date": "2022-11-25",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "title": "Adversarial Attacks Against Network Intrusion Detection in IoT Systems",
      "abstract": "Deep learning (DL) has gained popularity in network intrusion detection, due to its strong capability of recognizing subtle differences between normal and malicious network activities. Although a variety of methods have been designed to leverage DL models for security protection, whether these systems are vulnerable to adversarial examples (AEs) is unknown. In this article, we design a novel adversarial attack against DL-based network intrusion detection systems (NIDSs) in the Internet-of-Things environment, with only black-box accesses to the DL model in such NIDS. We introduce two techniques: 1) model extraction is adopted to replicate the black-box model with a small amount of training data and 2) a saliency map is then used to disclose the impact of each packet attribute on the detection results, and the most critical features. This enables us to efficiently generate AEs using conventional methods. With these tehniques, we successfully compromise one state-of-the-art NIDS, Kitsune: the adversary only needs to modify less than 0.005% of bytes in the malicious packets to achieve an average 94.31% attack success rate.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Han Qiu",
        "Tian Dong",
        "Tianwei Zhang",
        "Jialiang Lu",
        "G. Memmi",
        "Meikang Qiu"
      ],
      "citation_count": 213,
      "url": "https://www.semanticscholar.org/paper/4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "A. Salem",
        "Zhikun Zhang",
        "M. Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 152,
      "url": "https://www.semanticscholar.org/paper/9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "pdf_url": "",
      "publication_date": "2021-02-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5beafcd6c0222a2f45863058280873c1ef088ec4",
      "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks",
      "abstract": "Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called \"Simulator\", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online 1.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Chen Ma",
        "Li Chen",
        "Junhai Yong"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/5beafcd6c0222a2f45863058280873c1ef088ec4",
      "pdf_url": "https://arxiv.org/pdf/2009.00960",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "title": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey",
      "abstract": "During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "M. M. Real",
        "R. Salvador"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "pdf_url": "https://www.mdpi.com/2076-3417/11/15/6790/pdf?version=1627954373",
      "publication_date": "2021-07-23",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "title": "Watermarking Graph Neural Networks based on Backdoor Attacks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary.This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. We also explore our watermarking mechanism against an adaptive attacker with access to partial knowledge of the watermarked data. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks.",
      "year": 2021,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Jing Xu",
        "S. Picek"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "pdf_url": "https://repository.ubn.ru.nl//bitstream/handle/2066/295585/295585.pdf",
      "publication_date": "2021-10-21",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "title": "Preventing DNN Model IP Theft via Hardware Obfuscation",
      "abstract": "Training accurate deep learning (DL) models require large amounts of training data, significant work in labeling the data, considerable computing resources, and substantial domain expertise. In short, they are expensive to develop. Hence, protecting these models, which are valuable storehouses of intellectual properties (IP), against model stealing/cloning attacks is of paramount importance. Today\u2019s mobile processors feature Neural Processing Units (NPUs) to accelerate the execution of DL models. DL models executing on NPUs are vulnerable to hyperparameter extraction via side-channel attacks and model parameter theft via bus monitoring attacks. This paper presents a novel solution to defend against DL IP theft in NPUs during model distribution and deployment/execution via lightweight, keyed model obfuscation scheme. Unauthorized use of such models results in inaccurate classification. In addition, we present an ideal end-to-end deep learning trusted system composed of: 1) model distribution via hardware root-of-trust and public-key cryptography infrastructure (PKI) and 2) model execution via low-latency memory encryption. We demonstrate that our proposed obfuscation solution achieves IP protection objectives without requiring specialized training or sacrificing the model\u2019s accuracy. In addition, the proposed obfuscation mechanism preserves the output class distribution while degrading the model\u2019s accuracy for unauthorized parties, covering any evidence of a hacked model.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Brunno F. Goldstein",
        "Vinay C. Patil",
        "V. C. Ferreira",
        "A. S. Nery",
        "F. Fran\u00e7a",
        "S. Kundu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "pdf_url": "",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "pdf_url": "",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4ac66823cdb39b6a79d5b4bb6986634ecd5a7e9",
      "title": "An Efficient Time-Domain Electromagnetic Algorithm Based on LSTM Neural Network",
      "abstract": "Although neural networks have been applied in many fields since they were first introduced, the feasibility of applying it to predict the solution of Maxwell's equations remains open. In this letter, we investigate the feasibility of utilizing the long- and short-term memory (LSTM) neural network to solve the time-domain electromagnetic (TDEM) forward problems. With ground truth datasets being generated from the finite-difference time-domain (FDTD) method, a novel LSTM-TDEM model structure is proposed, and trained by a new feasible algorithm specifically designed for time-domain simulation. The strong approximation ability of the LSTM-TDEM method can accurately predict the electromagnetic field distributions for topologies of different geometries, materials, and excitations at different locations. The effectiveness of the proposed LSTM-TDEM method has been validated by several numerical experiments. The average relative error can be as small as 0.63% for 2-D case and 0.35% for 3-D case. It is worth noting that the training data are only 5% (198/4087) and 30% (660/2189) in 2-D and 3-D cases, respectively. Meanwhile, compared with the traditional FDTD method, the proposed method greatly reduces the calculation time, with its speedup ratio more than 1800 and 44\u00a0000 times over the FDTD method in 2-D and 3-D cases, respectively.",
      "year": 2021,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Fengbo Wu",
        "Mengning Fan",
        "Wenyuan Liu",
        "Bingyang Liang",
        "Yuanguo Zhou"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/d4ac66823cdb39b6a79d5b4bb6986634ecd5a7e9",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
      "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals\u2019 sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users\u2019 privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
      "year": 2021,
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "authors": [
        "A. R. Javed",
        "S. Rehman",
        "M. Khan",
        "M. Alazab",
        "H. Khan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "pdf_url": "http://qspace.qu.edu.qa/bitstream/10576/37656/2/3460392.pdf",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "title": "Imaging subsurface orebodies with airborne electromagnetic data using a recurrent neural network",
      "abstract": "Conventional interpretation of airborne electromagnetic data has been conducted by solving the inverse problem. However, with recent advances in machine learning (ML) techniques, a one-dimensional (1D) deep neural network inversion that predicts a 1D resistivity model using multi-frequency vertical magnetic fields and sensor height information at one location has been applied. Nevertheless, bacause the final interpretation of this 1D approach relies on connecting 1D resistivity models, 1D ML interpretation has low accuracy for the estimation of an isolated anomaly, as in conventional 1D inversion. Thus, we propose a two-dimensional (2D) interpretation technique that can overcome the limitations of 1D interpretation, and consider spatial continuity by using a recurrent neural network (RNN). We generated various 2D resistivity models, calculated the ratio of primary and induced secondary magnetic fields of vertical direction in ppm scale using vertical magnetic dipole source, and then trained the RNN using the resistivity models and the corresponding electromagnetic (EM) responses. To verify the validity of 2D RNN inversion, we applied the trained RNN to synthetic and field data. Through application of the field data, we demonstrated that the design of the training dataset is crucial to improve prediction performance in a 2D RNN inversion. In addition, we investigated changes in the RNN inversion results of field data dependent on the data preprocessing. We demonstrated that using two types of data, logarithmic transformed data and linear scale data, which having different patterns of input information can enhance the prediction performance of the EM inversion results.",
      "year": 2021,
      "venue": "Geophysics",
      "authors": [
        "M. Bang",
        "Seokmin Oh",
        "K. Noh",
        "S. Seol",
        "J. Byun"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "pdf_url": "",
      "publication_date": "2021-08-24",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b2212da1f8c968a60f70227623678a41fb63c3cf",
      "title": "Toward Invisible Adversarial Examples Against DNN-Based Privacy Leakage for Internet of Things",
      "abstract": "Deep neural networks (DNNs) can be utilized maliciously for compromising the privacy stored in electronic devices, e.g., identifying the images stored in a mobile phone connected to the Internet of Things (IoT). However, recent studies demonstrated that DNNs are vulnerable to adversarial examples, which are artificially designed perturbations in the original samples for misleading DNNs. Adversarial examples can be used to protect the DNN-based privacy leakage in mobile phones by replacing the photos with adversarial examples. To avoid affecting the normal use of photos, the adversarial examples need to be highly similar to original images. To handle a large number of photos stored in the devices at a proper time, the time efficiency of a method needs to be high enough. Previous methods cannot do well on both sides. In this article, we propose a broad class of selective gradient sign iterative algorithms to make adversarial examples useful in protecting the privacy of photos in IoT devices. By neglecting the unimportant image pixels in the iterative process of attacks according to the sort of first-order partial derivative, we control the optimization direction meticulously to reduce image distortions of adversarial examples without leveraging high time-consuming tricks. Extensive experimental results show that the proposed methods successfully fool the neural network classifiers for the image classification task with a small change in the visual effects and consume little calculating time simultaneously.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuyang Ding",
        "Shuai Zhang",
        "Mengkai Song",
        "Xiaocong Ding",
        "Fagen Li"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b2212da1f8c968a60f70227623678a41fb63c3cf",
      "pdf_url": "",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "373936d00c4a357579c4d375de0ce439e4e54d5f",
      "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Chen Chen",
        "Xuanli He",
        "Lingjuan Lyu",
        "Fangzhao Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2c47435498b8e67873cce1cc86cfbf8397b18286",
      "title": "Timing Black-Box Attacks: Crafting Adversarial Examples through Timing Leaks against DNNs on Embedded Devices",
      "abstract": "Deep neural networks (DNNs) have been applied to various industries. In particular, DNNs on embedded devices have attracted considerable interest because they allow real-time and distributed processing on site. However, adversarial examples (AEs), which add small perturbations to the input data of DNNs to cause misclassification, are serious threats to DNNs. In this paper, a novel black-box attack is proposed to craft AEs based only on processing time, i.e., the side-channel leaks from DNNs on embedded devices. Unlike several existing black-box attacks that utilize output probability, the proposed attack exploits the relationship between the number of activated nodes and processing time without using training data, model architecture, parameters, substitute models, or output probability. The perturbations for AEs are determined by the differential processing time based on the input data of the DNNs in the proposed attack. The experimental results show that the AEs of the proposed attack effectively cause an increase in the number of activated nodes and the misclassification of one of the incorrect labels against the DNNs on a microcontroller unit. Moreover, these results indicate that the attack can evade gradient-masking and confidence reduction countermeasures, which conceal the output probability, to prevent the crafting of AEs against several black-box attacks. Finally, the countermeasures against the attack are implemented and evaluated to clarify that the implementation of an activation function with data-dependent timing leaks is the cause of the proposed attack.",
      "year": 2021,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Tsunato Nakai",
        "Daisuke Suzuki",
        "T. Fujino"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2c47435498b8e67873cce1cc86cfbf8397b18286",
      "pdf_url": "https://doi.org/10.46586/tches.v2021.i3.149-175",
      "publication_date": null,
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
      "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
      "year": 2021,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Chengsi Gao",
        "Bing Li",
        "Ying Wang",
        "Weiwei Chen",
        "Lei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3453688.3461512",
      "publication_date": "2021-06-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44cc3c0d5a80fecd1b6285c73f79986852135162",
      "title": "ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access",
      "abstract": "Machine Learning (ML) models are progressively deployed in many real-world applications to perform a wide range of tasks, but are exposed to the security and privacy threats which aim to infer the details and even steal the functionality of the ML models. Despite extensive attacking efforts which rely on white-box or gray-box access, how to perform attacks with black-box access continues to be elusive. Aspiring to fill this gap, we move one step further and present ML-Stealer that can steal the functionality of any type of ML models with mere black-box access. With two algorithm designs, namely, synthetic data generation and replica model construction, ML-Stealer can construct a deep neural network (DNN)-based replica model which has the similar prediction functionality to the victim ML model. ML-Stealer does not require any knowledge about the victim model, nor does it enforce the access to statistical information or samples of the victim's training data. Experiment results demonstrate that ML-Stealer can achieve the consistent prediction results with the victim model of an averaged testing accuracy of 85.6%, and up to 93.6% at best.",
      "year": 2021,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Gaoyang Liu",
        "Shijie Wang",
        "Borui Wan",
        "Zekun Wang",
        "Chen Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/44cc3c0d5a80fecd1b6285c73f79986852135162",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ef2325d326986f6e9958332100fce2f00ae65a29",
      "title": "Emerging AI Security Threats for Autonomous Cars - Case Studies",
      "abstract": "Artificial Intelligence has made a significant contribution to autonomous vehicles, from object detection to path planning. However, AI models require a large amount of sensitive training data and are usually computationally intensive to build. The commercial value of such models motivates attackers to mount various attacks. Adversaries can launch model extraction attacks for monetization purposes or step-ping-stone towards other attacks like model evasion. In specific cases, it even results in destroying brand reputation, differentiation, and value proposition. In addition, IP laws and AI-related legalities are still evolving and are not uniform across countries. We discuss model extraction attacks in detail with two use-cases and a generic kill-chain that can compromise autonomous cars. It is essential to investigate strategies to manage and mitigate the risk of model theft.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Shanthi Lekkala",
        "Tanya Motwani",
        "Manojkumar Somabhai Parmar",
        "A. Phadke"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ef2325d326986f6e9958332100fce2f00ae65a29",
      "pdf_url": "",
      "publication_date": "2021-09-10",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "citation_count": 206,
      "url": "https://www.semanticscholar.org/paper/e4b1d7553020258d7e537e2cfa53865359389eac",
      "pdf_url": "",
      "publication_date": "2020-05-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3136dd4036331b4559b341560712942ca2e765e3",
      "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
      "abstract": "Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Mingfu Xue",
        "Chengxiang Yuan",
        "Heyi Wu",
        "Yushu Zhang",
        "Weiqiang Liu"
      ],
      "citation_count": 141,
      "url": "https://www.semanticscholar.org/paper/3136dd4036331b4559b341560712942ca2e765e3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09064510.pdf",
      "publication_date": "2020-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d76407a539addcb39bddef065c4061381591e78c",
      "title": "WAFFLE: Watermarking in Federated Learning",
      "abstract": "Federated learning is a distributed learning technique where machine learning models are trained on client devices in which the local training data resides. The training is coordinated via a central server which is, typically, controlled by the intended owner of the resulting model. By avoiding the need to transport the training data to the central server, federated learning improves privacy and efficiency. But it raises the risk of model theft by clients because the resulting model is available on every client device. Even if the application software used for local training may attempt to prevent direct access to the model, a malicious client may bypass any such restrictions by reverse engineering the application software. Watermarking is a well-known deterrence method against model theft by providing the means for model owners to demonstrate ownership of their models. Several recent deep neural network (DNN) watermarking techniques use backdooring: training the models with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several client devices. In this paper, we present WAFFLE, the first approach to watermark DNN models trained using federated learning. It introduces a retraining step at the server after each aggregation of local models into the global model. We show that WAFFLE efficiently embeds a resilient watermark into models incurring only negligible degradation in test accuracy (-0.17%), and does not require access to training data. We also introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+3.2%) overhead11The research report version of this paper is also available in https://arxiv.org/abs/2008.07298, and the code for reproducing our work can be found at https://github.com/ssg-research/WAFFLE.",
      "year": 2020,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "B. Atli",
        "Yuxi Xia",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 86,
      "url": "https://www.semanticscholar.org/paper/d76407a539addcb39bddef065c4061381591e78c",
      "pdf_url": "http://arxiv.org/pdf/2008.07298",
      "publication_date": "2020-08-17",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "title": "Model extraction from counterfactual explanations",
      "abstract": "Post-hoc explanation techniques refer to a posteriori methods that can be used to explain how black-box machine learning models produce their outcomes. Among post-hoc explanation techniques, counterfactual explanations are becoming one of the most popular methods to achieve this objective. In particular, in addition to highlighting the most important features used by the black-box model, they provide users with actionable explanations in the form of data instances that would have received a different outcome. Nonetheless, by doing so, they also leak non-trivial information about the model itself, which raises privacy issues. In this work, we demonstrate how an adversary can leverage the information provided by counterfactual explanations to build high-fidelity and high-accuracy model extraction attacks. More precisely, our attack enables the adversary to build a faithful copy of a target model by accessing its counterfactual explanations. The empirical evaluation of the proposed attack on black-box models trained on real-world datasets demonstrates that they can achieve high-fidelity and high-accuracy extraction even under low query budgets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "U. A\u00efvodji",
        "Alexandre Bolot",
        "S. Gambs"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "pdf_url": "",
      "publication_date": "2020-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "729fbe386e31e36ed5108ee276a6db223176a33d",
      "title": "Information Leakage by Model Weights on Federated Learning",
      "abstract": "Federated learning aggregates data from multiple sources while protecting privacy, which makes it possible to train efficient models in real scenes. However, although federated learning uses encrypted security aggregation, its decentralised nature makes it vulnerable to malicious attackers. A deliberate attacker can subtly control one or more participants and upload malicious model parameter updates, but the aggregation server cannot detect it due to encrypted privacy protection. Based on these problems, we find a practical and novel security risk in the design of federal learning. We propose an attack for conspired malicious participants to adjust the training data strategically so that the weight of a certain dimension in the aggregation model will rise or fall with a pattern. The trend of weights or parameters in the aggregation model forms meaningful signals, which is the risk of information leakage. The leakage is exposed to other participants in this federation but only available for participants who reach an agreement with the malicious participant, i.e., the receiver must be able to understand patterns of changes in weights. The attack effect is evaluated and verified on open-source code and data sets.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Xiaoyun Xu",
        "Jingzheng Wu",
        "Mutian Yang",
        "Tianyue Luo",
        "Xu Duan",
        "Weiheng Li",
        "Yanjun Wu",
        "Bin Wu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/729fbe386e31e36ed5108ee276a6db223176a33d",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "57cbdecb0b12f03a5c0fb95522fdc3494c87bd10",
      "title": "Modeling extra-deep electromagnetic logs using a deep neural network",
      "abstract": "Modern geosteering is heavily dependent on real-time interpretation of deep electromagnetic (EM) measurements. We have developed a methodology to construct a deep neural network (DNN) model trained to reproduce a full set of extra-deep EM logs consisting of 22 measurements per logging position. The model is trained in a 1D layered environment consisting of up to seven layers with different resistivity values. A commercial simulator provided by a tool vendor is used to generate a training data set. The data set size is limited because the simulator provided by the vendor is optimized for sequential execution. Therefore, we design a training data set that embraces the geologic rules and geosteering specifics supported by the forward model. We use this data set to produce an EM simulator based on a DNN without access to the proprietary information about the EM tool configuration or the original simulator source code. Despite using a relatively small training set size, the resulting DNN forward model is quite accurate for the considered examples: a multilayer synthetic case and a section of a published historical operation from the Goliat field. The observed average evaluation time of 0.15\u00a0ms per logging position makes it also suitable for future use as part of evaluation-hungry statistical and/or Monte Carlo inversion algorithms within geosteering workflows.",
      "year": 2020,
      "venue": "Geophysics",
      "authors": [
        "S. Alyaev",
        "M. Shahriari",
        "David Pardo",
        "\u00c1. J. Omella",
        "D. Larsen",
        "N. Jahani",
        "E. Suter"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/57cbdecb0b12f03a5c0fb95522fdc3494c87bd10",
      "pdf_url": "https://bird.bcamath.org/bitstream/20.500.11824/1409/1/geo2020-0389.1%20%282%29.pdf",
      "publication_date": "2020-05-18",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12393a8e6e7c750eae30c0c538663fa7b426f304",
      "title": "A New Privacy-Preserving Framework based on Edge-Fog-Cloud Continuum for Load Forecasting",
      "abstract": "As an essential part to intelligently fine-grained scheduling, planning and maintenance in smart grid and energy internet, short-term load forecasting makes great progress recently owing to the big data collected from smart meters and the leap forward in machine learning technologies. However, the centralized computing topology of classical electric information system, where individual electricity consumption data are frequently transmitted to the cloud center for load forecasting, tends to violate electric consumers\u2019 privacy as well as to increase the pressure on network bandwidth. To tackle the tricky issues, we propose a privacy-preserving framework based on the edge-fog-cloud continuum for smart grid. Specifically, 1) we gravitate the training of load forecasting models and forecasting workloads to distributed smart meters so that consumers\u2019 raw data are handled locally, and only the forecasting outputs that have been protected are reported to the cloud center via fog nodes; 2) we protect the local forecasting models that imply electricity features from model extraction attacks by model randomization; 3) we exploit a shuffle scheme among smart meters to protect the data ownership privacy, and utilize a re-encryption scheme to guarantee the forecasting data privacy. Finally, through comprehensive simulation and analysis, we validate our proposed privacy-preserving framework in terms of privacy protection, and computation and communication efficiency.",
      "year": 2020,
      "venue": "IEEE Wireless Communications and Networking Conference",
      "authors": [
        "S. Hou",
        "Hongjia Li",
        "Chang Yang",
        "Liming Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/12393a8e6e7c750eae30c0c538663fa7b426f304",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "83e2b8ab293b74f2b9347f22ac12b9175e96351e",
      "title": "Stored Grain Inventory Management Using Neural-Network-Based Parametric Electromagnetic Inversion",
      "abstract": "We present a neural network architecture to determine the volume and complex permittivity of grain stored in metal bins. The neural networks output the grain height, cone angle and complex permittivity of the grain, using the input of experimental field data ( $S$ -parameters) from an electromagnetic imaging system consisting of 24 transceivers installed in the bin. Key for practical applications, the neural networks are trained on synthetic data sets but generate the parametric information using experimental data as input, without the use of calibration objects or open-short-load measurements. To accomplish this, we formulate a data normalization scheme that enables the use of a loss function that directly compares measured $S$ -parameters and simulation model fields. The normalization strategy and the ability to train on synthetic data means we do not need to collect experimental training data. We demonstrate the applicability of this synthetically trained neural network to experimental data from two different bin geometries, and discuss the ability of these neural networks to successfully infer parameters that can be used for grain inventory management. Our neural-network-based approach enables rapid inference, providing a more cost-effective long-term solution than existing optimization-based parametric inversion methods.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Keeley Edwards",
        "N. Geddert",
        "Kennedy Krakalovich",
        "R. Kruk",
        "M. Asefi",
        "J. Lovetri",
        "C. Gilmore",
        "I. Jeffrey"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/83e2b8ab293b74f2b9347f22ac12b9175e96351e",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09260139.pdf",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f9f49997c404e386610289202a7c3812c6cbfe34",
      "title": "Differentially Private Machine Learning Model against Model Extraction Attack",
      "abstract": "Machine learning model is vulnerable to model extraction attacks since the attackers can send plenty of queries to infer the hyperparameters of the machine learning model thus stealing confidential information of the learning models. Therefore, there is a urgent need to defend against such an attack. Differential privacy is a promising technique to protect the valuable information. We propose a differential privacy-based method applied in the linear neural network to obfuscate the output of the machine learning model. The security and utility issue of injecting a noise layer to the linear neural network is mathematically analyzed. The experiment results show that our proposed method can lower the attacker's extraction rate while keeping high utility.",
      "year": 2020,
      "venue": "2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Zelei Cheng",
        "Zuotian Li",
        "Jiwei Zhang",
        "Shuhan Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f9f49997c404e386610289202a7c3812c6cbfe34",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "title": "Stealing Your Data from Compressed Machine Learning Models",
      "abstract": "Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Nuo Xu",
        "Qi Liu",
        "Tao Liu",
        "Zihao Liu",
        "Xiaochen Guo",
        "Wujie Wen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "title": "Securing Machine Learning Architectures and Systems",
      "abstract": "Machine learning (ML), and deep learning in particular, have become a critical workload as they are becoming increasingly applied at the core of a wide range of application spaces. Computer systems, from the architecture up, have been impacted by ML in two primary directions: (1) ML is an increasingly important computing workload, with new accelerators and systems targeted to support both training and inference at scale; and (2) ML supporting computer system decisions, both during design and run times, with new machine learning based algorithms controlling systems to optimize their performance, reliability and robustness. In this paper, we will explore the intersection of security, ML and computing systems, identifying both security challenges and opportunities. Machine learning systems are vulnerable to new attacks including adversarial attacks crafted to fool a classifier to the attacker's advantage, membership inference attacks attempting to compromise the privacy of the training data, and model extraction attacks seeking to recover the hyperparameters of a (secret) model. Architecture can be a target of these attacks when supporting ML (or is supported by ML), but also provides an opportunity to develop defenses against them, which we will illustrate with three examples from our recent work. First, we show how ML based hardware malware detectors can be attacked with adversarial perturbations to the Malware and how we can develop detectors that resist these attacks. Second, we show an example of microarchitectural side channel attacks that can be used to extract the secret parameters of a neural network and potential defenses against it. Finally, we discuss how hardware and systems can be used to make ML more robust against adversarial and other attacks.",
      "year": 2020,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Shirin Haji Amin Shirazi",
        "Hoda Naghibijouybari",
        "N. Abu-Ghazaleh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "pdf_url": "",
      "publication_date": "2020-09-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "title": "Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment",
      "abstract": "Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Gupta",
        "Erick Galinkin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "pdf_url": "",
      "publication_date": "2020-07-09",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "title": "Monitoring-based Differential Privacy Mechanism Against Query-Flooding Parameter Duplication Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this paper, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "title": "Mitigating Query-Flooding Parameter Duplication Attack on Regression Models with High-Dimensional Gaussian Mechanism",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Differential privacy (DP) has been considered a promising technique to mitigate this attack. However, we find that the vulnerability persists when regression models are being protected by current DP solutions. We show that the adversary can launch a query-flooding parameter duplication (QPD) attack to infer the model information by repeated queries. \nTo defend against the QPD attack on logistic and linear regression models, we propose a novel High-Dimensional Gaussian (HDG) mechanism to prevent unauthorized information disclosure without interrupting the intended services. In contrast to prior work, the proposed HDG mechanism will dynamically generate the privacy budget and random noise for different queries and their results to enhance the obfuscation. Besides, for the first time, HDG enables an optimal privacy budget allocation that automatically determines the minimum amount of noise to be added per user-desired privacy level on each dimension. We comprehensively evaluate the performance of HDG using real-world datasets and shows that HDG effectively mitigates the QPD attack while satisfying the privacy requirements. We also prepare to open-source the relevant codes to the community for further research.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Xiaoguang Li",
        "Hui Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "title": "Adversarial Imitation Attack",
      "abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Mingyi Zhou",
        "Jing Wu",
        "Yipeng Liu",
        "Xiaolin Huang",
        "Shuaicheng Liu",
        "Xiang Zhang",
        "Ce Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "pdf_url": "",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "title": "Model inversion attacks against collaborative inference",
      "abstract": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and datasets, to show their effectiveness and generalization. We also study the characteristics of deep learning models that make them susceptible to such inference privacy threats. This provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.",
      "year": 2019,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "R. Lee"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "pdf_url": "",
      "publication_date": "2019-12-09",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "title": "High-resolution palaeovalley classification from airborne\nelectromagnetic imaging and deep neural network training using\ndigital elevation model data",
      "abstract": "Abstract. Palaeovalleys are buried ancient river valleys that often form productive aquifers, especially in the semi-arid and arid areas of Australia. Delineating their extent and hydrostratigraphy is however a challenging task in groundwater system characterization. This study developed a methodology based on the deep learning super-resolution convolutional neural network (SRCNN) approach, to convert electrical conductivity (EC) estimates from an airborne electromagnetic (AEM) survey in South Australia to a high-resolution binary palaeovalley map. The SRCNN was trained and tested with a synthetic training dataset, where valleys were generated from readily available digital elevation model (DEM) data from the AEM survey area. Electrical conductivities typical of valley sediments were generated by Archie\u2019s Law, and subsequently blurred by down-sampling and bicubic interpolation to represent noise from the AEM survey, inversion and interpolation. After a model training step, the SRCNN successfully removed such noise, and reclassified the low-resolution, unimodal but skewed EC values into a high-resolution palaeovalley index following a bimodal distribution. The latter allows distinguishing valley from non-valley pixels. Furthermore, a realistic spatial connectivity structure of the palaeovalley was predicted when compared with borehole lithology logs and valley bottom flatness indicator. Overall the methodology permitted to better constrain the three-dimensional palaeovalley geometry from AEM images that are becoming more widely available for groundwater prospecting.\n",
      "year": 2019,
      "venue": "",
      "authors": [
        "Zhenjiao Jiang",
        "D. Mallants",
        "L. Peeters",
        "Lei Gao",
        "G. Mari\u00e9thoz"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "pdf_url": "https://hess.copernicus.org/articles/23/2561/2019/hess-23-2561-2019.pdf",
      "publication_date": "2019-01-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "title": "Proposed Guidelines for the Responsible Use of Explainable Machine Learning.",
      "abstract": "Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and security audits of ML models. Explainable ML (i.e. explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services. However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful black-boxes, e.g. fairwashing or scaffolding, and for other malevolent purposes like stealing models and sensitive training data. To promote best-practice discussions for this already in-flight technology, this short text presents internal definitions and a few examples before covering the proposed guidelines. This text concludes with a seemingly natural argument for the use of interpretable models and explanatory, debugging, and disparate impact testing methods in life- or mission-critical ML systems.",
      "year": 2019,
      "venue": "",
      "authors": [
        "Patrick Hall",
        "Navdeep Gill",
        "N. Schmidt"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "pdf_url": "",
      "publication_date": "2019-06-08",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b82d52b95040c0682b7a8baa9254eae79b068505",
      "title": "Adversarial Model Extraction on Graph Neural Networks",
      "abstract": "Along with the advent of deep neural networks came various methods of exploitation, such as fooling the classifier or contaminating its training data. Another such attack is known as model extraction, where provided API access to some black box neural network, the adversary extracts the underlying model. This is done by querying the model in such a way that the underlying neural network provides enough information to the adversary to be reconstructed. While several works have achieved impressive results with neural network extraction in the propositional domain, this problem has not yet been considered over the relational domain, where data samples are no longer considered to be independent and identically distributed (iid). Graph Neural Networks (GNNs) are a popular deep learning framework to perform machine learning tasks over relational data. In this work, we formalize an instance of GNN extraction, present a solution with preliminary results, and discuss our assumptions and future directions.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "David DeFazio",
        "Arti Ramesh"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/b82d52b95040c0682b7a8baa9254eae79b068505",
      "pdf_url": "",
      "publication_date": "2019-12-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data",
      "abstract": "As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.",
      "year": 2019,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Itay Mosafi",
        "Eli David",
        "N. Netanyahu"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "pdf_url": "https://arxiv.org/pdf/1912.03959",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "title": "Towards Privacy and Security of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, \\eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "pdf_url": "",
      "publication_date": "2019-11-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "title": "VAWS: Vulnerability Analysis of Neural Networks using Weight Sensitivity",
      "abstract": "The advancement in deep learning has taken the technology world by storm in the last decade. Although, there is enormous progress made in terms of algorithm performance, the security aspect of these algorithms has not received a lot of attention from the research community. As more industries start to adopt these algorithms the issue of security is becoming even more relevant. Security vulnerabilities in machine learning (ML), especially in deep neural networks (DNN), is becoming a concern. Various techniques have been proposed, including data manipulations and model stealing. However, most of them are focused on ML algorithms and target threat models that require access to training dataset. In this paper, we present a methodology that analyzes the DNN weight parameters under the threat model that assumes the attacker has the access to the weight memory only. This analysis is then used to develop an attack that manipulates weight parameters with respect to their sensitivity. To evaluate this attack, we implemented our methodology on a MLP trained on IRIS dataset and LeNet (DNN architecture) trained on MNIST dataset. Our experimental results demonstrate that alteration of model parameters results in subtle accuracy drop of the model. Depending on the applications such subtle changes can cause significant system malfunction or disruption, for example in vision-based industrial applications. Our results show that using our methodology a subtle accuracy drop can be achieved in a reasonable amount of time with very few parameter changes.",
      "year": 2019,
      "venue": "Midwest Symposium on Circuits and Systems",
      "authors": [
        "Muluken Hailesellasie",
        "Jacob Nelson",
        "Faiq Khalid",
        "S. R. Hasan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "pdf_url": "",
      "publication_date": "2019-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "title": "Quantifying (Hyper) Parameter Leakage in Machine Learning",
      "abstract": "Machine Learning models are extensively used for various multimedia applications and are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks that reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Extraction attacks proposed in the literature are empirically evaluated and lack a theoretical framework to measure the information leaked under such attacks. In this work, we propose a novel model-agnostic probabilistic framework, AIRAVATA, to quantify information leakage using partial knowledge and limited evidences from model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training for stealing the target model functionality. We use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in the literature to reason about the attack efficacy. This provides a practical tool to identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model and estimate the relative threats from different attacks.",
      "year": 2019,
      "venue": "IEEE International Conference on Multimedia Big Data",
      "authors": [
        "Vasisht Duddu",
        "D. V. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "pdf_url": "https://arxiv.org/pdf/1910.14409",
      "publication_date": "2019-10-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
      "abstract": "Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ``victim'' models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we study complex victim blackbox models, and an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ``knockoff'' with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as show that a reasonable knockoff of an image analysis API could be created for as little as $30.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 596,
      "url": "https://www.semanticscholar.org/paper/089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "pdf_url": "http://arxiv.org/pdf/1812.02766",
      "publication_date": "2018-12-06",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators",
      "abstract": "Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.",
      "year": 2018,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Lingxiao Wei",
        "Yannan Liu",
        "Bo Luo",
        "Yu LI",
        "Qiang Xu"
      ],
      "citation_count": 220,
      "url": "https://www.semanticscholar.org/paper/669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "pdf_url": "https://arxiv.org/pdf/1803.05847",
      "publication_date": "2018-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "title": "Stealing Neural Networks via Timing Side Channels",
      "abstract": "Deep learning is gaining importance in many applications. However, Neural Networks face several security and privacy threats. This is particularly significant in the scenario where Cloud infrastructures deploy a service with Neural Network model at the back end. Here, an adversary can extract the Neural Network parameters, infer the regularization hyperparameter, identify if a data point was part of the training data, and generate effective transferable adversarial examples to evade classifiers. This paper shows how a Neural Network model is susceptible to timing side channel attack. In this paper, a black box Neural Network extraction attack is proposed by exploiting the timing side channels to infer the depth of the network. Although, constructing an equivalent architecture is a complex search problem, it is shown how Reinforcement Learning with knowledge distillation can effectively reduce the search space to infer a target model. The proposed approach has been tested with VGG architectures on CIFAR10 data set. It is observed that it is possible to reconstruct substitute models with test accuracy close to the target models and the proposed approach is scalable and independent of type of Neural Network architectures.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "D. Samanta",
        "D. V. Rao",
        "V. Balas"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "pdf_url": "",
      "publication_date": "2018-12-31",
      "keywords_matched": [
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
      "abstract": "Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user\u2019s input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user\u2019s client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.",
      "year": 2018,
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "L. Hanzlik",
        "Yang Zhang",
        "Kathrin Grosse",
        "A. Salem",
        "Maximilian Augustin",
        "M. Backes",
        "Mario Fritz"
      ],
      "citation_count": 113,
      "url": "https://www.semanticscholar.org/paper/8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "pdf_url": "https://arxiv.org/pdf/1808.00590",
      "publication_date": "2018-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "544e7fe88816924a81962eb79fbb549571ec2217",
      "title": "Killing Four Birds with one Gaussian Process: The Relation between different Test-Time Attacks",
      "abstract": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. Consequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process Classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a different norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all information when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g. model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other.",
      "year": 2018,
      "venue": "International Conference on Pattern Recognition",
      "authors": [
        "Kathrin Grosse",
        "M. Smith",
        "M. Backes"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/544e7fe88816924a81962eb79fbb549571ec2217",
      "pdf_url": "https://arxiv.org/pdf/1806.02032",
      "publication_date": "2018-06-06",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "181fe8787937dbf7abf886042855be1bc6149f80",
      "title": "Model Extraction Warning in MLaaS Paradigm",
      "abstract": "Machine learning models deployed on the cloud are susceptible to several security threats including extraction attacks. Adversaries may abuse a model's prediction API to steal the model thus compromising model confidentiality, privacy of training data, and revenue from future query payments. This work introduces a model extraction monitor that quantifies the extraction status of models by continually observing the API query and response streams of users. We present two novel strategies that measure either the information gain or the coverage of the feature space spanned by user queries to estimate the learning rate of individual and colluding adversaries. Both approaches have low computational overhead and can easily be offered as services to model owners to warn them against state of the art extraction attacks. We demonstrate empirical performance results of these approaches for decision tree and neural network models using open source datasets and BigML MLaaS platform.",
      "year": 2017,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "M. Kesarwani",
        "B. Mukhoty",
        "V. Arya",
        "S. Mehta"
      ],
      "citation_count": 154,
      "url": "https://www.semanticscholar.org/paper/181fe8787937dbf7abf886042855be1bc6149f80",
      "pdf_url": "https://arxiv.org/pdf/1711.07221",
      "publication_date": "2017-11-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \nThe tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",
      "year": 2016,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Florian Tram\u00e8r",
        "Fan Zhang",
        "A. Juels",
        "M. Reiter",
        "Thomas Ristenpart"
      ],
      "citation_count": 1967,
      "url": "https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "pdf_url": "",
      "publication_date": "2016-08-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    }
  ]
}