[
  {
    "title": "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20-suya.pdf",
    "code_url": "https://github.com/suyeecav/Hybrid-Attack",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20fall_quiring_prepub.pdf",
    "code_url": "https://scaling-attacks.net/",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1904.02144.pdf",
    "code_url": "https://github.com/Jianbo-Lab/HSJA",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-xiang.pdf",
    "code_url": "https://github.com/inspire-group/PatchGuard",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://people.cs.uchicago.edu/~ravenben/publications/pdf/trapdoor-ccs20.pdf",
    "code_url": "https://github.com/Shawn-Shan/trapdoor",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1911.01559.pdf",
    "code_url": "https://github.com/alps-lab/imc",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460120.3485378",
    "code_url": "https://github.innominds.com/CGCL-codes/FeatureIndistinguishableAttack",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2102.02956.pdf",
    "code_url": "https://github.com/inspire-group/DetectorGuard",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "It's Not What It Looks Like: Manipulating Perceptual Hashing based Applications",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://gangw.cs.illinois.edu/PHashing.pdf",
    "code_url": "https://github.com/gyNancy/phash_public",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2112.05282.pdf",
    "code_url": "https://github.com/RamBoAttack/RamBoAttack.github.io",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2201.09650.pdf",
    "code_url": "https://github.com/cure-lab/ContraNet",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "AutoDA: Automated Decision-based Iterative Adversarial Attacks",
    "venue": "USENIX",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22_slides-fu-qi.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-li-huiying.pdf",
    "code_url": "https://sandlab.cs.uchicago.edu/blacklight",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Physical Hijacking Attacks against Object Trackers",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://dl.acm.org/doi/10.1145/3548606.3559390",
    "code_url": "https://github.com/purseclab/AttrackZone",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2205.10686.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Squint Hard Enough: Attacking Perceptual Hashing with Adversarial Machine Learning",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_146-prokos-prepub.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "The Space of Adversarial Strategies",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_256-sheatsley-prepub.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2303.06280.pdf",
    "code_url": "https://github.com/purseclab/AttrackZone",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "BounceAttack: A Query-Efficient Decision-based Adversarial Attack by Bouncing into the Wild",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a068/1RjEaEvldVS",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Sabre: Cutting through Adversarial Noise with Adaptive Spectral Filtering and Input Reconstruction",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a076/1RjEaLx3uAU",
    "code_url": "https://github.com/Mobile-Intelligence-Lab/SABRE",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Sabre: Cutting through Adversarial Noise with Adaptive Spectral Filtering and Input Reconstruction",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a161/1Ub24A2RzHi",
    "code_url": "https://github.com/Cryptology-Algorithm-Lab/Scores_Tell_Everything_about_Bob",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2307.07873.pdf",
    "code_url": "https://github.com/CGCL-codes/TransferAttackSurrogates",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Group-based Robustness: A General Framework for Customized Robustness in the Real World",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2306.16614.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "DorPatch: Distributed and Occlusion-Robust Adversarial Patch to Evade Certifiable Defenses",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-920-paper.pdf",
    "code_url": "https://github.com/CGCL-codes/DorPatch",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "UniID: Spoofing Face Authentication System by Universal Identity",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-1036-paper.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Enhance Stealthiness and Transferability of Adversarial Attacks with Class Activation Mapping Ensemble Attack",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-164-paper.pdf",
    "code_url": "https://github.com/DreamyRainforest/Class_Activation_Mapping_Ensemble_Attack",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Neural Invisibility Cloak: Concealing Adversary in Images via Compromised AI-driven Image Signal Processing",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhu-wenjun.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "Self-interpreting Adversarial Images",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-tingwei.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.1 Image"
  },
  {
    "title": "TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20-li-jinfeng.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.2 Text"
  },
  {
    "title": "Bad Characters: Imperceptible NLP Attacks",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2106.09898.pdf",
    "code_url": "https://github.com/nickboucher/imperceptible",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.2 Text"
  },
  {
    "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.06506.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.2 Text"
  },
  {
    "title": "No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2303.14443.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.2 Text"
  },
  {
    "title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-hussain.pdf",
    "code_url": "https://github.com/shehzeen/waveguard_defense",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Dompteur: Taming Audio Adversarial Examples",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-eisenhofer.pdf",
    "code_url": "https://github.com/RUB-SysSec/dompteur",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "EarArray: Defending against DolphinAttack via Acoustic Attenuation",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://www.ndss-symposium.org/ndss-paper/eararray-defending-against-dolphinattack-via-acoustic-attenuation/",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/1911.01840.pdf",
    "code_url": "https://github.com/FAKEBOB-adversarial-attack/FAKEBOB",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/1910.05262.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2007.06622.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "AdvPulse: Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via Subsecond Perturbations",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "http://www.winlab.rutgers.edu/~yychen/papers/li2020advpulse.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2110.09714.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2207.13192.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "SpecPatch: Human-in-the-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://cse.msu.edu/~qyan/paper/SpecPatch_CCS22.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Learning Normality is Enough: A Software-based Mitigation against Inaudible Voice Attacks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/conference/usenixsecurity23/presentation/li-xinfeng",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Understanding and Benchmarking the Commonality of Adversarial Examples",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a111/1Ub23jYBBHa",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a056/1RjEav0Daa4",
    "code_url": "https://github.com/TASER2023/TASER",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.07780.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "When Translators Refuse to Translate: A Novel Attack to Speech Translation Systems",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wu-haolin.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.3 Audio"
  },
  {
    "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2107.04284.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.4 Video"
  },
  {
    "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2203.16000.pdf",
    "code_url": "https://github.com/JosephCao0327/StyleFool",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.4 Video"
  },
  {
    "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2108.09513.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.5 Graph"
  },
  {
    "title": "Evading Classifiers by Morphing in the Dark",
    "venue": "ACM CCS",
    "year": 2017,
    "pdf_url": "https://arxiv.org/pdf/1705.07535.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
    "venue": "USENIX Security",
    "year": 2019,
    "pdf_url": "https://arxiv.org/pdf/1905.12386.pdf",
    "code_url": "http://www.tu-braunschweig.de/sec/research/code/imitator",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1911.02142.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Structural Attack against Graph Based Android Malware Detection",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://www4.comp.polyu.edu.hk/~csxluo/HRAT.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_347-eykholt-prepub.pdf",
    "code_url": "https://github.com/IBM/URET",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Adversarial Training for Raw-Binary Malware Classifiers",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-146-lucas.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-493-zhang-zhuo.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2303.08509.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2309.01866.pdf",
    "code_url": "https://github.com/gnipping/AdvDroidZero-Access-Instructions",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom Attacks",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://ieeexplore.ieee.org/document/11023369",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.6 Software"
  },
  {
    "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2208.12897.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.7 Hardware"
  },
  {
    "title": "DeepShuffle: A Lightweight Defense Framework against Adversarial Fault Injection Attacks on Deep Neural Networks in Multi-Tenant Cloud-FPGA",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a034/1RjEa9WUlPi",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.7 Hardware"
  },
  {
    "title": "Interpretable Deep Learning under Fire",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20spring_zhang_prepub.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.8 Interpret Method"
  },
  {
    "title": "\u201cIs your explanation stable?\u201d: A Robustness Evaluation Framework for Feature Attribution",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.01782.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.8 Interpret Method"
  },
  {
    "title": "AIRS: Explanation for Deep Reinforcement Learning based Security Applications",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-36-yu-jiahao.pdf",
    "code_url": "https://github.com/sherdencooper/AIRS",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.8 Interpret Method"
  },
  {
    "title": "SoK: Explainable Machine Learning in Adversarial Environments",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": null,
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.8 Interpret Method"
  },
  {
    "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-lovisotto.pdf",
    "code_url": "https://github.com/ssloxford/short-lived-adversarial-perturbations",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.09577.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2302.09491.pdf",
    "code_url": "https://github.com/DIG-Beihang/X-adv",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_278-man-prepub.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_349-cao-prepub.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "CAPatch: Physical Adversarial Patch against Image Captioning Systems",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-121-zhang-shibo.pdf",
    "code_url": "https://github.com/USSLab/CAPatch",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Exorcising \"Wraith\": Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-190-xiao-qifan.pdf",
    "code_url": "https://github.com/USSLab/CAPatch",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2401.03582.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Avara: A Uniform Evaluation System for Perceptibility Analysis Against Adversarial Object Evasion Attacks",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://drive.google.com/file/d/16qfqZpOED2W3wXmGibdDOIK5ctboend7/view",
    "code_url": "https://sites.google.com/view/avara-artifacts",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "hhttps://tianweiz07.github.io/Papers/24-ccs1.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Invisible but Detected: Physical Adversarial Shadow Attack and Defense on LiDAR Object Detection",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-kobayashi.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "From Threat to Trust: Exploiting Attention Mechanisms for Attacks and Defenses in Cooperative Perception",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-chenyi.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.9 Physical World"
  },
  {
    "title": "Adversarial Policy Training against Deep Reinforcement Learning",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21summer_wu-xian.pdf",
    "code_url": "https://github.com/psuwuxian/rl_attack",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.10 Reinforcement Learning"
  },
  {
    "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2402.03741",
    "code_url": "https://github.com/maoubo/SUB-PLAY",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.10 Reinforcement Learning"
  },
  {
    "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-derui.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.10 Reinforcement Learning"
  },
  {
    "title": "Cost-Aware Robust Tree Ensembles for Security Applications",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-chen-yizheng.pdf",
    "code_url": "https://github.com/surrealyz/growtrees",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "CADE: Detecting and Explaining Concept Drift Samples for Security Applications",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-yang-limin.pdf",
    "code_url": "https://github.com/whyisyoung/CADE",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Learning Security Classifiers with Verified Global Robustness Properties",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2105.11363.pdf",
    "code_url": "https://github.com/surrealyz/verified-global-properties",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "On the Robustness of Domain Constraints",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2105.08619.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://nesa.zju.edu.cn/download/dty_pdf_cert_rnn.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2002.12398.pdf",
    "code_url": "https://github.com/AI-secure/semantic-randomized-smoothing",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Transcend: Detecting Concept Drift in Malware Classification Models",
    "venue": "USENIX Security",
    "year": 2017,
    "pdf_url": "https://s2lab.cs.ucl.ac.uk/downloads/sec17-jordaney.pdf",
    "code_url": "https://s2lab.cs.ucl.ac.uk/projects/transcend/",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://s2lab.cs.ucl.ac.uk/downloads/transcending.pdf",
    "code_url": "https://s2lab.cs.ucl.ac.uk/projects/transcend/",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-vaishnavi.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec22-vaishnavi.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f830_paper.pdf",
    "code_url": "https://github.com/dongtsi/OWAD",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f508_paper.pdf",
    "code_url": "https://github.com/KaiWangGitHub/BARS",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2301.02905.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Continuous Learning for Android Malware Detection",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2302.04332.pdf",
    "code_url": "https://github.com/wagner-group/active-learning",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2202.01811.pdf",
    "code_url": "https://github.com/inspire-group/ObjectSeeker",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2202.03277.pdf",
    "code_url": "https://github.com/serval-uni-lu/realistic_adversarial_hardening",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2307.16630.pdf",
    "code_url": "https://github.com/Eyr3/TextCRS?tab=readme-ov-file",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.11005.pdf",
    "code_url": "https://github.com/andrew-cullen/ensemble-simplex-certifications",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "SoK: Efficiency Robustness of Dynamic Deep Learning Systems",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-rathnasuriya.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-li-jiate.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Robustifying ML-powered Network Classifiers with PANTS",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-jin-minhao.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "CertTA: Certified Robustness Made Practical for Learning-Based Traffic Analysis",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-yan-jinzhu.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/html/2506.05402v1",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.11 Robust Defense"
  },
  {
    "title": "Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-nasr.pdf",
    "code_url": "https://github.com/SPIN-UMass/BLANKET",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.12 Network Traffic"
  },
  {
    "title": "Pryde: A Modular Generalizable Workflow for Uncovering Evasion Attacks Against Stateful Firewall Deployments",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a144/1Ub242nYFoY",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.12 Network Traffic"
  },
  {
    "title": "Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a006/1RjE9LaYR0c",
    "code_url": "https://github.com/mahmoudkanazzal/MintA",
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.12 Network Traffic"
  },
  {
    "title": "Swallow: A Transfer-Robust Website Fingerprinting Attack via Consistent Feature Learning",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://ccs25files.zoolab.org/main/ccsfa/TaZ6VzOa/3719027.3744795.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.12 Network Traffic"
  },
  {
    "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2102.00918.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.13 Wireless Communication System"
  },
  {
    "title": "Adversarial Robustness for Tabular Data through Cost and Utility Awareness",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f924_paper.pdf",
    "code_url": null,
    "section": "1.1 Adversarial Attack & Defense",
    "subsection": "1.1.14 Tabular Data"
  },
  {
    "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20summer_fang_prepub.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-3_24498_paper.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2201.00763.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "FLAME: Taming Backdoors in Federated Learning",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-nguyen.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "EIFFeL: Ensuring Integrity for Federated Learning",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2112.12727.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Eluding Secure Aggregation in Federated Learning via Model Inconsistency",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2111.07380.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2210.10936.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2110.04350.pdf",
    "code_url": "https://github.com/SPIN-UMass/FRL",
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Securing Federated Sensitive Topic Classification against Poisoning Attacks",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_s112_paper.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2301.09508.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2201.02775.pdf",
    "code_url": "https://github.com/Qi-Pang/ADI",
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600b893/1NrbZhCP5ao",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2308.05832.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2304.08847.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2210.07714.pdf",
    "code_url": "https://github.com/TRUST-TUDa/crowdguard",
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Automatic Adversarial Adaption for Stealthy Poisoning Attacks in Federated Learning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-1366-paper.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2312.04432.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning \u2013 A Survey and Taxonomy",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2312.04432.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Byzantine-Robust Decentralized Federated Learning",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2406.10416",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "PoiSAFL: Scalable Poisoning Attack Framework to Byzantine-resilient Semi-asynchronous Federated Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-pang-xiaoyi.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-popovic.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.1 Federated Learning"
  },
  {
    "title": "Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation Agent",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20-pan.pdf",
    "code_url": null,
    "section": "1.2 Distributed Machine Learning",
    "subsection": "1.2.2 Normal Distributed Learning"
  },
  {
    "title": "Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://www.cs.cornell.edu/~shmat/shmat_oak20.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.1 Hijack Embedding"
  },
  {
    "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-schuster.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.2 Hijack Autocomplete Code"
  },
  {
    "title": "TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2301.02344.pdf",
    "code_url": "https://github.com/microsoft/CodeGenerationPoisoning",
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.2 Hijack Autocomplete Code"
  },
  {
    "title": "Poisoning the Unlabeled Dataset of Semi-Supervised Learning",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-carlini-poisoning.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.3 Semi-Supervised Learning"
  },
  {
    "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2101.02644.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.4 Recommender Systems"
  },
  {
    "title": "Reverse Attack: Black-box Attacks on Collaborative Recommendation",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://dl.acm.org/doi/abs/10.1145/3460120.3484805",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.4 Recommender Systems"
  },
  {
    "title": "Subpopulation Data Poisoning Attacks",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2006.14026.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.5 Classification"
  },
  {
    "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2111.04394.pdf",
    "code_url": "https://github.com/AhmedSalem2/Model-Hijacking",
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.5 Classification"
  },
  {
    "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-liu-hongbin.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.6 Constractive Learning"
  },
  {
    "title": "Preference Poisoning Attacks on Reward Model Learning",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2402.01920",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.6 Constractive Learning"
  },
  {
    "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2204.00032.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.7 Privacy"
  },
  {
    "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2308.08505.pdf",
    "code_url": "https://github.com/tianshuocong/TePA",
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.8 Test-Time Poisoning"
  },
  {
    "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-shan.pdf",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.9 Defense"
  },
  {
    "title": "Understanding Implosion in Text-to-Image Generative Models",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2409.12314",
    "code_url": null,
    "section": "1.3 Data Poisoning",
    "subsection": "1.3.9 Defense"
  },
  {
    "title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-tang-di.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Double-Cross Attacks: Subverting Active Learning Systems",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-vicarte.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Detecting AI Trojans Using Meta Neural Analysis",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/1910.03137.pdf",
    "code_url": "https://github.com/AI-secure/Meta-Nerual-Trojan-Detection",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2108.00352.pdf",
    "code_url": "https://github.com/jjy1994/BadEncoder",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://dl.acm.org/doi/10.1145/3372297.3423362",
    "code_url": "https://github.com/TemporaryAcc0unt/composite-attack",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484818",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "LoneNeuron: a Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://www.ittc.ku.edu/~bluo/download/liu2022ccs.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2022-12-paper.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "RAB: Provable Robustness Against Backdoor Attacks",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2003.08904.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "A Data-free Backdoor Injection Approach in Neural Networks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-573-lv.pdf",
    "code_url": "https://github.com/lvpeizhuo/Data-free_Backdoor",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Backdoor Attacks Against Dataset Distillation",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2301.01197.pdf",
    "code_url": "https://github.com/liuyugeng/baadd",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2301.06241.pdf",
    "code_url": "https://github.com/Megum1/BEAGLE",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Disguising Attacks with Explanation-Aware Backdoors",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://intellisec.de/pubs/2023-ieeesp.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2212.04687.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "AI-Guardian: Defeating Adversarial Attacks using Backdoors",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600a701/1NrbXZPyl7W",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600a755/1NrbYbKqcHS",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "NARCISSUS: A Practical Clean-Label Backdoor Attack with Limited Information",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2204.05255.pdf",
    "code_url": "https://github.com/ruoxi-jia-group/Narcissus-backdoor-attack",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity23-pan.pdf",
    "code_url": "https://github.com/ruoxi-jia-group/ASSET",
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "ODSCAN: Backdoor Scanning for Object Detection Models",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2312.02673.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2205.06900.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Distribution Preserving Backdoor Attack in Self-supervised Learning",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a029/1RjEa5rjsHK",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Backdooring Bias (B^2) into Stable Diffusion Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-naseh.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Watch the Watchers! On the Security Risks of Robustness-Enhancing Diffusion Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-li-changjiang.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Pretender: Universal Active Defense against Diffusion Finetuning Attacks",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-sun-zekun.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Rowhammer-Based Trojan Injection: One Bit Flip Is Sufficient for Backdooring DNNs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-li-xiang.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "From Purity to Peril: Backdooring Merged Models From \"Harmless\" Benign Components",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-lijin.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-lin-chenhao.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "Persistent Backdoor Attacks in Continual Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-guo-zhen.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.1 Image"
  },
  {
    "title": "T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-azizi.pdf",
    "code_url": "https://github.com/reza321/T-Miner",
    "section": "1.4 Backdoor",
    "subsection": "1.4.2 Text"
  },
  {
    "title": "Hidden Backdoors in Human-Centric Language Models",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2105.00164.pdf",
    "code_url": "https://github.com/lishaofeng/NLP_Backdoor",
    "section": "1.4 Backdoor",
    "subsection": "1.4.2 Text"
  },
  {
    "title": "Backdoor Pre-trained Models Can Transfer to All",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2111.00197.pdf",
    "code_url": "https://github.com/lishaofeng/NLP_Backdoor",
    "section": "1.4 Backdoor",
    "subsection": "1.4.2 Text"
  },
  {
    "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-pan-hidden.pdf",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.2 Text"
  },
  {
    "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.11225.pdf",
    "code_url": "https://github.com/AI-secure/TextGuard",
    "section": "1.4 Backdoor",
    "subsection": "1.4.2 Text"
  },
  {
    "title": "Graph Backdoor",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2006.11890.pdf",
    "code_url": "https://github.com/HarrialX/GraphBackdoor",
    "section": "1.4 Backdoor",
    "subsection": "1.4.3 Graph"
  },
  {
    "title": "Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2407.08935",
    "code_url": "https://github.com/Yuxin104/Opt-GDBA",
    "section": "1.4 Backdoor",
    "subsection": "1.4.3 Graph"
  },
  {
    "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-severi.pdf",
    "code_url": "https://github.com/ClonedOne/MalwareBackdoors",
    "section": "1.4 Backdoor",
    "subsection": "1.4.4 Software"
  },
  {
    "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600a906/1Js0DtfUrKw",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.5 Audio"
  },
  {
    "title": "MagBackdoor: Beware of Your Loudspeaker as Backdoor of Magnetic Attack for Malicious Command Injection",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://huskyachao.github.io/publication/magbackdoor-oakland23/",
    "code_url": null,
    "section": "1.4 Backdoor",
    "subsection": "1.4.5 Audio"
  },
  {
    "title": "Backdooring Multimodal Learning",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a031/1RjEa7rmaxW",
    "code_url": "https://github.com/multimodalbags/BAGS_Multimodal",
    "section": "1.4 Backdoor",
    "subsection": "1.4.6 Multimedia"
  },
  {
    "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2302.06279.pdf",
    "code_url": "https://github.com/GorkaAbad/Sneaky-Spikes",
    "section": "1.4 Backdoor",
    "subsection": "1.4.7 Neuromorphic Data"
  },
  {
    "title": "Blind Backdoors in Deep Learning Models",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.cs.cornell.edu/~shmat/shmat_usenix21blind.pdf",
    "code_url": "https://github.com/ebagdasa/backdoors101",
    "section": "1.5 ML Library Security",
    "subsection": "1.5.1 Loss"
  },
  {
    "title": "IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-125-christou.pdf",
    "code_url": null,
    "section": "1.5 ML Library Security",
    "subsection": "1.5.1 Loss"
  },
  {
    "title": "Towards Understanding and Detecting Cyberbullying in Real-world Images",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/ndss2021_7C-4_24260_paper.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.1 Cyberbullying"
  },
  {
    "title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2308.05596.pdf",
    "code_url": "https://github.com/xinleihe/toxic-prompt",
    "section": "1.6 AI4Security",
    "subsection": "1.6.1 Cyberbullying"
  },
  {
    "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-shen.pdf",
    "code_url": "https://github.com/TrustAIRLab/HateBench",
    "section": "1.6 AI4Security",
    "subsection": "1.6.1 Cyberbullying"
  },
  {
    "title": "FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-4_24403_paper.pdf",
    "code_url": "https://github.com/junjieliang672/FARE",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "From Grim Reality to Practical Solution: Malware Classification in Real-World Noise",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://henrygwb.github.io/publications/sp23.pdf",
    "code_url": "https://github.com/gnipping/morse",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Decoding the Secrets of Machine Learning in Windows Malware Classification: A Deep Dive into Datasets, Features, and Model Performance",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2307.14657.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "KAIROS: Practical Intrusion Detection and Investigation using Whole-system Provenance",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2308.05034.pdf",
    "code_url": "https://github.com/ProvenanceAnalytics/kairos",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "FLASH: A Comprehensive Approach to Intrusion Detection via Provenance Graph Representation Learning",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a139/1Ub23WQw20U",
    "code_url": "https://github.com/DART-Laboratory/Flash-IDS",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a012/1RjE9Q5gQrm",
    "code_url": "https://github.com/C0ldstudy/Argus",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.16940.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "GNNIC: Finding Long-Lost Sibling Functions with Abstract Similarity",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-492-paper.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Experimental Analyses of the Physical Surveillance Risks in Client-Side Content Scanning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-1401-paper.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Attributions for ML-based ICS Anomaly Detection: From Theory to Practice",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-216-paper.pdf",
    "code_url": "https://github.com/pwwl/ics-anomaly-attribution",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "DRAINCLoG: Detecting Rogue Accounts with Illegally-obtained NFTs using Classifiers Learned on Graphs",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2301.13577.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.04798.pdf",
    "code_url": "https://github.com/XXnormal/RAPIER",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2409.09272",
    "code_url": "https://github.com/LetterLiGo/SafeEar",
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "USD: NSFW Content Detection for Text-to-Image Models via Scene Graph",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-yuyang.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wu-yixin-generation.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "VoiceWukong: Benchmarking Deepfake Voice Detection",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-yan-ziwei.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-zhisheng.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2410.17910",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2405.04095",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "MM4flow: A Pre-trained Multi-modal Model for Versatile Network Traffic Analysis",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://ccs25files.zoolab.org/main/ccsfa/j43ciqBt/3719027.3744804.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2506.17162",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.2 Security Applications"
  },
  {
    "title": "WtaGraph: Web Tracking and Advertising Detection using Graph Neural Networks",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://zhiju.me/assets/files/WtaGraph_SP22.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.3 Advertisement Detection"
  },
  {
    "title": "Text Captcha Is Dead? A Large Scale Deployment and Empirical Studys",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://nesa.zju.edu.cn/download/Text%20Captcha%20Is%20Dead%20A%20Large%20Scale%20Deployment%20and%20Empirical%20Study.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.4 CAPTCHA"
  },
  {
    "title": "Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f243_paper.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.4 CAPTCHA"
  },
  {
    "title": "A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600b524/1Js0E2VGRhe",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.4 CAPTCHA"
  },
  {
    "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2103.03809.pdf",
    "code_url": "https://github.com/palmtreemodel/PalmTree",
    "section": "1.6 AI4Security",
    "subsection": "1.6.5 Code Analysis"
  },
  {
    "title": "CALLEE: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2111.01415.pdf",
    "code_url": "https://github.com/vul337/Callee",
    "section": "1.6 AI4Security",
    "subsection": "1.6.5 Code Analysis"
  },
  {
    "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2112.02125.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.5 Code Analysis"
  },
  {
    "title": "Raconteur: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
    "venue": "NDSS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2409.02074",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.5 Code Analysis"
  },
  {
    "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.03463.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.6 Chatbot"
  },
  {
    "title": "Towards a General Video-based Keystroke Inference Attack",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_338-yang_zhuolin-prepub.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.7 Side Channel Attack"
  },
  {
    "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2306.11924.pdf",
    "code_url": "https://github.com/computationalprivacy/dual-purpose-client-side-scanning",
    "section": "1.6 AI4Security",
    "subsection": "1.6.7 Side Channel Attack"
  },
  {
    "title": "Dos and Don'ts of Machine Learning in Computer Security",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22summer_arp.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.8 Guidline"
  },
  {
    "title": "\u201cSecurity is not my field, I\u2019m a stats guy\u201d: A Qualitative Root Cause Analysis of Barriers to Adversarial Machine Learning Defenses in Industry",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-324-mink.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.8 Guidline"
  },
  {
    "title": "Everybody\u2019s Got ML, Tell Me What Else You Have: Practitioners\u2019 Perception of ML-Based Security Tools and Explanations",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://gangw.cs.illinois.edu/Security_ML-user.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.8 Guidline"
  },
  {
    "title": "CERBERUS: Exploring Federated Prediction of Security Events",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.03050.pdf",
    "code_url": null,
    "section": "1.6 AI4Security",
    "subsection": "1.6.9 Security Event"
  },
  {
    "title": "VulChecker: Graph-based Vulnerability Localization in Source Code",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/conference/usenixsecurity23/presentation/mirsky",
    "code_url": "https://github.com/ymirsky/VulChecker",
    "section": "1.6 AI4Security",
    "subsection": "1.6.10 Vulnerability Discovery"
  },
  {
    "title": "On the Security Risks of AutoML",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22summer_pang.pdf",
    "code_url": null,
    "section": "1.7 AutoML Security",
    "subsection": "1.7.1 Security Analysis"
  },
  {
    "title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/2009.09663.pdf",
    "code_url": null,
    "section": "1.8 Hardware Related Security",
    "subsection": "1.8.1 Verification"
  },
  {
    "title": "NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_334-liu_qi-prepub.pdf",
    "code_url": null,
    "section": "1.8 Hardware Related Security",
    "subsection": "1.8.1 Verification"
  },
  {
    "title": "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-246-wang-jialai.pdf",
    "code_url": "https://github.com/vul337/Aegis",
    "section": "1.8 Hardware Related Security",
    "subsection": "1.8.1 Verification"
  },
  {
    "title": "DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2109.11495.pdf",
    "code_url": "https://github.com/dongtsi/DeepAID",
    "section": "1.9 Security Related Interpreting Method",
    "subsection": "1.9.1 Anomaly Detection"
  },
  {
    "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2309.05679.pdf",
    "code_url": "https://github.com/JenniferHo97/XAI-TREND-TEST",
    "section": "1.9 Security Related Interpreting Method",
    "subsection": "1.9.2 Faithfulness"
  },
  {
    "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2308.05362.pdf",
    "code_url": "https://github.com/E0HYL/FINER-explain",
    "section": "1.9 Security Related Interpreting Method",
    "subsection": "1.9.3 Security Applications"
  },
  {
    "title": "Who Are You (I Really Wanna Know)? Detecting Audio DeepFakes Through Vocal Tract Reconstruction",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22fall_blue.pdf",
    "code_url": null,
    "section": "1.10 Face Security",
    "subsection": "1.10.1 Deepfake Detection"
  },
  {
    "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://kaiyuanzhang.com/publications/SP23_ImU.pdf",
    "code_url": "https://github.com/njuaplusplus/imu",
    "section": "1.10 Face Security",
    "subsection": "1.10.2 Face Impersonation"
  },
  {
    "title": "DepthFake: Spoofing 3D Face Authentication with a 2D Photo",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600b710/1Js0EgNcf8A",
    "code_url": null,
    "section": "1.10 Face Security",
    "subsection": "1.10.2 Face Impersonation"
  },
  {
    "title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://yinzhicao.org/xfvschecker/XFVSChecker.pdf",
    "code_url": null,
    "section": "1.10 Face Security",
    "subsection": "1.10.3 Face Verification Systems"
  },
  {
    "title": "Deepfake Text Detection: Limitations and Opportunities",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2210.09421.pdf",
    "code_url": "https://github.com/jmpu/DeepfakeTextDetection",
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.1 Text Generation Detection"
  },
  {
    "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2303.14822",
    "code_url": "https://github.com/xinleihe/MGTBench",
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.1 Text Generation Detection"
  },
  {
    "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
    "venue": "CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2510.05173",
    "code_url": null,
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.1 Text Generation Detection"
  },
  {
    "title": "SoK: The Good, The Bad, and The Unbalanced: Measuring Structural Limitations of Deepfake Media Datasets",
    "venue": "USENIX Security",
    "year": 2024,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity24-layton.pdf",
    "code_url": null,
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.2 Deepfake"
  },
  {
    "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2409.09272",
    "code_url": "https://github.com/LetterLiGo/SafeEar",
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.2 Deepfake"
  },
  {
    "title": "\"Better Be Computer or I\u2019m Dumb\": A Large-Scale Evaluation of Humans as Audio Deepfake Detectors",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://cise.ufl.edu/~butler/pubs/ccs24-warren-deepfake.pdf",
    "code_url": null,
    "section": "1.10 AI Generation Security",
    "subsection": "1.10.2 Deepfake"
  },
  {
    "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
    "venue": "ACM CCS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2302.05319.pdf",
    "code_url": "https://github.com/eth-sri/sven",
    "section": "1.11 LLM Security",
    "subsection": "1.11.1 Code Analysis"
  },
  {
    "title": "DeGPT: Optimizing Decompiler Output with LLM",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-401-paper.pdf",
    "code_url": "https://github.com/PeiweiHu/DeGPT",
    "section": "1.11 LLM Security",
    "subsection": "1.11.1 Code Analysis"
  },
  {
    "title": "Raconteur: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
    "venue": "NDSS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2409.02074",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.1 Code Analysis"
  },
  {
    "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2409.12699",
    "code_url": "https://github.com/mahmoudkanazzal/PromSec",
    "section": "1.11 LLM Security",
    "subsection": "1.11.1 Code Analysis"
  },
  {
    "title": "We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-spracklen.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.1 Code Analysis"
  },
  {
    "title": "Transferable Multimodal Attack on Vision-Language Pre-training Models",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2305.12082.pdf",
    "code_url": "https://github.com/Yuchen413/text2image_safety",
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2404.06666",
    "code_url": "https://github.com/LetterLiGo/SafeGen_CCS2024",
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.14122",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2408.07728",
    "code_url": "https://github.com/DataSmithLab/Moderator",
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-qu-yiting.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "Are CAPTCHAs Still Bot-hard? Generalized Visual CAPTCHA Solving with Agentic Vision Language Model",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-teoh.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "From Meme to Threat: On the Hateful Meme Understanding and Induced Hateful Content Generation in Open-Source Vision Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-ma-yihan.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.2 Vision-Language Model"
  },
  {
    "title": "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2307.08715.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-yu-jiahao.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-song-minkyoo.pdf",
    "code_url": "https://doi.org/10.5281/zenodo.15628860",
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-jiawen.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Exposing the Guardrails: Reverse-Engineering and Jailbreaking Safety Filters in DALL\u00b7E Text-to-Image Pipelines",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-villa.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-krauss.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-lan.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-gong-xueluan.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-russinovich.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-xunguang.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-shenyi.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.3 Jailbreaking"
  },
  {
    "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.17400.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.4 Robustness"
  },
  {
    "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.05019.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2402.03214",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2306.05524v2",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-meng.pdf",
    "code_url": "https://zenodo.org/records/15807727",
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-ye-attacks.pdf",
    "code_url": "https://zenodo.org/records/14737003",
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "\"I Cannot Write This Because It Violates Our Content Policy\": Understanding Content Moderation Policies and User Experiences in Generative AI Products",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-gao-lan.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-akkus.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.5 Generated Concent"
  },
  {
    "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2308.13904.pdf",
    "code_url": "https://github.com/meng-wenlong/LMSanitator",
    "section": "1.11 LLM Security",
    "subsection": "1.11.6 Backdoor"
  },
  {
    "title": "EmbedX: Embedding-Based Cross-Trigger Backdoor Attack Against Large Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-yan-nan.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.6 Backdoor"
  },
  {
    "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-kim-hanna.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "Make Agent Defeat Agent: Automatic Detection of Taint-Style Vulnerabilities in LLM-based Agents",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-liu-fengyu.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-yanting.pdf",
    "code_url": "https://github.com/Wang-Yanting/TracLLM",
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-luo-zeren.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "Cloak, Honey, Trap: Proactive Defenses Against LLM Agents",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-ayzenshteyn.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-vekaria.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2509.07764",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.7 Agent Security"
  },
  {
    "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-chen-sizhe.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.8 Prompt Injection"
  },
  {
    "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-shafran.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.8 Prompt Injection"
  },
  {
    "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2410.05451",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.8 Prompt Injection"
  },
  {
    "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-yining.pdf",
    "code_url": null,
    "section": "1.11 LLM Security",
    "subsection": "1.11.9 Hallucination"
  },
  {
    "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20summer_salem_prepub.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Extracting Training Data from Large Language Models",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-carlini-extracting.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Analyzing Information Leakage of Updates to Natural Language Models",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://www.microsoft.com/en-us/research/uploads/prod/2020/09/ccs20.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2107.13190.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2103.11109.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Property Inference Attacks Against GANs",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://yangzhangalmo.github.io/papers/NDSS22-PIAGAN.pdf",
    "code_url": "https://github.com/Zhou-Junhao/PIA_GAN",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "MIRROR: Model Inversion for Deep Learning Network with High Fidelity",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2022-335-paper.pdf",
    "code_url": "https://model-inversion.github.io/mirror/",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2302.00539.pdf",
    "code_url": "https://github.com/microsoft/analysing_pii_leakage",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Timing Channels in Adaptive Neural Networks",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-125-paper.pdf",
    "code_url": "https://github.com/akinsanyaayomide/ADNNTimeLeaks",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2401.07205.pdf",
    "code_url": "https://github.com/ShimingWang98/Facial_Feature_Crafting_against_Inversion_based_Identity_Theft/tree/main",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.07389.pdf",
    "code_url": "https://github.com/guyAmit/Transpose-Attack-paper-NDSS24-/tree/main",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2409.09272",
    "code_url": "https://github.com/LetterLiGo/SafeEar",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2406.14114",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
    "venue": "ACM CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2404.17399",
    "code_url": "https://github.com/ethz-spylab/misleading-privacy-evals?tab=readme-ov-file",
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Towards a Re-evaluation of Data Forging Attacks in Practice",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-suliman.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wen.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2409.06280",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2506.24033",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.1 Data Recovery"
  },
  {
    "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20-leino.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-song.pdf",
    "code_url": "https://github.com/inspire-group/membership-inference-evaluation",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
    "venue": "NDSS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2101.01341.pdf",
    "code_url": "https://github.com/hyhmia/BlindMI",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1909.03935.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://yangzhangalmo.github.io/papers/CCS21-ContrastivePrivacy.pdf",
    "code_url": "https://github.com/xinleihe/ContrastiveLeaks",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks Against Recommender Systems",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://yangzhangalmo.github.io/papers/CCS21-RecommenderMIA.pdf",
    "code_url": "https://github.com/minxingzhang/MIARS",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2108.11023.pdf",
    "code_url": "https://github.com/minxingzhang/MIARS",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Auditing Membership Leakages of Multi-Exit Networks",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2208.11180.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2208.14933.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "On the Privacy Risks of Cell-Based NAS Architectures",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.01688.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks and Defenses in Neural Network Pruning",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-yuan-xiaoyong.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-yuan-xiaoyong.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2111.09679.pdf",
    "code_url": "https://github.com/privacytrustlab/ml_privacy_meter/tree/master/research/2022_enhanced_mia",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.08615.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.07983.pdf",
    "code_url": "https://github.com/S3L-official/SLMIA-SR",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2307.01610.pdf",
    "code_url": "https://github.com/DependableSystemsLab/MIA_defense_HAMP",
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks Against Vision-Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-hu-yuke.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/conference/usenixsecurity25/presentation/he-yu",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Enhanced Label-Only Membership Inference Attacks with Fewer Queries",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-li-hao.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zhang-kaiyuan.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2506.13972",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.2 Membership Inference Attack"
  },
  {
    "title": "Label Inference Attacks Against Vertical Federated Learning",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22summer_fu.pdf",
    "code_url": "https://github.com/minxingzhang/MIARS",
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "The Value of Collaboration in Convex Machine Learning with Differential Privacy",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1906.09679.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2012.02670.pdf",
    "code_url": "https://github.com/pasquini-dario/SplitNN_FSHA",
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Local and Central Differential Privacy for Robustness and Privacy in Federated Learning",
    "venue": "NDSS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2009.03561.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23summer_372-yue-prepub.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "PPA: Preference Profiling Attack Against Federated Learning",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_s171_paper.pdf",
    "code_url": "https://github.com/PPAattack/PPAattack",
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2205.08443.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "RoFL: Robustness of Secure Federated Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2107.03311.pdf",
    "code_url": "https://github.com/pps-lab/rofl-project-code",
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Scalable and Privacy-Preserving Federated Principal Component Analysis",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2304.00129.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Protecting Label Distribution in Cross-Silo Federated Learning",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a113/1Ub23mqt0hG",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2303.12233.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2408.16913",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-fan-boosting.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-fan-boosting.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Aion: Robust and Efficient Multi-Round Single-Mask Secure Aggregation Against Malicious Participants",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-liu-yizhong.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "SoK: On Gradient Leakage in Federated Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-du.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning with Client Momentum",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-gu-xiaolan.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "SLOTHE : Lazy Approximation of Non-Arithmetic Neural Network Functions over Encrypted Data",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-nam-slothe.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Sharpness-Aware Initialization: Improving Differentially Private Machine Learning from First Principles",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-zihao.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Task-Oriented Training Data Privacy Protection for Cloud-based Model Training",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-zhiqiang.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-xu-xiangrui.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "SoK: Gradient Inversion Attacks in Federated Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-carletti.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.3 Information Leakage in Distributed ML System"
  },
  {
    "title": "Privacy Risks of General-Purpose Language Models",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://nesa.zju.edu.cn/download/Privacy%20Risks%20of%20General-Purpose%20Language%20Models.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.4 Information Leakage in Embedding"
  },
  {
    "title": "Information Leakage in Embedding Models",
    "venue": "ACM CCS",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/2004.00053.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.4 Information Leakage in Embedding"
  },
  {
    "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2105.12049.pdf",
    "code_url": "https://github.com/mmalekzadeh/honest-but-curious-nets",
    "section": "2.1 Training Data",
    "subsection": "2.1.4 Information Leakage in Embedding"
  },
  {
    "title": "Stealing Links from Graph Neural Networks",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2105.12049.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "Inference Attacks Against Graph Neural Networks",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22summer_zhang-zhikun.pdf",
    "code_url": "https://github.com/Zhangzhk0819/GNN-Embedding-Leaks",
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2108.06504.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "Locally Private Graph Neural Networks",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2006.05535.pdf",
    "code_url": "https://github.com/sisaman/LPGNN",
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2204.06963.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "Group Property Inference Attacks Against Graph Neural Networks",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.01100.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "LPGNet: Link Private Graph Networks for Node Classification",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2205.03105.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
    "venue": "MDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2312.07861.pdf",
    "code_url": "https://github.com/GraphGuard/GraphGuard-Proactive",
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2501.10985",
    "code_url": "https://github.com/GraphGuard/GraphGuard-Proactive",
    "section": "2.1 Training Data",
    "subsection": "2.1.5 Graph Leakage"
  },
  {
    "title": "Machine Unlearning",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1912.03817.pdf",
    "code_url": "https://github.com/cleverhans-lab/machine-unlearning",
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "When Machine Unlearning Jeopardizes Privacy",
    "venue": "ACM CCS",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2005.02205.pdf",
    "code_url": "https://github.com/MinChen00/UnlearningLeaks",
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Graph Unlearning",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2103.14991.pdf",
    "code_url": "https://github.com/MinChen00/Graph-Unlearning",
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22fall_thudi.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Machine Unlearning of Features and Labels",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_s87_paper.pdf",
    "code_url": "https://github.com/alewarne/MachineUnlearning",
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.08230.pdf",
    "code_url": "https://github.com/TASI-LAB/Over-unlearning",
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.16136",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-naderloui.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-ye-duplication.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wang-cheng-long.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Split Unlearning",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2308.10422",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Rethinking Machine Unlearning in Image Generation Models",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2506.02761",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Prototype Surgery: Tailoring Neural Prototypes via Soft Labels for Efficient Machine Unlearning",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://ccs25files.zoolab.org/main/ccsfa/sV74FPQe/3719027.3744827.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.6 Unlearning"
  },
  {
    "title": "Are Attribute Inference Attacks Just Imputation?",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2209.01292.pdf",
    "code_url": "https://github.com/bargavj/EvaluatingDPML",
    "section": "2.1 Training Data",
    "subsection": "2.1.7 Attribute Inference Attack"
  },
  {
    "title": "Feature Inference Attack on Shapley Values",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://dl.acm.org/doi/abs/10.1145/3548606.3560573",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.7 Attribute Inference Attack"
  },
  {
    "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2211.05249.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.7 Attribute Inference Attack"
  },
  {
    "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-kabir.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.7 Attribute Inference Attack"
  },
  {
    "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2208.12348.pdf",
    "code_url": "https://github.com/johnmath/snap-sp23",
    "section": "2.1 Training Data",
    "subsection": "2.1.7 Property Inference Attack"
  },
  {
    "title": "SoK: Privacy-Preserving Data Synthesis",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2307.02106.pdf",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.8 Data Synthesis"
  },
  {
    "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2309.03081.pdf",
    "code_url": "https://github.com/link-zju/ORL-Auditor",
    "section": "2.1 Training Data",
    "subsection": "2.1.8 Dataset Auditing"
  },
  {
    "title": "SoK: Dataset Copyright Auditing in Machine Learning Systems",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2410.16618",
    "code_url": null,
    "section": "2.1 Training Data",
    "subsection": "2.1.8 Dataset Auditing"
  },
  {
    "title": "Exploring Connections Between Active Learning and Model Extraction",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://www.usenix.org/system/files/sec20-chandrasekaran.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1909.01838.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "DRMI: A Dataset Reduction Technology based on Mutual Information for Black-box Attacks",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-he-yingzhe.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "Entangled Watermarks as a Defense against Model Extraction",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21fall-jia.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
    "venue": "NDSS",
    "year": 2020,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2020/02/24178.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
    "venue": "USENIX Securiy",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22-chen-yufei.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2201.05889.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600a432/1He7YbsiH4c",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "SoK: Neural Network Extraction Through Physical Side Channels",
    "venue": "USENIX Security",
    "year": 2024,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity24-horvath.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
    "venue": "USENIX Security",
    "year": 2024,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity24-nayan.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.1 Model Extraction"
  },
  {
    "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2009.03015.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/system/files/sec23fall-prepub-444-yan-yifan.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2401.15239.pdf",
    "code_url": "https://github.com/lvpeizhuo/MEA-Defender",
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-Supervised Learning",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2209.03563.pdf",
    "code_url": "https://github.com/lvpeizhuo/SSL-WM",
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "Watermarking Language Models for Many Adaptive Users",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2209.03563.pdf",
    "code_url": "https://arxiv.org/pdf/2405.11109",
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "SoK: Watermarking for AI-Generated Content",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2411.18479",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-qu-watermarking.pdf",
    "code_url": "https://arxiv.org/pdf/2405.11109",
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "AUDIO WATERMARK: Dynamic and Harmless Watermark for Black-box Voice Dataset Copyright Protection",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-guo-hanqing.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "AudioMarkNet: Audio Watermarking for Deepfake Speech Detection",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-zong.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-chang-yijia-verification.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "LightShed: Defeating Perturbation-based Image Copyright Protections",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-foerster.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-lin-junhua.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.2 Model Watermark"
  },
  {
    "title": "Proof-of-Learning: Definitions and Practice",
    "venue": "IEEE S&P",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2103.05633.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "SoK: How Robust is Image Classification Deep Neural Network Watermarking?",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2108.04974.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
    "venue": "IEEE S&P",
    "year": 2022,
    "pdf_url": "https://nesa.zju.edu.cn/download/cjl_pdf_sp22.pdf",
    "code_url": "https://github.com/Testing4AI/DeepJudge",
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "https://arxiv.org/pdf/2201.11692.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "RAI2: Responsible Identity Audit Governing the Artificial Intelligence",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2201.11692.pdf",
    "code_url": "https://github.com/chichidd/RAI2",
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "ActiveDaemon: Unconscious DNN Dormancy and Waking Up via User-specific Invisible Token",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-588-paper.pdf",
    "code_url": "https://github.com/LANCEREN/ActiveDaemon",
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-huang-yujin.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.3 Model Owenership"
  },
  {
    "title": "PublicCheck: Public Integrity Verification for Services of Run-time Deep Models",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2203.10902.pdf",
    "code_url": null,
    "section": "2.2 Model",
    "subsection": "2.2.4 Model Integrity"
  },
  {
    "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2503.09022",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "On the Effectiveness of Prompt Stealing Attacks on In-the-Wild Prompts",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2025/223600a392/26hiTFMb8eQ",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "PRSA: Prompt Stealing Attacks against Real-World Prompt Services",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-yang-yong.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "Cross-Modal Prompt Inversion: Unifying Threats to Text and Image Generative AI Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-ye-inversion.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "Prompt Obfuscation for Large Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-pape.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
    "venue": "ACM CCS",
    "year": 2025,
    "pdf_url": "https://arxiv.org/pdf/2503.09291",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.1 Prompt Privacy"
  },
  {
    "title": "Codebreaker: Dynamic Extraction Attacks on Code Language Models",
    "venue": "IEEE S&P",
    "year": 2025,
    "pdf_url": "https://ieeexplore.ieee.org/document/11023359",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "LLMmap: Fingerprinting for Large Language Models",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-pasquini.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "Unlocking the Power of Differentially Private Zeroth-order Optimization for Fine-tuning LLMs",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-bao-ergute.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-dong-tian.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-liu-yupei.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-he-jinwen.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.2 Model Privacy"
  },
  {
    "title": "Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-wu-yixin-auditing.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.3 Data Privacy"
  },
  {
    "title": "Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-jin-weifei.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.3 Data Privacy"
  },
  {
    "title": "Effective PII Extraction from LLMs through Augmented Few-Shot Learning",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-cheng-shuai.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.3 Data Privacy"
  },
  {
    "title": "Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts",
    "venue": "USENIX Security",
    "year": 2025,
    "pdf_url": "https://www.usenix.org/system/files/usenixsecurity25-keum.pdf",
    "code_url": null,
    "section": "2.3 LLM Privacy",
    "subsection": "2.3.3 Data Privacy"
  },
  {
    "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models",
    "venue": "USENIX Security",
    "year": 2020,
    "pdf_url": "https://people.cs.uchicago.edu/~ravenben/publications/pdf/fawkes-usenix20.pdf",
    "code_url": "https://github.com/Shawn-Shan/fawkes",
    "section": "2.4 User Related Privacy",
    "subsection": "2.4.1 Image"
  },
  {
    "title": "Automatically Detecting Bystanders in Photos to Reduce Privacy Risks",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "http://vision.soic.indiana.edu/papers/bystander2020oakland.pdf",
    "code_url": null,
    "section": "2.4 User Related Privacy",
    "subsection": "2.4.1 Image"
  },
  {
    "title": "Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks",
    "venue": "IEEE S&P",
    "year": 2020,
    "pdf_url": "https://dl.acm.org/doi/abs/10.1145/3548606.3560571",
    "code_url": null,
    "section": "2.4 User Related Privacy",
    "subsection": "2.4.1 Image"
  },
  {
    "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
    "venue": "USENIX Security",
    "year": 2023,
    "pdf_url": "https://www.usenix.org/conference/usenixsecurity23/presentation/rosenberg",
    "code_url": "https://github.com/wi-pi/fairness_face_obfuscation",
    "section": "2.4 User Related Privacy",
    "subsection": "2.4.1 Image"
  },
  {
    "title": "SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://arxiv.org/pdf/2005.10296.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.1 3PC"
  },
  {
    "title": "BLAZE: Blazing Fast Privacy-Preserving Machine Learning",
    "venue": "NDSS",
    "year": 2020,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2020/02/24202-paper.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.1 3PC"
  },
  {
    "title": "Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2210.01988.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.1 3PC"
  },
  {
    "title": "Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2406.07948",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.1 3PC"
  },
  {
    "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning",
    "venue": "NDSS",
    "year": 2020,
    "pdf_url": "https://arxiv.org/pdf/1912.02631.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.2 4PC"
  },
  {
    "title": "Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning",
    "venue": "USENIX Security",
    "year": 2021,
    "pdf_url": "https://www.usenix.org/system/files/sec21-zheng.pdf",
    "code_url": "https://github.com/mc2-project/cerebro",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2208.08662.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "MPCDiff: Testing and Repairing MPC-Hardened Deep Learning Models",
    "venue": "NDSS",
    "year": 2023,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-380-paper.pdf",
    "code_url": "https://github.com/Qi-Pang/MPCDiff",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-512-paper.pdf",
    "code_url": "https://github.com/lightbulb128/Pencil",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "Securely Training Decision Trees Efficiently",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://eprint.iacr.org/2024/1077.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "CoGNN: Towards Secure and Efficient Collaborative Graph Learning",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://eprint.iacr.org/2024/987.pdf",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.3 SMPC"
  },
  {
    "title": "SoK: Cryptographic Neural-Network Computation",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://sokcryptonn.github.io/",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.4 Cryptographic NN Computation"
  },
  {
    "title": "From Individual Computation to Allied Optimization: Remodeling Privacy-Preserving Neural Inference with Function Input Tuning",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a101/1Ub238IknIs",
    "code_url": null,
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.4 Cryptographic NN Computation"
  },
  {
    "title": "BOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a130/1Ub23O2X00U",
    "code_url": "https://github.com/Clive2312/BOLT",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.4 Cryptographic NN Computation"
  },
  {
    "title": "Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://sokcryptonn.github.io/",
    "code_url": "https://github.com/eniac/flamingo",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.5 Secure Aggregation"
  },
  {
    "title": "ELSA: Secure Aggregation for Federated Learning with Malicious Actors",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://eprint.iacr.org/2022/1695.pdf",
    "code_url": "https://github.com/ucbsky/elsa",
    "section": "2.5 Private ML Protocols",
    "subsection": "2.5.5 Secure Aggregation"
  },
  {
    "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
    "venue": "USENIX Security",
    "year": 2022,
    "pdf_url": "https://www.usenix.org/system/files/sec22summer_liu-yugeng.pdf",
    "code_url": null,
    "section": "2.6 Platform",
    "subsection": "2.6.1 Inference Attack Measurement"
  },
  {
    "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://arxiv.org/pdf/2212.10986.pdf",
    "code_url": null,
    "section": "2.6 Platform",
    "subsection": "2.6.2 Survey"
  },
  {
    "title": "Federated Boosted Decision Trees with Differential Privacy",
    "venue": "ACM CCS",
    "year": 2022,
    "pdf_url": "http://dimacs.rutgers.edu/~graham/pubs/papers/dpxgboost.pdf",
    "code_url": null,
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.1 Tree Model"
  },
  {
    "title": "Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600b944/1NrbZkrFZi8",
    "code_url": null,
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.2 DP"
  },
  {
    "title": "Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a088/1Ub22UPYcsU",
    "code_url": null,
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.2 DP"
  },
  {
    "title": "Bounded and Unbiased Composite Differential Privacy",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2311.02324.pdf",
    "code_url": "https://github.com/CompositeDP/CompositeDP",
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.2 DP"
  },
  {
    "title": "Cohere: Managing Differential Privacy in Large Scale Systems",
    "venue": "IEEE S&P",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2301.08517.pdf",
    "code_url": "https://github.com/pps-lab/cohere",
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.2 DP"
  },
  {
    "title": "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks",
    "venue": "NDSS",
    "year": 2024,
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2024-1361-paper.pdf",
    "code_url": "https://github.com/Edison9419/ndss",
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.2 DP"
  },
  {
    "title": "Locally Differentially Private Frequency Estimation Based on Convolution Framework",
    "venue": "IEEE S&P",
    "year": 2023,
    "pdf_url": "https://www.computer.org/csdl/proceedings-article/sp/2023/933600c208/1NrbZx7nFkI",
    "code_url": null,
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.3 LDP"
  },
  {
    "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2406.19466",
    "code_url": null,
    "section": "2.7 Differential Privacy",
    "subsection": "2.7.3 LDP"
  },
  {
    "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
    "venue": "CCS",
    "year": 2024,
    "pdf_url": "https://arxiv.org/pdf/2405.06823",
    "code_url": "https://github.com/BHui97/PLeak",
    "section": "2.7 LLM Privacy",
    "subsection": "2.7.1 Prompt Privacy"
  }
]