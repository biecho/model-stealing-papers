{
  "source": "arxiv",
  "total": 194,
  "papers": [
    {
      "arxiv_id": "1904.02144",
      "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
      "abstract": "The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Jianbo Chen",
        "Michael I. Jordan",
        "Martin J. Wainwright"
      ],
      "pdf_url": "https://arxiv.org/pdf/1904.02144.pdf",
      "code_url": "https://github.com/Jianbo-Lab/HSJA",
      "url": "https://arxiv.org/abs/1904.02144",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "1911.01559",
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Ren Pang",
        "Hua Shen",
        "Xinyang Zhang",
        "Shouling Ji",
        "Yevgeniy Vorobeychik",
        "Xiapu Luo",
        "Alex Liu",
        "Ting Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.01559.pdf",
      "code_url": "https://github.com/alps-lab/imc",
      "url": "https://arxiv.org/abs/1911.01559",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2102.02956",
      "title": "DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks",
      "abstract": "State-of-the-art object detectors are vulnerable to localized patch hiding attacks, where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard as the first general framework for building provably robust object detectors against localized patch hiding attacks. DetectorGuard is inspired by recent advancements in robust image classification research; we ask: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (<1%) of clean performance.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Chong Xiang",
        "Prateek Mittal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2102.02956.pdf",
      "code_url": "https://github.com/inspire-group/DetectorGuard",
      "url": "https://arxiv.org/abs/2102.02956",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2112.05282",
      "title": "RamBoAttack: A Robust Query Efficient Deep Neural Network Decision Exploit",
      "abstract": "Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low distortion adversarial employing gradient estimation methods. We develop a robust query efficient attack capable of avoiding entrapment in a local minimum and misdirection from noisy gradients seen in gradient estimation methods. The attack method we propose, RamBoAttack, exploits the notion of Randomized Block Coordinate Descent to explore the hidden classifier manifold, targeting perturbations to manipulate only localized input features to address the issues of gradient estimation methods. Importantly, the RamBoAttack is more robust to the different sample inputs available to an adversary and the targeted class. Overall, for a given target class, RamBoAttack is demonstrated to be more robust at achieving a lower distortion within a given query budget. We curate our extensive results using the large-scale high-resolution ImageNet dataset and open-source our attack, test samples and artifacts on GitHub.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Viet Quoc Vo",
        "Ehsan Abbasnejad",
        "Damith C. Ranasinghe"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.05282.pdf",
      "code_url": "https://github.com/RamBoAttack/RamBoAttack.github.io",
      "url": "https://arxiv.org/abs/2112.05282",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2201.09650",
      "title": "What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction",
      "abstract": "Adversarial examples (AEs) pose severe threats to the applications of deep neural networks (DNNs) to safety-critical domains, e.g., autonomous driving. While there has been a vast body of AE defense solutions, to the best of our knowledge, they all suffer from some weaknesses, e.g., defending against only a subset of AEs or causing a relatively high accuracy loss for legitimate inputs. Moreover, most existing solutions cannot defend against adaptive attacks, wherein attackers are knowledgeable about the defense mechanisms and craft AEs accordingly. In this paper, we propose a novel AE detection framework based on the very nature of AEs, i.e., their semantic information is inconsistent with the discriminative features extracted by the target DNN model. To be specific, the proposed solution, namely ContraNet, models such contradiction by first taking both the input and the inference result to a generator to obtain a synthetic output and then comparing it against the original input. For legitimate inputs that are correctly inferred, the synthetic output tries to reconstruct the input. On the contrary, for AEs, instead of reconstructing the input, the synthetic output would be created to conform to the wrong label whenever possible. Consequently, by measuring the distance between the input and the synthetic output with metric learning, we can differentiate AEs from legitimate inputs. We perform comprehensive evaluations under various AE attack scenarios, and experimental results show that ContraNet outperforms existing solutions by a large margin, especially under adaptive attacks. Moreover, our analysis shows that successful AEs that can bypass ContraNet tend to have much-weakened adversarial semantics. We have also shown that ContraNet can be easily combined with adversarial training techniques to achieve further improved AE defense capabilities.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Yijun Yang",
        "Ruiyuan Gao",
        "Yu Li",
        "Qiuxia Lai",
        "Qiang Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.09650.pdf",
      "code_url": "https://github.com/cure-lab/ContraNet",
      "url": "https://arxiv.org/abs/2201.09650",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2205.10686",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box\" access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon.   In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Shawn Shan",
        "Wenxin Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.10686.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2205.10686",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2303.06280",
      "title": "Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks",
      "abstract": "Recent work has proposed stateful defense models (SDMs) as a compelling strategy to defend against a black-box attacker who only has query access to the model, as is common for online machine learning platforms. Such stateful defenses aim to defend against black-box attacks by tracking the query history and detecting and rejecting queries that are \"similar\" and thus preventing black-box attacks from finding useful gradients and making progress towards finding adversarial attacks within a reasonable query budget. Recent SDMs (e.g., Blacklight and PIHA) have shown remarkable success in defending against state-of-the-art black-box attacks. In this paper, we show that SDMs are highly vulnerable to a new class of adaptive black-box attacks. We propose a novel adaptive black-box attack strategy called Oracle-guided Adaptive Rejection Sampling (OARS) that involves two stages: (1) use initial query patterns to infer key properties about an SDM's defense; and, (2) leverage those extracted properties to design subsequent query patterns to evade the SDM's defense while making progress towards finding adversarial inputs. OARS is broadly applicable as an enhancement to existing black-box attacks - we show how to apply the strategy to enhance six common black-box attacks to be more effective against current class of SDMs. For example, OARS-enhanced versions of black-box attacks improved attack success rate against recent stateful defenses from almost 0% to to almost 100% for multiple datasets within reasonable query budgets.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Feng",
        "Ashish Hooda",
        "Neal Mangaokar",
        "Kassem Fawaz",
        "Somesh Jha",
        "Atul Prakash"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06280.pdf",
      "code_url": "https://github.com/purseclab/AttrackZone",
      "url": "https://arxiv.org/abs/2303.06280",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2307.07873",
      "title": "Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability",
      "abstract": "Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yechao Zhang",
        "Shengshan Hu",
        "Leo Yu Zhang",
        "Junyu Shi",
        "Minghui Li",
        "Xiaogeng Liu",
        "Wei Wan",
        "Hai Jin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07873.pdf",
      "code_url": "https://github.com/CGCL-codes/TransferAttackSurrogates",
      "url": "https://arxiv.org/abs/2307.07873",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2306.16614",
      "title": "Group-based Robustness: A General Framework for Customized Robustness in the Real World",
      "abstract": "Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss functions and 2) identify three new attack strategies. We show empirically that with comparable success rates, finding evasive samples using our new loss functions saves computation by a factor as large as the number of targeted classes, and finding evasive samples using our new attack strategies saves time by up to 99\\% compared to brute-force search methods. Finally, we propose a defense method that increases group-based robustness by up to 3.52$\\times$.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Weiran Lin",
        "Keane Lucas",
        "Neo Eyal",
        "Lujo Bauer",
        "Michael K. Reiter",
        "Mahmood Sharif"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.16614.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2306.16614",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.1 Image"
    },
    {
      "arxiv_id": "2106.09898",
      "title": "Bad Characters: Imperceptible NLP Attacks",
      "abstract": "Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Nicholas Boucher",
        "Ilia Shumailov",
        "Ross Anderson",
        "Nicolas Papernot"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.09898.pdf",
      "code_url": "https://github.com/nickboucher/imperceptible",
      "url": "https://arxiv.org/abs/2106.09898",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.2 Text"
    },
    {
      "arxiv_id": "2209.06506",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.06506.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.06506",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.2 Text"
    },
    {
      "arxiv_id": "2303.14443",
      "title": "No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning",
      "abstract": "The number of papers submitted to academic conferences is steadily rising in many scientific disciplines. To handle this growth, systems for automatic paper-reviewer assignments are increasingly used during the reviewing process. These systems use statistical topic models to characterize the content of submissions and automate the assignment to reviewers. In this paper, we show that this automation can be manipulated using adversarial learning. We propose an attack that adapts a given paper so that it misleads the assignment and selects its own reviewers. Our attack is based on a novel optimization strategy that alternates between the feature space and problem space to realize unobtrusive changes to the paper. To evaluate the feasibility of our attack, we simulate the paper-reviewer assignment of an actual security conference (IEEE S&P) with 165 reviewers on the program committee. Our results show that we can successfully select and remove reviewers without access to the assignment system. Moreover, we demonstrate that the manipulated papers remain plausible and are often indistinguishable from benign submissions.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Thorsten Eisenhofer",
        "Erwin Quiring",
        "Jonas M\u00f6ller",
        "Doreen Riepel",
        "Thorsten Holz",
        "Konrad Rieck"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14443.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2303.14443",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.2 Text"
    },
    {
      "arxiv_id": "1911.01840",
      "title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems",
      "abstract": "Speaker recognition (SR) is widely used in our daily life as a biometric authentication or identification mechanism. The popularity of SR brings in serious security concerns, as demonstrated by recent adversarial attacks. However, the impacts of such threats in the practical black-box setting are still open, since current attacks consider the white-box setting only. In this paper, we conduct the first comprehensive and systematic study of the adversarial attacks on SR systems (SRSs) to understand their security weakness in the practical blackbox setting. For this purpose, we propose an adversarial attack, named FAKEBOB, to craft adversarial samples. Specifically, we formulate the adversarial sample generation as an optimization problem, incorporated with the confidence of adversarial samples and maximal distortion to balance between the strength and imperceptibility of adversarial voices. One key contribution is to propose a novel algorithm to estimate the score threshold, a feature in SRSs, and use it in the optimization problem to solve the optimization problem. We demonstrate that FAKEBOB achieves 99% targeted attack success rate on both open-source and commercial systems. We further demonstrate that FAKEBOB is also effective on both open-source and commercial systems when playing over the air in the physical world. Moreover, we have conducted a human study which reveals that it is hard for human to differentiate the speakers of the original and adversarial voices. Last but not least, we show that four promising defense methods for adversarial attack from the speech recognition domain become ineffective on SRSs against FAKEBOB, which calls for more effective defense methods. We highlight that our study peeks into the security implications of adversarial attacks on SRSs, and realistically fosters to improve the security robustness of SRSs.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Guangke Chen",
        "Sen Chen",
        "Lingling Fan",
        "Xiaoning Du",
        "Zhe Zhao",
        "Fu Song",
        "Yang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.01840.pdf",
      "code_url": "https://github.com/FAKEBOB-adversarial-attack/FAKEBOB",
      "url": "https://arxiv.org/abs/1911.01840",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "1910.05262",
      "title": "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems",
      "abstract": "Automatic speech recognition and voice identification systems are being deployed in a wide array of applications, from providing control mechanisms to devices lacking traditional interfaces, to the automatic transcription of conversations and authentication of users. Many of these applications have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state of the art systems, with minimal impact on human comprehension. Processing pipelines for modern systems are comprised of signal preprocessing and feature extraction steps, whose output is fed to a machine-learned model. Prior work has focused on the models, using white-box knowledge to tailor model-specific attacks. We focus on the pipeline stages before the models, which (unlike the models) are quite similar across systems. As such, our attacks are black-box and transferable, and demonstrably achieve mistranscription and misidentification rates as high as 100% by modifying only a few frames of audio. We perform a study via Amazon Mechanical Turk demonstrating that there is no statistically significant difference between human perception of regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are generally not perceived by human subjects, but that are crucial for model accuracy. We also find that certain English language phonemes (in particular, vowels) are significantly more susceptible to our attack. We show that the attacks are effective when mounted over cellular networks, where signals are subject to degradation due to transcoding, jitter, and packet loss.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Muhammad Sajidur Rahman",
        "Washington Garcia",
        "Logan Blue",
        "Kevin Warren",
        "Anurag Swarnim Yadav",
        "Tom Shrimpton",
        "Patrick Traynor"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.05262.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1910.05262",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "2007.06622",
      "title": "SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems",
      "abstract": "Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible by the improved accuracy in neural networks. Like other systems based on neural networks, recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs. However, as we demonstrate in this paper, the end-to-end architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space. We demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work. We then demonstrate experimentally that attacks against these models almost universally fail to transfer. In so doing, we argue that substantial additional work is required to provide adequate mitigations in this space.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Kevin Warren",
        "Vincent Bindschaedler",
        "Nicolas Papernot",
        "Patrick Traynor"
      ],
      "pdf_url": "https://arxiv.org/pdf/2007.06622.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2007.06622",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "2110.09714",
      "title": "Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information",
      "abstract": "Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. The current \"black-box\" attacks all heavily rely on the knowledge of prediction/confidence scores to craft effective adversarial examples, which can be intuitively defended by service providers without returning these messages. In this paper, we propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100% success rates of attacks with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Baolin Zheng",
        "Peipei Jiang",
        "Qian Wang",
        "Qi Li",
        "Chao Shen",
        "Cong Wang",
        "Yunjie Ge",
        "Qingyang Teng",
        "Shenyi Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2110.09714.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2110.09714",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "2207.13192",
      "title": "Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception",
      "abstract": "Recently, adversarial machine learning attacks have posed serious security threats against practical audio signal classification systems, including speech recognition, speaker recognition, and music copyright detection. Previous studies have mainly focused on ensuring the effectiveness of attacking an audio signal classifier via creating a small noise-like perturbation on the original signal. It is still unclear if an attacker is able to create audio signal perturbations that can be well perceived by human beings in addition to its attack effectiveness. This is particularly important for music signals as they are carefully crafted with human-enjoyable audio characteristics.   In this work, we formulate the adversarial attack against music signals as a new perception-aware attack framework, which integrates human study into adversarial attack design. Specifically, we conduct a human study to quantify the human perception with respect to a change of a music signal. We invite human participants to rate their perceived deviation based on pairs of original and perturbed music signals, and reverse-engineer the human perception process by regression analysis to predict the human-perceived deviation given a perturbed signal. The perception-aware attack is then formulated as an optimization problem that finds an optimal perturbation signal to minimize the prediction of perceived deviation from the regressed human perception model. We use the perception-aware framework to design a realistic adversarial music attack against YouTube's copyright detector. Experiments show that the perception-aware attack produces adversarial music with significantly better perceptual quality than prior work.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Shangqing Zhao",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13192.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2207.13192",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "2311.07780",
      "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
      "abstract": "Audio adversarial examples (AEs) have posed significant security challenges to real-world speaker recognition systems. Most black-box attacks still require certain information from the speaker recognition model to be effective (e.g., keeping probing and requiring the knowledge of similarity scores). This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model. Although it is not feasible for an attacker to succeed with completely zero knowledge, we assume that the attacker only knows a short (or a few seconds) speech sample of a target speaker. Without any probing to gain further knowledge about the target model, we propose a new mechanism, called parrot training, to generate AEs against the target model. Motivated by recent advancements in voice conversion (VC), we propose to use the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech. Then, we use these parrot speech samples to train a parrot-trained(PT) surrogate model for the attacker. Under a joint transferability and perception framework, we investigate different ways to generate AEs on the PT model (called PT-AEs) to ensure the PT-AEs can be generated with high transferability to a black-box target model with good human perceptual quality. Real-world experiments show that the resultant PT-AEs achieve the attack success rates of 45.8% - 80.8% against the open-source models in the digital-line scenario and 47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon Echo, and Google Home, in the over-the-air scenario.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.07780.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2311.07780",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.3 Audio"
    },
    {
      "arxiv_id": "2107.04284",
      "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
      "abstract": "Widely deployed deep neural network (DNN) models have been proven to be vulnerable to adversarial perturbations in many applications (e.g., image, audio and text classifications). To date, there are only a few adversarial perturbations proposed to deviate the DNN models in video recognition systems by simply injecting 2D perturbations into video frames. However, such attacks may overly perturb the videos without learning the spatio-temporal features (across temporal frames), which are commonly extracted by DNN models for video recognition. To our best knowledge, we propose the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems. U3D has many advantages, such as (1) as the transfer-based attack, U3D can universally attack multiple DNN models for video recognition without accessing to the target DNN model; (2) the high transferability of U3D makes such universal black-box attack easy-to-launch, which can be further enhanced by integrating queries over the target model when necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the existing state-of-the-art defense schemes; (5) U3D can be efficiently generated with a few pre-learned parameters, and then immediately injected to attack real-time DNN-based video recognition systems. We have conducted extensive experiments to evaluate U3D on multiple DNN models and three large-scale video datasets. The experimental results demonstrate its superiority and practicality.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Shangyu Xie",
        "Han Wang",
        "Yu Kong",
        "Yuan Hong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.04284.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2107.04284",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.4 Video"
    },
    {
      "arxiv_id": "2203.16000",
      "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
      "abstract": "Video classification systems are vulnerable to adversarial attacks, which can create severe security problems in video verification. Current black-box attacks need a large number of queries to succeed, resulting in high computational overhead in the process of attack. On the other hand, attacks with restricted perturbations are ineffective against defenses such as denoising or adversarial training. In this paper, we focus on unrestricted perturbations and propose StyleFool, a black-box video adversarial attack via style transfer to fool the video classification system. StyleFool first utilizes color theme proximity to select the best style image, which helps avoid unnatural details in the stylized videos. Meanwhile, the target class confidence is additionally considered in targeted attacks to influence the output distribution of the classifier by moving the stylized video closer to or even across the decision boundary. A gradient-free method is then employed to further optimize the adversarial perturbations. We carry out extensive experiments to evaluate StyleFool on two standard datasets, UCF-101 and HMDB-51. The experimental results demonstrate that StyleFool outperforms the state-of-the-art adversarial attacks in terms of both the number of queries and the robustness against existing defenses. Moreover, 50% of the stylized videos in untargeted attacks do not need any query since they can already fool the video classification model. Furthermore, we evaluate the indistinguishability through a user study to show that the adversarial samples of StyleFool look imperceptible to human eyes, despite unrestricted perturbations.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Yuxin Cao",
        "Xi Xiao",
        "Ruoxi Sun",
        "Derui Wang",
        "Minhui Xue",
        "Sheng Wen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.16000.pdf",
      "code_url": "https://github.com/JosephCao0327/StyleFool",
      "url": "https://arxiv.org/abs/2203.16000",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.4 Video"
    },
    {
      "arxiv_id": "2108.09513",
      "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.   In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Jiaming Mu",
        "Binghui Wang",
        "Qi Li",
        "Kun Sun",
        "Mingwei Xu",
        "Zhuotao Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.09513.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2108.09513",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.5 Graph"
    },
    {
      "arxiv_id": "1705.07535",
      "title": "Evading Classifiers by Morphing in the Dark",
      "abstract": "Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
      "year": 2017,
      "venue": "ACM CCS",
      "authors": [
        "Hung Dang",
        "Yue Huang",
        "Ee-Chien Chang"
      ],
      "pdf_url": "https://arxiv.org/pdf/1705.07535.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1705.07535",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.6 Software"
    },
    {
      "arxiv_id": "1905.12386",
      "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
      "abstract": "In this paper, we present a novel attack against authorship attribution of source code. We exploit that recent attribution methods rest on machine learning and thus can be deceived by adversarial examples of source code. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer. The attack is guided by Monte-Carlo tree search that enables us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack has a substantial effect on two recent attribution methods, whose accuracy drops from over 88% to 1% under attack. Furthermore, we show that our attack can imitate the coding style of developers with high accuracy and thereby induce false attributions. We conclude that current approaches for authorship attribution are inappropriate for practical application and there is a need for resilient analysis techniques.",
      "year": 2019,
      "venue": "USENIX Security",
      "authors": [
        "Erwin Quiring",
        "Alwin Maier",
        "Konrad Rieck"
      ],
      "pdf_url": "https://arxiv.org/pdf/1905.12386.pdf",
      "code_url": "http://www.tu-braunschweig.de/sec/research/code/imitator",
      "url": "https://arxiv.org/abs/1905.12386",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.6 Software"
    },
    {
      "arxiv_id": "1911.02142",
      "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space [Extended Version]",
      "abstract": "Recent research efforts on adversarial machine learning (ML) have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This article makes three major contributions. Firstly, we propose a general formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the by-product of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. Secondly, building on our general formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations in terms of semantics and artifacts. We have tested our approach on a dataset with 150K Android apps from 2016 and 2018 which show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Thirdly, we explore the effectiveness of adversarial training as a possible approach to enforce robustness against adversarial samples, evaluating its effectiveness on the considered machine learning models under different scenarios. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial instance.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Jacopo Cortellazzi",
        "Feargus Pendlebury",
        "Daniel Arp",
        "Erwin Quiring",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.02142.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1911.02142",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.6 Software"
    },
    {
      "arxiv_id": "2303.08509",
      "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
      "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting \"never-executed\" function calls into malware code. The main challenges are two-fold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent.   To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges.   Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Heng Li",
        "Zhang Cheng",
        "Bang Wu",
        "Liheng Yuan",
        "Cuiying Gao",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08509.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2303.08509",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.6 Software"
    },
    {
      "arxiv_id": "2309.01866",
      "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "abstract": "The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ping He",
        "Yifan Xia",
        "Xuhong Zhang",
        "Shouling Ji"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01866.pdf",
      "code_url": "https://github.com/gnipping/AdvDroidZero-Access-Instructions",
      "url": "https://arxiv.org/abs/2309.01866",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.6 Software"
    },
    {
      "arxiv_id": "2208.12897",
      "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
      "abstract": "Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated circuits can bypass the security of critical infrastructures. Although researchers have proposed many techniques to detect HTs, several limitations exist, including: (i) a low success rate, (ii) high algorithmic complexity, and (iii) a large number of test patterns. Furthermore, the most pertinent drawback of prior detection techniques stems from an incorrect evaluation methodology, i.e., they assume that an adversary inserts HTs randomly. Such inappropriate adversarial assumptions enable detection techniques to claim high HT detection accuracy, leading to a \"false sense of security.\" Unfortunately, to the best of our knowledge, despite more than a decade of research on detecting HTs inserted during fabrication, there have been no concerted efforts to perform a systematic evaluation of HT detection techniques.   In this paper, we play the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL). ATTRITION evades eight detection techniques across two HT detection categories, showcasing its agnostic behavior. ATTRITION achieves average attack success rates of $47\\times$ and $211\\times$ compared to randomly inserted HTs against state-of-the-art HT detection techniques. We demonstrate ATTRITION's ability to evade detection techniques by evaluating designs ranging from the widely-used academic suites to larger designs such as the open-source MIPS and mor1kx processors to AES and a GPS module. Additionally, we showcase the impact of ATTRITION-generated HTs through two case studies (privilege escalation and kill switch) on the mor1kx processor. We envision that our work, along with our released HT benchmarks and models, fosters the development of better HT detection techniques.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Vasudev Gohil",
        "Hao Guo",
        "Satwik Patnaik",
        " Jeyavijayan",
        " Rajendran"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12897.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2208.12897",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.7 Hardware"
    },
    {
      "arxiv_id": "2209.01782",
      "title": "\"Is your explanation stable?\": A Robustness Evaluation Framework for Feature Attribution",
      "abstract": "Understanding the decision process of neural networks is hard. One vital method for explanation is to attribute its decision to pivotal features. Although many algorithms are proposed, most of them solely improve the faithfulness to the model. However, the real environment contains many random noises, which may leads to great fluctuations in the explanations. More seriously, recent works show that explanation algorithms are vulnerable to adversarial attacks. All of these make the explanation hard to trust in real scenarios.   To bridge this gap, we propose a model-agnostic method \\emph{Median Test for Feature Attribution} (MeTFA) to quantify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees. MeTFA has the following two functions: (1) examine whether one feature is significantly important or unimportant and generate a MeTFA-significant map to visualize the results; (2) compute the confidence interval of a feature attribution score and generate a MeTFA-smoothed map to increase the stability of the explanation. Experiments show that MeTFA improves the visual quality of explanations and significantly reduces the instability while maintaining the faithfulness. To quantitatively evaluate the faithfulness of an explanation under different noise settings, we further propose several robust faithfulness metrics. Experiment results show that the MeTFA-smoothed explanation can significantly increase the robust faithfulness. In addition, we use two scenarios to show MeTFA's potential in the applications. First, when applied to the SOTA explanation method to locate context bias for semantic segmentation models, MeTFA-significant explanations use far smaller regions to maintain 99\\%+ faithfulness. Second, when tested with different explanation-oriented attacks, MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against explanations.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yuyou Gan",
        "Yuhao Mao",
        "Xuhong Zhang",
        "Shouling Ji",
        "Yuwen Pu",
        "Meng Han",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01782.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.01782",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.8 Interpret Method"
    },
    {
      "arxiv_id": "2209.09577",
      "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
      "abstract": "Famous for its superior performance, deep learning (DL) has been popularly used within many applications, which also at the same time attracts various threats to the models. One primary threat is from adversarial attacks. Researchers have intensively studied this threat for several years and proposed dozens of approaches to create adversarial examples (AEs). But most of the approaches are only evaluated on limited models and datasets (e.g., MNIST, CIFAR-10). Thus, the effectiveness of attacking real-world DL models is not quite clear. In this paper, we perform the first systematic study of adversarial attacks on real-world DNN models and provide a real-world model dataset named RWM. Particularly, we design a suite of approaches to adapt current AE generation algorithms to the diverse real-world DL models, including automatically extracting DL models from Android apps, capturing the inputs and outputs of the DL models in apps, generating AEs and validating them by observing the apps' execution. For black-box DL models, we design a semantic-based approach to build suitable datasets and use them for training substitute models when performing transfer-based attacks. After analyzing 245 DL models collected from 62,583 real-world apps, we have a unique opportunity to understand the gap between real-world DL models and contemporary AE generation algorithms. To our surprise, the current AE generation algorithms can only directly attack 6.53% of the models. Benefiting from our approach, the success rate upgrades to 47.35%.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Zizhuang Deng",
        "Kai Chen",
        "Guozhu Meng",
        "Xiaodong Zhang",
        "Ke Xu",
        "Yao Cheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09577.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.09577",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.9 Physical World"
    },
    {
      "arxiv_id": "2302.09491",
      "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
      "abstract": "Adversarial attacks are valuable for evaluating the robustness of deep learning models. Existing attacks are primarily conducted on the visible light spectrum (e.g., pixel-wise texture perturbation). However, attacks targeting texture-free X-ray images remain underexplored, despite the widespread application of X-ray imaging in safety-critical scenarios such as the X-ray detection of prohibited items. In this paper, we take the first step toward the study of adversarial attacks targeted at X-ray prohibited item detection, and reveal the serious threats posed by such attacks in this safety-critical scenario. Specifically, we posit that successful physical adversarial attacks in this scenario should be specially designed to circumvent the challenges posed by color/texture fading and complex overlapping. To this end, we propose X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage. To resolve the issues associated with color/texture fading, we develop a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes, using the gradients of a surrogate model rather than directly generating adversarial textures. To place the printed 3D adversarial objects in luggage with complex overlapped instances, we design a policy-based reinforcement learning strategy to find locations eliciting strong attack performance in worst-case scenarios whereby the prohibited items are heavily occluded by other items. To verify the effectiveness of the proposed X-Adv, we conduct extensive experiments in both the digital and the physical world (employing a commercial X-ray security inspection system for the latter case). Furthermore, we present the physical-world X-ray adversarial attack dataset XAD.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Aishan Liu",
        "Jun Guo",
        "Jiakai Wang",
        "Siyuan Liang",
        "Renshuai Tao",
        "Wenbo Zhou",
        "Cong Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.09491.pdf",
      "code_url": "https://github.com/DIG-Beihang/X-adv",
      "url": "https://arxiv.org/abs/2302.09491",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.9 Physical World"
    },
    {
      "arxiv_id": "2401.03582",
      "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
      "abstract": "All vehicles must follow the rules that govern traffic behavior, regardless of whether the vehicles are human-driven or Connected Autonomous Vehicles (CAVs). Road signs indicate locally active rules, such as speed limits and requirements to yield or stop. Recent research has demonstrated attacks, such as adding stickers or projected colored patches to signs, that cause CAV misinterpretation, resulting in potential safety issues. Humans can see and potentially defend against these attacks. But humans can not detect what they can not observe. We have developed an effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is designed to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification. In this work, we formulate the threat model and requirements for an ILR-based traffic sign perception attack to succeed. We evaluate the effectiveness of the ILR attack with real-world experiments against two major traffic sign recognition architectures on four IR-sensitive cameras. Our black-box optimization methodology allows the attack to achieve up to a 100% attack success rate in indoor, static scenarios and a >80.5% attack success rate in our outdoor, moving vehicle scenarios. We find the latest state-of-the-art certifiable defense is ineffective against ILR attacks as it mis-certifies >33.5% of cases. To address this, we propose a detection strategy based on the physical properties of IR laser reflections which can detect 96% of ILR attacks.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Takami Sato",
        "Sri Hrushikesh Varma Bhupathiraju",
        "Michael Clifford",
        "Takeshi Sugawara",
        "Qi Alfred Chen",
        "Sara Rampazzi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03582.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2401.03582",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.9 Physical World"
    },
    {
      "arxiv_id": "2402.03741",
      "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
      "abstract": "Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Oubo Ma",
        "Yuwen Pu",
        "Linkang Du",
        "Yang Dai",
        "Ruo Wang",
        "Xiaolei Liu",
        "Yingcai Wu",
        "Shouling Ji"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03741",
      "code_url": "https://github.com/maoubo/SUB-PLAY",
      "url": "https://arxiv.org/abs/2402.03741",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.10 Reinforcement Learning"
    },
    {
      "arxiv_id": "2105.11363",
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "abstract": "Many recent works have proposed methods to train classifiers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classifier. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness.   In this paper, we present a framework and tools for training classifiers that satisfy global robustness properties. We define new notions of global robustness that are more suitable for security classifiers. We design a novel booster-fixer training framework to enforce global robustness properties. We structure our classifier as an ensemble of logic rules and design a new verifier to verify the properties. In our training algorithm, the booster increases the classifier's capacity, and the fixer enforces verified global robustness properties following counterexample guided inductive synthesis.   We show that we can train classifiers to satisfy different global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classifier's performance. For example, we train a Twitter spam account classifier to satisfy five global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn't satisfy any property.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Yue Qin",
        "Xiaojing Liao",
        "Suman Jana",
        "David Wagner"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.11363.pdf",
      "code_url": "https://github.com/surrealyz/verified-global-properties",
      "url": "https://arxiv.org/abs/2105.11363",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2105.08619",
      "title": "On the Robustness of Domain Constraints",
      "abstract": "Machine learning is vulnerable to adversarial examples-inputs designed to cause models to perform poorly. However, it is unclear if adversarial examples represent realistic inputs in the modeled domains. Diverse domains such as networks and phishing have domain constraints-complex relationships between features that an adversary must satisfy for an attack to be realized (in addition to any adversary-specific goals). In this paper, we explore how domain constraints limit adversarial capabilities and how adversaries can adapt their strategies to create realistic (constraint-compliant) examples. In this, we develop techniques to learn domain constraints from data, and show how the learned constraints can be integrated into the adversarial crafting process. We evaluate the efficacy of our approach in network intrusion and phishing datasets and find: (1) up to 82% of adversarial examples produced by state-of-the-art crafting algorithms violate domain constraints, (2) domain constraints are robust to adversarial examples; enforcing constraints yields an increase in model accuracy by up to 34%. We observe not only that adversaries must alter inputs to satisfy domain constraints, but that these constraints make the generation of valid adversarial examples far more challenging.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Yohan Beugin",
        "Michael J. Weisman",
        "Patrick McDaniel"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.08619.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2105.08619",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2002.12398",
      "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
      "abstract": "As machine learning (ML) systems become pervasive, safeguarding their security is critical. However, recently it has been demonstrated that motivated adversaries are able to mislead ML systems by perturbing test data using semantic transformations. While there exists a rich body of research providing provable robustness guarantees for ML models against $\\ell_p$ norm bounded adversarial perturbations, guarantees against semantic perturbations remain largely underexplored. In this paper, we provide TSS -- a unified framework for certifying ML robustness against general adversarial semantic transformations. First, depending on the properties of each transformation, we divide common transformations into two categories, namely resolvable (e.g., Gaussian blur) and differentially resolvable (e.g., rotation) transformations. For the former, we propose transformation-specific randomized smoothing strategies and obtain strong robustness certification. The latter category covers transformations that involve interpolation errors, and we propose a novel approach based on stratified sampling to certify the robustness. Our framework TSS leverages these certification strategies and combines with consistency-enhanced training to provide rigorous certification of robustness. We conduct extensive experiments on over ten types of challenging semantic transformations and show that TSS significantly outperforms the state of the art. Moreover, to the best of our knowledge, TSS is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset. For instance, our framework achieves 30.4% certified robust accuracy against rotation attack (within $\\pm 30^\\circ$) on ImageNet. Moreover, to consider a broader range of transformations, we show TSS is also robust against adaptive attacks and unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Linyi Li",
        "Maurice Weber",
        "Xiaojun Xu",
        "Luka Rimanic",
        "Bhavya Kailkhura",
        "Tao Xie",
        "Ce Zhang",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.12398.pdf",
      "code_url": "https://github.com/AI-secure/semantic-randomized-smoothing",
      "url": "https://arxiv.org/abs/2002.12398",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2301.02905",
      "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
      "abstract": "Encoder as a service is an emerging cloud service. Specifically, a service provider first pre-trains an encoder (i.e., a general-purpose feature extractor) via either supervised learning or self-supervised learning and then deploys it as a cloud service API. A client queries the cloud service API to obtain feature vectors for its training/testing inputs when training/testing its classifier (called downstream classifier). A downstream classifier is vulnerable to adversarial examples, which are testing inputs with carefully crafted perturbation that the downstream classifier misclassifies. Therefore, in safety and security critical applications, a client aims to build a robust downstream classifier and certify its robustness guarantees against adversarial examples.   What APIs should the cloud service provide, such that a client can use any certification method to certify the robustness of its downstream classifier against adversarial examples while minimizing the number of queries to the APIs? How can a service provider pre-train an encoder such that clients can build more certifiably robust downstream classifiers? We aim to answer the two questions in this work. For the first question, we show that the cloud service only needs to provide two APIs, which we carefully design, to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the APIs. For the second question, we show that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Wenjie Qu",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02905.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2301.02905",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2302.04332",
      "title": "Continuous Learning for Android Malware Detection",
      "abstract": "Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.   In this paper, we propose new methods to combat the concept drift problem of Android malware classifiers. Since machine learning technique needs to be continuously deployed, we use active learning: we select new samples for analysts to label, and then add the labeled samples to the training set to retrain the classifier. Our key idea is, similarity-based uncertainty is more robust against concept drift. Therefore, we combine contrastive learning with active learning. We propose a new hierarchical contrastive learning scheme, and a new sample selection technique to continuously train the Android malware classifier. Our evaluation shows that this leads to significant improvements, compared to previously published methods for active learning. Our approach reduces the false negative rate from 14% (for the best baseline) to 9%, while also reducing the false positive rate (from 0.86% to 0.48%). Also, our approach maintains more consistent performance across a seven-year time period than past methods.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Yizheng Chen",
        "Zhoujie Ding",
        "David Wagner"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04332.pdf",
      "code_url": "https://github.com/wagner-group/active-learning",
      "url": "https://arxiv.org/abs/2302.04332",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2202.01811",
      "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
      "abstract": "Object detectors, which are widely deployed in security-critical systems such as autonomous vehicles, have been found vulnerable to patch hiding attacks. An attacker can use a single physically-realizable adversarial patch to make the object detector miss the detection of victim objects and undermine the functionality of object detection applications. In this paper, we propose ObjectSeeker for certifiably robust object detection against patch hiding attacks. The key insight in ObjectSeeker is patch-agnostic masking: we aim to mask out the entire adversarial patch without knowing the shape, size, and location of the patch. This masking operation neutralizes the adversarial effect and allows any vanilla object detector to safely detect objects on the masked images. Remarkably, we can evaluate ObjectSeeker's robustness in a certifiable manner: we develop a certification procedure to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness. Our experiments demonstrate a significant (~10%-40% absolute and ~2-6x relative) improvement in certifiable robustness over the prior work, as well as high clean performance (~1% drop compared with undefended models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Chong Xiang",
        "Alexander Valtchanov",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.01811.pdf",
      "code_url": "https://github.com/inspire-group/ObjectSeeker",
      "url": "https://arxiv.org/abs/2202.01811",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2202.03277",
      "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
      "abstract": "While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealistic attacks. We shed light on the patterns that discriminate which unrealistic examples can be used for effective hardening. We release our code, datasets and models to support future research in exploring how to reduce the gap between unrealistic and realistic adversarial attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Salijona Dyrmishi",
        "Salah Ghamizi",
        "Thibault Simonetto",
        "Yves Le Traon",
        "Maxime Cordy"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.03277.pdf",
      "code_url": "https://github.com/serval-uni-lu/realistic_adversarial_hardening",
      "url": "https://arxiv.org/abs/2202.03277",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2307.16630",
      "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
      "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xinyu Zhang",
        "Hanbin Hong",
        "Yuan Hong",
        "Peng Huang",
        "Binghui Wang",
        "Zhongjie Ba",
        "Kui Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16630.pdf",
      "code_url": "https://github.com/Eyr3/TextCRS?tab=readme-ov-file",
      "url": "https://arxiv.org/abs/2307.16630",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2309.11005",
      "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
      "abstract": "Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\\%$ more samples at noise scale $\u03c3= 1$, with greater relative improvements observed as the difficulty of the predictive task increases.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Andrew C. Cullen",
        "Paul Montague",
        "Shijie Liu",
        "Sarah M. Erfani",
        "Benjamin I. P. Rubinstein"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11005.pdf",
      "code_url": "https://github.com/andrew-cullen/ensemble-simplex-certifications",
      "url": "https://arxiv.org/abs/2309.11005",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.11 Robust Defense"
    },
    {
      "arxiv_id": "2102.00918",
      "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
      "abstract": "Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Alireza Bahramali",
        "Milad Nasr",
        "Amir Houmansadr",
        "Dennis Goeckel",
        "Don Towsley"
      ],
      "pdf_url": "https://arxiv.org/pdf/2102.00918.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2102.00918",
      "source_section": "1.1 Adversarial Attack & Defense",
      "source_subsection": "1.1.13 Wireless Communication System"
    },
    {
      "arxiv_id": "2201.00763",
      "title": "DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection",
      "abstract": "Federated Learning (FL) allows multiple clients to collaboratively train a Neural Network (NN) model on their private data without revealing the data. Recently, several targeted poisoning attacks against FL have been introduced. These attacks inject a backdoor into the resulting model that allows adversary-controlled inputs to be misclassified. Existing countermeasures against backdoor attacks are inefficient and often merely aim to exclude deviating models from the aggregation. However, this approach also removes benign models of clients with deviating data distributions, causing the aggregated model to perform poorly for such clients.   To address this problem, we propose DeepSight, a novel model filtering approach for mitigating backdoor attacks. It is based on three novel techniques that allow to characterize the distribution of data used to train model updates and seek to measure fine-grained differences in the internal structure and outputs of NNs. Using these techniques, DeepSight can identify suspicious model updates. We also develop a scheme that can accurately cluster model updates. Combining the results of both components, DeepSight is able to identify and eliminate model clusters containing poisoned models with high attack impact. We also show that the backdoor contributions of possibly undetected poisoned models can be effectively mitigated with existing weight clipping-based defenses. We evaluate the performance and effectiveness of DeepSight and show that it can mitigate state-of-the-art backdoor attacks with a negligible impact on the model's performance on benign data.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Thien Duc Nguyen",
        "Markus Miettinen",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.00763.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2201.00763",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2112.12727",
      "title": "EIFFeL: Ensuring Integrity for Federated Learning",
      "abstract": "Federated learning (FL) enables clients to collaborate with a server to train a machine learning model. To ensure privacy, the server performs secure aggregation of updates from the clients. Unfortunately, this prevents verification of the well-formedness (integrity) of the updates as the updates are masked. Consequently, malformed updates designed to poison the model can be injected without detection. In this paper, we formalize the problem of ensuring \\textit{both} update privacy and integrity in FL and present a new system, \\textsf{EIFFeL}, that enables secure aggregation of \\textit{verified} updates. \\textsf{EIFFeL} is a general framework that can enforce \\textit{arbitrary} integrity checks and remove malformed updates from the aggregate, without violating privacy. Our empirical evaluation demonstrates the practicality of \\textsf{EIFFeL}. For instance, with $100$ clients and $10\\%$ poisoning, \\textsf{EIFFeL} can train an MNIST classification model to the same accuracy as that of a non-poisoned federated learner in just $2.4s$ per iteration.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Amrita Roy Chowdhury",
        "Chuan Guo",
        "Somesh Jha",
        "Laurens van der Maaten"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.12727.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2112.12727",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2111.07380",
      "title": "Eluding Secure Aggregation in Federated Learning via Model Inconsistency",
      "abstract": "Secure aggregation is a cryptographic protocol that securely computes the aggregation of its inputs. It is pivotal in keeping model updates private in federated learning. Indeed, the use of secure aggregation prevents the server from learning the value and the source of the individual model updates provided by the users, hampering inference and data attribution attacks. In this work, we show that a malicious server can easily elude secure aggregation as if the latter were not in place. We devise two different attacks capable of inferring information on individual private training datasets, independently of the number of users participating in the secure aggregation. This makes them concrete threats in large-scale, real-world federated learning applications. The attacks are generic and equally effective regardless of the secure aggregation protocol used. They exploit a vulnerability of the federated learning protocol caused by incorrect usage of secure aggregation and lack of parameter validation. Our work demonstrates that current implementations of federated learning with secure aggregation offer only a \"false sense of security\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Danilo Francati",
        "Giuseppe Ateniese"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.07380.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2111.07380",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2210.10936",
      "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
      "abstract": "Federated learning is vulnerable to poisoning attacks in which malicious clients poison the global model via sending malicious model updates to the server. Existing defenses focus on preventing a small number of malicious clients from poisoning the global model via robust federated learning methods and detecting malicious clients when there are a large number of them. However, it is still an open challenge how to recover the global model from poisoning attacks after the malicious clients are detected. A naive solution is to remove the detected malicious clients and train a new global model from scratch, which incurs large cost that may be intolerable for resource-constrained clients such as smartphones and IoT devices.   In this work, we propose FedRecover, which can recover an accurate global model from poisoning attacks with small cost for the clients. Our key idea is that the server estimates the clients' model updates instead of asking the clients to compute and communicate them during the recovery process. In particular, the server stores the global models and clients' model updates in each round, when training the poisoned global model. During the recovery process, the server estimates a client's model update in each round using its stored historical information. Moreover, we further optimize FedRecover to recover a more accurate global model using warm-up, periodic correction, abnormality fixing, and final tuning strategies, in which the server asks the clients to compute and communicate their exact model updates. Theoretically, we show that the global model recovered by FedRecover is close to or the same as that recovered by train-from-scratch under some assumptions. Empirically, our evaluation on four datasets, three federated learning methods, as well as untargeted and targeted poisoning attacks (e.g., backdoor attacks) shows that FedRecover is both accurate and efficient.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoyu Cao",
        "Jinyuan Jia",
        "Zaixi Zhang",
        "Neil Zhenqiang Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10936.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2210.10936",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2110.04350",
      "title": "FRL: Federated Rank Learning",
      "abstract": "Federated learning (FL) allows mutually untrusted clients to collaboratively train a common machine learning model without sharing their private/proprietary training data among each other. FL is unfortunately susceptible to poisoning by malicious clients who aim to hamper the accuracy of the commonly trained model through sending malicious model updates during FL's training process.   We argue that the key factor to the success of poisoning attacks against existing FL systems is the large space of model updates available to the clients, allowing malicious clients to search for the most poisonous model updates, e.g., by solving an optimization problem. To address this, we propose Federated Rank Learning (FRL). FRL reduces the space of client updates from model parameter updates (a continuous space of float numbers) in standard FL to the space of parameter rankings (a discrete space of integer values). To be able to train the global model using parameter ranks (instead of parameter weights), FRL leverage ideas from recent supermasks training mechanisms. Specifically, FRL clients rank the parameters of a randomly initialized neural network (provided by the server) based on their local training data. The FRL server uses a voting mechanism to aggregate the parameter rankings submitted by clients in each training epoch to generate the global ranking of the next training epoch.   Intuitively, our voting-based aggregation mechanism prevents poisoning clients from making significant adversarial modifications to the global model, as each client will have a single vote! We demonstrate the robustness of FRL to poisoning through analytical proofs and experimentation. We also show FRL's high communication efficiency. Our experiments demonstrate the superiority of FRL in real-world FL settings.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Hamid Mozaffari",
        "Virat Shejwalkar",
        "Amir Houmansadr"
      ],
      "pdf_url": "https://arxiv.org/pdf/2110.04350.pdf",
      "code_url": "https://github.com/SPIN-UMass/FRL",
      "url": "https://arxiv.org/abs/2110.04350",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2301.09508",
      "title": "BayBFed: Bayesian Backdoor Defense for Federated Learning",
      "abstract": "Federated learning (FL) allows participants to jointly train a machine learning model without sharing their private data with others. However, FL is vulnerable to poisoning attacks such as backdoor attacks. Consequently, a variety of defenses have recently been proposed, which have primarily utilized intermediary states of the global model (i.e., logits) or distance of the local models (i.e., L2-norm) from the global model to detect malicious backdoors. However, as these approaches directly operate on client updates, their effectiveness depends on factors such as clients' data distribution or the adversary's attack strategies. In this paper, we introduce a novel and more generic backdoor defense framework, called BayBFed, which proposes to utilize probability distributions over client updates to detect malicious updates in FL: it computes a probabilistic measure over the clients' updates to keep track of any adjustments made in the updates, and uses a novel detection algorithm that can leverage this probabilistic measure to efficiently detect and filter out malicious updates. Thus, it overcomes the shortcomings of previous approaches that arise due to the direct usage of client updates; as our probabilistic measure will include all aspects of the local client training strategies. BayBFed utilizes two Bayesian Non-Parametric extensions: (i) a Hierarchical Beta-Bernoulli process to draw a probabilistic measure given the clients' updates, and (ii) an adaptation of the Chinese Restaurant Process (CRP), referred by us as CRP-Jensen, which leverages this probabilistic measure to detect and filter out malicious updates. We extensively evaluate our defense approach on five benchmark datasets: CIFAR10, Reddit, IoT intrusion detection, MNIST, and FMNIST, and show that it can effectively detect and eliminate malicious updates in FL without deteriorating the benign performance of the global model.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Kavita Kumari",
        "Phillip Rieger",
        "Hossein Fereidooni",
        "Murtuza Jadliwala",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09508.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2301.09508",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2201.02775",
      "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems",
      "abstract": "Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the saliency score of ``victim'' participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Qi Pang",
        "Yuanyuan Yuan",
        "Shuai Wang",
        "Wenting Zheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.02775.pdf",
      "code_url": "https://github.com/Qi-Pang/ADI",
      "url": "https://arxiv.org/abs/2201.02775",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2308.05832",
      "title": "FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks",
      "abstract": "Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Ehsanul Kabir",
        "Zeyu Song",
        "Md Rafi Ur Rashid",
        "Shagufta Mehnaz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05832.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2308.05832",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2304.08847",
      "title": "BadVFL: Backdoor Attacks in Vertical Federated Learning",
      "abstract": "Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.   VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attacks in VFL is more challenging than in HFL, as the adversary i) does not have access to the labels during training and ii) cannot change the labels as she only has access to the feature embeddings. We present a first-of-its-kind clean-label backdoor attack in VFL, which consists of two phases: a label inference and a backdoor phase. We demonstrate the effectiveness of the attack on three different datasets, investigate the factors involved in its success, and discuss countermeasures to mitigate its impact.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Mohammad Naseri",
        "Yufei Han",
        "Emiliano De Cristofaro"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08847.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2304.08847",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2210.07714",
      "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
      "abstract": "Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.   This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior of neurons in hidden layers, and eliminates poisoned models through an iterative pruning scheme. CrowdGuard employs a server-located stacked clustering scheme to enhance its resilience to rogue client feedback. The evaluation results demonstrate that CrowdGuard achieves a 100% True-Positive-Rate and True-Negative-Rate across various scenarios, including IID and non-IID data distributions. Additionally, CrowdGuard withstands adaptive adversaries while preserving the original performance of protected models. To ensure confidentiality, CrowdGuard uses a secure and privacy-preserving architecture leveraging Trusted Execution Environments (TEEs) on both client and server sides.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Phillip Rieger",
        "Torsten Krau\u00df",
        "Markus Miettinen",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.07714.pdf",
      "code_url": "https://github.com/TRUST-TUDa/crowdguard",
      "url": "https://arxiv.org/abs/2210.07714",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2312.04432",
      "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04432.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2312.04432",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2312.04432",
      "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "Phillip Rieger",
        "Alexandra Dmitrienko",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04432.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2312.04432",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2406.10416",
      "title": "Byzantine-Robust Decentralized Federated Learning",
      "abstract": "Federated learning (FL) enables multiple clients to collaboratively train machine learning models without revealing their private training data. In conventional FL, the system follows the server-assisted architecture (server-assisted FL), where the training process is coordinated by a central server. However, the server-assisted FL framework suffers from poor scalability due to a communication bottleneck at the server, and trust dependency issues. To address challenges, decentralized federated learning (DFL) architecture has been proposed to allow clients to train models collaboratively in a serverless and peer-to-peer manner. However, due to its fully decentralized nature, DFL is highly vulnerable to poisoning attacks, where malicious clients could manipulate the system by sending carefully-crafted local models to their neighboring clients. To date, only a limited number of Byzantine-robust DFL methods have been proposed, most of which are either communication-inefficient or remain vulnerable to advanced poisoning attacks. In this paper, we propose a new algorithm called BALANCE (Byzantine-robust averaging through local similarity in decentralization) to defend against poisoning attacks in DFL. In BALANCE, each client leverages its own local model as a similarity reference to determine if the received model is malicious or benign. We establish the theoretical convergence guarantee for BALANCE under poisoning attacks in both strongly convex and non-convex settings. Furthermore, the convergence rate of BALANCE under poisoning attacks matches those of the state-of-the-art counterparts in Byzantine-free settings. Extensive experiments also demonstrate that BALANCE outperforms existing DFL methods and effectively defends against poisoning attacks.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Minghong Fang",
        "Zifan Zhang",
        " Hairi",
        "Prashant Khanduri",
        "Jia Liu",
        "Songtao Lu",
        "Yuchen Liu",
        "Neil Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10416",
      "code_url": null,
      "url": "https://arxiv.org/abs/2406.10416",
      "source_section": "1.2 Distributed Machine Learning",
      "source_subsection": "1.2.1 Federated Learning"
    },
    {
      "arxiv_id": "2301.02344",
      "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
      "abstract": "With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior attacks explicitly inject the insecure code payload into the training data, making the poison data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poison data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poison data by never explicitly including certain (suspicious) parts of the payload in the poison data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that can filter out suspicious sequences from the training data. Our evaluation against models of two sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for practitioners when selecting code used to train or tune code-suggestion models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hojjat Aghakhani",
        "Wei Dai",
        "Andre Manoel",
        "Xavier Fernandes",
        "Anant Kharkar",
        "Christopher Kruegel",
        "Giovanni Vigna",
        "David Evans",
        "Ben Zorn",
        "Robert Sim"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02344.pdf",
      "code_url": "https://github.com/microsoft/CodeGenerationPoisoning",
      "url": "https://arxiv.org/abs/2301.02344",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.2 Hijack Autocomplete Code"
    },
    {
      "arxiv_id": "2101.02644",
      "title": "Data Poisoning Attacks to Deep Learning Based Recommender Systems",
      "abstract": "Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.   In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Hai Huang",
        "Jiaming Mu",
        "Neil Zhenqiang Gong",
        "Qi Li",
        "Bin Liu",
        "Mingwei Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2101.02644.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2101.02644",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.4 Recommender Systems"
    },
    {
      "arxiv_id": "2006.14026",
      "title": "Subpopulation Data Poisoning Attacks",
      "abstract": "Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine learning algorithm to selectively change its output when it is deployed. In this work, we introduce a novel data poisoning attack called a \\emph{subpopulation attack}, which is particularly relevant when datasets are large and diverse. We design a modular framework for subpopulation attacks, instantiate it with different building blocks, and show that the attacks are effective for a variety of datasets and machine learning models. We further optimize the attacks in continuous domains using influence functions and gradient optimization methods. Compared to existing backdoor poisoning attacks, subpopulation attacks have the advantage of inducing misclassification in naturally distributed data points at inference time, making the attacks extremely stealthy. We also show that our attack strategy can be used to improve upon existing targeted attacks. We prove that, under some assumptions, subpopulation attacks are impossible to defend against, and empirically demonstrate the limitations of existing defenses against our attacks, highlighting the difficulty of protecting machine learning against this threat.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Matthew Jagielski",
        "Giorgio Severi",
        "Niklas Pousette Harger",
        "Alina Oprea"
      ],
      "pdf_url": "https://arxiv.org/pdf/2006.14026.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2006.14026",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.5 Classification"
    },
    {
      "arxiv_id": "2111.04394",
      "title": "Get a Model! Model Hijacking Attack Against Machine Learning Models",
      "abstract": "Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Ahmed Salem",
        "Michael Backes",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.04394.pdf",
      "code_url": "https://github.com/AhmedSalem2/Model-Hijacking",
      "url": "https://arxiv.org/abs/2111.04394",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.5 Classification"
    },
    {
      "arxiv_id": "2402.01920",
      "title": "Preference Poisoning Attacks on Reward Model Learning",
      "abstract": "Learning reward models from pairwise comparisons is a fundamental component in a number of domains, including autonomous control, conversational agents, and recommendation systems, as part of a broad goal of aligning automated decisions with user preferences. These approaches entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability by considering an attacker who can flip a small subset of preference comparisons to either promote or demote a target outcome. We propose two classes of algorithmic approaches for these attacks: a gradient-based framework, and several variants of rank-by-distance methods. Next, we evaluate the efficacy of best attacks in both these classes in successfully achieving malicious goals on datasets from three domains: autonomous control, recommendation system, and textual prompt-response preference learning. We find that the best attacks are often highly successful, achieving in the most extreme case 100\\% success rate with only 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary significantly across domains. In addition, we observe that the simpler and more scalable rank-by-distance approaches are often competitive with, and on occasion significantly outperform, gradient-based methods. Finally, we show that state-of-the-art defenses against other classes of poisoning attacks exhibit limited efficacy in our setting.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Junlin Wu",
        "Jiongxiao Wang",
        "Chaowei Xiao",
        "Chenguang Wang",
        "Ning Zhang",
        "Yevgeniy Vorobeychik"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01920",
      "code_url": null,
      "url": "https://arxiv.org/abs/2402.01920",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.6 Constractive Learning"
    },
    {
      "arxiv_id": "2204.00032",
      "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
      "abstract": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.   Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8x more precise inference on all other users' otherwise-private data points.   Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Florian Tram\u00e8r",
        "Reza Shokri",
        "Ayrton San Joaquin",
        "Hoang Le",
        "Matthew Jagielski",
        "Sanghyun Hong",
        "Nicholas Carlini"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.00032.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2204.00032",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.7 Privacy"
    },
    {
      "arxiv_id": "2308.08505",
      "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
      "abstract": "Deploying machine learning (ML) models in the wild is challenging as it suffers from distribution shifts, where the model trained on an original domain cannot generalize well to unforeseen diverse transfer domains. To address this challenge, several test-time adaptation (TTA) methods have been proposed to improve the generalization ability of the target pre-trained models under test data to cope with the shifted distribution. The success of TTA can be credited to the continuous fine-tuning of the target model according to the distributional hint from the test samples during test time. Despite being powerful, it also opens a new attack surface, i.e., test-time poisoning attacks, which are substantially different from previous poisoning attacks that occur during the training time of ML models (i.e., adversaries cannot intervene in the training process). In this paper, we perform the first test-time poisoning attack against four mainstream TTA methods, including TTT, DUA, TENT, and RPL. Concretely, we generate poisoned samples based on the surrogate models and feed them to the target TTA models. Experimental results show that the TTA methods are generally vulnerable to test-time poisoning attacks. For instance, the adversary can feed as few as 10 poisoned samples to degrade the performance of the target model from 76.20% to 41.83%. Our results demonstrate that TTA algorithms lacking a rigorous security assessment are unsuitable for deployment in real-life scenarios. As such, we advocate for the integration of defenses against test-time poisoning attacks into the design of TTA methods.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yun Shen",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.08505.pdf",
      "code_url": "https://github.com/tianshuocong/TePA",
      "url": "https://arxiv.org/abs/2308.08505",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.8 Test-Time Poisoning"
    },
    {
      "arxiv_id": "2409.12314",
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "abstract": "Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce \"model implosion,\" where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of \"supervised graph alignment\" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wenxin Ding",
        "Cathy Y. Li",
        "Shawn Shan",
        "Ben Y. Zhao",
        "Haitao Zheng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12314",
      "code_url": null,
      "url": "https://arxiv.org/abs/2409.12314",
      "source_section": "1.3 Data Poisoning",
      "source_subsection": "1.3.9 Defense"
    },
    {
      "arxiv_id": "1910.03137",
      "title": "Detecting AI Trojans Using Meta Neural Analysis",
      "abstract": "In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks, but they make undesirable assumptions about the attack strategies or require direct access to the trained models, which restricts their utility in practice.   This paper addresses these challenges by introducing a Meta Neural Trojan Detection (MNTD) pipeline that does not make assumptions on the attack strategies and only needs black-box access to models. The strategy is to train a meta-classifier that predicts whether a given target model is Trojaned. To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution. We then dynamically optimize a query set together with the meta-classifier to distinguish between Trojaned and benign models.   We evaluate MNTD with experiments on vision, speech, tabular data and natural language text datasets, and against different Trojan attacks such as data poisoning attack, model manipulation attack, and latent attack. We show that MNTD achieves 97% detection AUC score and significantly outperforms existing detection approaches. In addition, MNTD generalizes well and achieves high detection performance against unforeseen attacks. We also propose a robust MNTD pipeline which achieves 90% detection AUC even when the attacker aims to evade the detection with full knowledge of the system.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaojun Xu",
        "Qi Wang",
        "Huichen Li",
        "Nikita Borisov",
        "Carl A. Gunter",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.03137.pdf",
      "code_url": "https://github.com/AI-secure/Meta-Nerual-Trojan-Detection",
      "url": "https://arxiv.org/abs/1910.03137",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2108.00352",
      "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
      "abstract": "Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Jinyuan Jia",
        "Yupei Liu",
        "Neil Zhenqiang Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.00352.pdf",
      "code_url": "https://github.com/jjy1994/BadEncoder",
      "url": "https://arxiv.org/abs/2108.00352",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2003.08904",
      "title": "RAB: Provable Robustness Against Backdoor Attacks",
      "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Maurice Weber",
        "Xiaojun Xu",
        "Bojan Karla\u0161",
        "Ce Zhang",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.08904.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2003.08904",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2301.01197",
      "title": "Backdoor Attacks Against Dataset Distillation",
      "abstract": "Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Yugeng Liu",
        "Zheng Li",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.01197.pdf",
      "code_url": "https://github.com/liuyugeng/baadd",
      "url": "https://arxiv.org/abs/2301.01197",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2301.06241",
      "title": "BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense",
      "abstract": "Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Siyuan Cheng",
        "Guanhong Tao",
        "Yingqi Liu",
        "Shengwei An",
        "Xiangzhe Xu",
        "Shiwei Feng",
        "Guangyu Shen",
        "Kaiyuan Zhang",
        "Qiuling Xu",
        "Shiqing Ma",
        "Xiangyu Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06241.pdf",
      "code_url": "https://github.com/Megum1/BEAGLE",
      "url": "https://arxiv.org/abs/2301.06241",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2212.04687",
      "title": "Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",
      "abstract": "In this paper, we present a simple yet surprisingly effective technique to induce \"selective amnesia\" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) within a few minutes (about 30 times faster than training a model from scratch using the MNIST dataset), with only a small amount of clean data (0.1% of training data for TrojAI models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Rui Zhu",
        "Di Tang",
        "Siyuan Tang",
        "XiaoFeng Wang",
        "Haixu Tang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04687.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2212.04687",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2204.05255",
      "title": "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information",
      "abstract": "Backdoor attacks insert malicious data into a training set so that, during inference time, it misclassifies inputs that have been patched with a backdoor trigger as the malware specified label. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as \"clean-label attacks.\" Existing clean-label backdoor attacks require knowledge of the entire training set to be effective. Obtaining such knowledge is difficult or impossible because training data are often gathered from multiple sources (e.g., face images from different users). It remains a question whether backdoor attacks still present a real threat.   This paper provides an affirmative answer to this question by designing an algorithm to mount clean-label backdoor attacks based only on the knowledge of representative examples from the target class. With poisoning equal to or less than 0.5% of the target-class data and 0.05% of the training set, we can train a model to classify test examples from arbitrary classes into the target class when the examples are patched with a backdoor trigger. Our attack works well across datasets and models, even when the trigger presents in the physical world.   We explore the space of defenses and find that, surprisingly, our attack can evade the latest state-of-the-art defenses in their vanilla form, or after a simple twist, we can adapt to the downstream defenses. We study the cause of the intriguing effectiveness and find that because the trigger synthesized by our attack contains features as persistent as the original semantic features of the target class, any attempt to remove such triggers would inevitably hurt the model accuracy first.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Yi Zeng",
        "Minzhou Pan",
        "Hoang Anh Just",
        "Lingjuan Lyu",
        "Meikang Qiu",
        "Ruoxi Jia"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.05255.pdf",
      "code_url": "https://github.com/ruoxi-jia-group/Narcissus-backdoor-attack",
      "url": "https://arxiv.org/abs/2204.05255",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2312.02673",
      "title": "Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics",
      "abstract": "A backdoor attack in deep learning inserts a hidden backdoor in the model to trigger malicious behavior upon specific input patterns. Existing detection approaches assume a metric space (for either the original inputs or their latent representations) in which normal samples and malicious samples are separable. We show that this assumption has a severe limitation by introducing a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures the difference between normal samples and malicious samples.   To overcome this limitation, we move beyond looking for a perfect metric space that would work for different deep-learning models, and instead resort to more robust topological constructs. We propose TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection. The main idea of TED is to view a deep-learning model as a dynamical system that evolves inputs to outputs. In such a dynamical system, a benign input follows a natural evolution trajectory similar to other benign inputs. In contrast, a malicious sample displays a distinct trajectory, since it starts close to benign samples but eventually shifts towards the neighborhood of attacker-specified target samples to activate the backdoor.   Extensive evaluations are conducted on vision and natural language datasets across different network architectures. The results demonstrate that TED not only achieves a high detection rate, but also significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack. The code to reproduce the results is made public on GitHub.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xiaoxing Mo",
        "Yechao Zhang",
        "Leo Yu Zhang",
        "Wei Luo",
        "Nan Sun",
        "Shengshan Hu",
        "Shang Gao",
        "Yang Xiang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02673.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2312.02673",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2205.06900",
      "title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic",
      "abstract": "Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type. Our detector leverages the influence of the backdoor attack, independent of the backdoor embedding mechanism, on the landscape of the classifier's outputs prior to the softmax layer. For each class, a maximum margin statistic is estimated. Detection inference is then performed by applying an unsupervised anomaly detector to these statistics. Thus, our detector does not need any legitimate clean samples, and can efficiently detect backdoor attacks with arbitrary numbers of source classes. These advantages over several state-of-the-art methods are demonstrated on four datasets, for three different types of backdoor patterns, and for a variety of attack configurations. Finally, we propose a novel, general approach for backdoor mitigation once a detection is made. The mitigation approach was the runner-up at the first IEEE Trojan Removal Competition. The code is online available.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Hang Wang",
        "Zhen Xiang",
        "David J. Miller",
        "George Kesidis"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.06900.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2205.06900",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.1 Image"
    },
    {
      "arxiv_id": "2105.00164",
      "title": "Hidden Backdoors in Human-Centric Language Models",
      "abstract": "Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \\textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection, $95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Shaofeng Li",
        "Hui Liu",
        "Tian Dong",
        "Benjamin Zi Hao Zhao",
        "Minhui Xue",
        "Haojin Zhu",
        "Jialiang Lu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.00164.pdf",
      "code_url": "https://github.com/lishaofeng/NLP_Backdoor",
      "url": "https://arxiv.org/abs/2105.00164",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.2 Text"
    },
    {
      "arxiv_id": "2111.00197",
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "abstract": "Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Lujia Shen",
        "Shouling Ji",
        "Xuhong Zhang",
        "Jinfeng Li",
        "Jing Chen",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.00197.pdf",
      "code_url": "https://github.com/lishaofeng/NLP_Backdoor",
      "url": "https://arxiv.org/abs/2111.00197",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.2 Text"
    },
    {
      "arxiv_id": "2311.11225",
      "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification",
      "abstract": "Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hengzhi Pei",
        "Jinyuan Jia",
        "Wenbo Guo",
        "Bo Li",
        "Dawn Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11225.pdf",
      "code_url": "https://github.com/AI-secure/TextGuard",
      "url": "https://arxiv.org/abs/2311.11225",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.2 Text"
    },
    {
      "arxiv_id": "2006.11890",
      "title": "Graph Backdoor",
      "abstract": "One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Zhaohan Xi",
        "Ren Pang",
        "Shouling Ji",
        "Ting Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2006.11890.pdf",
      "code_url": "https://github.com/HarrialX/GraphBackdoor",
      "url": "https://arxiv.org/abs/2006.11890",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.3 Graph"
    },
    {
      "arxiv_id": "2407.08935",
      "title": "Distributed Backdoor Attacks on Federated Graph Learning and Certified Defenses",
      "abstract": "Federated graph learning (FedGL) is an emerging federated learning (FL) framework that extends FL to learn graph data from diverse sources. FL for non-graph data has shown to be vulnerable to backdoor attacks, which inject a shared backdoor trigger into the training data such that the trained backdoored FL model can predict the testing data containing the trigger as the attacker desires. However, FedGL against backdoor attacks is largely unexplored, and no effective defense exists.   In this paper, we aim to address such significant deficiency. First, we propose an effective, stealthy, and persistent backdoor attack on FedGL. Our attack uses a subgraph as the trigger and designs an adaptive trigger generator that can derive the effective trigger location and shape for each graph. Our attack shows that empirical defenses are hard to detect/remove our generated triggers. To mitigate it, we further develop a certified defense for any backdoored FedGL model against the trigger with any shape at any location. Our defense involves carefully dividing a testing graph into multiple subgraphs and designing a majority vote-based ensemble classifier on these subgraphs. We then derive the deterministic certified robustness based on the ensemble classifier and prove its tightness. We extensively evaluate our attack and defense on six graph datasets. Our attack results show our attack can obtain > 90% backdoor accuracy in almost all datasets. Our defense results show, in certain cases, the certified accuracy for clean testing graphs against an arbitrary trigger with size 20 can be close to the normal accuracy under no attack, while there is a moderate gap in other cases. Moreover, the certified backdoor accuracy is always 0 for backdoored testing graphs generated by our attack, implying our defense can fully mitigate the attack. Source code is available at: https://github.com/Yuxin104/Opt-GDBA.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuxin Yang",
        "Qiang Li",
        "Jinyuan Jia",
        "Yuan Hong",
        "Binghui Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08935",
      "code_url": "https://github.com/Yuxin104/Opt-GDBA",
      "url": "https://arxiv.org/abs/2407.08935",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.3 Graph"
    },
    {
      "arxiv_id": "2302.06279",
      "title": "Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.   This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Gorka Abad",
        "Oguzhan Ersoy",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.06279.pdf",
      "code_url": "https://github.com/GorkaAbad/Sneaky-Spikes",
      "url": "https://arxiv.org/abs/2302.06279",
      "source_section": "1.4 Backdoor",
      "source_subsection": "1.4.7 Neuromorphic Data"
    },
    {
      "arxiv_id": "2308.05596",
      "title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content",
      "abstract": "The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five model architectures and eight datasets demonstrating that LLMs with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. We find that prompt learning achieves around 10\\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xinlei He",
        "Savvas Zannettou",
        "Yun Shen",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05596.pdf",
      "code_url": "https://github.com/xinleihe/toxic-prompt",
      "url": "https://arxiv.org/abs/2308.05596",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.1 Cyberbullying"
    },
    {
      "arxiv_id": "2307.14657",
      "title": "Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance",
      "abstract": "Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.   This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classification using our dataset. Our results reveal that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We discover no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance. We also demonstrate how a larger number of families to classify make the classification harder, while a higher number of samples per family increases accuracy. Finally, we find that models trained on a uniform distribution of samples per family better generalize on unseen data.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Savino Dambra",
        "Yufei Han",
        "Simone Aonzo",
        "Platon Kotzias",
        "Antonino Vitale",
        "Juan Caballero",
        "Davide Balzarotti",
        "Leyla Bilge"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14657.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2307.14657",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2308.05034",
      "title": "Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance",
      "abstract": "Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Zijun Cheng",
        "Qiujian Lv",
        "Jinyuan Liang",
        "Yan Wang",
        "Degang Sun",
        "Thomas Pasquier",
        "Xueyuan Han"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05034.pdf",
      "code_url": "https://github.com/ProvenanceAnalytics/kairos",
      "url": "https://arxiv.org/abs/2308.05034",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2311.16940",
      "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
      "abstract": "Browser fingerprinting often provides an attractive alternative to third-party cookies for tracking users across the web. In fact, the increasing restrictions on third-party cookies placed by common web browsers and recent regulations like the GDPR may accelerate the transition. To counter browser fingerprinting, previous work proposed several techniques to detect its prevalence and severity. However, these rely on 1) centralized web crawls and/or 2) computationally intensive operations to extract and process signals (e.g., information-flow and static analysis). To address these limitations, we present FP-Fed, the first distributed system for browser fingerprinting detection. Using FP-Fed, users can collaboratively train on-device models based on their real browsing patterns, without sharing their training data with a central entity, by relying on Differentially Private Federated Learning (DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's performance on a set of 18.3k popular websites with different privacy levels, numbers of participants, and features extracted from the scripts. Our experiments show that FP-Fed achieves reasonably high detection performance and can perform both training and inference efficiently, on-device, by only relying on runtime signals extracted from the execution trace, without requiring any resource-intensive operation.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Meenatchi Sundaram Muthu Selva Annamalai",
        "Igor Bilogrevic",
        "Emiliano De Cristofaro"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16940.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2311.16940",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2301.13577",
      "title": "DRAINCLoG: Detecting Rogue Accounts with Illegally-obtained NFTs using Classifiers Learned on Graphs",
      "abstract": "As Non-Fungible Tokens (NFTs) continue to grow in popularity, NFT users have become targets of phishing attacks by cybercriminals, called \\textit{NFT drainers}. Over the last year, \\$100 million worth of NFTs were stolen by drainers, and their presence remains a serious threat to the NFT trading space. However, no work has yet comprehensively investigated the behaviors of drainers in the NFT ecosystem.   In this paper, we present the first study on the trading behavior of NFT drainers and introduce the first dedicated NFT drainer detection system. We collect 127M NFT transaction data from the Ethereum blockchain and 1,135 drainer accounts from five sources for the year 2022. We find that drainers exhibit significantly different transactional and social contexts from those of regular users. With these insights, we design \\textit{DRAINCLoG}, an automatic drainer detection system utilizing Graph Neural Networks. This system effectively captures the multifaceted web of interactions within the NFT space through two distinct graphs: the NFT-User graph for transaction contexts and the User graph for social contexts. Evaluations using real-world NFT transaction data underscore the robustness and precision of our model. Additionally, we analyze the security of \\textit{DRAINCLoG} under a wide variety of evasion attacks.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hanna Kim",
        "Jian Cui",
        "Eugene Jang",
        "Chanhee Lee",
        "Yongjae Lee",
        "Jin-Woo Chung",
        "Seungwon Shin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13577.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2301.13577",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2309.04798",
      "title": "Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic",
      "abstract": "Machine learning (ML) is promising in accurately detecting malicious flows in encrypted network traffic; however, it is challenging to collect a training dataset that contains a sufficient amount of encrypted malicious data with correct labels. When ML models are trained with low-quality training data, they suffer degraded performance. In this paper, we aim at addressing a real-world low-quality training dataset problem, namely, detecting encrypted malicious traffic generated by continuously evolving malware. We develop RAPIER that fully utilizes different distributions of normal and malicious traffic data in the feature space, where normal data is tightly distributed in a certain area and the malicious data is scattered over the entire feature space to augment training data for model training. RAPIER includes two pre-processing modules to convert traffic into feature vectors and correct label noises. We evaluate our system on two public datasets and one combined dataset. With 1000 samples and 45% noises from each dataset, our system achieves the F1 scores of 0.770, 0.776, and 0.855, respectively, achieving average improvements of 352.6%, 284.3%, and 214.9% over the existing methods, respectively. Furthermore, We evaluate RAPIER with a real-world dataset obtained from a security enterprise. RAPIER effectively achieves encrypted malicious traffic detection with the best F1 score of 0.773 and improves the F1 score of existing methods by an average of 272.5%.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Yuqi Qing",
        "Qilei Yin",
        "Xinhao Deng",
        "Yihao Chen",
        "Zhuotao Liu",
        "Kun Sun",
        "Ke Xu",
        "Jia Zhang",
        "Qi Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04798.pdf",
      "code_url": "https://github.com/XXnormal/RAPIER",
      "url": "https://arxiv.org/abs/2309.04798",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2409.09272",
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Kai Li",
        "Yifan Zheng",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09272",
      "code_url": "https://github.com/LetterLiGo/SafeEar",
      "url": "https://arxiv.org/abs/2409.09272",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2410.17910",
      "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
      "abstract": "Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations. Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks. To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning. Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining. By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks. Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies. Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods. Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Wei Qiao",
        "Yebo Feng",
        "Teng Li",
        "Zhuo Ma",
        "Yulong Shen",
        "JianFeng Ma",
        "Yang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17910",
      "code_url": null,
      "url": "https://arxiv.org/abs/2410.17910",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2405.04095",
      "title": "Combating Concept Drift with Explanatory Detection and Adaptation for Android Malware Classification",
      "abstract": "Machine learning-based Android malware classifiers achieve high accuracy in stationary environments but struggle with concept drift. The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has largely centered on detecting drift samples, with expert-led label revisions on these samples to guide model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   To combat concept drift, we propose DREAM, a novel system that improves drift detection and establishes an explanatory adaptation process. Our core idea is to integrate classifier and expert knowledge within a unified model. To achieve this, we embed malware explanations (or concepts) within the latent space of a contrastive autoencoder, while constraining sample reconstruction based on classifier predictions. This approach enhances classifier retraining in two key ways: 1) capturing the target classifier's characteristics to select more effective samples in drift detection and 2) enabling concept revisions that extend the classifier's semantics to provide stronger guidance for adaptation. Additionally, DREAM eliminates reliance on training data during real-time drift detection and provides a behavior-based drift explainer to support concept revision. Our evaluation shows that DREAM effectively improves the drift detection accuracy and reduces the expert analysis effort in adaptation across different malware datasets and classifiers. Notably, when updating a widely-used Drebin classifier, DREAM achieves the same accuracy with 76.6% fewer newly labeled samples compared to the best existing methods.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Yiling He",
        "Junchi Lei",
        "Zhan Qin",
        "Kui Ren",
        "Chun Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04095",
      "code_url": null,
      "url": "https://arxiv.org/abs/2405.04095",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2506.17162",
      "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
      "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability.   To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Side Liu",
        "Jiang Ming",
        "Guodong Zhou",
        "Xinyi Liu",
        "Jianming Fu",
        "Guojun Peng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.17162",
      "code_url": null,
      "url": "https://arxiv.org/abs/2506.17162",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.2 Security Applications"
    },
    {
      "arxiv_id": "2103.03809",
      "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding",
      "abstract": "Deep learning has demonstrated its strengths in numerous binary analysis tasks, including function boundary detection, binary code search, function prototype inference, value set analysis, etc. When applying deep learning to binary analysis tasks, we need to decide what input should be fed into the neural network model. More specifically, we need to answer how to represent an instruction in a fixed-length vector. The idea of automatically learning instruction representations is intriguing, however the existing schemes fail to capture the unique characteristics of disassembly. These schemes ignore the complex intra-instruction structures and mainly rely on control flow in which the contextual information is noisy and can be influenced by compiler optimizations.   In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora. PalmTree utilizes three pre-training tasks to capture various characteristics of assembly language. These training tasks overcome the problems in existing schemes, thus can help to generate high-quality representations. We conduct both intrinsic and extrinsic evaluations, and compare PalmTree with other instruction embedding schemes. PalmTree has the best performance for intrinsic metrics, and outperforms the other instruction embedding schemes for all downstream tasks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Xuezixiang Li",
        "Qu Yu",
        "Heng Yin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2103.03809.pdf",
      "code_url": "https://github.com/palmtreemodel/PalmTree",
      "url": "https://arxiv.org/abs/2103.03809",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.5 Code Analysis"
    },
    {
      "arxiv_id": "2111.01415",
      "title": "Callee: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning",
      "abstract": "Recovering binary programs' call graphs is crucial for inter-procedural analysis tasks and applications based on them.transfer One of the core challenges is recognizing targets of indirect calls (i.e., indirect callees). Existing solutions all have high false positives and negatives, making call graphs inaccurate. In this paper, we propose a new solution Callee combining transfer learning and contrastive learning. The key insight is that, deep neural networks (DNNs) can automatically identify patterns concerning indirect calls, which can be more efficient than designing approximation algorithms or heuristic rules to handle various cases. Inspired by the advances in question-answering applications, we utilize contrastive learning to answer the callsite-callee question. However, one of the toughest challenges is that DNNs need large datasets to achieve high performance, while collecting large-scale indirect-call ground-truths can be computational-expensive. Since direct calls and indirect calls share similar calling conventions, it is possible to transfer knowledge learned from direct calls to indirect ones. Therefore, we leverage transfer learning to pre-train DNNs with easy-to-collect direct calls and further fine-tune the indirect-call DNNs. We evaluate Callee on several groups of targets, and results show that our solution could match callsites to callees with an F1-Measure of 94.6%, much better than state-of-the-art solutions. Further, we apply Callee to binary code similarity detection and hybrid fuzzing, and found it could greatly improve their performance.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Wenyu Zhu",
        "Zhiyao Feng",
        "Zihan Zhang",
        "Jianjun Chen",
        "Zhijian Ou",
        "Min Yang",
        "Chao Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.01415.pdf",
      "code_url": "https://github.com/vul337/Callee",
      "url": "https://arxiv.org/abs/2111.01415",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.5 Code Analysis"
    },
    {
      "arxiv_id": "2112.02125",
      "title": "Examining Zero-Shot Vulnerability Repair with Large Language Models",
      "abstract": "Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Hammond Pearce",
        "Benjamin Tan",
        "Baleegh Ahmad",
        "Ramesh Karri",
        "Brendan Dolan-Gavitt"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.02125.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2112.02125",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.5 Code Analysis"
    },
    {
      "arxiv_id": "2409.02074",
      "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
      "abstract": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.",
      "year": 2025,
      "venue": "NDSS",
      "authors": [
        "Jiangyi Deng",
        "Xinfeng Li",
        "Yanjiao Chen",
        "Yijie Bai",
        "Haiqin Weng",
        "Yan Liu",
        "Tao Wei",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02074",
      "code_url": null,
      "url": "https://arxiv.org/abs/2409.02074",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.5 Code Analysis"
    },
    {
      "arxiv_id": "2209.03463",
      "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
      "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Wai Man Si",
        "Michael Backes",
        "Jeremy Blackburn",
        "Emiliano De Cristofaro",
        "Gianluca Stringhini",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03463.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.03463",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.6 Chatbot"
    },
    {
      "arxiv_id": "2306.11924",
      "title": "Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition",
      "abstract": "End-to-end encryption (E2EE) provides strong technical protections to individuals from interferences. Governments and law enforcement agencies around the world have however raised concerns that E2EE also allows illegal content to be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH) to detect known illegal content before it is shared, is seen as a promising solution to prevent the diffusion of illegal content while preserving encryption. While these proposals raise strong privacy concerns, proponents of the solutions have argued that the risk is limited as the technology has a limited scope: detecting known illegal content. In this paper, we show that modern perceptual hashing algorithms are actually fairly flexible pieces of technology and that this flexibility could be used by an adversary to add a secondary hidden feature to a client-side scanning system. More specifically, we show that an adversary providing the PH algorithm can ``hide\" a secondary purpose of face recognition of a target individual alongside its primary purpose of image copy detection. We first propose a procedure to train a dual-purpose deep perceptual hashing model by jointly optimizing for both the image copy detection and the targeted facial recognition task. Second, we extensively evaluate our dual-purpose model and show it to be able to reliably identify a target individual 67% of the time while not impacting its performance at detecting illegal content. We also show that our model is neither a general face detection nor a facial recognition model, allowing its secondary purpose to be hidden. Finally, we show that the secondary purpose can be enabled by adding a single illegal looking image to the database. Taken together, our results raise concerns that a deep perceptual hashing-based CSS system could turn billions of user devices into tools to locate targeted individuals.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Shubham Jain",
        "Ana-Maria Cretu",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.11924.pdf",
      "code_url": "https://github.com/computationalprivacy/dual-purpose-client-side-scanning",
      "url": "https://arxiv.org/abs/2306.11924",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.7 Side Channel Attack"
    },
    {
      "arxiv_id": "2209.03050",
      "title": "Cerberus: Exploring Federated Prediction of Security Events",
      "abstract": "Modern defenses against cyberattacks increasingly rely on proactive approaches, e.g., to predict the adversary's next actions based on past events. Building accurate prediction models requires knowledge from many organizations; alas, this entails disclosing sensitive information, such as network structures, security postures, and policies, which might often be undesirable or outright impossible. In this paper, we explore the feasibility of using Federated Learning (FL) to predict future security events. To this end, we introduce Cerberus, a system enabling collaborative training of Recurrent Neural Network (RNN) models for participating organizations. The intuition is that FL could potentially offer a middle-ground between the non-private approach where the training data is pooled at a central server and the low-utility alternative of only training local models. We instantiate Cerberus on a dataset obtained from a major security company's intrusion prevention product and evaluate it vis-a-vis utility, robustness, and privacy, as well as how participants contribute to and benefit from the system. Overall, our work sheds light on both the positive aspects and the challenges of using FL for this task and paves the way for deploying federated approaches to predictive security.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Mohammad Naseri",
        "Yufei Han",
        "Enrico Mariconti",
        "Yun Shen",
        "Gianluca Stringhini",
        "Emiliano De Cristofaro"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03050.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.03050",
      "source_section": "1.6 AI4Security",
      "source_subsection": "1.6.9 Security Event"
    },
    {
      "arxiv_id": "2009.09663",
      "title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
      "abstract": "Deep neural networks (DNNs) have become one of the enabling technologies in many safety-critical applications, e.g., autonomous driving and medical image analysis. DNN systems, however, suffer from various kinds of threats, such as adversarial example attacks and fault injection attacks. While there are many defense methods proposed against maliciously crafted inputs, solutions against faults presented in the DNN system itself (e.g., parameters and calculations) are far less explored. In this paper, we develop a novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification. The key to enabling such lightweight checking is that the smaller neural network only needs to produce approximate results for the initial task without sacrificing fault coverage much. We develop efficient and effective architecture and task exploration techniques to achieve optimized risk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve can reduce 90% of the risks at around 10% overhead.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Yu Li",
        "Min Li",
        "Bo Luo",
        "Ye Tian",
        "Qiang Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.09663.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2009.09663",
      "source_section": "1.8 Hardware Related Security",
      "source_subsection": "1.8.1 Verification"
    },
    {
      "arxiv_id": "2109.11495",
      "title": "DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications",
      "abstract": "Unsupervised Deep Learning (DL) techniques have been widely used in various security-related anomaly detection applications, owing to the great promise of being able to detect unforeseen threats and superior performance provided by Deep Neural Networks (DNN). However, the lack of interpretability creates key barriers to the adoption of DL models in practice. Unfortunately, existing interpretation approaches are proposed for supervised learning models and/or non-security domains, which are unadaptable for unsupervised DL models and fail to satisfy special requirements in security domains.   In this paper, we propose DeepAID, a general framework aiming to (1) interpret DL-based anomaly detection systems in security domains, and (2) improve the practicality of these systems based on the interpretations. We first propose a novel interpretation method for unsupervised DNNs by formulating and solving well-designed optimization problems with special constraints for security domains. Then, we provide several applications based on our Interpreter as well as a model-based extension Distiller to improve security systems by solving domain-specific problems. We apply DeepAID over three types of security-related anomaly detection systems and extensively evaluate our Interpreter with representative prior works. Experimental results show that DeepAID can provide high-quality interpretations for unsupervised DL models while meeting the special requirements of security domains. We also provide several use cases to show that DeepAID can help security operators to understand model decisions, diagnose system mistakes, give feedback to models, and reduce false positives.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Dongqi Han",
        "Zhiliang Wang",
        "Wenqi Chen",
        "Ying Zhong",
        "Su Wang",
        "Han Zhang",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2109.11495.pdf",
      "code_url": "https://github.com/dongtsi/DeepAID",
      "url": "https://arxiv.org/abs/2109.11495",
      "source_section": "1.9 Security Related Interpreting Method",
      "source_subsection": "1.9.1 Anomaly Detection"
    },
    {
      "arxiv_id": "2309.05679",
      "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing",
      "abstract": "While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \\ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Jinwen He",
        "Kai Chen",
        "Guozhu Meng",
        "Jiangshan Zhang",
        "Congyi Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05679.pdf",
      "code_url": "https://github.com/JenniferHo97/XAI-TREND-TEST",
      "url": "https://arxiv.org/abs/2309.05679",
      "source_section": "1.9 Security Related Interpreting Method",
      "source_subsection": "1.9.2 Faithfulness"
    },
    {
      "arxiv_id": "2308.05362",
      "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
      "abstract": "Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Yiling He",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05362.pdf",
      "code_url": "https://github.com/E0HYL/FINER-explain",
      "url": "https://arxiv.org/abs/2308.05362",
      "source_section": "1.9 Security Related Interpreting Method",
      "source_subsection": "1.9.3 Security Applications"
    },
    {
      "arxiv_id": "2210.09421",
      "title": "Deepfake Text Detection: Limitations and Opportunities",
      "abstract": "Recent advances in generative models for language have enabled the creation of convincing synthetic text or deepfake text. Prior work has demonstrated the potential for misuse of deepfake text to mislead content consumers. Therefore, deepfake text detection, the task of discriminating between human and machine-generated text, is becoming increasingly critical. Several defenses have been proposed for deepfake text detection. However, we lack a thorough understanding of their real-world applicability. In this paper, we collect deepfake text from 4 online services powered by Transformer-based tools to evaluate the generalization ability of the defenses on content in the wild. We develop several low-cost adversarial attacks, and investigate the robustness of existing defenses against an adaptive attacker. We find that many defenses show significant degradation in performance under our evaluation scenarios compared to their original claimed performance. Our evaluation shows that tapping into the semantic information in the text content is a promising approach for improving the robustness and generalization performance of deepfake text detection schemes.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Jiameng Pu",
        "Zain Sarwar",
        "Sifat Muhammad Abdullah",
        "Abdullah Rehman",
        "Yoonjin Kim",
        "Parantapa Bhattacharya",
        "Mobin Javed",
        "Bimal Viswanath"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.09421.pdf",
      "code_url": "https://github.com/jmpu/DeepfakeTextDetection",
      "url": "https://arxiv.org/abs/2210.09421",
      "source_section": "1.10 AI Generation Security",
      "source_subsection": "1.10.1 Text Generation Detection"
    },
    {
      "arxiv_id": "2303.14822",
      "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
      "abstract": "Nowadays, powerful large language models (LLMs) such as ChatGPT have demonstrated revolutionary power in a variety of tasks. Consequently, the detection of machine-generated texts (MGTs) is becoming increasingly crucial as LLMs become more advanced and prevalent. These models have the ability to generate human-like language, making it challenging to discern whether a text is authored by a human or a machine. This raises concerns regarding authenticity, accountability, and potential bias. However, existing methods for detecting MGTs are evaluated using different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework that encompasses various methodologies. Furthermore, it remains unclear how existing detection methods would perform against powerful LLMs. In this paper, we fill this gap by proposing the first benchmark framework for MGT detection against powerful LLMs, named MGTBench. Extensive evaluations on public datasets with curated texts generated by various powerful LLMs such as ChatGPT-turbo and Claude demonstrate the effectiveness of different detection methods. Our ablation study shows that a larger number of words in general leads to better performance and most detection methods can achieve similar performance with much fewer training samples. Moreover, we delve into a more challenging task: text attribution. Our findings indicate that the model-based detection methods still perform well in the text attribution task. To investigate the robustness of different detection methods, we consider three adversarial attacks, namely paraphrasing, random spacing, and adversarial perturbations. We discover that these attacks can significantly diminish detection effectiveness, underscoring the critical need for the development of more robust detection methods.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Xinlei He",
        "Xinyue Shen",
        "Zeyuan Chen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14822",
      "code_url": "https://github.com/xinleihe/MGTBench",
      "url": "https://arxiv.org/abs/2303.14822",
      "source_section": "1.10 AI Generation Security",
      "source_subsection": "1.10.1 Text Generation Detection"
    },
    {
      "arxiv_id": "2510.05173",
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
      "year": 2025,
      "venue": "CCS",
      "authors": [
        "Peigui Qi",
        "Kunsheng Tang",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Nenghai Yu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.05173",
      "code_url": null,
      "url": "https://arxiv.org/abs/2510.05173",
      "source_section": "1.10 AI Generation Security",
      "source_subsection": "1.10.1 Text Generation Detection"
    },
    {
      "arxiv_id": "2409.09272",
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Kai Li",
        "Yifan Zheng",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09272",
      "code_url": "https://github.com/LetterLiGo/SafeEar",
      "url": "https://arxiv.org/abs/2409.09272",
      "source_section": "1.10 AI Generation Security",
      "source_subsection": "1.10.2 Deepfake"
    },
    {
      "arxiv_id": "2302.05319",
      "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
      "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Jingxuan He",
        "Martin Vechev"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05319.pdf",
      "code_url": "https://github.com/eth-sri/sven",
      "url": "https://arxiv.org/abs/2302.05319",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.1 Code Analysis"
    },
    {
      "arxiv_id": "2409.02074",
      "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
      "abstract": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.",
      "year": 2025,
      "venue": "NDSS",
      "authors": [
        "Jiangyi Deng",
        "Xinfeng Li",
        "Yanjiao Chen",
        "Yijie Bai",
        "Haiqin Weng",
        "Yan Liu",
        "Tao Wei",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02074",
      "code_url": null,
      "url": "https://arxiv.org/abs/2409.02074",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.1 Code Analysis"
    },
    {
      "arxiv_id": "2409.12699",
      "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
      "abstract": "The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, they often introduce security vulnerabilities due to training on insecure open-source data. This highlights the need for ensuring secure and functional code generation. This paper introduces PromSec, an algorithm for prom optimization for secure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate code-clearing and generation as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. PromSec offers a cost-effective and practical solution for generating secure, functional code. Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that while a state-of-the-art approach fails to address all code vulnerabilities, PromSec effectively resolves them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operation time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study is a step in enhancing the trustworthiness of LLMs for secure and functional code generation, supporting their integration into real-world software development.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Mahmoud Nazzal",
        "Issa Khalil",
        "Abdallah Khreishah",
        "NhatHai Phan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12699",
      "code_url": "https://github.com/mahmoudkanazzal/PromSec",
      "url": "https://arxiv.org/abs/2409.12699",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.1 Code Analysis"
    },
    {
      "arxiv_id": "2305.12082",
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "abstract": "Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL$\\cdot$E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: \\url{https://github.com/Yuchen413/text2image_safety}.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yuchen Yang",
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Yinzhi Cao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12082.pdf",
      "code_url": "https://github.com/Yuchen413/text2image_safety",
      "url": "https://arxiv.org/abs/2305.12082",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.2 Vision-Language Model"
    },
    {
      "arxiv_id": "2404.06666",
      "title": "SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models",
      "abstract": "Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Yuchen Yang",
        "Jiangyi Deng",
        "Chen Yan",
        "Yanjiao Chen",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06666",
      "code_url": "https://github.com/LetterLiGo/SafeGen_CCS2024",
      "url": "https://arxiv.org/abs/2404.06666",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.2 Vision-Language Model"
    },
    {
      "arxiv_id": "2309.14122",
      "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
      "abstract": "Advanced text-to-image models such as DALL$\\cdot$E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Zhongjie Ba",
        "Jieming Zhong",
        "Jiachen Lei",
        "Peng Cheng",
        "Qinglong Wang",
        "Zhan Qin",
        "Zhibo Wang",
        "Kui Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14122",
      "code_url": null,
      "url": "https://arxiv.org/abs/2309.14122",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.2 Vision-Language Model"
    },
    {
      "arxiv_id": "2408.07728",
      "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
      "abstract": "We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Peiran Wang",
        "Qiyu Li",
        "Longxuan Yu",
        "Ziyao Wang",
        "Ang Li",
        "Haojian Jin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07728",
      "code_url": "https://github.com/DataSmithLab/Moderator",
      "url": "https://arxiv.org/abs/2408.07728",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.2 Vision-Language Model"
    },
    {
      "arxiv_id": "2307.08715",
      "title": "MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots",
      "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers.   In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. Jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of LLM chatbots.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08715.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2307.08715",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.3 Jailbreaking"
    },
    {
      "arxiv_id": "2311.17400",
      "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
      "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Lujia Shen",
        "Yuwen Pu",
        "Shouling Ji",
        "Changjiang Li",
        "Xuhong Zhang",
        "Chunpeng Ge",
        "Ting Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17400.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2311.17400",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.4 Robustness"
    },
    {
      "arxiv_id": "2311.05019",
      "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
      "abstract": "The potential misuse of ChatGPT and other Large Language Models (LLMs) has raised concerns regarding the dissemination of false information, plagiarism, academic dishonesty, and fraudulent activities. Consequently, distinguishing between AI-generated and human-generated content has emerged as an intriguing research topic. However, current text detection methods lack precision and are often restricted to specific tasks or domains, making them inadequate for identifying content generated by ChatGPT. In this paper, we propose an effective ChatGPT detector named DEMASQ, which accurately identifies ChatGPT-generated content. Our method addresses two critical factors: (i) the distinct biases in text composition observed in human- and machine-generated content and (ii) the alterations made by humans to evade previous detection methods. DEMASQ is an energy-based detection model that incorporates novel aspects, such as (i) optimization inspired by the Doppler effect to capture the interdependence between input text embeddings and output labels, and (ii) the use of explainable AI techniques to generate diverse perturbations. To evaluate our detector, we create a benchmark dataset comprising a mixture of prompts from both ChatGPT and humans, encompassing domains such as medical, open Q&A, finance, wiki, and Reddit. Our evaluation demonstrates that DEMASQ achieves high accuracy in identifying content generated by ChatGPT.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Kavita Kumari",
        "Alessandro Pegoraro",
        "Hossein Fereidooni",
        "Ahmad-Reza Sadeghi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.05019.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2311.05019",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.5 Generated Concent"
    },
    {
      "arxiv_id": "2402.03214",
      "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
      "abstract": "The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Anna Yoo Jeong Ha",
        "Josephine Passananti",
        "Ronik Bhaskar",
        "Shawn Shan",
        "Reid Southen",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03214",
      "code_url": null,
      "url": "https://arxiv.org/abs/2402.03214",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.5 Generated Concent"
    },
    {
      "arxiv_id": "2306.05524",
      "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
      "abstract": "With ChatGPT under the spotlight, utilizing large language models (LLMs) to assist academic writing has drawn a significant amount of debate in the community. In this paper, we aim to present a comprehensive study of the detectability of ChatGPT-generated content within the academic literature, particularly focusing on the abstracts of scientific papers, to offer holistic support for the future development of LLM applications and policies in academia. Specifically, we first present GPABench2, a benchmarking dataset of over 2.8 million comparative samples of human-written, GPT-written, GPT-completed, and GPT-polished abstracts of scientific writing in computer science, physics, and humanities and social sciences. Second, we explore the methodology for detecting ChatGPT content. We start by examining the unsatisfactory performance of existing ChatGPT detecting tools and the challenges faced by human evaluators (including more than 240 researchers or students). We then test the hand-crafted linguistic features models as a baseline and develop a deep neural framework named CheckGPT to better capture the subtle and deep semantic and linguistic patterns in ChatGPT written literature. Last, we conduct comprehensive experiments to validate the proposed CheckGPT framework in each benchmarking task over different disciplines. To evaluate the detectability of ChatGPT content, we conduct extensive experiments on the transferability, prompt engineering, and robustness of CheckGPT.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Zeyan Liu",
        "Zijun Yao",
        "Fengjun Li",
        "Bo Luo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.05524v2",
      "code_url": null,
      "url": "https://arxiv.org/abs/2306.05524",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.5 Generated Concent"
    },
    {
      "arxiv_id": "2308.13904",
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Chengkun Wei",
        "Wenlong Meng",
        "Zhikun Zhang",
        "Min Chen",
        "Minghu Zhao",
        "Wenjing Fang",
        "Lei Wang",
        "Zihui Zhang",
        "Wenzhi Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.13904.pdf",
      "code_url": "https://github.com/meng-wenlong/LMSanitator",
      "url": "https://arxiv.org/abs/2308.13904",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.6 Backdoor"
    },
    {
      "arxiv_id": "2509.07764",
      "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
      "abstract": "Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Haitao Hu",
        "Peng Chen",
        "Yanpeng Zhao",
        "Yuqi Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.07764",
      "code_url": null,
      "url": "https://arxiv.org/abs/2509.07764",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.7 Agent Security"
    },
    {
      "arxiv_id": "2410.05451",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner",
        "Chuan Guo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05451",
      "code_url": null,
      "url": "https://arxiv.org/abs/2410.05451",
      "source_section": "1.11 LLM Security",
      "source_subsection": "1.11.8 Prompt Injection"
    },
    {
      "arxiv_id": "2107.13190",
      "title": "TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing",
      "abstract": "Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator.   Our experimental evaluations on TableGAN-MCA have five main findings. First, TableGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Aoting Hu",
        "Renjie Xie",
        "Zhigang Lu",
        "Aiqun Hu",
        "Minhui Xue"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.13190.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2107.13190",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2103.11109",
      "title": "DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation",
      "abstract": "Recent success of deep neural networks (DNNs) hinges on the availability of large-scale dataset; however, training on such dataset often poses privacy risks for sensitive training information. In this paper, we aim to explore the power of generative models and gradient sparsity, and propose a scalable privacy-preserving generative model DATALENS. Comparing with the standard PATE privacy-preserving framework which allows teachers to vote on one-dimensional predictions, voting on the high dimensional gradient vectors is challenging in terms of privacy preservation. As dimension reduction techniques are required, we need to navigate a delicate tradeoff space between (1) the improvement of privacy preservation and (2) the slowdown of SGD convergence. To tackle this, we take advantage of communication efficient learning and propose a novel noise compression and aggregation approach TOPAGG by combining top-k compression for dimension reduction with a corresponding noise injection mechanism. We theoretically prove that the DATALENS framework guarantees differential privacy for its generated data, and provide analysis on its convergence. To demonstrate the practical usage of DATALENS, we conduct extensive experiments on diverse datasets including MNIST, Fashion-MNIST, and high dimensional CelebA, and we show that, DATALENS significantly outperforms other baseline DP generative models. In addition, we adapt the proposed TOPAGG approach, which is one of the key building blocks in DATALENS, to DP SGD training, and show that it is able to achieve higher utility than the state-of-the-art DP SGD approach in most cases. Our code is publicly available at https://github.com/AI-secure/DataLens.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Boxin Wang",
        "Fan Wu",
        "Yunhui Long",
        "Luka Rimanic",
        "Ce Zhang",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2103.11109.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2103.11109",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2302.00539",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Ahmed Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.00539.pdf",
      "code_url": "https://github.com/microsoft/analysing_pii_leakage",
      "url": "https://arxiv.org/abs/2302.00539",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2401.07205",
      "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
      "abstract": "With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Shiming Wang",
        "Zhe Ji",
        "Liyao Xiang",
        "Hao Zhang",
        "Xinbing Wang",
        "Chenghu Zhou",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07205.pdf",
      "code_url": "https://github.com/ShimingWang98/Facial_Feature_Crafting_against_Inversion_based_Identity_Theft/tree/main",
      "url": "https://arxiv.org/abs/2401.07205",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2311.07389",
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "abstract": "Deep neural networks are normally executed in the forward direction. However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks. Adversaries can exploit this capability to hide rogue models within seemingly legitimate models. In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets. Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models. We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models. Moreover, to mitigate this threat we propose a novel approach for detecting infected models.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Guy Amit",
        "Mosh Levy",
        "Yisroel Mirsky"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.07389.pdf",
      "code_url": "https://github.com/guyAmit/Transpose-Attack-paper-NDSS24-/tree/main",
      "url": "https://arxiv.org/abs/2311.07389",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2409.09272",
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Kai Li",
        "Yifan Zheng",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09272",
      "code_url": "https://github.com/LetterLiGo/SafeEar",
      "url": "https://arxiv.org/abs/2409.09272",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2406.14114",
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "abstract": "Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Shu Wang",
        "Kun Sun",
        "Yan Zhai"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14114",
      "code_url": null,
      "url": "https://arxiv.org/abs/2406.14114",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2404.17399",
      "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
      "abstract": "Empirical defenses for machine learning privacy forgo the provable guarantees of differential privacy in the hope of achieving higher utility while resisting realistic adversaries. We identify severe pitfalls in existing empirical privacy evaluations (based on membership inference attacks) that result in misleading conclusions. In particular, we show that prior evaluations fail to characterize the privacy leakage of the most vulnerable samples, use weak attacks, and avoid comparisons with practical differential privacy baselines. In 5 case studies of empirical privacy defenses, we find that prior evaluations underestimate privacy leakage by an order of magnitude. Under our stronger evaluation, none of the empirical defenses we study are competitive with a properly tuned, high-utility DP-SGD baseline (with vacuous provable guarantees).",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Michael Aerni",
        "Jie Zhang",
        "Florian Tram\u00e8r"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17399",
      "code_url": "https://github.com/ethz-spylab/misleading-privacy-evals?tab=readme-ov-file",
      "url": "https://arxiv.org/abs/2404.17399",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2409.06280",
      "title": "Anonymity Unveiled: A Practical Framework for Auditing Data Use in Deep Learning Models",
      "abstract": "The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns.   This work proposes MembershipTracker, a practical data auditing tool that can empower ordinary users to reliably detect the unauthorized use of their data in training DL models. We view data auditing through the lens of membership inference (MI). MembershipTracker consists of a lightweight data marking component to mark the target data with small and targeted changes, which can be strongly memorized by the model trained on them; and a specialized MI-based verification process to audit whether the model exhibits strong memorization on the target samples.   MembershipTracker only requires the users to mark a small fraction of data (0.005% to 0.1% in proportion to the training set), and it enables the users to reliably detect the unauthorized use of their data (average 0% FPR@100% TPR). We show that MembershipTracker is highly effective across various settings, including industry-scale training on the full-size ImageNet-1k dataset. We finally evaluate MembershipTracker under multiple classes of countermeasures.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Zitao Chen",
        "Karthik Pattabiraman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06280",
      "code_url": null,
      "url": "https://arxiv.org/abs/2409.06280",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2506.24033",
      "title": "Poisoning Attacks to Local Differential Privacy for Ranking Estimation",
      "abstract": "Local differential privacy (LDP) involves users perturbing their inputs to provide plausible deniability of their data. However, this also makes LDP vulnerable to poisoning attacks. In this paper, we first introduce novel poisoning attacks for ranking estimation. These attacks are intricate, as fake attackers do not merely adjust the frequency of target items. Instead, they leverage a limited number of fake users to precisely modify frequencies, effectively altering item rankings to maximize gains. To tackle this challenge, we introduce the concepts of attack cost and optimal attack item (set), and propose corresponding strategies for kRR, OUE, and OLH protocols. For kRR, we iteratively select optimal attack items and allocate suitable fake users. For OUE, we iteratively determine optimal attack item sets and consider the incremental changes in item frequencies across different sets. Regarding OLH, we develop a harmonic cost function based on the pre-image of a hash to select that supporting a larger number of effective attack items. Lastly, we present an attack strategy based on confidence levels to quantify the probability of a successful attack and the number of attack iterations more precisely. We demonstrate the effectiveness of our attacks through theoretical and empirical evidence, highlighting the necessity for defenses against these attacks. The source code and data have been made available at https://github.com/LDP-user/LDP-Ranking.git.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Pei Zhan",
        "Peng Tang",
        "Yangzhuo Li",
        "Puwen Wei",
        "Shanqing Guo"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.24033",
      "code_url": null,
      "url": "https://arxiv.org/abs/2506.24033",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.1 Data Recovery"
    },
    {
      "arxiv_id": "2101.01341",
      "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
      "abstract": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information.   In this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa.   BlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Bo Hui",
        "Yuchen Yang",
        "Haolin Yuan",
        "Philippe Burlina",
        "Neil Zhenqiang Gong",
        "Yinzhi Cao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2101.01341.pdf",
      "code_url": "https://github.com/hyhmia/BlindMI",
      "url": "https://arxiv.org/abs/2101.01341",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "1909.03935",
      "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
      "abstract": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Dingfan Chen",
        "Ning Yu",
        "Yang Zhang",
        "Mario Fritz"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.03935.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1909.03935",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2108.11023",
      "title": "EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning",
      "abstract": "Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "Wenjie Qu",
        "Neil Zhenqiang Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.11023.pdf",
      "code_url": "https://github.com/minxingzhang/MIARS",
      "url": "https://arxiv.org/abs/2108.11023",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2208.11180",
      "title": "Auditing Membership Leakages of Multi-Exit Networks",
      "abstract": "Relying on the fact that not all inputs require the same amount of computation to yield a confident prediction, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing to obtain predictions at intermediate layers of the model and thus save computation time and/or energy. However, current various designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks.   In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Zheng Li",
        "Yiyong Liu",
        "Xinlei He",
        "Ning Yu",
        "Michael Backes",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11180.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2208.11180",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2208.14933",
      "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
      "abstract": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called \\system, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely \\emph{distilled loss trajectory}, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6$\\times$ higher true-positive rate at a low false-positive rate of 0.1\\% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yiyong Liu",
        "Zhengyu Zhao",
        "Michael Backes",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.14933.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2208.14933",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2209.01688",
      "title": "On the Privacy Risks of Cell-Based NAS Architectures",
      "abstract": "Existing studies on neural architecture search (NAS) mainly focus on efficiently and effectively searching for network architectures with better performance. Little progress has been made to systematically understand if the NAS-searched architectures are robust to privacy attacks while abundant work has already shown that human-designed architectures are prone to privacy attacks. In this paper, we fill this gap and systematically measure the privacy risks of NAS architectures. Leveraging the insights from our measurement study, we further explore the cell patterns of cell-based NAS architectures and evaluate how the cell patterns affect the privacy risks of NAS-searched architectures. Through extensive experiments, we shed light on how to design robust NAS architectures against privacy attacks, and also offer a general methodology to understand the hidden correlation between the NAS-searched architectures and other privacy risks.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Hai Huang",
        "Zhikun Zhang",
        "Yun Shen",
        "Michael Backes",
        "Qi Li",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01688.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.01688",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2111.09679",
      "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
      "abstract": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensive \\textit{hypothesis testing framework} that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explain \\textit{why} different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform a \\textit{differential analysis} between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of the \\textit{Privacy Meter} software tool.",
      "year": 2022,
      "venue": "USENIX Security",
      "authors": [
        "Jiayuan Ye",
        "Aadyaa Maddi",
        "Sasi Kumar Murakonda",
        "Vincent Bindschaedler",
        "Reza Shokri"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.09679.pdf",
      "code_url": "https://github.com/privacytrustlab/ml_privacy_meter/tree/master/research/2022_enhanced_mia",
      "url": "https://arxiv.org/abs/2111.09679",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2209.08615",
      "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
      "abstract": "Membership inference (MI) attacks highlight a privacy weakness in present stochastic training methods for neural networks. It is not well understood, however, why they arise. Are they a natural consequence of imperfect generalization only? Which underlying causes should we address during training to mitigate these attacks? Towards answering such questions, we propose the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning. We offer causal graphs that quantitatively explain the observed MI attack performance achieved for $6$ attack variants. We refute several prior non-quantitative hypotheses that over-simplify or over-estimate the influence of underlying causes, thereby failing to capture the complex interplay between several factors. Our causal models also show a new connection between generalization and MI attacks via their shared causal factors. Our causal models have high predictive power ($0.90$), i.e., their analytical predictions match with observations in unseen experiments often, which makes analysis via them a pragmatic alternative.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Teodora Baluta",
        "Shiqi Shen",
        "S. Hitarth",
        "Shruti Tople",
        "Prateek Saxena"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.08615.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.08615",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2309.07983",
      "title": "SLMIA-SR: Speaker-Level Membership Inference Attacks against Speaker Recognition Systems",
      "abstract": "Membership inference attacks allow adversaries to determine whether a particular example was contained in the model's training dataset. While previous works have confirmed the feasibility of such attacks in various applications, none has focused on speaker recognition (SR), a promising voice-based biometric recognition technique. In this work, we propose SLMIA-SR, the first membership inference attack tailored to SR. In contrast to conventional example-level attack, our attack features speaker-level membership inference, i.e., determining if any voices of a given speaker, either the same as or different from the given inference voices, have been involved in the training of a model. It is particularly useful and practical since the training and inference voices are usually distinct, and it is also meaningful considering the open-set nature of SR, namely, the recognition speakers were often not present in the training data. We utilize intra-similarity and inter-dissimilarity, two training objectives of SR, to characterize the differences between training and non-training speakers and quantify them with two groups of features driven by carefully-established feature engineering to mount the attack. To improve the generalizability of our attack, we propose a novel mixing ratio training strategy to train attack models. To enhance the attack performance, we introduce voice chunk splitting to cope with the limited number of inference voices and propose to train attack models dependent on the number of inference voices. Our attack is versatile and can work in both white-box and black-box scenarios. Additionally, we propose two novel techniques to reduce the number of black-box queries while maintaining the attack performance. Extensive experiments demonstrate the effectiveness of SLMIA-SR.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Guangke Chen",
        "Yedi Zhang",
        "Fu Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07983.pdf",
      "code_url": "https://github.com/S3L-official/SLMIA-SR",
      "url": "https://arxiv.org/abs/2309.07983",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2307.01610",
      "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
      "abstract": "Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Zitao Chen",
        "Karthik Pattabiraman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.01610.pdf",
      "code_url": "https://github.com/DependableSystemsLab/MIA_defense_HAMP",
      "url": "https://arxiv.org/abs/2307.01610",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "2506.13972",
      "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
      "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Zhiqi Wang",
        "Chengyu Zhang",
        "Yuetian Chen",
        "Nathalie Baracaldo",
        "Swanand Kadhe",
        "Lei Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.13972",
      "code_url": null,
      "url": "https://arxiv.org/abs/2506.13972",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.2 Membership Inference Attack"
    },
    {
      "arxiv_id": "1906.09679",
      "title": "The Value of Collaboration in Convex Machine Learning with Differential Privacy",
      "abstract": "In this paper, we apply machine learning to distributed private data owned by multiple data owners, entities with access to non-overlapping training datasets. We use noisy, differentially-private gradients to minimize the fitness cost of the machine learning model using stochastic gradient descent. We quantify the quality of the trained model, using the fitness cost, as a function of privacy budget and size of the distributed datasets to capture the trade-off between privacy and utility in machine learning. This way, we can predict the outcome of collaboration among privacy-aware data owners prior to executing potentially computationally-expensive machine learning algorithms. Particularly, we show that the difference between the fitness of the trained machine learning model using differentially-private gradient queries and the fitness of the trained machine model in the absence of any privacy concerns is inversely proportional to the size of the training datasets squared and the privacy budget squared. We successfully validate the performance prediction with the actual performance of the proposed privacy-aware learning algorithms, applied to: financial datasets for determining interest rates of loans using regression; and detecting credit card frauds using support vector machines.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Nan Wu",
        "Farhad Farokhi",
        "David Smith",
        "Mohamed Ali Kaafar"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.09679.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1906.09679",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2012.02670",
      "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
      "abstract": "We investigate the security of Split Learning -- a novel collaborative machine learning framework that enables peak performance by requiring minimal resources consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack is able to overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning. To make our results reproducible, we made our code available at https://github.com/pasquini-dario/SplitNN_FSHA.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Giuseppe Ateniese",
        "Massimo Bernaschi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2012.02670.pdf",
      "code_url": "https://github.com/pasquini-dario/SplitNN_FSHA",
      "url": "https://arxiv.org/abs/2012.02670",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2009.03561",
      "title": "Local and Central Differential Privacy for Robustness and Privacy in Federated Learning",
      "abstract": "Federated Learning (FL) allows multiple participants to train machine learning models collaboratively by keeping their datasets local while only exchanging model updates. Alas, this is not necessarily free from privacy and robustness vulnerabilities, e.g., via membership, property, and backdoor attacks. This paper investigates whether and to what extent one can use differential Privacy (DP) to protect both privacy and robustness in FL. To this end, we present a first-of-its-kind evaluation of Local and Central Differential Privacy (LDP/CDP) techniques in FL, assessing their feasibility and effectiveness. Our experiments show that both DP variants do d fend against backdoor attacks, albeit with varying levels of protection-utility trade-offs, but anyway more effectively than other robustness defenses. DP also mitigates white-box membership inference attacks in FL, and our work is the first to show it empirically. Neither LDP nor CDP, however, defend against property inference. Overall, our work provides a comprehensive, re-usable measurement methodology to quantify the trade-offs between robustness/privacy and utility in differentially private FL.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Mohammad Naseri",
        "Jamie Hayes",
        "Emiliano De Cristofaro"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.03561.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2009.03561",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2205.08443",
      "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
      "abstract": "In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Dario Pasquini",
        "Mathilde Raynal",
        "Carmela Troncoso"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.08443.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2205.08443",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2107.03311",
      "title": "RoFL: Robustness of Secure Federated Learning",
      "abstract": "Even though recent years have seen many attacks exposing severe vulnerabilities in Federated Learning (FL), a holistic understanding of what enables these attacks and how they can be mitigated effectively is still lacking. In this work, we demystify the inner workings of existing (targeted) attacks. We provide new insights into why these attacks are possible and why a definitive solution to FL robustness is challenging. We show that the need for ML algorithms to memorize tail data has significant implications for FL integrity. This phenomenon has largely been studied in the context of privacy; our analysis sheds light on its implications for ML integrity. We show that certain classes of severe attacks can be mitigated effectively by enforcing constraints such as norm bounds on clients' updates. We investigate how to efficiently incorporate these constraints into secure FL protocols in the single-server setting. Based on this, we propose RoFL, a new secure FL system that extends secure aggregation with privacy-preserving input validation. Specifically, RoFL can enforce constraints such as $L_2$ and $L_\\infty$ bounds on high-dimensional encrypted model updates.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Hidde Lycklama",
        "Lukas Burkhalter",
        "Alexander Viand",
        "Nicolas K\u00fcchler",
        "Anwar Hithnawi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.03311.pdf",
      "code_url": "https://github.com/pps-lab/rofl-project-code",
      "url": "https://arxiv.org/abs/2107.03311",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2304.00129",
      "title": "Scalable and Privacy-Preserving Federated Principal Component Analysis",
      "abstract": "Principal component analysis (PCA) is an essential algorithm for dimensionality reduction in many data science domains. We address the problem of performing a federated PCA on private data distributed among multiple data providers while ensuring data confidentiality. Our solution, SF-PCA, is an end-to-end secure system that preserves the confidentiality of both the original data and all intermediate results in a passive-adversary model with up to all-but-one colluding parties. SF-PCA jointly leverages multiparty homomorphic encryption, interactive protocols, and edge computing to efficiently interleave computations on local cleartext data with operations on collectively encrypted data. SF-PCA obtains results as accurate as non-secure centralized solutions, independently of the data distribution among the parties. It scales linearly or better with the dataset dimensions and with the number of data providers. SF-PCA is more precise than existing approaches that approximate the solution by combining local analysis results, and between 3x and 250x faster than privacy-preserving alternatives based solely on secure multiparty computation or homomorphic encryption. Our work demonstrates the practical applicability of secure and federated PCA on private distributed datasets.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "David Froelicher",
        "Hyunghoon Cho",
        "Manaswitha Edupalli",
        "Joao Sa Sousa",
        "Jean-Philippe Bossuat",
        "Apostolos Pyrgelis",
        "Juan R. Troncoso-Pastoriza",
        "Bonnie Berger",
        "Jean-Pierre Hubaux"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00129.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2304.00129",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2303.12233",
      "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
      "abstract": "Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting. In this work we introduce LOKI, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters, and the weight gradients of data points between clients remain separate even through aggregation. With FedAVG and aggregation across 100 clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, and Tiny ImageNet. Using only a single training round, LOKI is able to leak 76-86% of all data samples.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Joshua C. Zhao",
        "Atul Sharma",
        "Ahmed Roushdy Elkordy",
        "Yahya H. Ezzeldin",
        "Salman Avestimehr",
        "Saurabh Bagchi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12233.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2303.12233",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2408.16913",
      "title": "Analyzing Inference Privacy Risks Through Gradients in Machine Learning",
      "abstract": "In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Zhuohang Li",
        "Andrew Lowy",
        "Jing Liu",
        "Toshiaki Koike-Akino",
        "Kieran Parsons",
        "Bradley Malin",
        "Ye Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16913",
      "code_url": null,
      "url": "https://arxiv.org/abs/2408.16913",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.3 Information Leakage in Distributed ML System"
    },
    {
      "arxiv_id": "2004.00053",
      "title": "Information Leakage in Embedding Models",
      "abstract": "Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains.   We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50\\%--70\\% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Congzheng Song",
        "Ananth Raghunathan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2004.00053.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2004.00053",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.4 Information Leakage in Embedding"
    },
    {
      "arxiv_id": "2105.12049",
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.12049.pdf",
      "code_url": "https://github.com/mmalekzadeh/honest-but-curious-nets",
      "url": "https://arxiv.org/abs/2105.12049",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.4 Information Leakage in Embedding"
    },
    {
      "arxiv_id": "2105.12049",
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2105.12049.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2105.12049",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2108.06504",
      "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
      "abstract": "Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Fan Wu",
        "Yunhui Long",
        "Ce Zhang",
        "Bo Li"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.06504.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2108.06504",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2006.05535",
      "title": "Locally Private Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated superior performance in learning node representations for various graph inference tasks. However, learning over graph data can raise privacy concerns when nodes represent people or human-related variables that involve sensitive or personal information. While numerous techniques have been proposed for privacy-preserving deep learning over non-relational data, there is less work addressing the privacy issues pertained to applying deep learning algorithms on graphs. In this paper, we study the problem of node data privacy, where graph nodes have potentially sensitive data that is kept private, but they could be beneficial for a central server for training a GNN over the graph. To address this problem, we develop a privacy-preserving, architecture-agnostic GNN learning algorithm with formal privacy guarantees based on Local Differential Privacy (LDP). Specifically, we propose an LDP encoder and an unbiased rectifier, by which the server can communicate with the graph nodes to privately collect their data and approximate the GNN's first layer. To further reduce the effect of the injected noise, we propose to prepend a simple graph convolution layer, called KProp, which is based on the multi-hop aggregation of the nodes' features acting as a denoising mechanism. Finally, we propose a robust training framework, in which we benefit from KProp's denoising capability to increase the accuracy of inference in the presence of noisy labels. Extensive experiments conducted over real-world datasets demonstrate that our method can maintain a satisfying level of accuracy with low privacy loss.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Sina Sajadmanesh",
        "Daniel Gatica-Perez"
      ],
      "pdf_url": "https://arxiv.org/pdf/2006.05535.pdf",
      "code_url": "https://github.com/sisaman/LPGNN",
      "url": "https://arxiv.org/abs/2006.05535",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2204.06963",
      "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
      "abstract": "Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yun Shen",
        "Yufei Han",
        "Zhikun Zhang",
        "Min Chen",
        "Ting Yu",
        "Michael Backes",
        "Yang Zhang",
        "Gianluca Stringhini"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.06963.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2204.06963",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2209.01100",
      "title": "Group Property Inference Attacks Against Graph Neural Networks",
      "abstract": "With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet.   In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA's success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Xiuling Wang",
        "Wendy Hui Wang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01100.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2209.01100",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2205.03105",
      "title": "LPGNet: Link Private Graph Networks for Node Classification",
      "abstract": "Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than \"trivially\" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Aashish Kolluri",
        "Teodora Baluta",
        "Bryan Hooi",
        "Prateek Saxena"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.03105.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2205.03105",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2312.07861",
      "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
      "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
      "year": 2024,
      "venue": "MDSS",
      "authors": [
        "Bang Wu",
        "He Zhang",
        "Xiangwen Yang",
        "Shuo Wang",
        "Minhui Xue",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07861.pdf",
      "code_url": "https://github.com/GraphGuard/GraphGuard-Proactive",
      "url": "https://arxiv.org/abs/2312.07861",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "2501.10985",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nian-Feng Tzeng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.10985",
      "code_url": "https://github.com/GraphGuard/GraphGuard-Proactive",
      "url": "https://arxiv.org/abs/2501.10985",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.5 Graph Leakage"
    },
    {
      "arxiv_id": "1912.03817",
      "title": "Machine Unlearning",
      "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Christopher A. Choquette-Choo",
        "Hengrui Jia",
        "Adelin Travers",
        "Baiwu Zhang",
        "David Lie",
        "Nicolas Papernot"
      ],
      "pdf_url": "https://arxiv.org/pdf/1912.03817.pdf",
      "code_url": "https://github.com/cleverhans-lab/machine-unlearning",
      "url": "https://arxiv.org/abs/1912.03817",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2005.02205",
      "title": "When Machine Unlearning Jeopardizes Privacy",
      "abstract": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "Michael Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.02205.pdf",
      "code_url": "https://github.com/MinChen00/UnlearningLeaks",
      "url": "https://arxiv.org/abs/2005.02205",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2103.14991",
      "title": "Graph Unlearning",
      "abstract": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to $62.5\\%$ higher F1 score and our proposed learning-based aggregation method achieves up to $112\\%$ higher F1 score.\\footnote{Our code is available at \\url{https://github.com/MinChen00/Graph-Unlearning}.}",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Min Chen",
        "Zhikun Zhang",
        "Tianhao Wang",
        "Michael Backes",
        "Mathias Humbert",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2103.14991.pdf",
      "code_url": "https://github.com/MinChen00/Graph-Unlearning",
      "url": "https://arxiv.org/abs/2103.14991",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2309.08230",
      "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services",
      "abstract": "The right to be forgotten requires the removal or \"unlearning\" of a user's data from machine learning models. However, in the context of Machine Learning as a Service (MLaaS), retraining a model from scratch to fulfill the unlearning request is impractical due to the lack of training data on the service provider's side (the server). Furthermore, approximate unlearning further embraces a complex trade-off between utility (model performance) and privacy (unlearning performance). In this paper, we try to explore the potential threats posed by unlearning services in MLaaS, specifically over-unlearning, where more information is unlearned than expected. We propose two strategies that leverage over-unlearning to measure the impact on the trade-off balancing, under black-box access settings, in which the existing machine unlearning attacks are not applicable. The effectiveness of these strategies is evaluated through extensive experiments on benchmark datasets, across various model architectures and representative unlearning approaches. Results indicate significant potential for both strategies to undermine model efficacy in unlearning scenarios. This study uncovers an underexplored gap between unlearning and contemporary MLaaS, highlighting the need for careful considerations in balancing data unlearning, model utility, and security.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Hongsheng Hu",
        "Shuo Wang",
        "Jiamin Chang",
        "Haonan Zhong",
        "Ruoxi Sun",
        "Shuang Hao",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08230.pdf",
      "code_url": "https://github.com/TASI-LAB/Over-unlearning",
      "url": "https://arxiv.org/abs/2309.08230",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2311.16136",
      "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
      "abstract": "Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ERASER framework for machinE unleaRning in MLaAS via an inferencE seRving-aware approach. ERASER strategically choose appropriate unlearning execution timing to address the inference service obsolescence issue. A novel inference consistency certification mechanism is proposed to avoid the violation of RTBF principle caused by postponed unlearning executions, thereby mitigating the undesirable exposure vulnerability. ERASER offers three groups of design choices to allow for tailor-made variants that best suit the specific environments and preferences of various MLaaS systems. Extensive empirical evaluations across various settings confirm ERASER's effectiveness, e.g., it can effectively save up to 99% of inference latency and 31% of computation overhead over the inference-oblivion baseline.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuke Hu",
        "Jian Lou",
        "Jiaqi Liu",
        "Wangze Ni",
        "Feng Lin",
        "Zhan Qin",
        "Kui Ren"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16136",
      "code_url": null,
      "url": "https://arxiv.org/abs/2311.16136",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2308.10422",
      "title": "Split Unlearning",
      "abstract": "We introduce Split Unlearning, a novel machine unlearning technology designed for Split Learning (SL), enabling the first-ever implementation of Sharded, Isolated, Sliced, and Aggregated (SISA) unlearning in SL frameworks. Particularly, the tight coupling between clients and the server in existing SL frameworks results in frequent bidirectional data flows and iterative training across all clients, violating the \"Isolated\" principle and making them struggle to implement SISA for independent and efficient unlearning. To address this, we propose SplitWiper with a new one-way-one-off propagation scheme, which leverages the inherently \"Sharded\" structure of SL and decouples neural signal propagation between clients and the server, enabling effective SISA unlearning even in scenarios with absent clients. We further design SplitWiper+ to enhance client label privacy, which integrates differential privacy and label expansion strategy to defend the privacy of client labels against the server and other potential adversaries. Experiments across diverse data distributions and tasks demonstrate that SplitWiper achieves 0% accuracy for unlearned labels, and 8% better accuracy for retained labels than non-SISA unlearning in SL. Moreover, the one-way-one-off propagation maintains constant overhead, reducing computational and communication costs by 99%. SplitWiper+ preserves 90% of label privacy when sharing masked labels with the server.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Guangsheng Yu",
        "Yanna Jiang",
        "Qin Wang",
        "Xu Wang",
        "Baihe Ma",
        "Caijun Sun",
        "Wei Ni",
        "Ren Ping Liu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.10422",
      "code_url": null,
      "url": "https://arxiv.org/abs/2308.10422",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2506.02761",
      "title": "Rethinking Machine Unlearning in Image Generation Models",
      "abstract": "With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at https://github.com/ryliu68/IGMU.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Renyang Liu",
        "Wenjie Feng",
        "Tianwei Zhang",
        "Wei Zhou",
        "Xueqi Cheng",
        "See-Kiong Ng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02761",
      "code_url": null,
      "url": "https://arxiv.org/abs/2506.02761",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.6 Unlearning"
    },
    {
      "arxiv_id": "2209.01292",
      "title": "Are Attribute Inference Attacks Just Imputation?",
      "abstract": "Models can expose sensitive information about their training data. In an attribute inference attack, an adversary has partial knowledge of some training records and access to a model trained on those records, and infers the unknown values of a sensitive feature of those records. We study a fine-grained variant of attribute inference we call \\emph{sensitive value inference}, where the adversary's goal is to identify with high confidence some records from a candidate set where the unknown attribute has a particular sensitive value. We explicitly compare attribute inference with data imputation that captures the training distribution statistics, under various assumptions about the training data available to the adversary. Our main conclusions are: (1) previous attribute inference methods do not reveal more about the training data from the model than can be inferred by an adversary without access to the trained model, but with the same knowledge of the underlying distribution as needed to train the attribute inference attack; (2) black-box attribute inference attacks rarely learn anything that cannot be learned without the model; but (3) white-box attacks, which we introduce and evaluate in the paper, can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model. Furthermore, we show that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk. The code for our experiments is available at \\url{https://github.com/bargavj/EvaluatingDPML}.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01292.pdf",
      "code_url": "https://github.com/bargavj/EvaluatingDPML",
      "url": "https://arxiv.org/abs/2209.01292",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.7 Attribute Inference Attack"
    },
    {
      "arxiv_id": "2211.05249",
      "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
      "abstract": "Although query-based systems (QBS) have become one of the main solutions to share data anonymously, building QBSes that robustly protect the privacy of individuals contributing to the dataset is a hard problem. Theoretical solutions relying on differential privacy guarantees are difficult to implement correctly with reasonable accuracy, while ad-hoc solutions might contain unknown vulnerabilities. Evaluating the privacy provided by QBSes must thus be done by evaluating the accuracy of a wide range of privacy attacks. However, existing attacks require time and expertise to develop, need to be manually tailored to the specific systems attacked, and are limited in scope. In this paper, we develop QuerySnout (QS), the first method to automatically discover vulnerabilities in QBSes. QS takes as input a target record and the QBS as a black box, analyzes its behavior on one or more datasets, and outputs a multiset of queries together with a rule to combine answers to them in order to reveal the sensitive attribute of the target record. QS uses evolutionary search techniques based on a novel mutation operator to find a multiset of queries susceptible to lead to an attack, and a machine learning classifier to infer the sensitive attribute from answers to the queries selected. We showcase the versatility of QS by applying it to two attack scenarios, three real-world datasets, and a variety of protection mechanisms. We show the attacks found by QS to consistently equate or outperform, sometimes by a large margin, the best attacks from the literature. We finally show how QS can be extended to QBSes that require a budget, and apply QS to a simple QBS based on the Laplace mechanism. Taken together, our results show how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSes to be automatically tested \"at the pressing of a button\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Ana-Maria Cretu",
        "Florimond Houssiau",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05249.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2211.05249",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.7 Attribute Inference Attack"
    },
    {
      "arxiv_id": "2208.12348",
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "abstract": "Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.   In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% higher success rate than Mahloujifar et al. while being 56.5x faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP's generality and effectiveness. An open-source implementation of SNAP can be found at https://github.com/johnmath/snap-sp23.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Harsh Chaudhari",
        "John Abascal",
        "Alina Oprea",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Jonathan Ullman"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12348.pdf",
      "code_url": "https://github.com/johnmath/snap-sp23",
      "url": "https://arxiv.org/abs/2208.12348",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.7 Property Inference Attack"
    },
    {
      "arxiv_id": "2307.02106",
      "title": "SoK: Privacy-Preserving Data Synthesis",
      "abstract": "As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yuzheng Hu",
        "Fan Wu",
        "Qinbin Li",
        "Yunhui Long",
        "Gonzalo Munilla Garrido",
        "Chang Ge",
        "Bolin Ding",
        "David Forsyth",
        "Bo Li",
        "Dawn Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02106.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2307.02106",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.8 Data Synthesis"
    },
    {
      "arxiv_id": "2309.03081",
      "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
      "abstract": "Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Shouling Ji",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.03081.pdf",
      "code_url": "https://github.com/link-zju/ORL-Auditor",
      "url": "https://arxiv.org/abs/2309.03081",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.8 Dataset Auditing"
    },
    {
      "arxiv_id": "2410.16618",
      "title": "SoK: Dataset Copyright Auditing in Machine Learning Systems",
      "abstract": "As the implementation of machine learning (ML) systems becomes more widespread, especially with the introduction of larger ML models, we perceive a spring demand for massive data. However, it inevitably causes infringement and misuse problems with the data, such as using unauthorized online artworks or face images to train ML models. To address this problem, many efforts have been made to audit the copyright of the model training dataset. However, existing solutions vary in auditing assumptions and capabilities, making it difficult to compare their strengths and weaknesses. In addition, robustness evaluations usually consider only part of the ML pipeline and hardly reflect the performance of algorithms in real-world ML applications. Thus, it is essential to take a practical deployment perspective on the current dataset copyright auditing tools, examining their effectiveness and limitations. Concretely, we categorize dataset copyright auditing research into two prominent strands: intrusive methods and non-intrusive methods, depending on whether they require modifications to the original dataset. Then, we break down the intrusive methods into different watermark injection options and examine the non-intrusive methods using various fingerprints. To summarize our results, we offer detailed reference tables, highlight key points, and pinpoint unresolved issues in the current literature. By combining the pipeline in ML systems and analyzing previous studies, we highlight several future directions to make auditing tools more suitable for real-world copyright protection requirements.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Linkang Du",
        "Xuanru Zhou",
        "Min Chen",
        "Chusong Zhang",
        "Zhou Su",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16618",
      "code_url": null,
      "url": "https://arxiv.org/abs/2410.16618",
      "source_section": "2.1 Training Data",
      "source_subsection": "2.1.8 Dataset Auditing"
    },
    {
      "arxiv_id": "1909.01838",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input.   To extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights.   We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2020,
      "venue": "USENIX Security",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alex Kurakin",
        "Nicolas Papernot"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.01838.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1909.01838",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.1 Model Extraction"
    },
    {
      "arxiv_id": "2201.05889",
      "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
      "abstract": "Pre-trained encoders are general-purpose feature extractors that can be used for many downstream tasks. Recent progress in self-supervised learning can pre-train highly effective encoders using a large volume of unlabeled data, leading to the emerging encoder as a service (EaaS). A pre-trained encoder may be deemed confidential because its training requires lots of data and computation resources as well as its public release may facilitate misuse of AI, e.g., for deepfakes generation. In this paper, we propose the first attack called StolenEncoder to steal pre-trained image encoders. We evaluate StolenEncoder on multiple target encoders pre-trained by ourselves and three real-world target encoders including the ImageNet encoder pre-trained by Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding encoder deployed as a paid EaaS. Our results show that our stolen encoders have similar functionality with the target encoders. In particular, the downstream classifiers built upon a target encoder and a stolen one have similar accuracy. Moreover, stealing a target encoder using StolenEncoder requires much less data and computation resources than pre-training it from scratch. We also explore three defenses that perturb feature vectors produced by a target encoder. Our results show these defenses are not enough to mitigate StolenEncoder.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yupei Liu",
        "Jinyuan Jia",
        "Hongbin Liu",
        "Neil Zhenqiang Gong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.05889.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2201.05889",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.1 Model Extraction"
    },
    {
      "arxiv_id": "2009.03015",
      "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
      "abstract": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.   AWT is the first end-to-end model to hide data in text by automatically learning -- without ground truth -- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Sahar Abdelnabi",
        "Mario Fritz"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.03015.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2009.03015",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.2 Model Watermark"
    },
    {
      "arxiv_id": "2401.15239",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15239.pdf",
      "code_url": "https://github.com/lvpeizhuo/MEA-Defender",
      "url": "https://arxiv.org/abs/2401.15239",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.2 Model Watermark"
    },
    {
      "arxiv_id": "2209.03563",
      "title": "SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning",
      "abstract": "Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation space, which causes any downstream classifier to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as CV and NLP, using different SSL models both contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning, pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the ownership of SSL models.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Peizhuo Lv",
        "Pan Li",
        "Shenchen Zhu",
        "Shengzhi Zhang",
        "Kai Chen",
        "Ruigang Liang",
        "Chang Yue",
        "Fan Xiang",
        "Yuling Cai",
        "Hualong Ma",
        "Yingjun Zhang",
        "Guozhu Meng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03563.pdf",
      "code_url": "https://github.com/lvpeizhuo/SSL-WM",
      "url": "https://arxiv.org/abs/2209.03563",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.2 Model Watermark"
    },
    {
      "arxiv_id": "2209.03563",
      "title": "SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning",
      "abstract": "Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation space, which causes any downstream classifier to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as CV and NLP, using different SSL models both contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning, pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the ownership of SSL models.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Peizhuo Lv",
        "Pan Li",
        "Shenchen Zhu",
        "Shengzhi Zhang",
        "Kai Chen",
        "Ruigang Liang",
        "Chang Yue",
        "Fan Xiang",
        "Yuling Cai",
        "Hualong Ma",
        "Yingjun Zhang",
        "Guozhu Meng"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03563.pdf",
      "code_url": "https://arxiv.org/pdf/2405.11109",
      "url": "https://arxiv.org/abs/2209.03563",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.2 Model Watermark"
    },
    {
      "arxiv_id": "2411.18479",
      "title": "SoK: Watermarking for AI-Generated Content",
      "abstract": "As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Xuandong Zhao",
        "Sam Gunn",
        "Miranda Christ",
        "Jaiden Fairoze",
        "Andres Fabrega",
        "Nicholas Carlini",
        "Sanjam Garg",
        "Sanghyun Hong",
        "Milad Nasr",
        "Florian Tramer",
        "Somesh Jha",
        "Lei Li",
        "Yu-Xiang Wang",
        "Dawn Song"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18479",
      "code_url": null,
      "url": "https://arxiv.org/abs/2411.18479",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.2 Model Watermark"
    },
    {
      "arxiv_id": "2103.05633",
      "title": "Proof-of-Learning: Definitions and Practice",
      "abstract": "Training machine learning (ML) models typically involves expensive iterative optimization. Once the model's final parameters are released, there is currently no mechanism for the entity which trained the model to prove that these parameters were indeed the result of this optimization procedure. Such a mechanism would support security of ML applications in several ways. For instance, it would simplify ownership resolution when multiple parties contest ownership of a specific model. It would also facilitate the distributed training across untrusted workers where Byzantine workers might otherwise mount a denial-of-service by returning incorrect model updates.   In this paper, we remediate this problem by introducing the concept of proof-of-learning in ML. Inspired by research on both proof-of-work and verified computations, we observe how a seminal training algorithm, stochastic gradient descent, accumulates secret information due to its stochasticity. This produces a natural construction for a proof-of-learning which demonstrates that a party has expended the compute require to obtain a set of model parameters correctly. In particular, our analyses and experiments show that an adversary seeking to illegitimately manufacture a proof-of-learning needs to perform *at least* as much work than is needed for gradient descent itself.   We also instantiate a concrete proof-of-learning mechanism in both of the scenarios described above. In model ownership resolution, it protects the intellectual property of models released publicly. In distributed training, it preserves availability of the training procedure. Our empirical evaluation validates that our proof-of-learning mechanism is robust to variance induced by the hardware (ML accelerators) and software stacks.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hengrui Jia",
        "Mohammad Yaghini",
        "Christopher A. Choquette-Choo",
        "Natalie Dullerud",
        "Anvith Thudi",
        "Varun Chandrasekaran",
        "Nicolas Papernot"
      ],
      "pdf_url": "https://arxiv.org/pdf/2103.05633.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2103.05633",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.3 Model Owenership"
    },
    {
      "arxiv_id": "2108.04974",
      "title": "SoK: How Robust is Image Classification Deep Neural Network Watermarking? (Extended Version)",
      "abstract": "Deep Neural Network (DNN) watermarking is a method for provenance verification of DNN models. Watermarking should be robust against watermark removal attacks that derive a surrogate model that evades provenance verification. Many watermarking schemes that claim robustness have been proposed, but their robustness is only validated in isolation against a relatively small set of attacks. There is no systematic, empirical evaluation of these claims against a common, comprehensive set of removal attacks. This uncertainty about a watermarking scheme's robustness causes difficulty to trust their deployment in practice. In this paper, we evaluate whether recently proposed watermarking schemes that claim robustness are robust against a large set of removal attacks. We survey methods from the literature that (i) are known removal attacks, (ii) derive surrogate models but have not been evaluated as removal attacks, and (iii) novel removal attacks. Weight shifting and smooth retraining are novel removal attacks adapted to the DNN watermarking schemes surveyed in this paper. We propose taxonomies for watermarking schemes and removal attacks. Our empirical evaluation includes an ablation study over sets of parameters for each attack and watermarking scheme on the CIFAR-10 and ImageNet datasets. Surprisingly, none of the surveyed watermarking schemes is robust in practice. We find that schemes fail to withstand adaptive attacks and known methods for deriving surrogate models that have not been evaluated as removal attacks. This points to intrinsic flaws in how robustness is currently evaluated. We show that watermarking schemes need to be evaluated against a more extensive set of removal attacks with a more realistic adversary model. Our source code and a complete dataset of evaluation results are publicly available, which allows to independently verify our conclusions.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Edward Jiang",
        "Xinda Li",
        "Florian Kerschbaum"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.04974.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2108.04974",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.3 Model Owenership"
    },
    {
      "arxiv_id": "2201.11692",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.11692.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2201.11692",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.3 Model Owenership"
    },
    {
      "arxiv_id": "2201.11692",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.11692.pdf",
      "code_url": "https://github.com/chichidd/RAI2",
      "url": "https://arxiv.org/abs/2201.11692",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.3 Model Owenership"
    },
    {
      "arxiv_id": "2203.10902",
      "title": "PublicCheck: Public Integrity Verification for Services of Run-time Deep Models",
      "abstract": "Existing integrity verification approaches for deep models are designed for private verification (i.e., assuming the service provider is honest, with white-box access to model parameters). However, private verification approaches do not allow model users to verify the model at run-time. Instead, they must trust the service provider, who may tamper with the verification results. In contrast, a public verification approach that considers the possibility of dishonest service providers can benefit a wider range of users. In this paper, we propose PublicCheck, a practical public integrity verification solution for services of run-time deep models. PublicCheck considers dishonest service providers, and overcomes public verification challenges of being lightweight, providing anti-counterfeiting protection, and having fingerprinting samples that appear smooth. To capture and fingerprint the inherent prediction behaviors of a run-time model, PublicCheck generates smoothly transformed and augmented encysted samples that are enclosed around the model's decision boundary while ensuring that the verification queries are indistinguishable from normal queries. PublicCheck is also applicable when knowledge of the target model is limited (e.g., with no knowledge of gradients or model parameters). A thorough evaluation of PublicCheck demonstrates the strong capability for model integrity breach detection (100% detection accuracy with less than 10 black-box API queries) against various model integrity attacks and model compression attacks. PublicCheck also demonstrates the smooth appearance, feasibility, and efficiency of generating a plethora of encysted samples for fingerprinting.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Shuo Wang",
        "Sharif Abuadbba",
        "Sidharth Agarwal",
        "Kristen Moore",
        "Ruoxi Sun",
        "Minhui Xue",
        "Surya Nepal",
        "Seyit Camtepe",
        "Salil Kanhere"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.10902.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2203.10902",
      "source_section": "2.2 Model",
      "source_subsection": "2.2.4 Model Integrity"
    },
    {
      "arxiv_id": "2503.09022",
      "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
      "abstract": "Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity.   In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Wenjie Qu",
        "Yuguang Zhou",
        "Yongji Wu",
        "Tingsong Xiao",
        "Binhang Yuan",
        "Yiming Li",
        "Jiaheng Zhang"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09022",
      "code_url": null,
      "url": "https://arxiv.org/abs/2503.09022",
      "source_section": "2.3 LLM Privacy",
      "source_subsection": "2.3.1 Prompt Privacy"
    },
    {
      "arxiv_id": "2503.09291",
      "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
      "abstract": "The inference process of modern large language models (LLMs) demands prohibitive computational resources, rendering them infeasible for deployment on consumer-grade devices. To address this limitation, recent studies propose distributed LLM inference frameworks, which employ split learning principles to enable collaborative LLM inference on resource-constrained hardware. However, distributing LLM layers across participants requires the transmission of intermediate outputs, which may introduce privacy risks to the original input prompts - a critical issue that has yet to be thoroughly explored in the literature.   In this paper, we rigorously examine the privacy vulnerabilities of distributed LLM inference frameworks by designing and evaluating three prompt inference attacks aimed at reconstructing input prompts from intermediate LLM outputs. These attacks are developed under various query and data constraints to reflect diverse real-world LLM service scenarios. Specifically, the first attack assumes an unlimited query budget and access to an auxiliary dataset sharing the same distribution as the target prompts. The second attack also leverages unlimited queries but uses an auxiliary dataset with a distribution differing from the target prompts. The third attack operates under the most restrictive scenario, with limited query budgets and no auxiliary dataset available. We evaluate these attacks on a range of LLMs, including state-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used models like GPT-2 and BERT for comparative analysis. Our experiments show that the first two attacks achieve reconstruction accuracies exceeding 90%, while the third achieves accuracies typically above 50%, even under stringent constraints. These findings highlight privacy risks in distributed LLM inference frameworks, issuing a strong alert on their deployment in real-world applications.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Xinjian Luo",
        "Ting Yu",
        "Xiaokui Xiao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09291",
      "code_url": null,
      "url": "https://arxiv.org/abs/2503.09291",
      "source_section": "2.3 LLM Privacy",
      "source_subsection": "2.3.1 Prompt Privacy"
    },
    {
      "arxiv_id": "2005.10296",
      "title": "SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",
      "abstract": "Performing machine learning (ML) computation on private data while maintaining data privacy, aka Privacy-preserving Machine Learning~(PPML), is an emergent field of research. Recently, PPML has seen a visible shift towards the adoption of the Secure Outsourced Computation~(SOC) paradigm due to the heavy computation that it entails. In the SOC paradigm, computation is outsourced to a set of powerful and specially equipped servers that provide service on a pay-per-use basis. In this work, we propose SWIFT, a robust PPML framework for a range of ML algorithms in SOC setting, that guarantees output delivery to the users irrespective of any adversarial behaviour. Robustness, a highly desirable feature, evokes user participation without the fear of denial of service.   At the heart of our framework lies a highly-efficient, maliciously-secure, three-party computation (3PC) over rings that provides guaranteed output delivery (GOD) in the honest-majority setting. To the best of our knowledge, SWIFT is the first robust and efficient PPML framework in the 3PC setting. SWIFT is as fast as (and is strictly better in some cases than) the best-known 3PC framework BLAZE (Patra et al. NDSS'20), which only achieves fairness. We extend our 3PC framework for four parties (4PC). In this regime, SWIFT is as fast as the best known fair 4PC framework Trident (Chaudhari et al. NDSS'20) and twice faster than the best-known robust 4PC framework FLASH (Byali et al. PETS'20).   We demonstrate our framework's practical relevance by benchmarking popular ML algorithms such as Logistic Regression and deep Neural Networks such as VGG16 and LeNet, both over a 64-bit ring in a WAN setting. For deep NN, our results testify to our claims that we provide improved security guarantee while incurring no additional overhead for 3PC and obtaining 2x improvement for 4PC.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Nishat Koti",
        "Mahak Pancholi",
        "Arpita Patra",
        "Ajith Suresh"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.10296.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2005.10296",
      "source_section": "2.5 Private ML Protocols",
      "source_subsection": "2.5.1 3PC"
    },
    {
      "arxiv_id": "2210.01988",
      "title": "Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning",
      "abstract": "The overhead of non-linear functions dominates the performance of the secure multiparty computation (MPC) based privacy-preserving machine learning (PPML). This work introduces a family of novel secure three-party computation (3PC) protocols, Bicoptor, which improve the efficiency of evaluating non-linear functions. The basis of Bicoptor is a new sign determination protocol, which relies on a clever use of the truncation protocol proposed in SecureML (S\\&P 2017). Our 3PC sign determination protocol only requires two communication rounds, and does not involve any preprocessing. Such sign determination protocol is well-suited for computing non-linear functions in PPML, e.g. the activation function ReLU, Maxpool, and their variants. We develop suitable protocols for these non-linear functions, which form a family of GPU-friendly protocols, Bicoptor. All Bicoptor protocols only require two communication rounds without preprocessing. We evaluate Bicoptor under a 3-party LAN network over a public cloud, and achieve more than 370,000 DReLU/ReLU or 41,000 Maxpool (find the maximum value of nine inputs) operations per second. Under the same settings and environment, our ReLU protocol has a one or even two orders of magnitude improvement to the state-of-the-art works, Falcon (PETS 2021) or Edabits (CRYPTO 2020), respectively without batch processing.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Lijing Zhou",
        "Ziyu Wang",
        "Hongrui Cui",
        "Qingrui Song",
        "Yu Yu"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.01988.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2210.01988",
      "source_section": "2.5 Private ML Protocols",
      "source_subsection": "2.5.1 3PC"
    },
    {
      "arxiv_id": "2406.07948",
      "title": "Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization",
      "abstract": "Multi-party training frameworks for decision trees based on secure multi-party computation enable multiple parties to train high-performance models on distributed private data with privacy preservation. The training process essentially involves frequent dataset splitting according to the splitting criterion (e.g. Gini impurity). However, existing multi-party training frameworks for decision trees demonstrate communication inefficiency due to the following issues: (1) They suffer from huge communication overhead in securely splitting a dataset with continuous attributes. (2) They suffer from huge communication overhead due to performing almost all the computations on a large ring to accommodate the secure computations for the splitting criterion.   In this paper, we are motivated to present an efficient three-party training framework, namely Ents, for decision trees by communication optimization. For the first issue, we present a series of training protocols based on the secure radix sort protocols to efficiently and securely split a dataset with continuous attributes. For the second issue, we propose an efficient share conversion protocol to convert shares between a small ring and a large ring to reduce the communication overhead incurred by performing almost all the computations on a large ring. Experimental results from eight widely used datasets show that Ents outperforms state-of-the-art frameworks by $5.5\\times \\sim 9.3\\times$ in communication sizes and $3.9\\times \\sim 5.3\\times$ in communication rounds. In terms of training time, Ents yields an improvement of $3.5\\times \\sim 6.7\\times$. To demonstrate its practicality, Ents requires less than three hours to securely train a decision tree on a widely used real-world dataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Guopeng Lin",
        "Weili Han",
        "Wenqiang Ruan",
        "Ruisheng Zhou",
        "Lushan Song",
        "Bingshuai Li",
        "Yunfeng Shao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07948",
      "code_url": null,
      "url": "https://arxiv.org/abs/2406.07948",
      "source_section": "2.5 Private ML Protocols",
      "source_subsection": "2.5.1 3PC"
    },
    {
      "arxiv_id": "1912.02631",
      "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning",
      "abstract": "Machine learning has started to be deployed in fields such as healthcare and finance, which propelled the need for and growth of privacy-preserving machine learning (PPML). We propose an actively secure four-party protocol (4PC), and a framework for PPML, showcasing its applications on four of the most widely-known machine learning algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC protocol tolerating at most one malicious corruption is practically efficient as compared to the existing works. We use the protocol to build an efficient mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and Garbled worlds. Our framework operates in the offline-online paradigm over rings and is instantiated in an outsourced setting for machine learning. Also, we propose conversions especially relevant to privacy-preserving machine learning. The highlights of our framework include using a minimal number of expensive circuits overall as compared to ABY3. This can be seen in our technique for truncation, which does not affect the online cost of multiplication and removes the need for any circuits in the offline phase. Our B2A conversion has an improvement of $\\mathbf{7} \\times$ in rounds and $\\mathbf{18} \\times$ in the communication complexity. The practicality of our framework is argued through improvements in the benchmarking of the aforementioned algorithms when compared with ABY3. All the protocols are implemented over a 64-bit ring in both LAN and WAN settings. Our improvements go up to $\\mathbf{187} \\times$ for the training phase and $\\mathbf{158} \\times$ for the prediction phase when observed over LAN and WAN.",
      "year": 2020,
      "venue": "NDSS",
      "authors": [
        "Harsh Chaudhari",
        "Rahul Rachuri",
        "Ajith Suresh"
      ],
      "pdf_url": "https://arxiv.org/pdf/1912.02631.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/1912.02631",
      "source_section": "2.5 Private ML Protocols",
      "source_subsection": "2.5.2 4PC"
    },
    {
      "arxiv_id": "2208.08662",
      "title": "Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy",
      "abstract": "Secure multi-party computation-based machine learning, referred to as MPL, has become an important technology to utilize data from multiple parties with privacy preservation. While MPL provides rigorous security guarantees for the computation process, the models trained by MPL are still vulnerable to attacks that solely depend on access to the models. Differential privacy could help to defend against such attacks. However, the accuracy loss brought by differential privacy and the huge communication overhead of secure multi-party computation protocols make it highly challenging to balance the 3-way trade-off between privacy, efficiency, and accuracy.   In this paper, we are motivated to resolve the above issue by proposing a solution, referred to as PEA (Private, Efficient, Accurate), which consists of a secure DPSGD protocol and two optimization methods. First, we propose a secure DPSGD protocol to enforce DPSGD in secret sharing-based MPL frameworks. Second, to reduce the accuracy loss led by differential privacy noise and the huge communication overhead of MPL, we propose two optimization methods for the training process of MPL: (1) the data-independent feature extraction method, which aims to simplify the trained model structure; (2) the local data-based global model initialization method, which aims to speed up the convergence of the model training. We implement PEA in two open-source MPL frameworks: TF-Encrypted and Queqiao. The experimental results on various datasets demonstrate the efficiency and effectiveness of PEA. E.g. when $\u03b5$ = 2, we can train a differentially private classification model with an accuracy of 88% for CIFAR-10 within 7 minutes under the LAN setting. This result significantly outperforms the one from CryptGPU, one SOTA MPL framework: it costs more than 16 hours to train a non-private deep neural network model on CIFAR-10 with the same accuracy.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Wenqiang Ruan",
        "Mingxin Xu",
        "Wenjing Fang",
        "Li Wang",
        "Lei Wang",
        "Weili Han"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.08662.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2208.08662",
      "source_section": "2.5 Private ML Protocols",
      "source_subsection": "2.5.3 SMPC"
    },
    {
      "arxiv_id": "2212.10986",
      "title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning",
      "abstract": "Deploying machine learning models in production may allow adversaries to infer sensitive information about training data. There is a vast literature analyzing different types of inference risks, ranging from membership inference to reconstruction attacks. Inspired by the success of games (i.e., probabilistic experiments) to study security properties in cryptography, some authors describe privacy inference risks in machine learning using a similar game-based style. However, adversary capabilities and goals are often stated in subtly different ways from one presentation to the other, which makes it hard to relate and compose results. In this paper, we present a game-based framework to systematize the body of knowledge on privacy inference risks in machine learning. We use this framework to (1) provide a unifying structure for definitions of inference risks, (2) formally establish known relations among definitions, and (3) to uncover hitherto unknown relations that would have been difficult to spot otherwise.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Ahmed Salem",
        "Giovanni Cherubin",
        "David Evans",
        "Boris K\u00f6pf",
        "Andrew Paverd",
        "Anshuman Suri",
        "Shruti Tople",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.10986.pdf",
      "code_url": null,
      "url": "https://arxiv.org/abs/2212.10986",
      "source_section": "2.6 Platform",
      "source_subsection": "2.6.2 Survey"
    },
    {
      "arxiv_id": "2311.02324",
      "title": "Bounded and Unbiased Composite Differential Privacy",
      "abstract": "The objective of differential privacy (DP) is to protect privacy by producing an output distribution that is indistinguishable between any two neighboring databases. However, traditional differentially private mechanisms tend to produce unbounded outputs in order to achieve maximum disturbance range, which is not always in line with real-world applications. Existing solutions attempt to address this issue by employing post-processing or truncation techniques to restrict the output results, but at the cost of introducing bias issues. In this paper, we propose a novel differentially private mechanism which uses a composite probability density function to generate bounded and unbiased outputs for any numerical input data. The composition consists of an activation function and a base function, providing users with the flexibility to define the functions according to the DP constraints. We also develop an optimization algorithm that enables the iterative search for the optimal hyper-parameter setting without the need for repeated experiments, which prevents additional privacy overhead. Furthermore, we evaluate the utility of the proposed mechanism by assessing the variance of the composite probability density function and introducing two alternative metrics that are simpler to compute than variance estimation. Our extensive evaluation on three benchmark datasets demonstrates consistent and significant improvement over the traditional Laplace and Gaussian mechanisms. The proposed bounded and unbiased composite differentially private mechanism will underpin the broader DP arsenal and foster future privacy-preserving studies.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Kai Zhang",
        "Yanjun Zhang",
        "Ruoxi Sun",
        "Pei-Wei Tsai",
        "Muneeb Ul Hassan",
        "Xin Yuan",
        "Minhui Xue",
        "Jinjun Chen"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02324.pdf",
      "code_url": "https://github.com/CompositeDP/CompositeDP",
      "url": "https://arxiv.org/abs/2311.02324",
      "source_section": "2.7 Differential Privacy",
      "source_subsection": "2.7.2 DP"
    },
    {
      "arxiv_id": "2301.08517",
      "title": "Cohere: Managing Differential Privacy in Large Scale Systems",
      "abstract": "The need for a privacy management layer in today's systems started to manifest with the emergence of new systems for privacy-preserving analytics and privacy compliance. As a result, many independent efforts have emerged that try to provide system support for privacy. Recently, the scope of privacy solutions used in systems has expanded to encompass more complex techniques such as Differential Privacy (DP). The use of these solutions in large-scale systems imposes new challenges and requirements. Careful planning and coordination are necessary to ensure that privacy guarantees are maintained across a wide range of heterogeneous applications and data systems. This requires new solutions for managing and allocating scarce and non-replenishable privacy resources. In this paper, we introduce Cohere, a new system that simplifies the use of DP in large-scale systems. Cohere implements a unified interface that allows heterogeneous applications to operate on a unified view of users' data. In this work, we further address two pressing system challenges that arise in the context of real-world deployments: ensuring the continuity of privacy-based applications (i.e., preventing privacy budget depletion) and effectively allocating scarce shared privacy resources (i.e., budget) under complex preferences. Our experiments show that Cohere achieves a 6.4--28x improvement in utility compared to the state-of-the-art across a range of complex workloads.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Nicolas K\u00fcchler",
        "Emanuel Opel",
        "Hidde Lycklama",
        "Alexander Viand",
        "Anwar Hithnawi"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.08517.pdf",
      "code_url": "https://github.com/pps-lab/cohere",
      "url": "https://arxiv.org/abs/2301.08517",
      "source_section": "2.7 Differential Privacy",
      "source_subsection": "2.7.2 DP"
    },
    {
      "arxiv_id": "2406.19466",
      "title": "Data Poisoning Attacks to Locally Differentially Private Frequent Itemset Mining Protocols",
      "abstract": "Local differential privacy (LDP) provides a way for an untrusted data collector to aggregate users' data without violating their privacy. Various privacy-preserving data analysis tasks have been studied under the protection of LDP, such as frequency estimation, frequent itemset mining, and machine learning. Despite its privacy-preserving properties, recent research has demonstrated the vulnerability of certain LDP protocols to data poisoning attacks. However, existing data poisoning attacks are focused on basic statistics under LDP, such as frequency estimation and mean/variance estimation. As an important data analysis task, the security of LDP frequent itemset mining has yet to be thoroughly examined. In this paper, we aim to address this issue by presenting novel and practical data poisoning attacks against LDP frequent itemset mining protocols. By introducing a unified attack framework with composable attack operations, our data poisoning attack can successfully manipulate the state-of-the-art LDP frequent itemset mining protocols and has the potential to be adapted to other protocols with similar structures. We conduct extensive experiments on three datasets to compare the proposed attack with four baseline attacks. The results demonstrate the severity of the threat and the effectiveness of the proposed attack.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Wei Tong",
        "Haoyu Chen",
        "Jiacheng Niu",
        "Sheng Zhong"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.19466",
      "code_url": null,
      "url": "https://arxiv.org/abs/2406.19466",
      "source_section": "2.7 Differential Privacy",
      "source_subsection": "2.7.3 LDP"
    },
    {
      "arxiv_id": "2405.06823",
      "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
      "abstract": "Large Language Models (LLMs) enable a new ecosystem with many downstream applications, called LLM applications, with different natural language processing tasks. The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform. Therefore, an LLM application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an LLM application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness.   In this paper, we design a novel, closed-box prompt leaking attack framework, called PLeak, to optimize an adversarial query such that when the attacker sends it to a target LLM application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt.   We evaluate PLeak in both offline settings and for real-world LLM applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that PLeak can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/BHui97/PLeak.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Philippe Burlina",
        "Yinzhi Cao"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06823",
      "code_url": "https://github.com/BHui97/PLeak",
      "url": "https://arxiv.org/abs/2405.06823",
      "source_section": "2.7 LLM Privacy",
      "source_subsection": "2.7.1 Prompt Privacy"
    }
  ]
}