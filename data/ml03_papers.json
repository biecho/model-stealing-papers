{
  "owasp_id": "ML03",
  "owasp_name": "Model Inversion Attack",
  "total": 43,
  "updated": "2026-01-28",
  "papers": [
    {
      "paper_id": "seed_9ab242f3",
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss",
        "Katherine Lee",
        "Adam Roberts",
        "Tom B. Brown",
        "D. Song",
        "\u00da. Erlingsson",
        "Alina Oprea",
        "Colin Raffel"
      ],
      "author_details": [
        {
          "name": "Nicholas Carlini",
          "h_index": 35,
          "citation_count": 10308,
          "affiliations": []
        },
        {
          "name": "Florian Tram\u00e8r",
          "h_index": 51,
          "citation_count": 33525,
          "affiliations": [
            "ETH Z\u00fcrich"
          ]
        },
        {
          "name": "Eric Wallace",
          "h_index": 32,
          "citation_count": 12288,
          "affiliations": [
            "UC Berkeley"
          ]
        },
        {
          "name": "Matthew Jagielski",
          "h_index": 36,
          "citation_count": 11099,
          "affiliations": []
        },
        {
          "name": "Ariel Herbert-Voss",
          "h_index": 9,
          "citation_count": 64764,
          "affiliations": []
        },
        {
          "name": "Katherine Lee",
          "h_index": 20,
          "citation_count": 39401,
          "affiliations": []
        },
        {
          "name": "Adam Roberts",
          "h_index": 40,
          "citation_count": 69243,
          "affiliations": []
        },
        {
          "name": "Tom B. Brown",
          "h_index": 25,
          "citation_count": 82310,
          "affiliations": []
        },
        {
          "name": "D. Song",
          "h_index": 136,
          "citation_count": 105349,
          "affiliations": []
        },
        {
          "name": "\u00da. Erlingsson",
          "h_index": 36,
          "citation_count": 14408,
          "affiliations": []
        },
        {
          "name": "Alina Oprea",
          "h_index": 32,
          "citation_count": 9229,
          "affiliations": []
        },
        {
          "name": "Colin Raffel",
          "h_index": 58,
          "citation_count": 68459,
          "affiliations": []
        }
      ],
      "max_h_index": 136,
      "url": "https://openalex.org/W3112689365",
      "pdf_url": "https://arxiv.org/pdf/2012.07805",
      "doi": "https://doi.org/10.48550/arxiv.2012.07805",
      "citation_count": 2513,
      "influential_citation_count": 263,
      "reference_count": 75,
      "is_open_access": false,
      "publication_date": "2020-12-14",
      "tldr": "This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model, and finds that larger models are more vulnerable than smaller models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "training-data-extraction",
        "memorization"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_a823e69e",
      "title": "Task-Oriented Training Data Privacy Protection for Cloud-based Model Training",
      "abstract": "The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",
      "year": 2015,
      "venue": "IEEE Access",
      "authors": [
        "S. M. Riazul Islam",
        "Daehan Kwak",
        "Md. Humaun Kabir",
        "M. Hossain",
        "K. Kwak"
      ],
      "author_details": [
        {
          "name": "S. M. Riazul Islam",
          "h_index": 3,
          "citation_count": 2578,
          "affiliations": []
        },
        {
          "name": "Daehan Kwak",
          "h_index": 22,
          "citation_count": 4552,
          "affiliations": [
            "Kean Univeristy"
          ]
        },
        {
          "name": "Md. Humaun Kabir",
          "h_index": 2,
          "citation_count": 2475,
          "affiliations": []
        },
        {
          "name": "M. Hossain",
          "h_index": 6,
          "citation_count": 2612,
          "affiliations": []
        },
        {
          "name": "K. Kwak",
          "h_index": 37,
          "citation_count": 13131,
          "affiliations": []
        }
      ],
      "max_h_index": 37,
      "url": "https://openalex.org/W1943579973",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/7042252/07113786.pdf",
      "doi": "https://doi.org/10.1109/access.2015.2437951",
      "citation_count": 2474,
      "influential_citation_count": 78,
      "reference_count": 110,
      "is_open_access": true,
      "publication_date": "2015-06-01",
      "tldr": "An intelligent collaborative security model to minimize security risk is proposed; how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context is discussed; and various IoT and eHealth policies and regulations are addressed to determine how they can facilitate economies and societies in terms of sustainable development.",
      "fields_of_study": [
        "Computer Science",
        "Business"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "cloud-training",
        "data-privacy"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/6287639/7042252/07113786.pdf"
    },
    {
      "paper_id": "seed_0c25d4b0",
      "title": "Effective PII Extraction from LLMs through Augmented Few-Shot Learning",
      "abstract": "Abstract Educational technology innovations leveraging large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (eg, question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs\u2010based innovations in authentic educational contexts. To address this, we conducted a systematic scoping review of 118 peer\u2010reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The findings revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation. Additionally, we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency and insufficient privacy and beneficence considerations. The findings were summarised into three recommendations for future studies, including updating existing innovations with state\u2010of\u2010the\u2010art models (eg, GPT\u20103/4), embracing the initiative of open\u2010sourcing models/systems, and adopting a human\u2010centred approach throughout the developmental process. As the intersection of AI and education is continuously evolving, the findings of this study can serve as an essential reference point for researchers, allowing them to leverage the strengths, learn from the limitations, and uncover potential research opportunities enabled by ChatGPT and other generative AI models. Practitioner notes What is currently known about this topic Generating and analysing text\u2010based content are time\u2010consuming and laborious tasks. Large language models are capable of efficiently analysing an unprecedented amount of textual content and completing complex natural language processing and generation tasks. Large language models have been increasingly used to develop educational technologies that aim to automate the generation and analysis of textual content, such as automated question generation and essay scoring. What this paper adds A comprehensive list of different educational tasks that could potentially benefit from LLMs\u2010based innovations through automation. A structured assessment of the practicality and ethicality of existing LLMs\u2010based innovations from seven important aspects using established frameworks. Three recommendations that could potentially support future studies to develop LLMs\u2010based innovations that are practical and ethical to implement in authentic educational contexts. Implications for practice and/or policy Updating existing innovations with state\u2010of\u2010the\u2010art models may further reduce the amount of manual effort required for adapting existing models to different educational tasks. The reporting standards of empirical research that aims to develop educational technologies using large language models need to be improved. Adopting a human\u2010centred approach throughout the developmental process could contribute to resolving the practical and ethical challenges of large language models in education.",
      "year": 2023,
      "venue": "British Journal of Educational Technology",
      "authors": [
        "Lixiang Yan",
        "Lele Sha",
        "Linxuan Zhao",
        "Yuheng Li",
        "Roberto Mart\u00ednez Maldonado",
        "Guanliang Chen",
        "Xinyu Li",
        "Yueqiao Jin",
        "D. Ga\u0161evi\u0107"
      ],
      "author_details": [
        {
          "name": "Lixiang Yan",
          "h_index": 19,
          "citation_count": 1850,
          "affiliations": []
        },
        {
          "name": "Lele Sha",
          "h_index": 12,
          "citation_count": 882,
          "affiliations": []
        },
        {
          "name": "Linxuan Zhao",
          "h_index": 14,
          "citation_count": 1056,
          "affiliations": []
        },
        {
          "name": "Yuheng Li",
          "h_index": 10,
          "citation_count": 798,
          "affiliations": []
        },
        {
          "name": "Roberto Mart\u00ednez Maldonado",
          "h_index": 7,
          "citation_count": 906,
          "affiliations": []
        },
        {
          "name": "Guanliang Chen",
          "h_index": 19,
          "citation_count": 2186,
          "affiliations": []
        },
        {
          "name": "Xinyu Li",
          "h_index": 12,
          "citation_count": 939,
          "affiliations": []
        },
        {
          "name": "Yueqiao Jin",
          "h_index": 11,
          "citation_count": 1034,
          "affiliations": []
        },
        {
          "name": "D. Ga\u0161evi\u0107",
          "h_index": 82,
          "citation_count": 28774,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://openalex.org/W4385632485",
      "pdf_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bjet.13370",
      "doi": "https://doi.org/10.1111/bjet.13370",
      "citation_count": 496,
      "influential_citation_count": 21,
      "reference_count": 94,
      "is_open_access": true,
      "publication_date": "2023-03-17",
      "tldr": "A systematic scoping review of 118 peer\u2010reviewed papers published since 2017 revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "PII-extraction",
        "few-shot"
      ],
      "open_access_pdf": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bjet.13370"
    },
    {
      "paper_id": "2004.00053",
      "title": "Information Leakage in Embedding Models",
      "abstract": "Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains.   We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50\\%--70\\% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.",
      "year": 2020,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Congzheng Song",
        "A. Raghunathan"
      ],
      "author_details": [
        {
          "name": "Congzheng Song",
          "h_index": 17,
          "citation_count": 8858,
          "affiliations": []
        },
        {
          "name": "A. Raghunathan",
          "h_index": 16,
          "citation_count": 3271,
          "affiliations": []
        }
      ],
      "max_h_index": 17,
      "url": "https://arxiv.org/abs/2004.00053",
      "citation_count": 325,
      "influential_citation_count": 32,
      "reference_count": 83,
      "is_open_access": true,
      "publication_date": "2020-03-31",
      "tldr": "This work develops three classes of attacks to systematically study information that might be leaked by embeddings, and extensively evaluates the attacks on various state-of-the-art embedding models in the text domain.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "embedding-leakage",
        "pre-training"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2004.00053"
    },
    {
      "paper_id": "2302.00539",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nils Lukas",
        "A. Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B'eguelin"
      ],
      "author_details": [
        {
          "name": "Nils Lukas",
          "h_index": 10,
          "citation_count": 886,
          "affiliations": []
        },
        {
          "name": "A. Salem",
          "h_index": 17,
          "citation_count": 3182,
          "affiliations": []
        },
        {
          "name": "Robert Sim",
          "h_index": 12,
          "citation_count": 1388,
          "affiliations": []
        },
        {
          "name": "Shruti Tople",
          "h_index": 20,
          "citation_count": 2828,
          "affiliations": []
        },
        {
          "name": "Lukas Wutschitz",
          "h_index": 7,
          "citation_count": 576,
          "affiliations": []
        },
        {
          "name": "Santiago Zanella-B'eguelin",
          "h_index": 6,
          "citation_count": 471,
          "affiliations": []
        }
      ],
      "max_h_index": 20,
      "url": "https://arxiv.org/abs/2302.00539",
      "citation_count": 324,
      "influential_citation_count": 31,
      "reference_count": 87,
      "is_open_access": false,
      "publication_date": "2023-02-01",
      "tldr": "Novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks are introduced, showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of P II sequences, and a subtle connection between record-level membership inference and PII reconstruction are shown.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "PII-leakage",
        "language-models"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_9cbf640c",
      "title": "Private Investigator: Extracting Personally Identifiable Information from Large Language Models Using Optimized Prompts",
      "abstract": "Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Steven M. Williamson",
        "Victor R. Prybutok"
      ],
      "author_details": [
        {
          "name": "Steven M. Williamson",
          "h_index": 3,
          "citation_count": 377,
          "affiliations": []
        },
        {
          "name": "Victor R. Prybutok",
          "h_index": 9,
          "citation_count": 608,
          "affiliations": []
        }
      ],
      "max_h_index": 9,
      "url": "https://openalex.org/W4390829176",
      "pdf_url": "https://www.mdpi.com/2076-3417/14/2/675/pdf?version=1705396255",
      "doi": "https://doi.org/10.3390/app14020675",
      "citation_count": 293,
      "influential_citation_count": 13,
      "reference_count": 37,
      "is_open_access": true,
      "publication_date": "2024-01-12",
      "tldr": "Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care.",
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "PII-extraction",
        "optimized-prompts"
      ],
      "open_access_pdf": "https://www.mdpi.com/2076-3417/14/2/675/pdf?version=1705396255"
    },
    {
      "paper_id": "seed_fa4cdd9d",
      "title": "Privacy Risks of General-Purpose Language Models",
      "abstract": "Recently, a new paradigm of building general-purpose language models (e.g., Google's Bert and OpenAI's GPT-2) in Natural Language Processing (NLP) for text feature extraction, a standard procedure in NLP systems that converts texts to vectors (i.e., embeddings) for downstream modeling, has arisen and starts to find its application in various downstream NLP tasks and real world systems (e.g., Google's search engine [6]). To obtain general-purpose text embeddings, these language models have highly complicated architectures with millions of learnable parameters and are usually pretrained on billions of sentences before being utilized. As is widely recognized, such a practice indeed improves the state-of-the-art performance of many downstream NLP tasks. However, the improved utility is not for free. We find the text embeddings from general-purpose language models would capture much sensitive information from the plain text. Once being accessed by the adversary, the embeddings can be reverse-engineered to disclose sensitive information of the victims for further harassment. Although such a privacy risk can impose a real threat to the future leverage of these promising NLP tools, there are neither published attacks nor systematic evaluations by far for the mainstream industry-level language models. To bridge this gap, we present the first systematic study on the privacy risks of 8 state-of-the-art language models with 4 diverse case studies. By constructing 2 novel attack classes, our study demonstrates the aforementioned privacy risks do exist and can impose practical threats to the application of general-purpose language models on sensitive data covering identity, genome, healthcare and location. For example, we show the adversary with nearly no prior knowledge can achieve about 75% accuracy when inferring the precise disease site from Bert embeddings of patients' medical descriptions. As possible countermeasures, we propose 4 different defenses (via rounding, differential privacy, adversarial training and subspace projection) to obfuscate the unprotected embeddings for mitigation purpose. With extensive evaluations, we also provide a preliminary analysis on the utility-privacy trade-off brought by each defense, which we hope may foster future mitigation researches.",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xudong Pan",
        "Mi Zhang",
        "S. Ji",
        "Min Yang"
      ],
      "author_details": [
        {
          "name": "Xudong Pan",
          "h_index": 14,
          "citation_count": 940,
          "affiliations": []
        },
        {
          "name": "Mi Zhang",
          "h_index": 15,
          "citation_count": 1008,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Min Yang",
          "h_index": 15,
          "citation_count": 1196,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://openalex.org/W3046764764",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152761.pdf",
      "doi": "https://doi.org/10.1109/sp40000.2020.00095",
      "citation_count": 269,
      "influential_citation_count": 10,
      "reference_count": 86,
      "is_open_access": true,
      "publication_date": "2020-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "LM-privacy",
        "embedding-privacy"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152761.pdf"
    },
    {
      "paper_id": "2012.02670",
      "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
      "abstract": "We investigate the security of Split Learning -- a novel collaborative machine learning framework that enables peak performance by requiring minimal resources consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack is able to overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning. To make our results reproducible, we made our code available at https://github.com/pasquini-dario/SplitNN_FSHA.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Dario Pasquini",
        "G. Ateniese",
        "M. Bernaschi"
      ],
      "author_details": [
        {
          "name": "Dario Pasquini",
          "h_index": 11,
          "citation_count": 624,
          "affiliations": []
        },
        {
          "name": "G. Ateniese",
          "h_index": 50,
          "citation_count": 20201,
          "affiliations": []
        },
        {
          "name": "M. Bernaschi",
          "h_index": 33,
          "citation_count": 5359,
          "affiliations": []
        }
      ],
      "max_h_index": 50,
      "url": "https://arxiv.org/abs/2012.02670",
      "citation_count": 197,
      "influential_citation_count": 25,
      "reference_count": 71,
      "is_open_access": true,
      "publication_date": "2020-12-04",
      "tldr": "This paper exposes vulnerabilities of the split learning protocol and demonstrates its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets and extending previously devised attacks for Federated Learning.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "split-learning",
        "inference-attack"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2012.02670"
    },
    {
      "paper_id": "2108.06504",
      "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
      "abstract": "Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Fan Wu",
        "Yunhui Long",
        "Ce Zhang",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Fan Wu",
          "h_index": 10,
          "citation_count": 447,
          "affiliations": [
            "University of Illinois at Urbana-Champaign"
          ]
        },
        {
          "name": "Yunhui Long",
          "h_index": 11,
          "citation_count": 1237,
          "affiliations": []
        },
        {
          "name": "Ce Zhang",
          "h_index": 14,
          "citation_count": 692,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 69,
          "citation_count": 27430,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://arxiv.org/abs/2108.06504",
      "citation_count": 115,
      "influential_citation_count": 34,
      "reference_count": 39,
      "is_open_access": true,
      "publication_date": "2021-08-14",
      "tldr": "This paper proposes a privacy attack LINKTELLER via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob, and empirically shows that LINK TELLER is able to recover a significant amount of private edges in different settings, both including inductive and transductive datasets, under different graph densities, significantly outperforming existing baselines.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "link-recovery",
        "influence-analysis"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2108.06504"
    },
    {
      "paper_id": "seed_30d1ae53",
      "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
      "abstract": "Secure multi-party machine learning allows several parties to build a model on their pooled data to increase utility while not explicitly sharing data with each other. We show that such multi-party computation can cause leakage of global dataset properties between the parties even when parties obtain only black-box access to the final model. In particular, a ``curious'' party can infer the distribution of sensitive attributes in other parties' data with high accuracy. This raises concerns regarding the confidentiality of properties pertaining to the whole dataset as opposed to individual data records. We show that our attack can leak population-level properties in datasets of different types, including tabular, text, and graph data. To understand and measure the source of leakage, we consider several models of correlation between a sensitive attribute and the rest of the data. Using multiple machine learning models, we show that leakage occurs even if the sensitive attribute is not included in the training data and has a low correlation with other attributes or the target variable.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wanrong Zhang",
        "Shruti Tople",
        "O. Ohrimenko"
      ],
      "author_details": [
        {
          "name": "Wanrong Zhang",
          "h_index": 10,
          "citation_count": 339,
          "affiliations": []
        },
        {
          "name": "Shruti Tople",
          "h_index": 20,
          "citation_count": 2828,
          "affiliations": []
        },
        {
          "name": "O. Ohrimenko",
          "h_index": 23,
          "citation_count": 3053,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://openalex.org/W3171497996",
      "pdf_url": "https://arxiv.org/pdf/2006.07267",
      "doi": "https://doi.org/10.48550/arxiv.2006.07267",
      "citation_count": 89,
      "influential_citation_count": 8,
      "reference_count": 55,
      "is_open_access": false,
      "publication_date": "2020-06-12",
      "tldr": "This work shows that multi-party computation can cause leakage of global dataset properties between the parties even when parties obtain only black-box access to the final model, and considers several models of correlation between a sensitive attribute and the rest of the data.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "multi-party-ML",
        "property-leakage"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2209.01292",
      "title": "Are Attribute Inference Attacks Just Imputation?",
      "abstract": "Models can expose sensitive information about their training data. In an attribute inference attack, an adversary has partial knowledge of some training records and access to a model trained on those records, and infers the unknown values of a sensitive feature of those records. We study a fine-grained variant of attribute inference we call \\emph{sensitive value inference}, where the adversary's goal is to identify with high confidence some records from a candidate set where the unknown attribute has a particular sensitive value. We explicitly compare attribute inference with data imputation that captures the training distribution statistics, under various assumptions about the training data available to the adversary. Our main conclusions are: (1) previous attribute inference methods do not reveal more about the training data from the model than can be inferred by an adversary without access to the trained model, but with the same knowledge of the underlying distribution as needed to train the attribute inference attack; (2) black-box attribute inference attacks rarely learn anything that cannot be learned without the model; but (3) white-box attacks, which we introduce and evaluate in the paper, can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model. Furthermore, we show that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk. The code for our experiments is available at \\url{https://github.com/bargavj/EvaluatingDPML}.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "author_details": [
        {
          "name": "Bargav Jayaraman",
          "h_index": 12,
          "citation_count": 1429,
          "affiliations": []
        },
        {
          "name": "David Evans",
          "h_index": 5,
          "citation_count": 148,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://arxiv.org/abs/2209.01292",
      "citation_count": 70,
      "influential_citation_count": 7,
      "reference_count": 37,
      "is_open_access": true,
      "publication_date": "2022-09-02",
      "tldr": "It is shown that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk, and white-box attacks can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "attribute-inference",
        "imputation"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560663"
    },
    {
      "paper_id": "seed_086ec9de",
      "title": "Property Inference Attacks Against GANs",
      "abstract": "While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN\u2019s training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.",
      "year": 2022,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Junhao Zhou",
        "Yufei Chen",
        "Chao Shen",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Junhao Zhou",
          "h_index": 2,
          "citation_count": 72,
          "affiliations": []
        },
        {
          "name": "Yufei Chen",
          "h_index": 7,
          "citation_count": 189,
          "affiliations": []
        },
        {
          "name": "Chao Shen",
          "h_index": 20,
          "citation_count": 1393,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 40,
          "citation_count": 7620,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 40,
      "url": "https://openalex.org/W3213807399",
      "pdf_url": "https://doi.org/10.14722/ndss.2022.23019",
      "doi": "https://doi.org/10.14722/ndss.2022.23019",
      "citation_count": 68,
      "influential_citation_count": 12,
      "reference_count": 72,
      "is_open_access": true,
      "publication_date": "2021-11-15",
      "tldr": "This paper proposes the first set of training dataset property inference attacks against GANs and proposes a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black- box setting.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "generative"
      ],
      "model_types": [
        "gan"
      ],
      "tags": [
        "property-inference",
        "GAN-attack"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2022.23019"
    },
    {
      "paper_id": "seed_908a8f14",
      "title": "MIRROR: Model Inversion for Deep Learning Network with High Fidelity",
      "abstract": "Model inversion reverse-engineers input samples from a given model, and hence poses serious threats to information confidentiality.We propose a novel inversion technique based on StyleGAN, whose generator has a special architecture that forces the decomposition of an input to styles of various granularities such that the model can learn them separately in training.During sample generation, the generator transforms a latent value to parameters controlling these styles to compose a sample.In our inversion, given a target label of some subject model to invert (e.g., a private face based identity recognition model), our technique leverages a StyleGAN trained on public data from the same domain (e.g., a public human face dataset), uses the gradient descent or genetic search algorithm, together with distribution based clipping, to find a proper parameterization of the styles such that the generated sample is correctly classified to the target label (by the subject model) and recognized by humans.The results show that our inverted samples have high fidelity, substantially better than those by existing state-of-the-art techniques.",
      "year": 2022,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Shengwei An",
        "Guanhong Tao",
        "Qiuling Xu",
        "Yingqi Liu",
        "Guangyu Shen",
        "Yuan Yao",
        "Jingwei Xu",
        "X. Zhang"
      ],
      "author_details": [
        {
          "name": "Shengwei An",
          "h_index": 17,
          "citation_count": 1032,
          "affiliations": []
        },
        {
          "name": "Guanhong Tao",
          "h_index": 27,
          "citation_count": 2840,
          "affiliations": []
        },
        {
          "name": "Qiuling Xu",
          "h_index": 13,
          "citation_count": 753,
          "affiliations": []
        },
        {
          "name": "Yingqi Liu",
          "h_index": 13,
          "citation_count": 1832,
          "affiliations": []
        },
        {
          "name": "Guangyu Shen",
          "h_index": 20,
          "citation_count": 1240,
          "affiliations": []
        },
        {
          "name": "Yuan Yao",
          "h_index": 59,
          "citation_count": 14677,
          "affiliations": []
        },
        {
          "name": "Jingwei Xu",
          "h_index": 12,
          "citation_count": 593,
          "affiliations": []
        },
        {
          "name": "X. Zhang",
          "h_index": 85,
          "citation_count": 296271,
          "affiliations": []
        }
      ],
      "max_h_index": 85,
      "url": "https://openalex.org/W4226117300",
      "pdf_url": "https://doi.org/10.14722/ndss.2022.24335",
      "doi": "https://doi.org/10.14722/ndss.2022.24335",
      "citation_count": 68,
      "influential_citation_count": 17,
      "reference_count": 71,
      "is_open_access": true,
      "tldr": "A novel inversion technique based on StyleGAN, whose generator has a special architecture that forces the decomposition of an input to styles of various granularities such that the model can learn them separately in training.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "gan"
      ],
      "tags": [
        "model-inversion",
        "StyleGAN",
        "high-fidelity"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2022.24335"
    },
    {
      "paper_id": "seed_28ac2152",
      "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
      "abstract": "Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy enhancement requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy enhancement should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "K. Yue",
        "Richeng Jin",
        "Chau-Wai Wong",
        "D. Baron",
        "H. Dai"
      ],
      "author_details": [
        {
          "name": "K. Yue",
          "h_index": 6,
          "citation_count": 147,
          "affiliations": []
        },
        {
          "name": "Richeng Jin",
          "h_index": 10,
          "citation_count": 301,
          "affiliations": []
        },
        {
          "name": "Chau-Wai Wong",
          "h_index": 17,
          "citation_count": 742,
          "affiliations": []
        },
        {
          "name": "D. Baron",
          "h_index": 27,
          "citation_count": 6136,
          "affiliations": []
        },
        {
          "name": "H. Dai",
          "h_index": 49,
          "citation_count": 9766,
          "affiliations": []
        }
      ],
      "max_h_index": 49,
      "url": "https://openalex.org/W4281609047",
      "pdf_url": "https://arxiv.org/pdf/2206.04055",
      "doi": "https://doi.org/10.48550/arxiv.2206.04055",
      "citation_count": 68,
      "influential_citation_count": 9,
      "reference_count": 77,
      "is_open_access": true,
      "publication_date": "2022-06-08",
      "tldr": "It is shown that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning and argued that privacy enhancement should not be treated as a byproduct of gradient compression.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "gradient-obfuscation",
        "false-security"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2206.04055"
    },
    {
      "paper_id": "2303.12233",
      "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
      "abstract": "Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting. In this work we introduce LOKI, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters, and the weight gradients of data points between clients remain separate even through aggregation. With FedAVG and aggregation across 100 clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, and Tiny ImageNet. Using only a single training round, LOKI is able to leak 76-86% of all data samples.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Joshua C. Zhao",
        "Atul Sharma",
        "A. Elkordy",
        "Yahya H. Ezzeldin",
        "A. Avestimehr",
        "S. Bagchi"
      ],
      "author_details": [
        {
          "name": "Joshua C. Zhao",
          "h_index": 5,
          "citation_count": 141,
          "affiliations": []
        },
        {
          "name": "Atul Sharma",
          "h_index": 8,
          "citation_count": 230,
          "affiliations": []
        },
        {
          "name": "A. Elkordy",
          "h_index": 9,
          "citation_count": 447,
          "affiliations": []
        },
        {
          "name": "Yahya H. Ezzeldin",
          "h_index": 9,
          "citation_count": 618,
          "affiliations": []
        },
        {
          "name": "A. Avestimehr",
          "h_index": 53,
          "citation_count": 12012,
          "affiliations": []
        },
        {
          "name": "S. Bagchi",
          "h_index": 53,
          "citation_count": 8902,
          "affiliations": []
        }
      ],
      "max_h_index": 53,
      "url": "https://arxiv.org/abs/2303.12233",
      "citation_count": 54,
      "influential_citation_count": 10,
      "reference_count": 54,
      "is_open_access": true,
      "publication_date": "2023-03-21",
      "tldr": "Loki is introduced, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "data-reconstruction",
        "model-manipulation"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2303.12233"
    },
    {
      "paper_id": "2105.12049",
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "M. Malekzadeh",
        "A. Borovykh",
        "Deniz Gunduz"
      ],
      "author_details": [
        {
          "name": "M. Malekzadeh",
          "h_index": 13,
          "citation_count": 955,
          "affiliations": []
        },
        {
          "name": "A. Borovykh",
          "h_index": 12,
          "citation_count": 1190,
          "affiliations": []
        },
        {
          "name": "Deniz Gunduz",
          "h_index": 2,
          "citation_count": 51,
          "affiliations": []
        }
      ],
      "max_h_index": 13,
      "url": "https://arxiv.org/abs/2105.12049",
      "citation_count": 44,
      "influential_citation_count": 2,
      "reference_count": 87,
      "is_open_access": true,
      "publication_date": "2021-05-25",
      "tldr": "This work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "secret-encoding",
        "attribute-leakage"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2105.12049"
    },
    {
      "paper_id": "seed_e8f3ac3f",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "M. Malekzadeh",
        "A. Borovykh",
        "Deniz Gunduz"
      ],
      "author_details": [
        {
          "name": "M. Malekzadeh",
          "h_index": 13,
          "citation_count": 955,
          "affiliations": []
        },
        {
          "name": "A. Borovykh",
          "h_index": 12,
          "citation_count": 1190,
          "affiliations": []
        },
        {
          "name": "Deniz Gunduz",
          "h_index": 2,
          "citation_count": 51,
          "affiliations": []
        }
      ],
      "max_h_index": 13,
      "url": "https://arxiv.org/abs/2105.12049",
      "citation_count": 44,
      "influential_citation_count": 2,
      "reference_count": 87,
      "is_open_access": true,
      "publication_date": "2021-05-25",
      "tldr": "This work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "link-stealing",
        "GNN-privacy"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2105.12049"
    },
    {
      "paper_id": "seed_f53dc38c",
      "title": "Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks under Federated Learning \u2013 A Survey and Taxonomy",
      "abstract": "Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data. However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors). Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model. To address the deficiencies of existing defenses, we take a generic and completely different approach to detect poisoning (targeted and untargeted) attacks. We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights. This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions. We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognition. We demonstrate that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Hossein Fereidooni",
        "Alessandro Pegoraro",
        "P. Rieger",
        "A. Dmitrienko",
        "Ahmad Sadeghi"
      ],
      "author_details": [
        {
          "name": "Hossein Fereidooni",
          "h_index": 20,
          "citation_count": 2269,
          "affiliations": []
        },
        {
          "name": "Alessandro Pegoraro",
          "h_index": 5,
          "citation_count": 115,
          "affiliations": []
        },
        {
          "name": "P. Rieger",
          "h_index": 13,
          "citation_count": 1401,
          "affiliations": []
        },
        {
          "name": "A. Dmitrienko",
          "h_index": 26,
          "citation_count": 4849,
          "affiliations": []
        },
        {
          "name": "Ahmad Sadeghi",
          "h_index": 7,
          "citation_count": 610,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://arxiv.org/abs/2312.04432",
      "citation_count": 40,
      "influential_citation_count": 2,
      "reference_count": 77,
      "is_open_access": false,
      "publication_date": "2023-12-07",
      "tldr": "FreqFed is presented, a novel aggregation mechanism that transforms the model updates into the frequency domain, where it is demonstrated that FreqFed can mitigate poisoning attacks effectively with a negligible impact on the utility of the aggregated model.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "survey",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "gradient-inversion",
        "taxonomy",
        "privacy-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2209.01100",
      "title": "Group Property Inference Attacks Against Graph Neural Networks",
      "abstract": "With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet.   In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA's success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xiuling Wang",
        "Wendy Hui Wang"
      ],
      "author_details": [
        {
          "name": "Xiuling Wang",
          "h_index": 6,
          "citation_count": 125,
          "affiliations": []
        },
        {
          "name": "Wendy Hui Wang",
          "h_index": 24,
          "citation_count": 1881,
          "affiliations": []
        }
      ],
      "max_h_index": 24,
      "url": "https://arxiv.org/abs/2209.01100",
      "citation_count": 40,
      "influential_citation_count": 4,
      "reference_count": 53,
      "is_open_access": true,
      "publication_date": "2022-09-02",
      "tldr": "This work designs a set of defense mechanisms against the GPIA attacks, and demonstrates empirically that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "property-inference",
        "group-level"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2209.01100"
    },
    {
      "paper_id": "2208.12348",
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "abstract": "Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.   In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% higher success rate than Mahloujifar et al. while being 56.5x faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP's generality and effectiveness. An open-source implementation of SNAP can be found at https://github.com/johnmath/snap-sp23.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Harsh Chaudhari",
        "John Abascal",
        "Alina Oprea",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Jonathan Ullman"
      ],
      "author_details": [
        {
          "name": "Harsh Chaudhari",
          "h_index": 8,
          "citation_count": 493,
          "affiliations": []
        },
        {
          "name": "John Abascal",
          "h_index": 4,
          "citation_count": 126,
          "affiliations": []
        },
        {
          "name": "Alina Oprea",
          "h_index": 32,
          "citation_count": 9229,
          "affiliations": []
        },
        {
          "name": "Matthew Jagielski",
          "h_index": 36,
          "citation_count": 11099,
          "affiliations": []
        },
        {
          "name": "Florian Tram\u00e8r",
          "h_index": 51,
          "citation_count": 33525,
          "affiliations": [
            "ETH Z\u00fcrich"
          ]
        },
        {
          "name": "Jonathan Ullman",
          "h_index": 40,
          "citation_count": 5634,
          "affiliations": [
            "Northeastern University"
          ]
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/2208.12348",
      "citation_count": 37,
      "influential_citation_count": 6,
      "reference_count": 51,
      "is_open_access": true,
      "publication_date": "2022-08-25",
      "tldr": "An efficient property inference attack, SNAP, is designed, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-basedProperty inference attack by Mahloujifar et al.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "property-inference",
        "poisoning-assisted"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2208.12348"
    },
    {
      "paper_id": "seed_316d3721",
      "title": "Feature Inference Attack on Shapley Values",
      "abstract": "As a solution concept in cooperative game theory, Shapley value is highly\\nrecognized in model interpretability studies and widely adopted by the leading\\nMachine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and\\nIBM. However, as the Shapley value-based model interpretability methods have\\nbeen thoroughly studied, few researchers consider the privacy risks incurred by\\nShapley values, despite that interpretability and privacy are two foundations\\nof machine learning (ML) models.\\n In this paper, we investigate the privacy risks of Shapley value-based model\\ninterpretability methods using feature inference attacks: reconstructing the\\nprivate model inputs based on their Shapley value explanations. Specifically,\\nwe present two adversaries. The first adversary can reconstruct the private\\ninputs by training an attack model based on an auxiliary dataset and black-box\\naccess to the model interpretability services. The second adversary, even\\nwithout any background knowledge, can successfully reconstruct most of the\\nprivate features by exploiting the local linear correlations between the model\\ninputs and outputs. We perform the proposed attacks on the leading MLaaS\\nplatforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The\\nexperimental results demonstrate the vulnerability of the state-of-the-art\\nShapley value-based model interpretability methods used in the leading MLaaS\\nplatforms and highlight the significance and necessity of designing\\nprivacy-preserving model interpretability methods in future studies. To our\\nbest knowledge, this is also the first work that investigates the privacy risks\\nof Shapley values.\\n",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinjian Luo",
        "Yangfan Jiang",
        "X. Xiao"
      ],
      "author_details": [
        {
          "name": "Xinjian Luo",
          "h_index": 8,
          "citation_count": 408,
          "affiliations": []
        },
        {
          "name": "Yangfan Jiang",
          "h_index": 4,
          "citation_count": 78,
          "affiliations": []
        },
        {
          "name": "X. Xiao",
          "h_index": 32,
          "citation_count": 3405,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://openalex.org/W4308361263",
      "pdf_url": "https://doi.org/10.1145/3548606.3560573",
      "doi": "https://doi.org/10.1145/3548606.3560573",
      "citation_count": 33,
      "influential_citation_count": 6,
      "reference_count": 71,
      "is_open_access": true,
      "publication_date": "2022-11-07",
      "tldr": "The experimental results demonstrate the vulnerability of the state-of-the-art Shapley value-based model interpretability methods used in the leading MLaaS platforms and highlight the significance and necessity of designing privacy-preserving model interpretable methods in future studies.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "Shapley-values",
        "feature-inference",
        "XAI-attack"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560573"
    },
    {
      "paper_id": "2205.08443",
      "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
      "abstract": "In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Dario Pasquini",
        "Mathilde Raynal",
        "C. Troncoso"
      ],
      "author_details": [
        {
          "name": "Dario Pasquini",
          "h_index": 11,
          "citation_count": 624,
          "affiliations": []
        },
        {
          "name": "Mathilde Raynal",
          "h_index": 5,
          "citation_count": 85,
          "affiliations": []
        },
        {
          "name": "C. Troncoso",
          "h_index": 36,
          "citation_count": 4927,
          "affiliations": []
        }
      ],
      "max_h_index": 36,
      "url": "https://arxiv.org/abs/2205.08443",
      "citation_count": 32,
      "influential_citation_count": 8,
      "reference_count": 80,
      "is_open_access": true,
      "publication_date": "2022-05-17",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "decentralized-ML",
        "P2P-privacy"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2205.08443"
    },
    {
      "paper_id": "2205.03105",
      "title": "LPGNet: Link Private Graph Networks for Node Classification",
      "abstract": "Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than \"trivially\" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Aashish Kolluri",
        "Teodora Baluta",
        "Bryan Hooi",
        "Prateek Saxena"
      ],
      "author_details": [
        {
          "name": "Aashish Kolluri",
          "h_index": 7,
          "citation_count": 879,
          "affiliations": []
        },
        {
          "name": "Teodora Baluta",
          "h_index": 8,
          "citation_count": 314,
          "affiliations": []
        },
        {
          "name": "Bryan Hooi",
          "h_index": 1,
          "citation_count": 31,
          "affiliations": []
        },
        {
          "name": "Prateek Saxena",
          "h_index": 11,
          "citation_count": 346,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://arxiv.org/abs/2205.03105",
      "citation_count": 31,
      "influential_citation_count": 4,
      "reference_count": 72,
      "is_open_access": true,
      "publication_date": "2022-05-06",
      "tldr": "A new neural network architecture called LPGNet is presented, which provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training, and offers consistently better privacy-utility tradeoffs than DPGCN.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "link-privacy",
        "node-classification"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560705"
    },
    {
      "paper_id": "seed_440edd44",
      "title": "PPA: Preference Profiling Attack Against Federated Learning",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Chunyi Zhou",
        "Yansong Gao",
        "Anmin Fu",
        "Kai Chen",
        "Zhiyang Dai",
        "Zhi Zhang",
        "Minhui Xue",
        "Yuqing Zhang"
      ],
      "author_details": [
        {
          "name": "Chunyi Zhou",
          "h_index": 5,
          "citation_count": 402,
          "affiliations": []
        },
        {
          "name": "Yansong Gao",
          "h_index": 27,
          "citation_count": 3957,
          "affiliations": []
        },
        {
          "name": "Anmin Fu",
          "h_index": 15,
          "citation_count": 825,
          "affiliations": []
        },
        {
          "name": "Kai Chen",
          "h_index": 17,
          "citation_count": 1029,
          "affiliations": []
        },
        {
          "name": "Zhiyang Dai",
          "h_index": 3,
          "citation_count": 44,
          "affiliations": []
        },
        {
          "name": "Zhi Zhang",
          "h_index": 17,
          "citation_count": 983,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 32,
          "citation_count": 4765,
          "affiliations": []
        },
        {
          "name": "Yuqing Zhang",
          "h_index": 4,
          "citation_count": 79,
          "affiliations": []
        }
      ],
      "max_h_index": 32,
      "url": "https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_s171_paper.pdf",
      "citation_count": 29,
      "influential_citation_count": 1,
      "reference_count": 78,
      "is_open_access": true,
      "publication_date": "2022-02-10",
      "tldr": "This work proposes a new type of privacy inference attack, coined Preference Profiling Attack (PPA), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "preference-profiling",
        "FL-privacy"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2023.23171"
    },
    {
      "paper_id": "2204.06963",
      "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
      "abstract": "Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yun Shen",
        "Yufei Han",
        "Zhikun Zhang",
        "Min Chen",
        "Tingyue Yu",
        "Michael Backes",
        "Yang Zhang",
        "G. Stringhini"
      ],
      "author_details": [
        {
          "name": "Yun Shen",
          "h_index": 16,
          "citation_count": 1497,
          "affiliations": []
        },
        {
          "name": "Yufei Han",
          "h_index": 18,
          "citation_count": 950,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 17,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "Min Chen",
          "h_index": 9,
          "citation_count": 641,
          "affiliations": []
        },
        {
          "name": "Tingyue Yu",
          "h_index": 4,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 51,
          "citation_count": 14080,
          "affiliations": [
            "Department of Physics, University of Namibia",
            "Centre for Space Research, North-West University"
          ]
        },
        {
          "name": "Yang Zhang",
          "h_index": 82,
          "citation_count": 35162,
          "affiliations": []
        },
        {
          "name": "G. Stringhini",
          "h_index": 53,
          "citation_count": 10769,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://arxiv.org/abs/2204.06963",
      "citation_count": 16,
      "influential_citation_count": 1,
      "reference_count": 91,
      "is_open_access": false,
      "publication_date": "2022-04-14",
      "tldr": "This paper proposes a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes to recover edges and demonstrates the effectiveness and applicability of the attack.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "node-embedding",
        "memory-recovery"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_c09c2704",
      "title": "SoK: On Gradient Leakage in Federated Learning",
      "abstract": "Federated learning (FL) facilitates collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from shared gradients in FL, a vulnerability known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a timeline to trace their evolution and develops a systematization to uncover their inherent threats. By rethinking GIA in practical FL systems, three fundamental aspects influencing GIA's effectiveness are identified: \\textit{training setup}, \\textit{model}, and \\textit{post-processing}. Guided by these aspects, we perform extensive theoretical and empirical evaluations of SOTA GIAs across diverse settings. Our findings highlight that GIA is notably \\textit{constrained}, \\textit{fragile}, and \\textit{easily defensible}. Specifically, GIAs exhibit inherent limitations against practical local training settings. Additionally, their effectiveness is highly sensitive to the trained model, and even simple post-processing techniques applied to gradients can serve as effective defenses. Our work provides crucial insights into the limited threats of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiacheng Du",
        "Jiahui Hu",
        "Zhibo Wang",
        "Peng Sun",
        "N. Gong",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Jiacheng Du",
          "h_index": 4,
          "citation_count": 61,
          "affiliations": []
        },
        {
          "name": "Jiahui Hu",
          "h_index": 18,
          "citation_count": 1012,
          "affiliations": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Zhibo Wang",
          "h_index": 9,
          "citation_count": 238,
          "affiliations": []
        },
        {
          "name": "Peng Sun",
          "h_index": 4,
          "citation_count": 84,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 9,
          "citation_count": 269,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 14,
          "citation_count": 655,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://openalex.org/W4394642568",
      "pdf_url": "https://arxiv.org/pdf/2404.05403",
      "doi": "https://doi.org/10.48550/arxiv.2404.05403",
      "citation_count": 15,
      "influential_citation_count": 1,
      "reference_count": 0,
      "is_open_access": false,
      "publication_date": "2024-04-08",
      "tldr": "This work starts with a survey of GIAs that establishes a timeline to trace their evolution and develops a systematization to uncover their inherent threats, which provides crucial insights into the limited threats of GIAs in practical FL systems.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Review"
      ],
      "paper_type": "survey",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "SoK",
        "gradient-leakage"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2312.07861",
      "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
      "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Bang Wu",
        "He Zhang",
        "Xiangwen Yang",
        "Shuo Wang",
        "Minhui Xue",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "author_details": [
        {
          "name": "Bang Wu",
          "h_index": 11,
          "citation_count": 509,
          "affiliations": []
        },
        {
          "name": "He Zhang",
          "h_index": 10,
          "citation_count": 443,
          "affiliations": []
        },
        {
          "name": "Xiangwen Yang",
          "h_index": 8,
          "citation_count": 273,
          "affiliations": []
        },
        {
          "name": "Shuo Wang",
          "h_index": 4,
          "citation_count": 40,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 3,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Shirui Pan",
          "h_index": 5,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Xingliang Yuan",
          "h_index": 4,
          "citation_count": 38,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://arxiv.org/abs/2312.07861",
      "citation_count": 13,
      "influential_citation_count": 0,
      "reference_count": 79,
      "is_open_access": false,
      "publication_date": "2023-12-13",
      "tldr": "This paper proposes a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "data-misuse-detection",
        "MLaaS"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2211.05249",
      "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
      "abstract": "Although query-based systems (QBS) have become one of the main solutions to share data anonymously, building QBSes that robustly protect the privacy of individuals contributing to the dataset is a hard problem. Theoretical solutions relying on differential privacy guarantees are difficult to implement correctly with reasonable accuracy, while ad-hoc solutions might contain unknown vulnerabilities. Evaluating the privacy provided by QBSes must thus be done by evaluating the accuracy of a wide range of privacy attacks. However, existing attacks require time and expertise to develop, need to be manually tailored to the specific systems attacked, and are limited in scope. In this paper, we develop QuerySnout (QS), the first method to automatically discover vulnerabilities in QBSes. QS takes as input a target record and the QBS as a black box, analyzes its behavior on one or more datasets, and outputs a multiset of queries together with a rule to combine answers to them in order to reveal the sensitive attribute of the target record. QS uses evolutionary search techniques based on a novel mutation operator to find a multiset of queries susceptible to lead to an attack, and a machine learning classifier to infer the sensitive attribute from answers to the queries selected. We showcase the versatility of QS by applying it to two attack scenarios, three real-world datasets, and a variety of protection mechanisms. We show the attacks found by QS to consistently equate or outperform, sometimes by a large margin, the best attacks from the literature. We finally show how QS can be extended to QBSes that require a budget, and apply QS to a simple QBS based on the Laplace mechanism. Taken together, our results show how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSes to be automatically tested \"at the pressing of a button\".",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Ana-Maria Cre\u0163u",
        "F. Houssiau",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "author_details": [
        {
          "name": "Ana-Maria Cre\u0163u",
          "h_index": 8,
          "citation_count": 238,
          "affiliations": []
        },
        {
          "name": "F. Houssiau",
          "h_index": 11,
          "citation_count": 471,
          "affiliations": []
        },
        {
          "name": "Antoine Cully",
          "h_index": 28,
          "citation_count": 3699,
          "affiliations": []
        },
        {
          "name": "Yves-Alexandre de Montjoye",
          "h_index": 15,
          "citation_count": 5175,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://arxiv.org/abs/2211.05249",
      "citation_count": 12,
      "influential_citation_count": 0,
      "reference_count": 61,
      "is_open_access": true,
      "publication_date": "2022-11-07",
      "tldr": "This paper develops QuerySnout, the first method to automatically discover vulnerabilities in query-based systems, and shows how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSs to be automatically tested \"at the pressing of a button\".",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "query-based-systems",
        "automated-discovery"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2211.05249"
    },
    {
      "paper_id": "seed_bbdae0dd",
      "title": "Protecting Label Distribution in Cross-Silo Federated Learning",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yangfan Jiang",
        "Xinjian Luo",
        "Yuncheng Wu",
        "Xiaokui Xiao",
        "Bengchin Ooi"
      ],
      "author_details": [
        {
          "name": "Yangfan Jiang",
          "h_index": 5,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Xinjian Luo",
          "h_index": 8,
          "citation_count": 408,
          "affiliations": []
        },
        {
          "name": "Yuncheng Wu",
          "h_index": 6,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Xiaokui Xiao",
          "h_index": 7,
          "citation_count": 108,
          "affiliations": []
        },
        {
          "name": "Bengchin Ooi",
          "h_index": 8,
          "citation_count": 253,
          "affiliations": []
        }
      ],
      "max_h_index": 8,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a113/1Ub23mqt0hG",
      "citation_count": 8,
      "influential_citation_count": 0,
      "reference_count": 63,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "label-distribution",
        "cross-silo"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2408.16913",
      "title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning",
      "abstract": "In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhuohang Li",
        "Andrew Lowy",
        "Jing Liu",
        "T. Koike-Akino",
        "K. Parsons",
        "Bradley Malin",
        "Ye Wang"
      ],
      "author_details": [
        {
          "name": "Zhuohang Li",
          "h_index": 15,
          "citation_count": 968,
          "affiliations": []
        },
        {
          "name": "Andrew Lowy",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Jing Liu",
          "h_index": 5,
          "citation_count": 67,
          "affiliations": []
        },
        {
          "name": "T. Koike-Akino",
          "h_index": 5,
          "citation_count": 73,
          "affiliations": []
        },
        {
          "name": "K. Parsons",
          "h_index": 23,
          "citation_count": 2294,
          "affiliations": []
        },
        {
          "name": "Bradley Malin",
          "h_index": 4,
          "citation_count": 129,
          "affiliations": []
        },
        {
          "name": "Ye Wang",
          "h_index": 5,
          "citation_count": 87,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://arxiv.org/abs/2408.16913",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 112,
      "is_open_access": true,
      "publication_date": "2024-08-29",
      "tldr": "The inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning is demonstrated, and a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures is presented.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "gradient-privacy",
        "systematic-analysis"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690304"
    },
    {
      "paper_id": "seed_22a31921",
      "title": "SoK: Gradient Inversion Attacks in Federated Learning",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Vincenzo Carletti",
        "P. Foggia",
        "Carlo Mazzocca",
        "Giuseppe Parrella",
        "M. Vento"
      ],
      "author_details": [
        {
          "name": "Vincenzo Carletti",
          "h_index": 17,
          "citation_count": 944,
          "affiliations": []
        },
        {
          "name": "P. Foggia",
          "h_index": 38,
          "citation_count": 8591,
          "affiliations": []
        },
        {
          "name": "Carlo Mazzocca",
          "h_index": 2,
          "citation_count": 63,
          "affiliations": []
        },
        {
          "name": "Giuseppe Parrella",
          "h_index": 1,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "M. Vento",
          "h_index": 47,
          "citation_count": 12438,
          "affiliations": []
        }
      ],
      "max_h_index": 47,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-carletti.pdf",
      "citation_count": 7,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "survey",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "SoK",
        "gradient-inversion"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2401.07205",
      "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
      "abstract": "With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Shiming Wang",
        "Zhe Ji",
        "Liyao Xiang",
        "Hao Zhang",
        "Xinbing Wang",
        "Cheng Zhou",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Shiming Wang",
          "h_index": 2,
          "citation_count": 7,
          "affiliations": []
        },
        {
          "name": "Zhe Ji",
          "h_index": 3,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Liyao Xiang",
          "h_index": 3,
          "citation_count": 38,
          "affiliations": []
        },
        {
          "name": "Hao Zhang",
          "h_index": 9,
          "citation_count": 2025,
          "affiliations": []
        },
        {
          "name": "Xinbing Wang",
          "h_index": 10,
          "citation_count": 275,
          "affiliations": []
        },
        {
          "name": "Cheng Zhou",
          "h_index": 17,
          "citation_count": 1159,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 9,
          "citation_count": 290,
          "affiliations": []
        }
      ],
      "max_h_index": 17,
      "url": "https://arxiv.org/abs/2401.07205",
      "citation_count": 5,
      "influential_citation_count": 1,
      "reference_count": 39,
      "is_open_access": false,
      "publication_date": "2024-01-14",
      "tldr": "Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "model-inversion-defense",
        "facial-features",
        "edge-computing"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2311.07389",
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "abstract": "Deep neural networks are normally executed in the forward direction. However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks. Adversaries can exploit this capability to hide rogue models within seemingly legitimate models. In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets. Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models. We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models. Moreover, to mitigate this threat we propose a novel approach for detecting infected models.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Guy Amit",
        "Mosh Levy",
        "Yisroel Mirsky"
      ],
      "author_details": [
        {
          "name": "Guy Amit",
          "h_index": 4,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Mosh Levy",
          "h_index": 5,
          "citation_count": 217,
          "affiliations": []
        },
        {
          "name": "Yisroel Mirsky",
          "h_index": 22,
          "citation_count": 4807,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://arxiv.org/abs/2311.07389",
      "citation_count": 5,
      "influential_citation_count": 2,
      "reference_count": 67,
      "is_open_access": false,
      "publication_date": "2023-11-13",
      "tldr": "This work focuses on the data exfiltration attack and shows that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "dataset-stealing",
        "bidirectional-training",
        "rogue-model"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2501.10985",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nianfeng Tzeng"
      ],
      "author_details": [
        {
          "name": "Jiadong Lou",
          "h_index": 10,
          "citation_count": 263,
          "affiliations": []
        },
        {
          "name": "Xu Yuan",
          "h_index": 2,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Rui Zhang",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Xingliang Yuan",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Neil Gong",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Nianfeng Tzeng",
          "h_index": 4,
          "citation_count": 82,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://arxiv.org/abs/2501.10985",
      "citation_count": 4,
      "influential_citation_count": 0,
      "reference_count": 53,
      "is_open_access": false,
      "publication_date": "2025-01-19",
      "tldr": "This work proposes a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy and achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "link-stealing-defense",
        "training-graph"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2406.14114",
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "abstract": "Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shu Wang",
        "Kun Sun",
        "Yan Zhai"
      ],
      "author_details": [
        {
          "name": "Shu Wang",
          "h_index": 8,
          "citation_count": 334,
          "affiliations": []
        },
        {
          "name": "Kun Sun",
          "h_index": 3,
          "citation_count": 22,
          "affiliations": []
        },
        {
          "name": "Yan Zhai",
          "h_index": 2,
          "citation_count": 8,
          "affiliations": []
        }
      ],
      "max_h_index": 8,
      "url": "https://arxiv.org/abs/2406.14114",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 83,
      "is_open_access": true,
      "publication_date": "2024-06-20",
      "tldr": "A dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution, is presented.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "generative",
        "llm"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "data-boundary",
        "trustworthiness",
        "GenAI"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670299"
    },
    {
      "paper_id": "seed_5a89b9a8",
      "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
      "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a target model with limited access, have gained increasing attention in recent years. However, there is currently no consensus on a formal definition of data reconstruction attacks or appropriate evaluation metrics for measuring their quality. This lack of rigorous definitions and universal metrics has hindered further advancement in this field. In this paper, we address this issue in the vision domain by proposing a unified attack taxonomy and formal definitions of data reconstruction attacks. We first propose a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity. Additionally, we leverage large language models (LLMs) as a substitute for human judgment, enabling visual evaluation with an emphasis on high-quality reconstructions. Using our proposed taxonomy and metrics, we present a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research. Empirical results, primarily from a memorization perspective, not only validate the effectiveness of our metrics but also offer valuable insights for designing new attacks.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Rui Wen",
        "Yiyong Liu",
        "Michael Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Rui Wen",
          "h_index": 10,
          "citation_count": 804,
          "affiliations": []
        },
        {
          "name": "Yiyong Liu",
          "h_index": 3,
          "citation_count": 203,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 16,
          "citation_count": 877,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 12,
          "citation_count": 598,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://openalex.org/W4417136254",
      "pdf_url": "https://arxiv.org/pdf/2506.07888",
      "doi": "https://doi.org/10.48550/arxiv.2506.07888",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 76,
      "is_open_access": false,
      "publication_date": "2025-06-09",
      "tldr": "This paper proposes a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity, and presents a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "survey",
      "domains": [],
      "model_types": [],
      "tags": [
        "SoK",
        "data-reconstruction",
        "benchmark"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_1a68fdb2",
      "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
      "abstract": "Federated learning (FL) enables collaborative model training among multiple clients without the need to expose raw data. Its ability to safeguard privacy, at the heart of FL, has recently been a hot-button debate topic. To elaborate, several studies have introduced a type of attacks known as gradient leakage attacks (GLAs), which exploit the gradients shared during training to reconstruct clients' raw data. On the flip side, some literature, however, contends no substantial privacy risk in practical FL environments due to the effectiveness of such GLAs being limited to overly relaxed conditions, such as small batch sizes and knowledge of clients' data distributions. This paper bridges this critical gap by empirically demonstrating that clients' data can still be effectively reconstructed, even within realistic FL environments. Upon revisiting GLAs, we recognize that their performance failures stem from their inability to handle the gradient matching problem. To alleviate the performance bottlenecks identified above, we develop FedLeak, which introduces two novel techniques, partial gradient matching and gradient regularization. Moreover, to evaluate the performance of FedLeak in real-world FL environments, we formulate a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices. Under this protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby underscoring the significant vulnerability in FL systems and the urgent need for more effective defense methods.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Mingyuan Fan",
        "Fuyi Wang",
        "Cen Chen",
        "Jianying Zhou"
      ],
      "author_details": [
        {
          "name": "Mingyuan Fan",
          "h_index": 2,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Fuyi Wang",
          "h_index": 2,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Cen Chen",
          "h_index": 4,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Jianying Zhou",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        }
      ],
      "max_h_index": 4,
      "url": "https://openalex.org/W4417256413",
      "pdf_url": "https://arxiv.org/pdf/2506.08435",
      "doi": "https://doi.org/10.48550/arxiv.2506.08435",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "publication_date": "2025-06-10",
      "tldr": "To alleviate the performance bottlenecks identified above, FedLeak is developed, which introduces two novel techniques, partial gradient matching and gradient regularization, and a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices is formulated.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "gradient-leakage",
        "realistic-settings"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_9139fe98",
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into daily routines, yet they raise significant privacy and safety concerns. Recent research proposes collaborative inference, which outsources the early-layer inference to ensure data locality, and introduces model safety auditing based on inner neuron patterns. Both techniques expose the LLM's Internal States (ISs), which are traditionally considered irreversible to inputs due to optimization challenges and the highly abstract representations in deep layers. In this work, we challenge this assumption by proposing four inversion attacks that significantly improve the semantic similarity and token matching rate of inverted inputs. Specifically, we first develop two white-box optimization-based attacks tailored for low-depth and high-depth ISs. These attacks avoid local minima convergence, a limitation observed in prior work, through a two-phase inversion process. Then, we extend our optimization attack under more practical black-box weight access by leveraging the transferability between the source and the derived LLMs. Additionally, we introduce a generation-based attack that treats inversion as a translation task, employing an inversion model to reconstruct inputs. Extensive evaluation of short and long prompts from medical consulting and coding assistance datasets and 6 LLMs validates the effectiveness of our inversion attacks. Notably, a 4,112-token long medical consulting prompt can be nearly perfectly inverted with 86.88 F1 token matching from the middle layer of Llama-3 model. Finally, we evaluate four practical defenses that we found cannot perfectly prevent ISs inversion and draw conclusions for future mitigation design.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tian Dong",
        "Yan Meng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Zhen Liu",
        "Haojin Zhu"
      ],
      "author_details": [
        {
          "name": "Tian Dong",
          "h_index": 4,
          "citation_count": 108,
          "affiliations": []
        },
        {
          "name": "Yan Meng",
          "h_index": 3,
          "citation_count": 31,
          "affiliations": []
        },
        {
          "name": "Shaofeng Li",
          "h_index": 13,
          "citation_count": 939,
          "affiliations": []
        },
        {
          "name": "Guoxing Chen",
          "h_index": 5,
          "citation_count": 164,
          "affiliations": []
        },
        {
          "name": "Zhen Liu",
          "h_index": 4,
          "citation_count": 68,
          "affiliations": []
        },
        {
          "name": "Haojin Zhu",
          "h_index": 6,
          "citation_count": 83,
          "affiliations": []
        }
      ],
      "max_h_index": 13,
      "url": "https://openalex.org/W4414421755",
      "pdf_url": "https://arxiv.org/pdf/2507.16372",
      "doi": "https://doi.org/10.48550/arxiv.2507.16372",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 64,
      "is_open_access": false,
      "publication_date": "2025-07-22",
      "tldr": "This work develops two white-box optimization-based attacks tailored for low-depth and high-depth ISs and introduces a generation-based attack that treats inversion as a translation task, employing an inversion model to reconstruct inputs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "internal-state-inversion",
        "collaborative-inference"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_974e8369",
      "title": "Timing Channels in Adaptive Neural Networks",
      "abstract": "Current machine learning systems offer great predictive power but also require significant computational resources.As a result, the promise of a class of optimized machine learning models, called adaptive neural networks (ADNNs), has seen recent wide appeal.These models make dynamic decisions about the amount of computation to perform based on the given input, allowing for fast predictions on \"easy\" input.While various considerations of ADNNs have been extensively researched, how these input-dependent optimizations might introduce vulnerabilities has been hitherto under-explored.Our work is the first to demonstrate and evaluate timing channels due to the optimizations of ADNNs with the capacity to leak sensitive attributes about a user's input.We empirically study six ADNNs types and demonstrate how an attacker can significantly improve their ability to infer sensitive attributes, such as class label, of another user's input from an observed timing measurement.Our results show that timing information can increase an attacker's probability of correctly inferring the attribute of the user's input by up to a factor of 9.89x.Our empirical evaluation uses four different datasets, including those containing sensitive medical and demographic information, and considers leakage across a variety of sensitive attributes of the user's input.We conclude by demonstrating how timing channels can be exploited across the public internet in two fictitious web applications -Fictitious Health Company and Fictitious HR -that make use of ADNNs for serving predictions to their clients.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Ayomide Akinsanya",
        "Tegan Brennan"
      ],
      "author_details": [
        {
          "name": "Ayomide Akinsanya",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Tegan Brennan",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        }
      ],
      "max_h_index": 1,
      "url": "https://openalex.org/W4391724747",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.24125",
      "doi": "https://doi.org/10.14722/ndss.2024.24125",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 61,
      "is_open_access": true,
      "tldr": "This work is the first to demonstrate and evaluate timing channels due to the optimizations of ADNNs with the capacity to leak sensitive attributes about a user\u2019s input and demonstrate how timing channels can be exploited across the public internet in two fictitious web applications that make use of ADNNs for serving predictions to their clients.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "timing-channel",
        "adaptive-NN",
        "side-channel"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.24125"
    },
    {
      "paper_id": "seed_5d66c988",
      "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
      "abstract": "Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and performance maintenance. The utility metric is designed to promote consistency between the gradients of key parameters associated with robust data and those derived from clients' data, thus maintaining model performance. Furthermore, the privacy metric guides the generation of robust data towards enlarging the semantic gap with clients' data. Theoretical analysis supports the effectiveness of Refiner, and empirical evaluations on multiple benchmark datasets demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Mingyuan Fan",
        "Cen Chen",
        "Chengyu Wang",
        "Wenmeng Zhou",
        "Jun Huang",
        "Ximeng Liu",
        "Wenzhong Guo"
      ],
      "author_details": [
        {
          "name": "Mingyuan Fan",
          "h_index": 7,
          "citation_count": 147,
          "affiliations": []
        },
        {
          "name": "Cen Chen",
          "h_index": 4,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Chengyu Wang",
          "h_index": 20,
          "citation_count": 1354,
          "affiliations": []
        },
        {
          "name": "Wenmeng Zhou",
          "h_index": 6,
          "citation_count": 187,
          "affiliations": []
        },
        {
          "name": "Jun Huang",
          "h_index": 24,
          "citation_count": 1742,
          "affiliations": []
        },
        {
          "name": "Ximeng Liu",
          "h_index": 12,
          "citation_count": 707,
          "affiliations": []
        },
        {
          "name": "Wenzhong Guo",
          "h_index": 8,
          "citation_count": 471,
          "affiliations": []
        }
      ],
      "max_h_index": 24,
      "url": "https://openalex.org/W4310827164",
      "pdf_url": "https://arxiv.org/pdf/2212.02042",
      "doi": "https://doi.org/10.48550/arxiv.2212.02042",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 93,
      "is_open_access": true,
      "publication_date": "2022-12-05",
      "tldr": "A novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data is explored, and empirical evaluations demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "gradient-leakage-defense",
        "data-refining"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2212.02042"
    },
    {
      "paper_id": "seed_07bac000",
      "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
      "abstract": "As machine learning (ML) technologies become more prevalent in privacy-sensitive areas like healthcare and finance, eventually incorporating sensitive information in building data-driven algorithms, it is vital to scrutinize whether these data face any privacy leakage risks. One potential threat arises from an adversary querying trained models using the public, non-sensitive attributes of entities in the training data to infer their private, sensitive attributes, a technique known as the attribute inference attack. This attack is particularly deceptive because, while it may perform poorly in predicting sensitive attributes across the entire dataset, it excels at predicting the sensitive attributes of records from a few vulnerable groups, a phenomenon known as disparate vulnerability. This paper illustrates that an adversary can take advantage of this disparity to carry out a series of new attacks, showcasing a threat level beyond previous imagination. We first develop a novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset. We then introduce two targeted variations of the attribute inference attack that can identify and exploit a vulnerable subset of the training data, marking the first instances of targeted attacks in this category, achieving significantly higher accuracy than untargeted versions. We are also the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ehsanul Kabir",
        "Lucas Craig",
        "Shagufta Mehnaz"
      ],
      "author_details": [
        {
          "name": "Ehsanul Kabir",
          "h_index": 1,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Lucas Craig",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Shagufta Mehnaz",
          "h_index": 13,
          "citation_count": 694,
          "affiliations": []
        }
      ],
      "max_h_index": 13,
      "url": "https://openalex.org/W4415981201",
      "pdf_url": "https://arxiv.org/pdf/2504.04033",
      "doi": "https://doi.org/10.48550/arxiv.2504.04033",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 39,
      "is_open_access": false,
      "publication_date": "2025-04-05",
      "tldr": "A novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset and is the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "targeted-attack",
        "disparate-privacy"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5d56148b",
      "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
      "abstract": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jinwen He",
        "Yiyang Lu",
        "Zijin Lin",
        "Kai Chen",
        "Yue Zhao"
      ],
      "author_details": [
        {
          "name": "Jinwen He",
          "h_index": 4,
          "citation_count": 166,
          "affiliations": [
            "Institute of Information Engineering, Chinese Academy of Sciences"
          ]
        },
        {
          "name": "Yiyang Lu",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Zijin Lin",
          "h_index": 3,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Kai Chen",
          "h_index": 2,
          "citation_count": 38,
          "affiliations": []
        },
        {
          "name": "Yue Zhao",
          "h_index": 6,
          "citation_count": 214,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://openalex.org/W4414684146",
      "pdf_url": "https://arxiv.org/pdf/2506.19563",
      "doi": "https://doi.org/10.48550/arxiv.2506.19563",
      "citation_count": 1,
      "influential_citation_count": 1,
      "reference_count": 54,
      "is_open_access": false,
      "publication_date": "2025-06-24",
      "tldr": "PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "privacy-breach-detection",
        "semantic-consistency"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5f19093b",
      "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning",
      "abstract": "Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xiangru Xu",
        "Zhize Li",
        "Yufei Han",
        "Bin Wang",
        "Jiqiang Liu",
        "Wei Wang"
      ],
      "author_details": [
        {
          "name": "Xiangru Xu",
          "h_index": 3,
          "citation_count": 241,
          "affiliations": []
        },
        {
          "name": "Zhize Li",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Yufei Han",
          "h_index": 6,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Bin Wang",
          "h_index": 12,
          "citation_count": 410,
          "affiliations": []
        },
        {
          "name": "Jiqiang Liu",
          "h_index": 3,
          "citation_count": 22,
          "affiliations": []
        },
        {
          "name": "Wei Wang",
          "h_index": 2,
          "citation_count": 6,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://openalex.org/W7116111594",
      "pdf_url": "https://doi.org/10.48550/arxiv.2512.15460",
      "doi": "https://doi.org/10.48550/arxiv.2512.15460",
      "citation_count": 0,
      "influential_citation_count": 0,
      "reference_count": 56,
      "is_open_access": false,
      "publication_date": "2025-12-17",
      "tldr": "This work derives a tight and computable upper bound for InvLoss and explores its implications from three perspectives, showing that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "federated-learning"
      ],
      "model_types": [],
      "tags": [
        "DRA-risk",
        "risk-assessment"
      ],
      "open_access_pdf": ""
    }
  ]
}