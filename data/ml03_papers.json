{
  "updated": "2026-01-06",
  "total": 215,
  "owasp_id": "ML03",
  "owasp_name": "Model Inversion Attack",
  "description": "Attacks that reconstruct or recover private training data from machine learning\n        models. This includes model inversion attacks that recover input features,\n        gradient leakage attacks, training data reconstruction, attribute inference,\n        and techniques to extract sensitive information about training samples from\n        model outputs or gradients. Focus is on privacy of TRAINING DATA, not the model.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "37ef78b60cb09e136e13df735fc58ade8348d671",
      "title": "Finding the PISTE: Towards Understanding Privacy Leaks in Vertical Federated Learning Systems",
      "abstract": "Vertical Federated Learning (VFL) is a collaborative learning paradigm where participants share the same sample space while splitting the feature space. In VFL, local participants host their bottom models for feature extraction and collaboratively train a classifier by exchanging intermediate results with the server owning the labels. Both local training data and bottom models contain privacy-sensitive information and are considered the intellectual property of each participant, and thus should be protected by the design of VFL. Our study exposes the fundamental susceptibility of VFL systems to privacy leaks, which arise from the collaboration between the server and clients during both training and testing. Based on our findings, we propose PISTE, a model-agnostic framework of privacy stealing attacks against VFL. PISTE delivers three privacy inference attacks, i.e., model stealing, data reconstruction, and property inference attacks on five benchmark datasets and four different model architectures. We further discuss four potential countermeasures. Experimental results show that all of them cannot prevent all three privacy stealing attacks in PISTE. In summary, our study demonstrates the inherent yet rarely uncovered vulnerability of VFL on leaking data and model privacy.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiangru Xu",
        "Wei Wang",
        "Zheng Chen",
        "Bin Wang",
        "Chao Li",
        "Li Duan",
        "Zhen Han",
        "Yufei Han"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/37ef78b60cb09e136e13df735fc58ade8348d671",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "title": "Balancing Security and Efficiency in GAI-Driven Semantic Communication: Challenges, Solutions, and Future Paths",
      "abstract": "The convergence of artificial intelligence (AI) and wireless communications has driven the emergence of semantic communication (SC), a paradigm that prioritizes context-aware semantic exchange over traditional bit-level transmission. Although enhancing efficiency and task-specific reliability, this advancing capability is accompanied by significant security challenges that remain underexplored. In this paper, we provide an overview of security challenges in SC systems, with a particular focus on the confidentiality, integrity, and availability of the wireless transmission and generative AI (GAI) models. To defend against risks of model confidentiality compromise and semantic feature leakage, we propose a solution integrating trusted execution environments (TEEs) for secure model inference and adversarial cryptography for the protection of semantics over realistic wireless channels. Test results show it achieves close-to-black-box attack resistance in model stealing effectiveness, and the BLEU scores of eavesdropping attackers are effectively reduced to below 0.1 across various SNR levels. Finally, we discuss potential open issues and solutions for enhancing the SC security, paving the way for future research in this critical area. The proposed framework demonstrates promising results in enhancing both model and data confidentiality, contributing to the development of secure SC systems for 6G networks.",
      "year": 2025,
      "venue": "IEEE Network",
      "authors": [
        "Qianyun Zhang",
        "Jiting Shi",
        "Weihao Zeng",
        "Xinyu Xu",
        "Zhenyu Guan",
        "Shufeng Li",
        "Zhijing Qin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4c167947e7fe8fd78118504627930f1935b11924",
      "title": "MER-Inspector: Assessing Model Extraction Risks from An Attack-Agnostic Perspective",
      "abstract": "Information leakage issues in machine learning-based Web applications have attracted increasing attention. While the risk of data privacy leakage has been rigorously analyzed, the theory of model function leakage, known as Model Extraction Attacks (MEAs), has not been well studied. In this paper, we are the first to understand MEAs theoretically from an attack-agnostic perspective and to propose analytical metrics for evaluating model extraction risks. By using the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a regularized kernel classification problem and then derive the fidelity gap and generalization error bounds of the attack performance. Based on these theoretical analyses, we propose a new theoretical metric called Model Recovery Complexity (MRC), which measures the distance of weight changes between the victim and surrogate models to quantify risk. Additionally, we find that victim model accuracy, which shows a strong positive correlation with model extraction risk, can serve as an empirical metric. By integrating these two metrics, we propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to compare the extraction risks of models under different model architectures by utilizing relative metric values. We conduct extensive experiments on 16 model architectures and 5 datasets. The experimental results demonstrate that the proposed metrics have a high correlation with model extraction risks, and MER-Inspector can accurately compare the extraction risks of any two models with up to 89.58%.",
      "year": 2025,
      "venue": "The Web Conference",
      "authors": [
        "Xinwei Zhang",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Bai",
        "Huadi Zheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4c167947e7fe8fd78118504627930f1935b11924",
      "pdf_url": "",
      "publication_date": "2025-04-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "title": "Adversarial Autoencoder based Model Extraction Attacks for Collaborative DNN Inference at Edge",
      "abstract": "Deep neural networks (DNNs) are influencing a wide range of applications from safety-critical to security-sensitive use cases. In many such use cases, the DNN inference process relies on distributed systems involving IoT devices and edge/cloud severs as participants where a pre-trained DNN model is partitioned/split onto multiple parts and the participants collaboratively execute them. However, often such collaboration requires dynamic DNN partitioning information to be exchanged among the participants over unsecured network or via relays/hops which can lead to novel privacy vulnerabilities. In this paper, we propose a DNN model extraction attack that exploits such vulnerabilities to not only extract the original input data, but also reconstruct the entire victim DNN model. Specifically, the proposed attack model utilizes extracted/leaked data and adversarial autoencoders to generate and train a shadow model that closely mimics the behavior of the original victim model. The proposed attack is query-free and does not require the attacker to have any prior information about the victim model and input data. Using an IoT-edge hardware testbed running collaborative DNN inference, we demonstrate the effectiveness of the proposed attack model in extracting the victim model with high levels of certainty across many realistic scenarios.",
      "year": 2025,
      "venue": "IEEE/IFIP Network Operations and Management Symposium",
      "authors": [
        "Manal Zneit",
        "Xiaojie Zhang",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
      "abstract": "Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Shang Wang",
        "Bo Liu",
        "Leo Yu Zhang",
        "Wanlei Zhou",
        "Yang Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
      "abstract": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ganghua Wang",
        "Yuhong Yang",
        "Jie Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "title": "Obfuscation for Deep Neural Networks against Model Extraction: Attack Taxonomy and Defense Optimization",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yulian Sun",
        "Vedant Bonde",
        "Li Duan",
        "Yong Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "title": "Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel",
      "abstract": "Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-world physical devices. User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive. Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning. However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level. In this paper, we introduce a generic physical side-channel attack, ScaAR, that extracts user interactions with neural networks by leveraging electromagnetic (EM) emissions of physical devices. Our proposed attack is implementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA). Experimental results demonstrate that, through the EM side channel, ScaAR can effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuoran Liu",
        "Senna van Hoek",
        "P'eter Horv'ath",
        "Dirk Lauret",
        "Xiaoyun Xu",
        "L. Batina"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6971c737329185c98d62432dcbcbd36c03c04c6b",
      "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?",
      "abstract": "Deep Neural Networks (DNNs) have attracted significant attention, and their internal models are now considered valuable intellectual assets. Extracting these internal models through access to a DNN is conceptually similar to extracting a secret key via oracle access to a block cipher. Consequently, cryptanalytic techniques, particularly differential-like attacks, have been actively explored recently. ReLU-based DNNs are the most commonly and widely deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024) assume access to exact output logits, which are usually invisible, more recent works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting, where only the final classification result (e.g.,\"dog\"or\"car\") is available to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that model extraction is feasible in polynomial time even under this restricted setting. In this paper, we first show that the assumptions underlying their attack become increasingly unrealistic as the attack-target depth grows. In practice, satisfying these assumptions requires an exponential number of queries with respect to the attack depth, implying that the attack does not always run in polynomial time. To address this critical limitation, we propose a novel attack method called CrossLayer Extraction. Instead of directly extracting the secret parameters (e.g., weights and biases) of a specific neuron, which incurs exponential cost, we exploit neuron interactions across layers to extract this information from deeper layers. This technique significantly reduces query complexity and mitigates the limitations of existing model extraction approaches.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Akira Ito",
        "Takayuki Miura",
        "Yosuke Todo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6971c737329185c98d62432dcbcbd36c03c04c6b",
      "pdf_url": "",
      "publication_date": "2025-10-08",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0c77335d4c20505a99178221b377755a5ece20a",
      "title": "Security of Approximate Neural Networks against Power Side-channel Attacks",
      "abstract": "Emerging low-energy computing technologies, in particular approximate computing, are becoming increasingly relevant in key applications. A significant use case for these technologies is reduced energy consumption in Artificial Neural Networks (ANNs), an increasingly pressing concern with the rapid growth of AI deployments. It is essential we understand the security implications of approximate computing in an ANN context before this practice becomes commonplace. In this work, we examine the test case of approximate ANN processing elements (PE) in terms of information leakage via the power side channel. We perform a weight extraction correlation Power Analysis (CPA) attack under three approximation scenarios: overclocking, voltage scaling, and circuit level bitwise approximation. We demonstrate that as the degree of approximation increases the Signal to Noise Ratio (SNR) of power traces rapidly degrades. We show that the Measurement to Disclosure (MTD) increases for all approximate techniques. An MTD of 48 under precise computing is increased to at minimum 200 (bitwise approximate circuit at $\\mathbf{2 5 \\%}$ approximation), and under some approximation scenarios $\\gt1024$. i.e. an increase in attack difficulty of at least x4 and potentially over x20. A relative Security-Power-Delay (SPD) analysis reveals that, in addition to the across the board improvement vs precise computing, voltage and clock scaling both significantly outperform approximate circuits with voltage scaling as the highest performing technique.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Aditya Japa",
        "Jack Miskelly",
        "M. O'Neill",
        "Chongyan Gu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f0c77335d4c20505a99178221b377755a5ece20a",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "title": "Catch the Star: Weight Recovery Attack Using Side-Channel Star Map Against DNN Accelerator",
      "abstract": "The rapid development of Artificial Intelligence (AI) technology must be connected to the arithmetic support of high-performance hardware. However, when the deep neural network (DNN) accelerator performs inference tasks at the edge end, the sensitive data of DNN will generate leakage through side-channel information. The adversary can recover the model structure and weight parameters of DNN by using the side-channel information, which seriously affects the protection of necessary intellectual property (IP) of DNN, so the hardware security of the DNN accelerator is critical. In the current research of Side-channel attack (SCA) for matrix multiplication units, such as systolic arrays, the linear multiplication operation leads to a more extensive weights search space for the SCA, and extracting all the weight parameters requires higher attack conditions. This article proposes a new power SCA method, which includes a Collision-Correlation Power Analysis (Collision-CPA) and Correlation-based Weight Search Algorithm (C-WSA) to address the problem. The Collision-CPA reduces the attack conditions for the SCA by building multiple Hamming Distance (HD)-based power leakage models for the systolic array. Meanwhile, the C-WSA dramatically reduces the weights search space. In addition, the concept of a Side-channel star map (SCSM) is proposed for the first time in this article, and the adversary can quickly and accurately locate the correct weight information in the SCSM. Through experiments, we recover all the weight parameters of a $3\\times 3$ systolic array based on 100000 power traces, in which the weight search space is reduced by up to 97.7%. For the DNN accelerator at the edge, especially the systolic array structure, our proposed novel SCA aligns more with practical attack scenarios, with lower attack conditions, and higher attack efficiency.",
      "year": 2025,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiang Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ec45414afec5007925db471273111da8bf389234",
      "title": "Arithmetic Masking Countermeasure to Mitigate Side-Channel-Based Model Extraction Attack on DNN Accelerator",
      "abstract": null,
      "year": 2025,
      "venue": "ACNS Workshops",
      "authors": [
        "Hirokatsu Yamasaki",
        "Kota Yoshida",
        "Yuta Fukuda",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ec45414afec5007925db471273111da8bf389234",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2c361d27c70f4b61da39732c338c54145a4cb82",
      "title": "Exploiting Power Side-Channel Vulnerabilities in XGBoost Accelerator",
      "abstract": "XGBoost (eXtreme Gradient Boosting), a widelyused decision tree algorithm, plays a crucial role in applications such as ransomware and fraud detection. While its performance is well-established, its security against model extraction on hardware platforms like Field Programmable Gate Arrays (FPGAs) has not been fully explored. In this paper, we demonstrate a significant vulnerability where sensitive model data can be leaked from an XGBoost implementation through side-channel attacks (SCAs). By analyzing variations in power consumption, we show how an attacker can infer node features within the XGBoost model, leading to the extraction of critical data. We conduct an experiment using the XGBoost accelerator FAXID on the Sakura-X platform, demonstrating a method to deduce model decisions by monitoring power consumptions. The results show that on average 367k tests are sufficient to leak sensitive values. Our findings underscore the need for improved hardware and algorithmic protections to safeguard machine learning models from these types of attacks.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Yimeng Xiao",
        "Archit Gajjar",
        "Aydin Aysu",
        "Paul Franzon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2c361d27c70f4b61da39732c338c54145a4cb82",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "86c0661fdb12bcde0709e91416510fda72f10f13",
      "title": "A Comprehensive Survey of Model Extraction Attacks: Current Trends, Defenses, and Future Directions",
      "abstract": "Model extraction attacks pose a significant threat to Machine Learning (ML) systems, especially in cloud-based services like Machine Learning as a Service (MLaaS). Attacks aim to steal proprietary models by replicating their functionality or extracting their internal parameters. This paper reviews model extraction attack types, examining existing defensive techniques and weaknesses in current defenses. Promising defense mechanisms are discussed, including adaptive privacy budgets, hybrid defense strategies, combining multiple methods, and hardwarebased security solutions. Emerging attack models like collaborative attacks in federated learning environments are explored. Future research focuses on adaptive defenses and Artificial Intelligence (AI)-driven detection methods to improve model robustness and contribute to more resilient machine learning systems.",
      "year": 2025,
      "venue": "2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)",
      "authors": [
        "Quazi Rian Hasnaine",
        "Yaodan Hu",
        "Mohamed I. Ibrahem",
        "Mostafa M. Fouda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/86c0661fdb12bcde0709e91416510fda72f10f13",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68f0633a0be1a7009c4caf765529d898b2450044",
      "title": "SONNI: Secure Oblivious Neural Network Inference",
      "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS) model, the client encrypts data using homomorphic encryption and uploads it to a server for computation. The result is then sent back to the client for decryption. It has become more and more common for the computation to be outsourced to third-party servers. In this paper we identify a weakness in this protocol that enables a completely undetectable novel model-stealing attack that we call the Silver Platter attack. This attack works even under multikey encryption that prevents a simple collusion attack to steal model parameters. We also propose a mitigation that protects privacy even in the presence of a malicious server and malicious client or model provider (majority dishonest). When compared to a state-of-the-art but small encrypted model with 32k parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while batching capability is reduced by 0.2%. Our approach uses a novel results-checking protocol that ensures the computation was performed correctly without violating honest clients' data privacy. Even with collusion between the client and the server, they are unable to steal model parameters. Additionally, the model provider cannot learn any client data if maliciously working with the server.",
      "year": 2025,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Luke Sperling",
        "Sandeep S. Kulkarni"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68f0633a0be1a7009c4caf765529d898b2450044",
      "pdf_url": "",
      "publication_date": "2025-04-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "title": "Detecting Generative Model Inversion Attacks for Protecting Intellectual Property of Deep Neural Networks",
      "abstract": "Recently, protecting the Intellectual Property (IP) of deep neural networks (DNNs) has attracted attention from researchers. This is because training DNN models can be costly especially when acquiring and labeling training data require domain expertise. DNN watermarking and fingerprinting are two techniques proposed to prevent DNN IP infringement. Although these two techniques achieve high performance on defending against previously proposed DNN stealing attacks, researchers recently show that both of them are ineffective against generative model inversion attacks. Specifically, an adversary inverts training data from well-trained DNNs and uses the inverted data to train DNNs from scratch such that DNN watermarking and fingerprinting are both bypassed. This novel model stealing strategy shows that data inverted from victim models can be effectively exploited by adversaries, which poses a new threat to the IP protection of DNNs. To combat this new threat, one potential solution is to enable defenders to prove ownership on data inverted from models being protected. If the training data of a suspected model, which can be disclosed via the judicial process, are proven to be data inverted from victim models, then IP infringement is detected. This research direction is currently underexplored. In this paper, we fill the gap in the literature to investigate countermeasures against this emerging threat. We propose a simple but effective method, called InverseDataInspector (IDI), to detect whether data are inverted from victim models. Specifically, our method first extracts features from both the inverted data and victim models. These features are then combined and used for training classifiers. Experimental results demonstrate that our method achieves high performance on detecting inverted data and also generalizes to new generative model inversion methods that are not seen when training classifiers.",
      "year": 2025,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Yiding Yu",
        "W. Zong",
        "Wenjing Su",
        "Yang-Wai Chow",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "pdf_url": "",
      "publication_date": "2025-10-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
      "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical machine learning, primarily to solve classification, regression and generative tasks. However, its rapid development raises critical security challenges in the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines adversarial threats unique to QML systems, focusing on vulnerabilities in cloud-based deployments, hybrid architectures, and quantum generative models. Key attack vectors include model stealing via transpilation or output extraction, data poisoning through quantum-specific perturbations, reverse engineering of proprietary variational quantum circuits, and backdoor attacks. Adversaries exploit noise-prone quantum hardware and insufficiently secured QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership, and functionality. Defense mechanisms leverage quantum properties to counter these threats. Noise signatures from training hardware act as non-invasive watermarks, while hardware-aware obfuscation techniques and ensemble strategies disrupt cloning attempts. Emerging solutions also adapt classical adversarial training and differential privacy to quantum settings, addressing vulnerabilities in quantum neural networks and generative architectures. However, securing QML requires addressing open challenges such as balancing noise levels for reliability and security, mitigating cross-platform attacks, and developing quantum-classical trust frameworks. This chapter summarizes recent advances in attacks and defenses, offering a roadmap for researchers and practitioners to build robust, trustworthy QML systems resilient to evolving adversarial landscapes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Archisman Ghosh",
        "Satwik Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
      "abstract": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuochen Yang",
        "Fok Kar Wai",
        "V. Thing"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "80b18d8f790620e175cde50af13e78d03ec424b8",
      "title": "The Implementation of Neural Network Encryption Algorithms for Side-Channel Attack Protection",
      "abstract": "\u00a0With the rapid development of deep learning in the field of side-channel analysis, neural network-trained encryption algorithms have demonstrated numerous advantages, providing novel ideas for resisting side-channel attacks. We implemented a bit neural network-based block encryption scheme resistant to side-channel attacks. Experimental verification shows that the complete algorithm combined with this scheme exhibits correctness, reliability, efficiency, and resistance to side-channel attacks. This scheme has two significant advantages: First, bit networks can achieve functions that multilayer perceptrons (MLPs) cannot perform. For example, using the AES encryption algorithm, we successfully reduced the column mixing network loss during MLP training from 0.25 to 0. Second, bit networks can integrate with MLPs without intermediate value leakage issues. Once combined with MLPs, the generalization capability of the AES round operation model is significantly enhanced, while ensuring that the number",
      "year": 2025,
      "venue": "Journal of Computing and Electronic Information Management",
      "authors": [
        "Bo Chen",
        "Yi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/80b18d8f790620e175cde50af13e78d03ec424b8",
      "pdf_url": "",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cb00fa94e62cb3d215a29e3bd0b1bcf02085a6ec",
      "title": "Understanding Neural Networks in Profiled Side-Channel Analysis",
      "abstract": "Side-channel analysis (SCA) capitalizes on unintentionally leaked information to extract sensitive data from cryptographic systems. Over recent years, deep learning has shown effectiveness in analyzing the diverse forms of SCA signals. However, due to the absence of a comprehensive understanding, constructing effective networks tailored for a variety of cryptographic systems becomes a considerable challenge. This paper proposes a novel methodology designed to deconstruct networks intended for SCA, with the goal of enhancing our understanding of the mechanisms by which these complex systems process diverse SCA signals. Our approach begins with a f-ANOVA-based method to pinpoint pivotal parameter amidst a plethora of adjustable ones. Thereafter, network visualization technique is harnessed to investigate the impact of variations in these key parameters. Through experiments, we have distilled principles for network formulation that accommodate the unique characteristics inherent in side-channel signals. The experimental outcomes highlight notable improvements when parameters are set according to the proposed principles.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Yimeng Chen",
        "Bo Wang",
        "Changshan Su",
        "Ao Li",
        "Gen Li",
        "Yuxing Tang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/cb00fa94e62cb3d215a29e3bd0b1bcf02085a6ec",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "title": "Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks",
      "abstract": "Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks. For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Zi Liang",
        "Haoyang Li",
        "Ronghua Li",
        "Huadi Zheng",
        "Haibo Hu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "pdf_url": "",
      "publication_date": "2025-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8cf823eb4c9309221953480a9b98c7700136c546",
      "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment",
      "abstract": "With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tushar Nayan",
        "Ziqi Zhang",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8cf823eb4c9309221953480a9b98c7700136c546",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "title": "Stealing AI Model Weights Through Covert Communication Channels",
      "abstract": "AI models are often regarded as valuable intellectual property due to the high cost of their development, the competitive advantage they provide, and the proprietary techniques involved in their creation. As a result, AI model stealing attacks pose a serious concern for AI model providers. In this work, we present a novel attack targeting wireless devices equipped with AI hardware accelerators. The attack unfolds in two phases. In the first phase, the victim's device is compromised with a hardware Trojan (HT) designed to covertly leak model weights through a hidden communication channel, without the victim realizing it. In the second phase, the adversary uses a nearby wireless device to intercept the victim's transmission frames during normal operation and incrementally reconstruct the complete weight matrix. The proposed attack is agnostic to both the AI model architecture and the hardware accelerator used. We validate our approach through a hardware-based demonstration involving four diverse AI models of varying types and sizes. We detail the design of the HT and the covert channel, highlighting their stealthy nature. Additionally, we analyze the impact of bit error rates on the reception and propose an error mitigation technique. The effectiveness of the attack is evaluated based on the accuracy of the reconstructed models with stolen weights and the time required to extract them. Finally, we explore potential defense mechanisms.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Valentin Barbaza",
        "Al\u00e1n Rodrigo D\u00edaz Rizo",
        "Hassan Aboushady",
        "Spyridon Raptis",
        "H. Stratigopoulos"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "pdf_url": "",
      "publication_date": "2025-09-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms",
      "abstract": "Online collaborative medical prediction platforms offer convenience and real-time feedback by leveraging massive electronic health records. However, growing concerns about privacy and low prediction quality can deter patient participation and doctor cooperation. In this paper, we first clarify the privacy attacks, namely attribute attacks targeting patients and model extraction attacks targeting doctors, and specify the corresponding privacy principles. We then propose a privacy-preserving mechanism and integrate it into a novel one-shot distributed learning framework, aiming to simultaneously meet both privacy requirements and prediction performance objectives. Within the framework of statistical learning theory, we theoretically demonstrate that the proposed distributed learning framework can achieve the optimal prediction performance under specific privacy requirements. We further validate the developed privacy-preserving collaborative medical prediction platform through both toy simulations and real-world data experiments.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shao-Bo Lin",
        "Xiaotong Liu",
        "Yao Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "pdf_url": "",
      "publication_date": "2025-07-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "title": "On the Interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction",
      "abstract": "Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.",
      "year": 2025,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine",
        "Rinad Akel",
        "Ihab Sbeity",
        "Silvia Giordano",
        "Marc Langheinrich",
        "Omran Ayoub"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "pdf_url": "",
      "publication_date": "2025-05-13",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "135ea890e84a64ea2b6786cc795b94e214b70050",
      "title": "FlatD: Protecting Deep Neural Network Program from Reversing Attacks",
      "abstract": "The emergence of Deep Learning compilers provides automated optimization and compilation across Deep Learning frameworks and hardware platforms, which enhances the performance of AI service and benefits the deployment to edge devices and low-power processors. However, deep neural network (DNN) programs generated by Deep Learning compilers introduce a new attack interface. They are targeted by new model extraction attacks that can fully or partially rebuild the DNN model by reversing the DNN programs. Unfortunately, no defense countermeasure is designed to hinder this kind of attack. To address the issue, we investigate all of the state-of-the-art reversing-based model extraction attacks and identify an essential component shared across the frameworks. Based on this observation, we propose FlatD, the first defense framework for DNN programs toward reversing-based model extraction attacks. FlatD manipulates and conceals the original Control Flow Graphs of DNN programs based on Control Flow Flattening. Unlike traditional Control Flow Flattening, FlatD ensures the DNN programs are challenging for attackers to recover their Control Flow Graphs and gain necessary information statically. Our evaluation shows that, compared to the traditional Control Flow Flattening (O-LLVM), FlatD provides more effective and stealthy protection to DNN programs with similar performance and lower scale.",
      "year": 2025,
      "venue": "2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
      "authors": [
        "Jinquan Zhang",
        "Zihao Wang",
        "Dinghao Wu",
        "Pei Wang",
        "Rui Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/135ea890e84a64ea2b6786cc795b94e214b70050",
      "pdf_url": "",
      "publication_date": "2025-04-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "title": "Side-Channel-Assisted Reverse-Engineering of Encrypted DNN Hardware Accelerator IP and Attack Surface Exploration",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized numerous application domains with their unparalleled performance. As the models become larger and more complex, hardware DNN accelerators are increasingly popular. Field-Programmable Gate Array (FPGA)-based DNN accelerators offer near-Application Specific Integrated Circuit (ASIC) efficiency and exceptional flexibility, establishing them as one of the primary hardware platforms for rapidly evolving deep learning implementations, particularly on edge devices. This prominence renders them lucrative targets for attackers. Existing attacks aimed at compromising the confidentiality of DNN models deployed on FPGA DNN accelerators often assume complete knowledge of the accelerators. However, this assumption does not hold for real-world, proprietary, high-performance FPGA DNN accelerators. In this study, we introduce a comprehensive and effective reverse-engineering methodology for demystifying FPGA DNN accelerator soft Intellectual Property (IP) cores. We demonstrate its application on the cutting-edge AMD-Xilinx Deep Learning Processing Unit (DPU). Our method relies on schematic analysis and, innovatively, electromagnetic (EM) side-channel analysis to reveal the data flow and scheduling of the DNN accelerators. To the best of our knowledge, this research is the first successful endeavor to reverse-engineer a commercial encrypted DNN accelerator IP. Moreover, we investigate attack surfaces exposed by the reverse-engineering findings, including the successful recovery of DNN model architectures and extraction of model parameters. These outcomes pose a significant threat to real-world commercial FPGA-DNN acceleration systems. We discuss potential countermeasures and offer recommendations for FPGA-based IP protection.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Gongye Cheng",
        "Yukui Luo",
        "Xiaolin Xu",
        "Yunsi Fei"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "faa68f195393a1ca36b840396278d971d3b6920a",
      "title": "A Priori Knowledge-Based Physics-Informed Neural Networks for Electromagnetic Inverse Scattering",
      "abstract": "Based on the physics-informed neural network (PINN) method, a two-step inverse scattering method is proposed to improve the efficiency and accuracy of the inversion in this work. The first step is to calculate the total fields and the initial solution of permittivity distribution in the domain of interest (DoI) by a traditional inversion algorithm, the distorted finite-difference-frequency-domain-based iterative method (DFIM), as a priori information for the cascaded PINNs. The second step is to use the calculated a priori information as additional parts of the data loss term in the proposed PINN framework for network training. Several typical numerical examples and one experimental example are considered to validate the proposed method. Inversion results show that the proposed method has good accuracy, efficiency, and robustness to noise. Compared with the data-driven deep learning methods in electromagnetic inversion, the proposed method belongs to an unsupervised learning framework and can handle more general problems. Compared with the traditional inverse algorithms, it is more efficient and accurate. In general, the proposed two-step method inherits the advantages of both traditional deep learning methods and inverse scattering methods. Importantly, it also establishes the bridge between traditional inverse scattering algorithms and deep learning methods.",
      "year": 2024,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Yifeng Hu",
        "Xiao-Hua Wang",
        "Huiming Zhou",
        "Lei Wang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/faa68f195393a1ca36b840396278d971d3b6920a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "title": "A realistic model extraction attack against graph neural networks",
      "abstract": null,
      "year": 2024,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hanjin Tong",
        "Wanlei Zhou"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/36d37aaf4bb16db3e5cc37c8b4ff55d4babb71ba",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9af4ee4d5255739283cdd728864168a7e6336393",
      "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Yue Teng",
        "Ke He",
        "Qiongkai Xu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9af4ee4d5255739283cdd728864168a7e6336393",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b892ac05369526432384a4cdf1d4d087f8bc45de",
      "title": "Beyond Slow Signs in High-fidelity Model Extraction",
      "abstract": "Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al. [1] further enhanced by Canales-Mart\\'inez et al. [2] for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Hanna Foerster",
        "Robert D. Mullins",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b892ac05369526432384a4cdf1d4d087f8bc45de",
      "pdf_url": "",
      "publication_date": "2024-06-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
      "abstract": "The past few years have seen an intense research interest in the practical needs of the \u201cright to be forgotten\u201d, which has motivated researchers to develop machine unlearning methods to unlearn a fraction of training data and its lineage. While existing machine unlearning methods prioritize the protection of individuals\u2019 private data, they over-look investigating the unlearned models\u2019 susceptibility to adversarial attacks and security breaches. In this work, we uncover a novel security vulnerability of machine unlearning based on the insight that adversarial vulnerabilities can be bol-stered, especially for adversarially robust models. To exploit this observed vulnerability, we pro-pose a novel attack called Adv ersarial U nlearning A ttack (AdvUA), which aims to generate a small fraction of malicious unlearning requests during the unlearning process. AdvUA causes a significant reduction of adversarial robustness in the unlearned model compared to the original model, providing an entirely new capability for adversaries that is infeasible in conventional machine learning pipelines. Notably, we also show that AdvUA can effectively enhance model stealing attacks by extracting additional decision boundary information, further emphasizing the breadth and significance of our research. We also conduct both theoretical analysis and computational complexity of AdvUA. Extensive numerical studies are performed to demonstrate the effectiveness and efficiency of the proposed attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chenxu Zhao",
        "Wei Qian",
        "Yangyi Li",
        "Aobo Chen",
        "Mengdi Huai"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhenyi Wang",
        "Yihan Wu",
        "Heng Huang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "title": "Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting",
      "abstract": "Model stealing attacks involve creating copies of machine learning models that have similar functionalities to the original model without proper authorization. Such attacks raise significant concerns about the intellectual property of the machine learning models. Nonetheless, current defense mechanisms against such attacks tend to exhibit certain drawbacks, notably in terms of utility, and robustness. For example, watermarking-based defenses require victim models to be retrained for embedding watermarks, which can potentially impact the main task performance. Moreover, other defenses, especially fingerprinting-based methods, often rely on specific samples like adversarial examples to verify ownership of the target model. These approaches might prove less robust against adaptive attacks, such as model stealing with adversarial training. It remains unclear whether normal examples, as opposed to adversarial ones, can effectively reflect the characteristics of stolen models. To tackle these challenges, we propose a novel method that leverages a neural network as a decoder to inverse the suspicious model\u2019s outputs. Inspired by model inversion attacks, we argue that this decoding process will unveil hidden patterns inherent in the original outputs of the suspicious model. Drawing from these decoding outcomes, we calculate specific metrics to determine the legitimacy of the suspicious models. We validate the efficacy of our defense technique against diverse model stealing attacks, specifically within the domain of classification tasks based on deep neural networks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Shuai Zhou",
        "Tianqing Zhu",
        "Dayong Ye",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "40d2bc169204b49a9cfa659b5398791ac5860caf",
      "title": "Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through Data-Free Model Extraction Attacks?",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yu-Lin Zhuang",
        "Chuan Shi",
        "Mengmei Zhang",
        "Jinghui Chen",
        "Lingjuan Lyu",
        "Pan Zhou"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/40d2bc169204b49a9cfa659b5398791ac5860caf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "title": "Privacy Implications of Explainable AI in Data-Driven Systems",
      "abstract": "Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.",
      "year": 2024,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "pdf_url": "",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "title": "Revisiting Black-box Ownership Verification for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data, enabling applications in various domains. Yet, GNNs are vulnerable to model extraction attacks, imposing risks to intellectual property. To mitigate model extraction attacks, model ownership verification is considered an effective method. However, throughout a series of empirical studies, we found that the existing GNN ownership verification methods either mandate unrealistic conditions or present unsatisfactory accuracy under the most practical settings\u2014the black-box setting where the verifier only requires access to the final output (e.g., posterior probability) of the target model and the suspect model.Inspired by the studies, we propose a new, black-box GNN ownership verification method that involves local independent models and shadow surrogate models to train a classifier for performing ownership verification. Our method boosts the verification accuracy by exploiting two insights: (1) We consider the overall behaviors of the target model for decision-making, better utilizing its holistic fingerprinting; (2) We enrich the fingerprinting of the target model by masking a subset of features of its training data, injecting extra information to facilitate ownership verification.To assess the effectiveness of our proposed method, we perform an intensive series of evaluations with 5 popular datasets, 5 mainstream GNN architectures, and 16 different settings. Our method achieves nearly perfect accuracy with a marginal impact on the target model in all cases, significantly outperforming the existing methods and enlarging their practicality. We also demonstrate that our method maintains robustness against adversarial attempts to evade the verification.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ruikai Zhou",
        "Kang Yang",
        "Xiuling Wang",
        "Wendy Hui Wang",
        "Jun Xu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c67af67a82170c03e7d39f2f143ef74e8367d0a4",
      "title": "Defending against model extraction attacks with OOD feature learning and decision boundary confusion",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & security",
      "authors": [
        "Chuang Liang",
        "Jie Huang",
        "Zeping Zhang",
        "Shuaishuai Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c67af67a82170c03e7d39f2f143ef74e8367d0a4",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "title": "Alignment-Aware Model Extraction Attacks on Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f2109f7f2412308738e03a60f79946cb1ad2aa7a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "title": "Knowledge Distillation-Based Model Extraction Attack using Private Counterfactual Explanations",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fatima Ezzeddine",
        "Omran Ayoub",
        "Silvia Giordano"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/94dd5cd82b6a082dff3dd697d63191325cf6830b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "title": "Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks",
      "abstract": "Deep neural network (DNN) intellectual property (IP) models must be kept undisclosed to avoid revealing trade secrets. Recent works have devised machine learning techniques that leverage on side-channel information leakage of the target platform to reverse engineer DNN architectures. However, these works fail to perform successful attacks on DNNs that have undergone performance optimizations (i.e., operator fusion) using DNN compilers, e.g., Apache tensor virtual machine (TVM). We propose a two-phase attack framework to infer the layer sequences of optimized DNNs through side-channel information leakage. In the first phase, we use a recurrent network with multihead attention components to learn the intra and interlayer fusion patterns from GPU traces of TVM-optimized DNNs, in order to accurately predict the operation distribution. The second phase uses a model to learn the run-time temporal correlations between operations and layers, which enables the prediction of layer sequence. An encoding strategy is proposed to overcome the convergence issues faced by existing learning-based methods when inferring the layer sequences of optimized DNNs. Extensive experiments show that our learning-based framework outperforms state-of-the-art DNN model extraction techniques. Our framework is also the first to effectively reverse engineer both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) using side-channel leakage.",
      "year": 2024,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Guiyuan Jiang",
        "Xinwang Liu",
        "Peilan He",
        "Siew-Kei Lam"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "befa5df856721b10c50d034594fbc7194d96e386",
      "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
      "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hanbo Huang",
        "Yihan Li",
        "Bowen Jiang",
        "Bowen Jiang",
        "Lin Liu",
        "Ruoyu Sun",
        "Zhuotao Liu",
        "Shiyu Liang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/befa5df856721b10c50d034594fbc7194d96e386",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "title": "Bio-Rollup: a new privacy protection solution for biometrics based on two-layer scalability-focused blockchain",
      "abstract": "The increased use of artificial intelligence generated content (AIGC) among vast user populations has heightened the risk of private data leaks. Effective auditing and regulation remain challenging, further compounding the risks associated with the leaks involving model parameters and user data. Blockchain technology, renowned for its decentralized consensus mechanism and tamper-resistant properties, is emerging as an ideal tool for documenting, auditing, and analyzing the behaviors of all stakeholders in machine learning as a service (MLaaS). This study centers on biometric recognition systems, addressing pressing privacy and security concerns through innovative endeavors. We conducted experiments to analyze six distinct deep neural networks, leveraging a dataset quality metric grounded in the query output space to quantify the value of the transfer datasets. This analysis revealed the impact of imbalanced datasets on training accuracy, thereby bolstering the system\u2019s capacity to detect model data thefts. Furthermore, we designed and implemented a novel Bio-Rollup scheme, seamlessly integrating technologies such as certificate authority, blockchain layer two scaling, and zero-knowledge proofs. This innovative scheme facilitates lightweight auditing through Merkle proofs, enhancing efficiency while minimizing blockchain storage requirements. Compared to the baseline approach, Bio-Rollup restores the integrity of the biometric system and simplifies deployment procedures. It effectively prevents unauthorized use through certificate authorization and zero-knowledge proofs, thus safeguarding user privacy and offering a passive defense against model stealing attacks.",
      "year": 2024,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Jian Yun",
        "Yusheng Lu",
        "Xinyang Liu",
        "Jingdan Guan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "pdf_url": "https://doi.org/10.7717/peerj-cs.2268",
      "publication_date": "2024-09-09",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "title": "MarginFinger: Controlling Generated Fingerprint Distance to Classification boundary Using Conditional GANs",
      "abstract": "Deep neural networks (DNNs) are widely employed across various domains, with their training costs making them crucial assets for model owners. However, the rise of Machine Learning as a Service has made models more accessible, but also increases the risk of leakage. Attackers can successfully steal models through internal leaks or API access, emphasizing the critical importance of protecting intellectual property. Several watermarking methods have been proposed, embedding secret watermarks of model owners into models. However, watermarking requires tampering with the model's training process to embed the watermark, which may lead to a decrease in utility. Recently, some fingerprinting techniques have emerged to generate fingerprint samples near the classification boundary to detect pirated models. Nevertheless, these methods lack distance constraints and suffer from high training costs. To address these issues, we propose to utilize conditional generative network to generate fingerprint data points, enabling a better exploration of the model's decision boundary. By incorporating margin loss during GAN training, we can control the distance between generated data points and classification boundary to ensure the robustness and uniqueness of our method. Moreover, our method does not require additional training of proxy models, enhancing the efficiency of fingerprint acquisition. To validate the effectiveness of our approach, we evaluate it on CIFAR-10 and Tiny-ImageNet, considering three types of model extraction attacks, fine-tuning, pruning, and transfer learning attacks. The results demonstrate that our method achieves ARUC values of 0.186 and 0.153 on CIFAR-10 and Tiny-ImageNet datasets, respectively, representing a remarkable improvement of 400% and 380% compared to the current leading baseline. The source code is available at https://github.com/wason981/MarginFinger.",
      "year": 2024,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Weixing Liu",
        "Sheng-hua Zhong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3652583.3658058",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "title": "Streamlining DNN Obfuscation to Defend Against Model Stealing Attacks",
      "abstract": "Side-channel-based Deep Neural Network (DNN) model stealing has become a major concern with the advent of learning-based attacks. In respond to this threat, defence mechanisms have been presented to obfuscate the DNN execution, making it difficult to infer the correlation between side-channel information and DNN architecture. However, state-of-the-art (SOTA) DNN obfuscation is time-consuming, requires expert-level changes in existing DNN compilers (e.g., Tensor Virtual Machine (TVM)), and often relies on prior knowledge of the attack models. In this work, we study the impact of various obfuscation levels on the defence effectiveness, and present a streamlined DNN obfuscation process that is extremely fast and is agnostic to any attack models. Our study reveals that by just modifying the scheduling of DNN operations on the GPU, we can achieve comparable defense performance as the SOTA in an attack agnostic manner. We also propose a simple algorithm that determines an effective scheduling configuration for mitigating DNN model stealing at a fraction of a time required by SOTA obfuscation methods. Our method can be easily integrated into existing DNN compilers as a security feature, even by non-experts, to protect their DNN against side-channel attacks.",
      "year": 2024,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Siew-Kei Lam",
        "Guiyuan Jiang",
        "Peilan He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "55414dc21b091006bf868b28008c9fc30fa38dca",
      "title": "Model Extraction Attack against On-device Deep Learning with Power Side Channel",
      "abstract": "The proliferation of on-device deep learning models in resource-constrained environments has led to significant advancements in privacy-preserving machine learning. However, the deployment of these models also introduces new security challenges, one of which is the vulnerability to model extraction attacks. In this paper, we investigate a novel attack with power side channel to extract on-device deep learning model deployed, which poses a substantial threat to on-device deep learning systems. By carefully monitoring power consumption during inference, an adversary can gain insights into the model\u2019s internal behavior, potentially compromising the model\u2019s intellectual property and sensitive data. Through experiments on a real-world embedded device (Jetson Nano) and various types of deep learning models, we demonstrate that the proposed attack can extract models with high fidelity. Based on experiments, we find that the power side channel-assisted model extraction attack can achieve high attacking success rate, up to 96.7% and 87.5% under close world and open world settings. This research sheds light on the evolving landscape of security threats in the context of on-device DL and provides valuable insights into safeguarding these models from potential adversaries.",
      "year": 2024,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Jiali Liu",
        "Han Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/55414dc21b091006bf868b28008c9fc30fa38dca",
      "pdf_url": "",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "title": "Beowulf: Mitigating Model Extraction Attacks Via Reshaping Decision Regions",
      "abstract": "Machine Learning as a Service (MLaaS) enables resource-constrained users to access well-trained models through a publicly accessible Application Programming Interface (API) on a pay-per-query basis. Nevertheless, model owners may face the potential threats of model extraction attacks where malicious users replicate valuable commercial models based on query results. Existing defenses against model extraction attacks, however, either sacrifice prediction accuracy or fail to thwart more advanced attacks. In this paper, we propose a novel model extraction defense, dubbed Beowulf 1 , which draws inspiration from theoretical findings that models with complex and narrow decision regions are difficult to be reproduced. Rather than arbitrarily altering decision regions, which may jeopardize the predictive capacity of the victim model, we introduce a dummy class, carefully synthesized using both random and adversarial noises. The random noise broadens the coverage of the dummy class, and the adversarial noise impacts decision regions near decision boundaries with normal classes. To further improve the model utility, we propose to employ data augmentation methods to seamlessly integrate the dummy class and the normal classes. Extensive evaluations on CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets demonstrate that Beowulf can significantly reduce the extraction accuracy of 6 state-of-the-art model extraction attacks by as much as 80%. Moreover, we show that Beowulf is also robust to adaptive model extraction attacks.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xueluan Gong",
        "Rubin Wei",
        "Ziyao Wang",
        "Yuchen Sun",
        "Jiawen Peng",
        "Yanjiao Chen",
        "Qian Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670267",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1a3d692b5a9eb5e94026cac9c76ff83cd9adb7d2",
      "title": "Privacy-preserving inference resistant to model extraction attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Expert systems with applications",
      "authors": [
        "Junyoung Byun",
        "Yujin Choi",
        "Jaewook Lee",
        "Saerom Park"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1a3d692b5a9eb5e94026cac9c76ff83cd9adb7d2",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "title": "GanTextKnockoff: stealing text sentiment analysis model functionality using synthetic data",
      "abstract": "Today, black-box machine learning models are often subject to extraction attacks that aim to retrieve their internal information. Black-box model extraction attacks are typically conducted by providing input data and, based on observing the output results, constructing a new model that functions equivalently to the original. This process is usually carried out by leveraging available data from public repositories or synthetic data generated by generative models. Most model extraction attack methods using synthetic data have been concentrated in the field of computer vision, with minimal research focused on model extraction in natural language processing. In this paper, we propose a method that utilizes synthetic textual data to construct a new model with high accuracy and similarity to the original black-box sentiment analysis model.",
      "year": 2024,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran",
        "Viet-Binh Do"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "pdf_url": "",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "title": "Extracting DNN Architectures via Runtime Profiling on Mobile GPUs",
      "abstract": "Deep Neural Networks (DNNs) have become invaluable intellectual property for AI providers due to advancements fueled by a decade of research and development. However, recent studies have demonstrated the effectiveness of model extraction attacks, which threaten this value by stealing DNN models. These attacks can lead to misuse of personal data, safety risks in critical systems, and the spread of misinformation. This paper explores model extraction attacks on DNN models deployed on mobile devices, using runtime profiles as a side-channel. Since mobile devices are resource constrained, DNN deployments require optimization efforts to reduce latency. The main hurdle in extracting DNN architectures in this scenario is that optimization techniques, such as operator-level and graph-level fusion, can obfuscate the association between runtime profile operators and their corresponding DNN layers, posing challenges for adversaries to accurately predict the computation performed. To overcome this, we propose a novel method analyzing GPU call profiles to identify the original DNN architecture. Our approach achieves full accuracy in extracting DNN architectures from a predefined set, even when layer information is obscured. For unseen architectures, a layer-by-layer hyperparameter extraction method guided by sub-layer patterns is introduced, also achieving high accuracy. This research achieves two firsts: 1) targeting mobile GPUs for DNN architecture extraction and 2) successfully extracting architectures from optimized models with fused layers.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Dong Hyub Kim",
        "Jonah O\u2019Brien Weiss",
        "Sandip Kundu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d853215713815135bd419b5748c99c1ec03192ad",
      "title": "A New Approach in Mitigating Adversarial Attacks on Machine Learning",
      "abstract": "Machine learning is a powerful tool that has the potential to transform many industries, and thus is open to security attacks. Such attacks on machine learning algorithms are known as adversarial attacks. Adversarial attacks are designed to deceive or mislead machine learning models by introducing malicious input data, modifying existing data, or exploiting weaknesses in the algorithms used to train the models. These attacks can be targeted, deliberate, and sophisticated, leading to serious consequences such as incorrect decision-making, data breaches, and loss of intellectual property. Poisoning attacks, evasion attacks, model stealing, and model inversion attacks are some examples of adversarial attacks. At the moment, most researchers are focusing on a defense approach to mitigate these attacks. This approach aims to create a strong defense system that can detect and respond to attacks in real-time, prevent unauthorized access to systems and data, and mitigate the impact of security breaches. Unfortunately, this approach has some disadvantages, one of which is limited effectiveness. Despite the use of multiple defense measures, determined attackers can still find ways to breach systems and access sensitive data. This is due to the nature of the defense approach, which never addresses the root of the problem and thus can lead to the repetition of such attacks. In this paper, a new approach is proposed, namely using the forensic approach. The proposed approach will investigate attacks against machine learning, identify the root cause of the attack, determine the extent of the damage, and gather information that can be used to prevent similar incidents in the future.",
      "year": 2024,
      "venue": "IEEE Symposium on Wireless Technology and Applications",
      "authors": [
        "Abomakhleb Abdulruhman I Ahmad",
        "K. A. Jalil"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d853215713815135bd419b5748c99c1ec03192ad",
      "pdf_url": "",
      "publication_date": "2024-07-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "title": "Model Extraction Attack Without Natural Images",
      "abstract": null,
      "year": 2024,
      "venue": "ACNS Workshops",
      "authors": [
        "Kota Yoshida",
        "Takeshi Fujino"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9295ae78a4a20d7d43cf84a5d68c70912df9ce6b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e1e53971438e42df0a29fa448b3398331d63f94a",
      "title": "Enabling Power Side-Channel Attack Simulation on Mixed-Signal Neural Network Accelerators",
      "abstract": "Due to the tremendous success of Deep Learning with neural networks (NNs) in recent years and the simultaneous leap of embedded, low-power devices (e.g. wearables, smart-phones, IoT, and smart sensors), enabling the inference of those NNs in power-constrained environments gave rise to specialized NN accelerators. One paradigm followed by many of those accelerators was the transition from digital domain computing towards performing operations in the analog domain, turning them from digital to mixed-signal NN accelerators. While power-efficiency and inference accuracy have been researched with increasing interest, security and protection against a side-channel attack (SCA) have not found much attention. However, side-channels pose a major security concern by allowing an attacker to steal valuable knowledge about proprietary NNs deployed on accelerators. In order to evaluate mixed-signal NNs accelerators concerning SCA robustness, its tendency to leak information through the side-channel needs investigation. In this work, we propose a methodology for enabling side-channel analysis of mixed-signal NNs accelerators, which shows reasonable accuracy in an early development stage. The approach enables the reuse of large portions of design sources for simulation and production while providing flexibility and fast development cycles for changes to the analog design.",
      "year": 2024,
      "venue": "Coins",
      "authors": [
        "Simon Wilhelmst\u00e4tter",
        "Joschua Conrad",
        "Devanshi Upadhyaya",
        "I. Polian",
        "M. Ortmanns"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e1e53971438e42df0a29fa448b3398331d63f94a",
      "pdf_url": "",
      "publication_date": "2024-07-29",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "title": "On the Security Vulnerabilities of MRAM-Based in-Memory Computing Architectures Against Model Extraction Attacks",
      "abstract": "This paper studies the security vulnerabilities of embedded nonvolatile memory (eNVM)-based in-memory computing (IMC) architectures to model extraction attacks (MEAs). These attacks allow the reconstruction of private training data from trained model parameters thereby leaking sensitive user information. The presence of analog noise in eNVM-based IMC computation suggests that they may be intrinsically robust to MEA. However, we show that this conjecture is false. Specifically, we consider the scenario where an attacker aims to retrieve model parameters via input-output query access, and propose three attacks that exploit the statistics of the IMC computation. We demonstrate the efficacy of these attacks in extracting the model parameters of the last layer of a ResNet-20 network from the bitcell array of an MRAM-based IMC prototype in 22 nm process. Employing the proposed MEAs, the attacker obtains a CIFAR-10 accuracy within 0.1 % of that of a $N=64$ dimensional, $7 \\mathrm{b} \\times 4 \\mathrm{b}$ fixed-point digital baseline. To the best of our knowledge, this is the first work to demonstrate MEAs for eNVM-based IMC on a real-life IC prototype. Our results indicate the critical importance of investigating the security vulnerabilities of IMCs in general, and eNVM-based IMCs, in particular.",
      "year": 2024,
      "venue": "International Conference on Computer Aided Design",
      "authors": [
        "Saion K. Roy",
        "Naresh R. Shanbhag"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "pdf_url": "",
      "publication_date": "2024-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "title": "Watermarking Counterfactual Explanations",
      "abstract": "Counterfactual (CF) explanations for ML model predictions provide actionable recourse recommendations to individuals adversely impacted by predicted outcomes. However, despite being preferred by end-users, CF explanations have been shown to pose significant security risks in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on the underlying proprietary ML model. To address this security challenge, we propose CFMark, a novel model-agnostic watermarking framework for detecting unauthorized model extraction attacks relying on CF explanations. CFMark involves a novel bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks using these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme. At the same time, the embedded watermark does not compromise the quality of the CF explanations. We evaluate CFMark across diverse real-world datasets, CF explanation methods, and model extraction techniques. Our empirical results demonstrate CFMark's effectiveness, achieving an F-1 score of ~0.89 in identifying unauthorized model extraction attacks using watermarked CF explanations. Importantly, this watermarking incurs only a negligible degradation in the quality of generated CF explanations (i.e., ~1.3% degradation in validity and ~1.6% in proximity). Our work establishes a critical foundation for the secure deployment of CF explanations in real-world applications.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hangzhi Guo",
        "Amulya Yadav"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "pdf_url": "",
      "publication_date": "2024-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7727e2a4116a63759d0a942b31732c2014007029",
      "title": "LDPKiT: Superimposing Remote Queries for Privacy-Preserving Local Model Training",
      "abstract": "Users of modern Machine Learning (ML) cloud services face a privacy conundrum -- on one hand, they may have concerns about sending private data to the service for inference, but on the other hand, for specialized models, there may be no alternative but to use the proprietary model of the ML service. In this work, we present LDPKiT, a framework for non-adversarial, privacy-preserving model extraction that leverages a user's private in-distribution data while bounding privacy leakage. LDPKiT introduces a novel superimposition technique that generates approximately in-distribution samples, enabling effective knowledge transfer under local differential privacy (LDP). Experiments on Fashion-MNIST, SVHN, and PathMNIST demonstrate that LDPKiT consistently improves utility while maintaining privacy, with benefits that become more pronounced at stronger noise levels. For example, on SVHN, LDPKiT achieves nearly the same inference accuracy at $\\epsilon=1.25$ as at $\\epsilon=2.0$, yielding stronger privacy guarantees with less than a 2% accuracy reduction. We further conduct sensitivity analyses to examine the effect of dataset size on performance and provide a systematic analysis of latent space representations, offering theoretical insights into the accuracy gains of LDPKiT.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Kexin Li",
        "Aastha Mehta",
        "David Lie"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7727e2a4116a63759d0a942b31732c2014007029",
      "pdf_url": "",
      "publication_date": "2024-05-25",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "32f77115199e3f31aa3c08179456a5a1b71001b5",
      "title": "A More General Electromagnetic Inverse Scattering Method Based on Physics-Informed Neural Network",
      "abstract": "Based on the computational framework of physics-informed neural networks (PINNs), an unsupervised deep learning method is developed for inverse problems, which features good accuracy, high efficiency, and good generality. When considering the case of multifrequency inversion, a frequency scale factor is introduced to address the scale difference problem brought by the different frequency terms and the occurrence of gradient explosion during the network training. In addition, to improve the efficiency and accuracy, a dynamic sampling strategy is proposed. Four numerical examples and one experimental example are considered to validate the effectiveness of the proposed method. The inversion results show that the proposed PINN method achieves good accuracy, efficiency, and generality, especially for electrically large and high-contrast scatterers. Moreover, the method shows good robustness against noise. Compared with traditional data-driven deep learning methods, the proposed method is efficient because it operates in an unsupervised manner and exhibits good generalization across different inversion tasks. Compared with traditional quantitative inverse scattering algorithms, the proposed method can overcome their limitations in dealing with extremely high-contrast or electrically large targets. In general, the proposed PINN not only inherits high inversion quality when compared with traditional deep learning methods but also has better generality than the traditional inverse scattering methods.",
      "year": 2023,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Yifeng Hu",
        "Xiao\u2010Hua Wang",
        "Huiming Zhou",
        "Lei Wang",
        "Bing-Zhong Wang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/32f77115199e3f31aa3c08179456a5a1b71001b5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "81cd9575100643a3463465ec19e90ee78e122f93",
      "title": "SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap",
      "abstract": "A crucial module of the widely applied machine learning (ML) model is the model training phase, which involves large-scale training data, often including sensitive private data. ML models trained on these sensitive data suffer from significant privacy concerns since ML models can intentionally or unintendedly leak information about training data. Adversaries can exploit this information to perform privacy attacks, including model extraction, membership inference, and model inversion. While a model extraction attack steals and replicates a trained model functionality, and membership inference infers the data sample's inclusiveness to the training set, a model inversion attack has the goal of inferring the training data sample's sensitive attribute value or reconstructing the training sample (i.e., image/audio/text). Distinct and inconsistent characteristics of model inversion attack make this attack even more challenging and consequential, opening up model inversion attack as a more prominent and increasingly expanding research paradigm. Thereby, to flourish research in this relatively underexplored model inversion domain, we conduct the first-ever systematic literature review of the model inversion attack landscape. We characterize model inversion attacks and provide a comprehensive taxonomy based on different dimensions. We illustrate foundational perspectives emphasizing methodologies and key principles of the existing attacks and defense techniques. Finally, we discuss challenges and open issues in the existing model inversion attacks, focusing on the roadmap for future research directions.",
      "year": 2023,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "S. Dibbo"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/81cd9575100643a3463465ec19e90ee78e122f93",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "040f56924f53619d73508930f66a035d62214f79",
      "title": "Explanation leaks: Explanation-guided model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "Information Sciences",
      "authors": [
        "Anli Yan",
        "Teng Huang",
        "Lishan Ke",
        "Xiaozhang Liu",
        "Qi Chen",
        "Changyu Dong"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/040f56924f53619d73508930f66a035d62214f79",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1b0e8f3727f452d8ef13950ae61c631be8956306",
      "title": "Model Extraction Attacks Revisited",
      "abstract": "Model extraction (ME) attacks represent one major threat to Machine-Learning-as-a-Service (MLaaS) platforms by \"stealing\" the functionality of confidential machine-learning models through querying black-box APIs. Over seven years have passed since ME attacks were first conceptualized in the seminal work [75]. During this period, substantial advances have been made in both ME attacks and MLaaS platforms, raising the intriguing question: How has the vulnerability of MLaaS platforms to ME attacks been evolving? In this work, we conduct an in-depth study to answer this critical question. Specifically, we characterize the vulnerability of current, mainstream MLaaS platforms to ME attacks from multiple perspectives including attack strategies, learning techniques, surrogatemodel design, and benchmark tasks. Many of our findings challenge previously reported results, suggesting emerging patterns of ME vulnerability. Further, by analyzing the vulnerability of the same MLaaS platforms using historical datasets from the past four years, we retrospectively characterize the evolution of ME vulnerability over time, leading to a set of interesting findings. Finally, we make suggestions about improving the current practice of MLaaS in terms of attack robustness. Our study sheds light on the current state of ME vulnerability in the wild and points to several promising directions for future research.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Jiacheng Liang",
        "Ren Pang",
        "Changjiang Li",
        "Ting Wang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/1b0e8f3727f452d8ef13950ae61c631be8956306",
      "pdf_url": "https://arxiv.org/pdf/2312.05386",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d2efd249eaffbf3675957444271ca97330156a1",
      "title": "Protecting Regression Models With Personalized Local Differential Privacy",
      "abstract": "The equation-solving model extraction attack is an intuitively simple but devastating attack to steal confidential information of regression models through a sufficient number of queries. Complete mitigation is difficult. Thus, the development of countermeasures is focused on degrading the attack effectiveness as much as possible without losing the model utilities. We investigate a novel personalized local differential privacy mechanism to defend against the attack. We obfuscate the model by adding high-dimensional Gaussian noise on model coefficients. Our solution can adaptively produce the noise to protect the model on the fly. We thoroughly evaluate the performance of our mechanisms using real-world datasets. The experiment shows that the proposed scheme outperforms the existing differential-privacy-enabled solution, i.e., 4 times more queries are required to achieve the same attack result. We also plan to publish the relevant codes to the community for further research.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiaoguang Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/3d2efd249eaffbf3675957444271ca97330156a1",
      "pdf_url": "",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "title": "A Threshold Implementation-Based Neural Network Accelerator With Power and Electromagnetic Side-Channel Countermeasures",
      "abstract": "With the recent advancements in machine learning (ML) theory, a lot of energy-efficient neural network (NN) accelerators have been developed. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. This work introduces a threshold implementation (TI) masking-based NN accelerator that secures model parameters and inputs against power and electromagnetic (EM) side-channel attacks. The 0.159 mm2 demonstration in 28 nm runs at 125 MHz at 0.95 V and limits the area and energy overhead to 64% and $5.5\\times $ , respectively, while demonstrating security even greater than 2M traces. The accelerator also secures model parameters through encryption and the inputs against horizontal power analysis (HPA) attacks.",
      "year": 2023,
      "venue": "IEEE Journal of Solid-State Circuits",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "pdf_url": "",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "title": "Peek into the Black-Box: Interpretable Neural Network using SAT Equations in Side-Channel Analysis",
      "abstract": "Deep neural networks (DNN) have become a significant threat to the security of cryptographic implementations with regards to side-channel analysis (SCA), as they automatically combine the leakages without any preprocessing needed, leading to a more efficient attack. However, these DNNs for SCA remain mostly black-box algorithms that are very difficult to interpret. Benamira et al. recently proposed an interpretable neural network called Truth Table Deep Convolutional Neural Network (TT-DCNN), which is both expressive and easier to interpret. In particular, a TT-DCNN has a transparent inner structure that can entirely be transformed into SAT equations after training. In this work, we analyze the SAT equations extracted from a TT-DCNN when applied in SCA context, eventually obtaining the rules and decisions that the neural networks learned when retrieving the secret key from the cryptographic primitive (i.e., exact formula). As a result, we can pinpoint the critical rules that the neural network uses to locate the exact Points of Interest (PoIs). We validate our approach first on simulated traces for higher-order masking. However, applying TT-DCNN on real traces is not straightforward. We propose a method to adapt TT-DCNN for application on real SCA traces containing thousands of sample points. Experimental validation is performed on software-based ASCADv1 and hardware-based AES_HD_ext datasets. In addition, TT-DCNN is shown to be able to learn the exact countermeasure in a best-case setting.",
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Trevor Yap",
        "Adrien Benamira",
        "S. Bhasin",
        "Thomas Peyrin"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/10276/9724",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dbe1361a697f46d111818f5e1d291f0e8ae858f3",
      "title": "Fast Bayesian Inversion of Airborne Electromagnetic Data Based on the Invertible Neural Network",
      "abstract": "The inversion of airborne electromagnetic (AEM) data suffers from severe nonuniqueness in the solution. Bayesian inference provides the means to estimate structural uncertainty with a rich suite of statistical information. However, conventional Bayesian methods are computationally demanding in nonlinear inversions, especially considering the huge volumes of observational data, and thus are not feasible in practice. In this study, we develop a fast Bayesian inversion operator based on the invertible neural network (INN) to fully explore the posterior distribution and quantitatively evaluate the model uncertainty. The INN uses a latent variable to capture the information loss during measurement and constructs bijective mappings between the AEM data and the resistivity model. We also introduce another noise variable into the INN to account for data uncertainties. Synthetic tests demonstrate that the INN can effectively recover the posterior distribution from a relatively small ensemble of predicted resistivity models whose AEM responses show a significant agreement with the true signal. We also apply the INN inversion operator to a field dataset and obtain results consistent with previous studies. The INN shows considerable adaptability to field observations and strong noise robustness. Meanwhile, the INN delivers the inversion result with posterior model distribution for 23 366 AEM time series in 20 s on a common PC. The inversion efficiency can be further improved for large datasets due to its natural parallelizability. The proposed INN method can support fast Bayesian inversion of AEM data and offer tremendous potential for near real-time uncertainty evaluation of underground structures.",
      "year": 2023,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Sihong Wu",
        "Qinghua Huang",
        "Li Zhao"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/dbe1361a697f46d111818f5e1d291f0e8ae858f3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "title": "Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhenyi Wang",
        "Li Shen",
        "Tongliang Liu",
        "Tiehang Duan",
        "Yanjun Zhu",
        "Dongling Zhan",
        "D. Doermann",
        "Mingchen Gao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "title": "Detection of Crucial Power Side Channel Data Leakage in Neural Networks",
      "abstract": "Neural network (NN) accelerators are now extensively utilized in a range of applications that need a high degree of security, such as driverless cars, NLP, and image recognition. Due to privacy issues and the high cost, hardware implementations contained within NN Propagators were often not accessible for general populace. Additionally with power and time data, accelerators also disclose critical data by electro-magnetic (EM) sided channels. Within this study, we demonstrate a side-channel information-based attack that can successfully steal models from large-scale NN accelerators deployed on real-world hardware. The use of these accelerators is widespread. The proposed method of attack consists of two distinct phases: 1) Using EM side-channel data to estimate networking's underlying architecture; 2) Using margin-dependent, attackers learning actively in estimating parameters, notably weights. Deducing the underlying network structure from EM sidechannel data. Inferring the underlying network structure from EM sidechannel data. Experimental findings demonstrate that the disclosed attack technique can be used to precisely retrieve the large-scale NN via the use of EM side-channel information leaking. Overall, our attack shows how critical it is to conceal electromagnetic (EM) traces for massive NN accelerators in practical settings.",
      "year": 2023,
      "venue": "International Telecommunication Networks and Applications Conference",
      "authors": [
        "A. A. Ahmed",
        "Mohammad Kamrul Hasan",
        "Nurhizam Safie Mohd Satar",
        "N. Nafi",
        "A. Aman",
        "S. Islam",
        "Saif Aamer Fadhil"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "pdf_url": "",
      "publication_date": "2023-11-29",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "title": "Hardware-Software Co-design for Side-Channel Protected Neural Network Inference",
      "abstract": "Physical side-channel attacks are a major threat to stealing confidential data from devices. There has been a recent surge in such attacks on edge machine learning (ML) hardware to extract the model parameters. Consequently, there has also been work, although limited, on building corresponding defenses against such attacks. Current solutions take either fully software-or fully hardware-centric approaches, which are limited in performance and flexibility, respectively. In this paper, we propose the first hardware-software co-design solution for building side-channel-protected ML hardware. Our solution targets edge devices and addresses both performance and flexibility needs. To that end, we develop a secure RISCV-based coprocessor design that can execute a neural network implemented in C/C++. Our coprocessor uses masking to execute various neural network operations like weighted summations, activation functions, and output layer computation in a sidechannel secure fashion. We extend the original RV32I instruction set with custom instructions to control the masking gadgets inside the secure coprocessor. We further use the custom instructions to implement easy-to-use APIs that are exposed to the end-user as a shared library. Finally, we demonstrate the empirical sidechannel security of the design up to 1M traces.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Avinash L. Varna",
        "Raghavan Kumar",
        "Aydin Aysu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c65feaf2681a5676204a43101edff4d897d37ffb",
      "title": "Explanation-based data-free model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "World wide web (Bussum)",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Hongyang Yan",
        "Xiaozhang Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c65feaf2681a5676204a43101edff4d897d37ffb",
      "pdf_url": "",
      "publication_date": "2023-06-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
      "abstract": "Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanmin Tang",
        "Jing Yu",
        "Keke Gai",
        "Xiangyang Qu",
        "Yue Hu",
        "Gang Xiong",
        "Qi Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "title": "Holistic Implicit Factor Evaluation of Model Extraction Attacks",
      "abstract": "Model extraction attacks (MEAs) allow adversaries to replicate a surrogate model analogous to the target model's decision pattern. While several attacks and defenses have been studied in-depth, the underlying reasons behind our susceptibility to them often remain unclear. Analyzing these implication influence factors helps to promote secure deep learning (DL) systems, it requires studying extraction attacks in various scenarios to determine the success of different attacks and the hallmarks of DLs. However, understanding, implementing, and evaluating even a single attack requires extremely high technical effort, making it impractical to study the vast number of unique extraction attack scenarios. To this end, we present a first-of-its-kind holistic evaluation of implication factors for MEAs which relies on the attack process abstracted from state-of-the-art MEAs. Specifically, we concentrate on four perspectives. we consider the impact of the task accuracy, model architecture, and robustness of the target model on MEAs, as well as the impact of the model architecture of the surrogate model on MEAs. Our empirical evaluation includes an ablation study over sixteen model architectures and four image datasets. Surprisingly, our study shows that improving the robustness of the target model via adversarial training is more vulnerable to model extraction attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Anli Yan",
        "Hongyang Yan",
        "Li Hu",
        "Xiaozhang Liu",
        "Teng Huang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "51e6982bb24b76d018d96432e032703e7ea35ef4",
      "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against Decision Tree Models",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/51e6982bb24b76d018d96432e032703e7ea35ef4",
      "pdf_url": "http://arxiv.org/pdf/2302.02162",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "title": "MERCURY: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator",
      "abstract": "DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks. Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents MERCURY, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN accelerator. The key insight of MERCURY is to model the side-channel extraction process as a sequence-to-sequence problem. The adversary can leverage a time-to-digital converter (TDC) to remotely collect the power trace of the target model\u2019s inference. Then he uses a learning model to automatically recover the architecture details of the victim model from the power trace without any prior knowledge. The adversary can further use the attention mechanism to localize the leakage points that contribute most to the attack. Evaluation results indicate that MERCURY can keep the error rate of model extraction below 1%.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Technology",
      "authors": [
        "Xi-ai Yan",
        "Xiaoxuan Lou",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Chip-Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/171839/2/_DR_NTU_An_Automated_Remote_Side_channel_Attack_to_FPGA_based_DNN_Accelerators.pdf",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "148adb6df70218017aba770047cacb3c9e745411",
      "title": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks",
      "abstract": "Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096 neurons in VGG-19, the overheads are between 2.8% and 11%.",
      "year": 2023,
      "venue": "International Conference on Cyber Security Cryptography and Machine Learning",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/148adb6df70218017aba770047cacb3c9e745411",
      "pdf_url": "http://arxiv.org/pdf/2303.18132",
      "publication_date": "2023-03-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "title": "Defense Against Model Extraction Attacks on Recommender Systems",
      "abstract": "The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.",
      "year": 2023,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Sixiao Zhang",
        "Hongzhi Yin",
        "Hongxu Chen",
        "Cheng Long"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635751",
      "publication_date": "2023-10-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "title": "Model Extraction Attacks on Split Federated Learning",
      "abstract": "Federated Learning (FL) is a popular collaborative learning scheme involving multiple clients and a server. FL focuses on protecting clients' data but turns out to be highly vulnerable to Intellectual Property (IP) threats. Since FL periodically collects and distributes the model parameters, a free-rider can download the latest model and thus steal model IP. Split Federated Learning (SFL), a recent variant of FL that supports training with resource-constrained clients, splits the model into two, giving one part of the model to clients (client-side model), and the remaining part to the server (server-side model). Thus SFL prevents model leakage by design. Moreover, by blocking prediction queries, it can be made resistant to advanced IP threats such as traditional Model Extraction (ME) attacks. While SFL is better than FL in terms of providing IP protection, it is still vulnerable. In this paper, we expose the vulnerability of SFL and show how malicious clients can launch ME attacks by querying the gradient information from the server side. We propose five variants of ME attack which differs in the gradient usage as well as in the data assumptions. We show that under practical cases, the proposed ME attacks work exceptionally well for SFL. For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90% accuracy with less than 2% accuracy degradation with VGG-11 on CIFAR-10.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jingtao Li",
        "A. S. Rakin",
        "Xing Chen",
        "Li Yang",
        "Zhezhi He",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "pdf_url": "http://arxiv.org/pdf/2303.08581",
      "publication_date": "2023-03-13",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "title": "AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the decision-making processes of AI models. However, the data used for such explanations can pose security and privacy risks. Existing literature identifies attacks on machine learning models, including membership inference, model inversion, and model extraction attacks. These attacks target either the model or the training data, depending on the settings and parties involved. XAI tools can increase the vulnerability of model extraction attacks, which is a concern when model owners prefer black-box access, thereby keeping model parameters and architecture private. To exploit this risk, we propose AUTOLYCUS, a novel retraining (learning) based model extraction attack framework against interpretable models under black-box settings. As XAI tools, we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley values (SHAP) to infer decision boundaries and create surrogate models that replicate the functionality of the target model. LIME and SHAP are mainly chosen for their realistic yet information-rich explanations, coupled with their extensive adoption, simplicity, and usability. We evaluate AUTOLYCUS on six machine learning datasets, measuring the accuracy and similarity of the surrogate model to the target model. The results show that AUTOLYCUS is highly effective, requiring significantly fewer queries compared to state-of-the-art attacks, while maintaining comparable accuracy and similarity. We validate its performance and transferability on multiple interpretable ML models, including decision trees, logistic regression, naive bayes, and k-nearest neighbor. Additionally, we show the resilience of AUTOLYCUS against proposed countermeasures.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "pdf_url": "",
      "publication_date": "2023-02-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "title": "Deep-Learning Model Extraction Through Software-Based Power Side-Channel",
      "abstract": "Deep learning (DL) techniques have been increasingly applied across various applications, facing a growing number of security threats. One such threat is model extraction, an attack that steals the Intellectual Property of DL models, either by recovering the same functionality or retrieving high-fidelity models. Current model extraction methods can be categorized as learning-based or cryptanalytic, with the latter relying on model queries and computational methods to recover parameters. However, these are limited to shallow neural networks and are computationally prohibitive for deeper DL models. In this paper, we propose leveraging software-based power analysis, specifically the Intel Running Average Power Limit (RAPL) technique, for DL model extraction. RAPL allows us to measure power leakage of the most popular activation function, ReLU, through a software interface. Consequently, the ReLU branch direction can be leaked in the software power side-channel, a vulnerability common in many state-of-the-art DL frameworks. We introduce a novel methodology for model extraction Algorithm from input gradient assisted by side channel information. We implement our attack on the oneDNN framework, the most popular library on Intel processors. Compared to prior work, our model extraction, assisted by the software power side-channel, only requires 0.8% of the queries to retrieve as-layer MLP. We also successfully apply our method to a common Convolutional Neural Network (CNN) - Lenet-5. To the best of our knowledge, this is the first work that extracts CNN models with more than 5 layers based solely on queries and software.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Xiang Zhang",
        "A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "pdf_url": "",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "title": "Data-Free Model Extraction Attacks in the Context of Object Detection",
      "abstract": "A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. We find that the proposed model extraction method achieves significant results by using reasonable queries. The discovery of this object detection vulnerability will support future prospects for securing such models.",
      "year": 2023,
      "venue": "International Conference on Virtual Storytelling",
      "authors": [
        "Harshit Shah",
        "G. Aravindhan",
        "Pavan Kulkarni",
        "Yuvaraj Govidarajulu",
        "Manojkumar Somabhai Parmar"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "pdf_url": "https://arxiv.org/pdf/2308.05127",
      "publication_date": "2023-08-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "title": "Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers",
      "abstract": "Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction, the complexity of the attack is relatively low and we highlight the urgent need for practicable protections that could fit the strong memory and latency requirements of such platforms.",
      "year": 2023,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "pdf_url": "",
      "publication_date": "2023-11-02",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "761cb683cf1bf94b878f6d7527600c2c62aee796",
      "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
      "abstract": "While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yi Xie",
        "Jie Zhang",
        "Shiqian Zhao",
        "Tianwei Zhang",
        "Xiaofeng Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/761cb683cf1bf94b878f6d7527600c2c62aee796",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "title": "Scalable Scan-Chain-Based Extraction of Neural Network Models",
      "abstract": "Scan chains have greatly improved hardware testability while introducing security breaches for confidential data. Scan-chain attacks have extended their scope from cryptoprocessors to AI edge devices. The recently proposed scan-chain-based neural network (NN) model extraction attack (lCCAD 2021) made it possible to achieve fine-grained extraction and is multiple orders of magnitude more efficient both in queries and accuracy than its coarse-grained mathematical counterparts. However, both query formulation complexity and constraint solver failures increase drastically with network depth/size. We demonstrate a more powerful adversary, who is capable of improving scalability while maintaining accuracy, by relaxing high-fidelity constraints to formulate an approximate-fidelity-based layer-constrained least-squares extraction using random queries. We conduct our extraction attack on neural network inference topologies of different depths and sizes, targeting the MNIST digit recognition task. The results show that our method outperforms the scan-chain attack proposed in ICCAD 2021 by an average increase in the extracted neural network's functional accuracy of \u2248 32% and 2\u20133 orders of reduction in queries. Furthermore, we demonstrated that our attack is highly effective even in the presence of countermeasures against adversarial samples.",
      "year": 2023,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Shui Jiang",
        "S. Potluri",
        "Tsungyi Ho"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0fa016157057c5203793d328430a7d86ababd7cd",
      "title": "Voltage Scaling-Agnostic Counteraction of Side-Channel Neural Net Reverse Engineering via Machine Learning Compensation and Multi-Level Shuffling",
      "abstract": "This work proposes a voltage scaling-agnostic counteraction against neural network weight reverse engineering via side-channel attacks. Multi-level shuffling and machine learning-based dual power compensation are introduced. State-of-the-art protection ($\\gt200\\cdot 10^{6}$ MTD) is achieved at low power overhead (1.76$\\times $) and zero latency overhead.",
      "year": 2023,
      "venue": "2023 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)",
      "authors": [
        "Qiang Fang",
        "Longyang Lin",
        "Hui Zhang",
        "Tianqi Wang",
        "Massimo Alioto"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0fa016157057c5203793d328430a7d86ababd7cd",
      "pdf_url": "",
      "publication_date": "2023-06-11",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "title": "RemovalNet: DNN Fingerprint Removal Attacks",
      "abstract": "With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this article, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named <sc>RemovalNet</sc>, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the <italic>fidelity</italic>, <italic>effectiveness</italic>, and <italic>efficiency</italic> of the <sc>RemovalNet</sc> against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the <sc>RemovalNet</sc> is <italic>effective</italic>. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is <inline-formula><tex-math notation=\"LaTeX\">$\\times 100$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u00d7</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"qin-ieq1-3315064.gif\"/></alternatives></inline-formula> times higher than that of the baseline attacks, (2) the <sc>RemovalNet</sc> is <italic>efficient</italic>. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the <sc>RemovalNet</sc> saves nearly 85% of computational resources at most, (3) the <sc>RemovalNet</sc> achieves high <italic>fidelity</italic> that the created surrogate model maintains high accuracy after the DNN fingerprint removal process.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zhengguang Li",
        "Kunzhe Huang",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "pdf_url": "http://arxiv.org/pdf/2308.12319",
      "publication_date": "2023-08-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "pdf_url": "https://arxiv.org/pdf/2309.01786",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8f03f195bacd67b350450aa2a793e7b9861afb17",
      "title": "SparseLock: Securing Neural Network Models in Deep Learning Accelerators",
      "abstract": "Securing neural networks (NNs) against model extraction and parameter exfiltration attacks is an important problem primarily because modern NNs take a lot of time and resources to build and train. We observe that there are no countermeasures (CMs) against recently proposed attacks on sparse NNs and there is no single CM that effectively protects against all types of known attacks for both sparse as well as dense NNs. In this paper, we propose SparseLock, a comprehensive CM that protects against all types of attacks including some of the very recently proposed ones for which no CM exists as of today. We rely on a novel compression algorithm and binning strategy. Our security guarantees are based on the inherent hardness of bin packing and inverse bin packing problems. We also perform a battery of statistical and information theory based tests to successfully show that we leak very little information and side channels in our architecture are akin to random sources. In addition, we show a performance benefit of 47.13% over the nearest competing secure architecture.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Nivedita Shrivastava",
        "S. Sarangi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8f03f195bacd67b350450aa2a793e7b9861afb17",
      "pdf_url": "",
      "publication_date": "2023-11-05",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "title": "Theoretical Limits of Provable Security Against Model Extraction by Efficient Observational Defenses",
      "abstract": "Can we hope to provide provable security against model extraction attacks? As a step towards a theoretical study of this question, we unify and abstract a wide range of \u201cobservational\u201d model extraction defenses (OMEDs) - roughly, those that attempt to detect model extraction by analyzing the distribution over the adversary's queries. To accompany the abstract OMED, we define the notion of complete OMEDs - when benign clients can freely interact with the model - and sound OMEDs - when adversarial clients are caught and prevented from reverse engineering the model. Our formalism facilitates a simple argument for obtaining provable security against model extraction by complete and sound OMEDs, using (average-case) hardness assumptions for PAC-learning, in a way that abstracts current techniques in the prior literature. The main result of this work establishes a partial computational incompleteness theorem for the OMED: any efficient OMED for a machine learning model computable by a polynomial size decision tree that satisfies a basic form of completeness cannot satisfy soundness, unless the subexponential Learning Parity with Noise (LPN) assumption does not hold. To prove the incompleteness theorem, we introduce a class of model extraction attacks called natural Covert Learning attacks based on a connection to the Covert Learning model of Canetti and Karchmer (TCC '21), and show that such attacks circumvent any defense within our abstract mechanism in a black-box, nonadaptive way. As a further technical contribution, we extend the Covert Learning algorithm of Canetti and Karchmer to work over any \u201cconcise\u201d product distribution (albeit for juntas of a logarithmic number of variables rather than polynomial size decision trees), by showing that the technique of learning with a distributional inverter of Binnendyk et al. (ALT '22) remains viable in the Covert Learning setting.",
      "year": 2023,
      "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "title": "NNLeak: An AI-Oriented DNN Model Extraction Attack through Multi-Stage Side Channel Analysis",
      "abstract": "Side channel analysis (SCA) attacks have become emerging threats to AI algorithms and deep neural network (DNN) models. However, most existing SCA attacks focus on extracting models deployed on embedded devices, such as microcontrollers. Accurate SCA attacks on extracting DNN models deployed on AI accelerators are largely missing, leaving researchers with an (improper) assumption that DNNs on AI accelerators may be immune to SCA attacks due to their complexity. In this paper, we propose a novel method, namely NNLeak to extract complete DNN models on FPGA-based AI accelerators. To achieve this goal, NNLeak first exploits simple power analysis (SPA) to identify model architecture. Then a multi-stage correlation power analysis (CPA) is designed to recover model weights accurately. Finally, NNLeak determines the activation functions of DNN models through an AI-oriented classifier. The efficacy of NNLeak is validated on FPGA implementations of two DNN models, including multilayer perceptron (MLP) and LeNet. Experimental results show that NNLeak can successfully extract complete DNN models within 2000 power traces.",
      "year": 2023,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Ya Gao",
        "Haocheng Ma",
        "Mingkai Yan",
        "Jiaji He",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "title": "Stolen Risks of Models with Security Properties",
      "abstract": "Verifiable robust machine learning, as a new trend of ML security defense, enforces security properties (e.g., Lipschitzness, Monotonicity) on machine learning models and achieves satisfying accuracy-security trade-off. Such security properties identify a series of evasion strategies of ML security attackers and specify logical constraints on their effects on a classifier (e.g., the classifier is monotonically increasing along some feature dimensions). However, little has been done so far to understand the side effect of those security properties on the model privacy. In this paper, we aim at better understanding the privacy impacts on security properties of robust ML models. Particularly, we report the first measurement study to investigate the model stolen risks of robust models satisfying four security properties (i.e., LocalInvariance, Lipschitzness, SmallNeighborhood, and Monotonicity). Our findings bring to light the factors that influence model stealing attacks and defense performance on models trained with security properties. In addition, to train an ML model satisfying goals in accuracy, security, and privacy, we propose a novel technique, called BoundaryFuzz, which introduces a privacy property into verifiable robust training frameworks to defend against model stealing attacks on robust models. Experimental results demonstrate the defense effectiveness of BoundaryFuzz.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yue Qin",
        "Zhuoqun Fu",
        "Chuyun Deng",
        "Xiaojing Liao",
        "Jia Zhang",
        "Haixin Duan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "title": "Model Stealing Attacks On FHE-based Privacy-Preserving Machine Learning through Adversarial Examples",
      "abstract": null,
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9b1d486d20cb915ee42d19e556c3587298eec5d6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "title": "Adversarial machine learning in cybersecurity: Mitigating evolving threats in AI-powered defense systems",
      "abstract": "The increasing integration of artificial intelligence (AI) in cybersecurity has enhanced the ability to detect and mitigate cyber threats in real-time. However, adversarial machine learning (AML) has emerged as a significant challenge, enabling attackers to manipulate AI models and bypass security measures. This study explores the evolving landscape of AML threats and the vulnerabilities they introduce to AI-powered defense systems. The research identifies key adversarial attack techniques, including evasion, poisoning, model inversion, and model extraction, which threaten the integrity and effectiveness of AI-driven cybersecurity mechanisms. This study evaluates various mitigation strategies to address these threats, such as adversarial Training, model hardening, defensive Distillation, and hybrid AI approaches. Through experimental analysis, we assess the robustness of AI defense systems under adversarial attack and measure their effectiveness using key performance metrics, including model accuracy, false positive rates, and computational efficiency. The findings indicate that while adversarial Training improves model resilience, adaptive attack techniques continue to challenge existing defenses, necessitating continuous advancements in cybersecurity frameworks. This research highlights the need for a multi-layered security approach that integrates AI-based anomaly detection, human-AI hybrid security models, and adaptive learning techniques to counter adversarial threats effectively. Additionally, it discusses the broader implications of AML in cybersecurity, including policy considerations, ethical concerns, and future research directions. The study recommends strategies for enhancing AI-powered cyber defense systems to maintain security, reliability, and resilience against evolving adversarial threats.",
      "year": 2023,
      "venue": "World Journal of Advanced Engineering Technology and Sciences",
      "authors": [
        "Ebuka Mmaduekwe Paul",
        "Ugochukwu Mmaduekwe Stanley",
        "Joseph Darko Kessie",
        "Mukhtar Dolapo Salawudeen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "pdf_url": "",
      "publication_date": "2023-12-30",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "title": "Power Side-Channel Attacks and Defenses for Neural Network Accelerators",
      "abstract": "Neural networks are becoming increasingly utilized in a range of real-world applications, often involving privacy-sensitive or safety-critical tasks like medical image analysis or autonomous driving. Despite their usefulness, designing and training neural networks (NNs) can be costly, both in terms of financial and energy expenses [4]. Gathering and labeling training data, actual training, and fine-tuning require considerable resources. The network models themselves are also considered confidential intellectual property (IP). Additionally, the carbon footprint of model training and development has a significant impact on the environment [5].",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b244d97e5cca7db3aa81b5ccd9e46f763b685687",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "66da20e195569d1fb5f72cb52f4ebf595ec4792c",
      "title": "Multiple-model and time-sensitive dynamic active learning for recurrent graph convolutional network model extraction attacks",
      "abstract": null,
      "year": 2023,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "Zhuo Zeng",
        "Chengliang Wang",
        "Fei Ma",
        "Peifeng Wang",
        "Hongqian Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66da20e195569d1fb5f72cb52f4ebf595ec4792c",
      "pdf_url": "",
      "publication_date": "2023-07-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3c26202058ff573620e70e340c9c8cafb2094678",
      "title": "Model Extraction Attacks on DistilBERT",
      "abstract": null,
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Amro Salman",
        "Ayman Saeed",
        "Khalid Elmadani",
        "S. Babiker"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3c26202058ff573620e70e340c9c8cafb2094678",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "title": "Image Translation-Based Deniable Encryption against Model Extraction Attack",
      "abstract": "In cloud storage applications, data owners\u2019 original images are usually encrypted before being outsourced to the cloud for preserving data owners\u2019 privacy. However, in deep learning model-based image encryption methods, an adversary can conduct the model extraction attack to reveal the model parameters and thus restore the privacy information by obtaining numerous encrypted images. In this paper, we propose an image translation-based deniable encryption (ITDE) scheme to achieve encryption deniability and defend against model extraction attacks. Differing from traditional encryption methods in which encrypted images are visually meaningless, ITDE applies image translation to generate encrypted images in the form of human faces. Moreover, ITDE provides deniability for data owners to keep the encryption parameters private. To defend against model extraction attacks, the defense mechanism is introduced in our proposed ITDE to preserve deep learning models. Experimental results demonstrate the superiority of our proposed methods in terms of encryption deniability and privacy preservation.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Yiling Chen",
        "Yuanzhi Yao",
        "Nenghai Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "pdf_url": "",
      "publication_date": "2023-10-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "title": "Side-Channel Attack Analysis on In-Memory Computing Architectures",
      "abstract": "In-memory computing (IMC) systems have great potential for accelerating data-intensive tasks such as deep neural networks (DNNs). As DNN models are generally highly proprietary, the neural network architectures become valuable targets for attacks. In IMC systems, since the whole model is mapped on chip and weight memory read can be restricted, the pre-mapped DNN model acts as a \u201cblack box\u201d for users. However, the localized and stationary weight and data patterns may subject IMC systems to other attacks. In this article, we propose a side-channel attack methodology on IMC architectures. We show that it is possible to extract model architectural information from power trace measurements without any prior knowledge of the neural network. We first developed a simulation framework that can emulate the dynamic power traces of the IMC macros. We then performed side-channel leakage analysis to reverse engineer model information such as the stored layer type, layer sequence, output channel/feature size and convolution kernel size from power traces of the IMC macros. Based on the extracted information, full networks can potentially be reconstructed without any knowledge of the neural network. Finally, we discuss potential countermeasures for building IMC systems that offer resistance to these model extraction attack.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Ziyu Wang",
        "Fanruo Meng",
        "Yongmo Park",
        "J. Eshraghian",
        "Wei D. Lu"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "pdf_url": "https://arxiv.org/pdf/2209.02792",
      "publication_date": "2022-09-06",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "title": "Reverse Engineering Neural Network Folding with Remote FPGA Power Analysis",
      "abstract": "Specialized hardware accelerators in the form of FPGAs are widely being used for neural network implementations. By that, they also become the target of power analysis attacks that try to reverse engineer the embedded secret information, in the form of model parameters. However, most of these attacks assume rather simple implementations, not realistic frameworks. Layer folding is used in such accelerators to optimize the network under given area constraints with various degrees of parallel and sequential operations. In this paper, we show that folding does mislead existing power side-channel attacks on frameworks such as FINN. We show how we can extract the folding parameters successfully and use that information to subsequently also recover the number of neurons\u2013something not reliably possible without knowing the folding information. Following the methodologies of both profiling side-channel attacks and machine learning, our approach can extract the amount of neurons with 98% accuracy on a test device, compared to 44-79% accuracy based on related work under the same test conditions and datasets. Furthermore, we show how a classifier that is based on regression can detect previously unknown parameters, which has not been shown before. To verify our results under different environmental conditions, we test the target device in a climate chamber under various temperature ranges and still reach accuracies of at least 93%.",
      "year": 2022,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "pdf_url": "",
      "publication_date": "2022-05-15",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "title": "A Neural Network-Based Hybrid Framework for Least-Squares Inversion of Transient Electromagnetic Data",
      "abstract": "Inversion of large-scale time-domain transient electromagnetic (TEM) surveys is computationally expensive and time-consuming. The calculation of partial derivatives for the Jacobian matrix is by far the most computationally intensive task, as this requires calculation of a significant number of forward responses. We propose to accelerate the inversion process by predicting partial derivatives using an artificial neural network. Network training data for resistivity models for a broad range of geological settings are generated by computing partial derivatives as symmetric differences between two forward responses. Given that certain applications have larger tolerances for modeling inaccuracy and varying degrees of flexibility throughout the different phases of interpretation, we present four inversion schemes that provide a tunable balance between computational time and inversion accuracy when modeling TEM datasets. We improve speed and maintain accuracy with a hybrid framework, where the neural network derivatives are used initially and switched to full numerical derivatives in the final iterations. We also present a full neural network solution where neural network forward and derivatives are used throughout the inversion. In a least-squares inversion framework, a speedup factor exceeding 70 is obtained on the calculation of derivatives, and the inversion process is expedited ~36 times when the full neural network solution is used. Field examples show that the full nonlinear inversion and the hybrid approach gives identical results, whereas the full neural network inversion results in higher deviation but provides a reasonable indication about the overall subsurface geology.",
      "year": 2022,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "M. Asif",
        "T. Bording",
        "P. Maurya",
        "Bo Zhang",
        "G. Fiandaca",
        "D. Grombacher",
        "A. Christiansen",
        "E. Auken",
        "J. Larsen"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/83fa3f08be844c1808086a2d2ff941ee1dd58853",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "title": "Towards explainable model extraction attacks",
      "abstract": "One key factor able to boost the applications of artificial intelligence (AI) in security\u2010sensitive domains is to leverage them responsibly, which is engaged in providing explanations for AI. To date, a plethora of explainable artificial intelligence (XAI) has been proposed to help users interpret model decisions. However, given its data\u2010driven nature, the explanation itself is potentially susceptible to a high risk of exposing privacy. In this paper, we first show that the existing XAI is vulnerable to model extraction attacks and then present an XAI\u2010aware dual\u2010task model extraction attack (DTMEA). DTMEA can attack a target model with explanation services, that is, it can extract both the classification and explanation tasks of the target model. More specifically, the substitution model extracted by DTMEA is a multitask learning architecture, consisting of a sharing layer and two task\u2010specific layers for classification and explanation. To reveal which explanation technologies are more vulnerable to expose privacy information, we conduct an empirical evaluation of four major explanation types in the benchmark data set. Experimental results show that the attack accuracy of DTMEA outperforms the predicted\u2010only method with up to 1.25%, 1.53%, 9.25%, and 7.45% in MNIST, Fashion\u2010MNIST, CIFAR\u201010, and CIFAR\u2010100, respectively. By exposing the potential threats on explanation technologies, our research offers the insights to develop effective tools that are able to trade off security\u2010sensitive relationships.",
      "year": 2022,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Xiaozhang Liu",
        "Hongyang Yan",
        "Teng Huang",
        "Xianmin Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "pdf_url": "",
      "publication_date": "2022-09-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e6d21d219b1f3321ed2354c229946373d779897a",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box'' access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon. In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shawn Shan",
        "Wen-Luan Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e6d21d219b1f3321ed2354c229946373d779897a",
      "pdf_url": "https://arxiv.org/pdf/2205.10686",
      "publication_date": "2022-05-21",
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "33158daa5df8197872a06e415f2f277b026d9988",
      "title": "High-Fidelity Model Extraction Attacks via Remote Power Monitors",
      "abstract": "This paper shows the first side-channel attack on neural network (NN) IPs through a remote power monitor. We demonstrate that a remote monitor implemented with time-to-digital converters can be exploited to steal the weights from a hardware implementation of NN inference. Such an attack alleviates the need to have physical access to the target device and thus expands the attack vector to multi-tenant cloud FPGA platforms. Our results quantify the effectiveness of the attack on an FPGA implementation of NN inference and compare it to an attack with physical access. We demonstrate that it is indeed possible to extract the weights using DPA with 25000 traces if the SNR is sufficient. The paper, therefore, motivates secure virtualization-to protect the confidentiality of high-valued NN model IPs in multi-tenant execution environments, platform developers need to employ strong countermeasures against physical side-channel attacks.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Anuj Dubey",
        "Emre Karabulut",
        "Amro Awad",
        "Aydin Aysu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33158daa5df8197872a06e415f2f277b026d9988",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "title": "DNNCloak: Secure DNN Models Against Memory Side-channel Based Reverse Engineering Attacks",
      "abstract": "As deep neural networks (DNN) expand their attention into various domains and the high cost of training a model, the structure of a DNN model has become a valuable intellectual property and needs to be protected. However, reversing DNN models by exploiting side-channel leakage has been demonstrated in various ways. Even if the model is encrypted and the processing hardware units are trusted, the attacker can still extract the model\u2019s structure and critical parameters through side channels, potentially posing significant commercial risks. In this paper, we begin by analyzing representative memory side-channel attacks on DNN models and identifying the primary causes of leakage. We also find that the full encryption used to protect model parameters could add extensive overhead. Based on our observations, we propose DNNCloak, a lightweight and secure framework aiming at mitigating reverse engineering attacks on common DNN architectures. DNNCloak includes a set of obfuscation schemes that increase the difficulty of reverse-engineering the DNN structure. Additionally, DNNCloak reduces the overhead of full weights encryption with an efficient matrix permutation scheme, resulting in reduced memory access time and enhanced security against retraining attacks on the model parameters. At last, we show how DNNCloak can defend DNN models from side-channel attacks effectively, with minimal performance overhead.",
      "year": 2022,
      "venue": "ICCD",
      "authors": [
        "Yuezhi Che",
        "Rujia Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "title": "Counteract Side-Channel Analysis of Neural Networks by Shuffling",
      "abstract": "Machine learning is becoming an essential part in almost every electronic device. Implementations of neural networks are mostly targeted towards computational performance or memory footprint. Nevertheless, security is also an important part in order to keep the network secret and protect the intellectual property associated to the network. Especially, since neural network implementations are demonstrated to be vulnerable to side-channel analysis, powerful and computational cheap countermeasures are in demand. In this work, we apply a shuffling countermeasure to a microcontroller implementation of a neural network to prevent side-channel analysis. The countermeasure is effective while the computational overhead is low. We investigate the extensions necessary for our countermeasure, and how shuffling increases the effort for an attack in theory. In addition, we demonstrate the increase in effort for an attacker through experiments on real side-channel measurements. Based on the mechanism of shuffling and our experimental results, we conclude that an attack on a commonly used neural network with shuffling is no longer feasible in a reasonable amount of time.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manuel Brosch",
        "Matthias Probst",
        "G. Sigl"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ec461790ee249f9df979ac014243291e3a40794f",
      "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
      "abstract": "A variety of explanation methods have been proposed in recent years to help users gain insights into the results returned by neural networks, which are otherwise complex and opaque black-boxes. However, explanations give rise to potential side-channels that can be leveraged by an adversary for mounting attacks on the system. In particular, post-hoc explanation methods that highlight input dimensions according to their importance or relevance to the result also leak information that weakens security and privacy. In this work, we perform the first systematic characterization of the privacy and security risks arising from various popular explanation techniques. First, we propose novel explanation-guided black-box evasion attacks that lead to 10 times reduction in query count for the same success rate. We show that the adversarial advantage from explanations can be quantified as a reduction in the total variance of the estimated gradient. Second, we revisit the membership information leaked by common explanations. Contrary to observations in prior studies, via our modified attacks we show significant leakage of membership information (above 100% improvement over prior results), even in a much stricter black-box setting. Finally, we study explanation-guided model extraction attacks and demonstrate adversarial gains through a large reduction in query count.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Pengrui Quan",
        "Supriyo Chakraborty",
        "J. Jeyakumar",
        "Mani Srivastava"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/ec461790ee249f9df979ac014243291e3a40794f",
      "pdf_url": "https://arxiv.org/pdf/2206.14004",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "title": "On the Robustness of Dataset Inference",
      "abstract": "Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs. Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods. The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence. Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI (at the cost of incurring some accuracy loss) by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade. Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Sebastian Szyller",
        "Rui Zhang",
        "Jian Liu",
        "N. Asokan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "pdf_url": "http://arxiv.org/pdf/2210.13631",
      "publication_date": "2022-10-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "95e2bfb5e01dc7cd666d59247c8849171aaf4e12",
      "title": "Exploration into the Explainability of Neural Network Models for Power Side-Channel Analysis",
      "abstract": "In this work, we present a comprehensive analysis of explainability of Neural Network (NN) models in the context of power Side-Channel Analysis (SCA), to gain insight into which features or Points of Interest (PoI) contribute the most to the classification decision. Although many existing works claim state-of-the-art accuracy in recovering secret key from cryptographic implementations, it remains to be seen whether the models actually learn representations from the leakage points. In this work, we evaluated the reasoning behind the success of a NN model, by validating the relevance scores of features derived from the network to the ones identified by traditional statistical PoI selection methods. Thus, utilizing the explainability techniques as a standard validation technique for NN models is justified.",
      "year": 2022,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Anupam Golder",
        "Ashwin Bhat",
        "A. Raychowdhury"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/95e2bfb5e01dc7cd666d59247c8849171aaf4e12",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3526241.3530346",
      "publication_date": "2022-06-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4908fdc1feff153269670f0f8aac837346042553",
      "title": "Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System",
      "abstract": "The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Zhendong Wang",
        "Xiaoming Zeng",
        "Xulong Tang",
        "Danfeng Zhang",
        "Xingbo Hu",
        "Yang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4908fdc1feff153269670f0f8aac837346042553",
      "pdf_url": "http://arxiv.org/pdf/2208.13720",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "title": "A ThreshoId-ImpIementation-Based Neural-Network Accelerator Securing Model Parameters and Inputs Against Power Side-Channel Attacks",
      "abstract": "Neural network (NN) hardware accelerators are being widely deployed on low-power loT nodes for energy-efficient decision making. Embedded NN implementations can use locally stored proprietary models, and may operate over private inputs (e.g., health monitors with patient-specific biomedical classifiers [6]), which must not be disclosed. Side-channel attacks (SCA) are a major concern in embedded systems where physical access to the operating hardware can allow attackers to recover secret data by exploiting information leakage through power consumption, timing and electromagnetic emissions [1, 7, 8]. As shown in Fig. 34.3.1, SCA on embedded NN implementations can reveal the model parameters [9] as well as the inputs [10]. To address these concerns, we present an energy - efficient ASlC solution for protecting both the model parameters and the input data against power-based SCA.",
      "year": 2022,
      "venue": "IEEE International Solid-State Circuits Conference",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "pdf_url": "",
      "publication_date": "2022-02-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "title": "Model Extraction Attack and Defense on Deep Generative Models",
      "abstract": "The security issues of machine learning have aroused much attention and model extraction attack is one of them. The definition of model extraction attack is that an adversary can collect data through query access to a victim model and train a substitute model with it in order to steal the functionality of the target model. At present, most of the related work has focused on the research of model extraction attack against discriminative models while this paper pays attention to deep generative models. First, considering the difference of an adversary` goals, the attacks are taxonomized into two different types: accuracy extraction attack and fidelity extraction attack and the effect is evaluated by 1-NN accuracy. Attacks among three main types of deep generative models and the influence of the number of queries are also researched. Finally, this paper studies different defensive techniques to safeguard the models according to their architectures.",
      "year": 2022,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Sheng Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/2189/1/012024",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eb39dda2df56270599f2a28bc6433c84c1704949",
      "title": "Extracted BERT Model Leaks More Information than You Think!",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xuanli He",
        "Chen Chen",
        "L. Lyu",
        "Qiongkai Xu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eb39dda2df56270599f2a28bc6433c84c1704949",
      "pdf_url": "https://arxiv.org/pdf/2210.11735",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "title": "Play the Imitation Game: Model Extraction Attack against Autonomous Driving Localization",
      "abstract": "The security of the Autonomous Driving (AD) system has been gaining researchers\u2019 and public\u2019s attention recently. Given that AD companies have invested a huge amount of resources in developing their AD models, e.g., localization models, these models, especially their parameters, are important intellectual property and deserve strong protection. In this work, we examine whether the confidentiality of production-grade Multi-Sensor Fusion (MSF) models, in particular, Error-State Kalman Filter (ESKF), can be stolen from an outside adversary. We propose a new model extraction attack called TaskMaster that can infer the secret ESKF parameters under black-box assumption. In essence, TaskMaster trains a substitutional ESKF model to recover the parameters, by observing the input and output to the targeted AD system. To precisely recover the parameters, we combine a set of techniques, like gradient-based optimization, search-space reduction and multi-stage optimization. The evaluation result on real-world vehicle sensor dataset shows that TaskMaster is practical. For example, with 25 seconds AD sensor data for training, the substitutional ESKF model reaches centimeter-level accuracy, comparing with the ground-truth model.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Qifan Zhang",
        "Junjie Shen",
        "Mingtian Tan",
        "Zhe Zhou",
        "Zhou Li",
        "Qi Alfred Chen",
        "Haipeng Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3564625.3567977",
      "publication_date": "2022-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f74980d18e194246618eca55faad7858fe57d08c",
      "title": "Leveraging Ferroelectric Stochasticity and In-Memory Computing for DNN IP Obfuscation",
      "abstract": "With the emergence of the Internet of Things (IoT), deep neural networks (DNNs) are widely used in different domains, such as computer vision, healthcare, social media, and defense. The hardware-level architecture of a DNN can be built using an in-memory computing-based design, which is loaded with the weights of a well-trained DNN model. However, such hardware-based DNN systems are vulnerable to model stealing attacks where an attacker reverse-engineers (REs) and extracts the weights of the DNN model. In this work, we propose an energy-efficient defense technique that combines a ferroelectric field effect transistor (FeFET)-based reconfigurable physically unclonable function (PUF) with an in-memory FeFET XNOR to thwart model stealing attacks. We leverage the inherent stochasticity in the FE domains to build a PUF that helps to corrupt the neural network\u2019s (NN) weights when an adversarial attack is detected. We showcase the efficacy of the proposed defense scheme by performing experiments on graph-NNs (GNNs), a particular type of DNN. The proposed defense scheme is a first of its kind that evaluates the security of GNNs. We investigate the effect of corrupting the weights on different layers of the GNN on the accuracy degradation of the graph classification application for two specific error models of corrupting the FeFET-based PUFs and five different bioinformatics datasets. We demonstrate that our approach successfully degrades the inference accuracy of the graph classification by corrupting any layer of the GNN after a small rewrite pulse.",
      "year": 2022,
      "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
      "authors": [
        "Likhitha Mankali",
        "N. Rangarajan",
        "Swetaki Chatterjee",
        "Shubham Kumar",
        "Y. Chauhan",
        "O. Sinanoglu",
        "Hussam Amrouch"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f74980d18e194246618eca55faad7858fe57d08c",
      "pdf_url": "https://doi.org/10.1109/jxcdc.2022.3217043",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ea29c488e74411c15097b925f69ca19ea734af2f",
      "title": "Neural Network-Based Entropy: A New Metric for Evaluating Side-Channel Attacks",
      "abstract": null,
      "year": 2022,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Jiafeng Cheng",
        "Nengyuan Sun",
        "Wenrui Liu",
        "Zhao Peng",
        "Chunyang Wang",
        "Caiban Sun",
        "Yufei Wang",
        "Yijian Bi",
        "Yiming Wen",
        "Hongliu Zhang",
        "Pengcheng Zhang",
        "S. Kose",
        "Weize Yu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ea29c488e74411c15097b925f69ca19ea734af2f",
      "pdf_url": "",
      "publication_date": "2022-08-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c082d0fdc703fd06f99a93f4683416f987d335ff",
      "title": "Privacy-Preserving DNN Model Authorization against Model Theft and Feature Leakage",
      "abstract": "Today\u2019s intelligent services are built on well-trained deep neural network (DNN) models, which usually require large private datasets along with a high cost for model training. It consequently makes the model providers cherish the pre-trained DNN models and only distribute them to authorized users. However, malicious users can steal these valuable models for abuse, illegal copy and redistribution. Attackers can also extract private features from even authorized models to leak partial training datasets. They both violate privacy. Existing techniques from secure community attempt to avoid parameter leakage during model authorization but yet cannot solve privacy issues sufficiently. In this paper, we propose a privacy-preserving model authorization approach, AgAuth, to resist the aforementioned privacy threats. We devise a novel scheme called Information-Agnostic Conversion (IAC) for forwarding procedure to eliminate residual features in model parameters. Based on it, we then propose Inference-on-Ciphertext (CiFer) mechanism for DNN reasoning, which includes three stages in each forwarding. The Encrypt phase first converts the proprietary model parameters to demonstrate uniform distribution. The Forward stage per-forms forwarding function without decryption at authorized side. Specifically, this stage just computes over ciphertext. The Decrypt phase finally recovers the information-agnostic outputs to informative output tensor for real-world services. In addition, we implement a prototype and conduct extensive experiments to evaluate its performance. The qualitative and quantitative results demonstrate that our solution AgAuth is privacy-preserving to defend against model theft and feature leakage, without accuracy loss or notable performance decrease.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Qiushi Li",
        "Ju Ren",
        "Yuezhi Zhou",
        "Yaoxue Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c082d0fdc703fd06f99a93f4683416f987d335ff",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "title": "A Survey on Side-Channel-based Reverse Engineering Attacks on Deep Neural Networks",
      "abstract": "Hardware side-channels have been exploited to leak sensitive information. With the emergence of deep learning, their hardware platforms have also been scrutinized for side-channel information leakage. It has been shown that the structure, weights, and input samples of deep neural networks (DNN) can all be the victim of reverse engineering attacks that rely on side-channel information leakage. In this paper, we survey existing work on hardware side-channel-based reverse engineering attacks on DNNs as well as the countermeasures.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Yuntao Liu",
        "Michael Zuzak",
        "Daniel Xing",
        "Isaac McDaniel",
        "Priya Mittu",
        "Olsan Ozbay",
        "Abir Akib",
        "Ankur Srivastava"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "title": "HWGN2: Side-channel Protected Neural Networks through Secure and Private Function Evaluation",
      "abstract": "Recent work has highlighted the risks of intellectual property (IP) piracy of deep learning (DL) models from the side-channel leakage of DL hardware accelerators. In response, to provide side-channel leakage resiliency to DL hardware accelerators, several approaches have been proposed, mainly borrowed from the methodologies devised for cryptographic implementations. Therefore, as expected, the same challenges posed by the complex design of such countermeasures should be dealt with. This is despite the fact that fundamental cryptographic approaches, specifically secure and private function evaluation, could potentially improve the robustness against side-channel leakage. To examine this and weigh the costs and benefits, we introduce hardware garbled NN (HWGN2), a DL hardware accelerator implemented on FPGA. HWGN2 also provides NN designers with the flexibility to protect their IP in real-time applications, where hardware resources are heavily constrained, through a hardware-communication cost trade-off. Concretely, we apply garbled circuits, implemented using a MIPS architecture that achieves up to 62.5x fewer logical and 66x less memory utilization than the state-of-the-art approaches at the price of communication overhead. Further, the side-channel resiliency of HWGN2 is demonstrated by employing the test vector leakage assessment (TVLA) test against both power and electromagnetic side-channels. This is in addition to the inherent feature of HWGN2: it ensures the privacy of users' input, including the architecture of NNs. We also demonstrate a natural extension to the malicious security modeljust as a by-product of our implementation.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mohammad J. Hashemi",
        "Steffi Roy",
        "Domenic Forte",
        "F. Ganji"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bda00fec01defdb86c5d5ac165c9a1599e74d056",
      "pdf_url": "http://arxiv.org/pdf/2208.03806",
      "publication_date": "2022-08-07",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "title": "Matryoshka: Stealing Functionality of Private ML Data by Hiding Models in Model",
      "abstract": "In this paper, we present a novel insider attack called Matryoshka, which employs an irrelevant scheduled-to-publish DNN model as a carrier model for covert transmission of multiple secret models which memorize the functionality of private ML data stored in local data centers. Instead of treating the parameters of the carrier model as bit strings and applying conventional steganography, we devise a novel parameter sharing approach which exploits the learning capacity of the carrier model for information hiding. Matryoshka simultaneously achieves: (i) High Capacity -- With almost no utility loss of the carrier model, Matryoshka can hide a 26x larger secret model or 8 secret models of diverse architectures spanning different application domains in the carrier model, neither of which can be done with existing steganography techniques; (ii) Decoding Efficiency -- once downloading the published carrier model, an outside colluder can exclusively decode the hidden models from the carrier model with only several integer secrets and the knowledge of the hidden model architecture; (iii) Effectiveness -- Moreover, almost all the recovered models have similar performance as if it were trained independently on the private data; (iv) Robustness -- Information redundancy is naturally implemented to achieve resilience against common post-processing techniques on the carrier before its publishing; (v) Covertness -- A model inspector with different levels of prior knowledge could hardly differentiate a carrier model from a normal model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xudong Pan",
        "Yifan Yan",
        "Sheng Zhang",
        "Mi Zhang",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "pdf_url": "http://arxiv.org/pdf/2206.14371",
      "publication_date": "2022-06-29",
      "keywords_matched": [
        "stealing functionality"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9286db7b087e6ba805d0526aeeeccfccf540069c",
      "title": "TP-NET: Training Privacy-Preserving Deep Neural Networks under Side-Channel Power Attacks",
      "abstract": "Privacy in deep learning is receiving tremendous attention with its wide applications in industry and academics. Recent studies have shown the internal structure of a deep neural network is easily inferred via side-channel power attacks in the training process. To address this pressing privacy issue, we propose TP-NET, a novel solution for training privacy-preserving deep neural networks under side-channel power attacks. The key contribution of TP-NET is the introduction of randomness into the internal structure of a deep neural network and the training process. Specifically, the workflow of TP-NET includes three steps: First, Independent Sub-network Construction, which generates multiple independent sub-networks via randomly se-lecting nodes in each hidden layer. Second, Sub-network Random Training, which randomly trains multiple sub-networks such that power traces keep random in the temporal domain. Third, Prediction, which outputs the predictions made by the most accu-rate sub-network to achieve high classification performance. The performance of TP-NET is evaluated under side-channel power attacks. The experimental results on two benchmark datasets demonstrate that TP-NET decreases the inference accuracy on the number of hidden nodes by at least 38.07% while maintaining competitive classification accuracy compared with traditional deep neural networks. Finally, a theoretical analysis shows that the power consumption of TP-NET depends on the number of sub-networks, the structure of each sub-network, and atomic operations in the training process.",
      "year": 2022,
      "venue": "International Symposium on Smart Electronic Systems",
      "authors": [
        "Hui Hu",
        "Jessa Gegax-Randazzo",
        "Clay Carper",
        "Mike Borowczak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9286db7b087e6ba805d0526aeeeccfccf540069c",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e3bc283af5746de4ede17c123d39e02e819edcab",
      "title": "Characterizing Side-Channel Leakage of DNN Classifiers though Performance Counters",
      "abstract": "Rapid advancements in Deep Neural Networks (DNN) have led to their deployment in a wide range of com-mercial applications. DNN classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Like other applications, they have been shown to be vulnerable to side-channel information leakage. There have been several proof-of-concept attacks demon-strating the extraction of their model parameters and input data. However, no prior study has examined the possibility of using side-channels to extract the DNN classifier's decision or output. In this initial study, we aim to understand if there exists a correlation between the output class selected by a classifier and side-channel information collected while running the inference process on a CPU. Our initial evaluation shows that with the proposed approach it is possible to accurately recover the output class for model inputs via multiple side-channels: primarily power, but also branch mispredictions and cache misses.",
      "year": 2022,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Saikat Majumdar",
        "Mohammad Hossein Samavatian",
        "R. Teodorescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3bc283af5746de4ede17c123d39e02e819edcab",
      "pdf_url": "",
      "publication_date": "2022-06-27",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "title": "Enhancing Cybersecurity in Edge AI through Model Distillation and Quantization: A Robust and Efficient Approach",
      "abstract": "The rapid proliferation of Edge AI has introduced significant cybersecurity challenges, including adversarial attacks, model theft, and data privacy concerns. Traditional deep learning models deployed on edge devices often suffer from high computational complexity and memory requirements, making them vulnerable to exploitation. This paper explores the integration of model distillation and quantization techniques to enhance the security and efficiency of Edge AI systems. Model distillation reduces model complexity by transferring knowledge from a large, cumbersome model (teacher) to a compact, efficient one (student), thereby improving resilience against adversarial manipulations. \nQuantization further optimizes the student model by reducing bit precision, minimizing attack surfaces while maintaining performance. \nWe present a comprehensive analysis of how these techniques mitigate cybersecurity threats such as model inversion, membership inference, and evasion attacks. Additionally, we evaluate trade-offs between model accuracy, latency, and robustness in resource-constrained edge environments. Experimental results on benchmark datasets demonstrate that distilled and quantized models achieve comparable accuracy to their full-precision counterparts while significantly reducing vulnerability to cyber threats. Our findings highlight the potential of distillation and quantization as key enablers for secure, lightweight, and high-performance Edge AI deployments.",
      "year": 2022,
      "venue": "International Journal for Sciences and Technology",
      "authors": [
        "Mangesh Pujari",
        "Anshul Goel",
        "Ashwin Sharma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "pdf_url": "",
      "publication_date": "2022-11-25",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5018c07b5581cf9398173b7311c02ad085b6349e",
      "title": "An Enhanced Adversarial Attacks Method on Power System Based on Model Extraction Algorithm",
      "abstract": "Artificial intelligence algorithms fit connections between features and problems from a data-driven perspective. Artificial intelligence algorithms perform iterative training of data through neural network, which provides a new thinking dimension for researchers. The researches on the power grid field are no longer limited to modeling and analysis through traditional physical mechanism methods. However, there are security risks on the neural network model established by artificial intelligence algorithms. Attackers apply model extraction attacks to structure a substitute model of target power system model, which supports other attack algorithms to attack the power system and ultimately affects the normal operation of power system. This paper proposes an enhancement method for adversarial attacks on power system based on model extraction algorithm and tests the promoting effect of adversarial sample attack in various scenarios.",
      "year": 2022,
      "venue": "2022 IEEE 6th Conference on Energy Internet and Energy System Integration (EI2)",
      "authors": [
        "Yucheng Ma",
        "Qi Wang",
        "Zengji Liu",
        "Chao Hong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5018c07b5581cf9398173b7311c02ad085b6349e",
      "pdf_url": "",
      "publication_date": "2022-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "A. Salem",
        "Zhikun Zhang",
        "M. Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 152,
      "url": "https://www.semanticscholar.org/paper/9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "pdf_url": "",
      "publication_date": "2021-02-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction",
      "abstract": "We investigate whether model extraction can be used to \u2018steal\u2019 the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.",
      "year": 2021,
      "venue": "ACM Conference on Recommender Systems",
      "authors": [
        "Zhenrui Yue",
        "Zhankui He",
        "Huimin Zeng",
        "Julian McAuley"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460231.3474275",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "title": "Stealing Neural Network Structure Through Remote FPGA Side-Channel Analysis",
      "abstract": "Deep Neural Network (DNN) models have been extensively developed by companies for a wide range of applications. The development of a customized DNN model with great performance requires costly investments, and its structure (layers and hyper-parameters) is considered intellectual property and holds immense value. However, in this paper, we found the model secret is vulnerable when a cloud-based FPGA accelerator executes it. We demonstrate an end-to-end attack based on remote power side-channel analysis and machine-learning-based secret inference against different DNN models. The evaluation result shows that an attacker can reconstruct the layer and hyper-parameter sequence at over 90% accuracy using our method, which can significantly reduce her model development workload. We believe the threat presented by our attack is tangible, and new defense mechanisms should be developed against this threat.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yicheng Zhang",
        "Rozhin Yasaei",
        "Hao Chen",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "pdf_url": "",
      "publication_date": "2021-02-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "title": "MEGEX: Data-Free Model Extraction Attack Against Gradient-Based Explainable AI",
      "abstract": "Explainable AI encourages machine learning applications in the real world, whereas data-free model extraction attacks (DFME), in which an adversary steals a trained machine learning model by creating input queries with generative models instead of collecting training data, have attracted attention as a serious threat. In this paper, we propose MEGEX, a data-free model extraction attack against explainable AI that provides gradient-based explanations for inference results, and investigate whether the gradient-based explanations increase the vulnerability to the data-free model extraction attacks. In MEGEX, an adversary leverages explanations by Vanilla Gradient as derivative values for training a generative model. We prove that MEGEX is identical to white-box data-free knowledge distillation, whereby the adversary can train the generative model with the exact gradients. Our experiments show that the adversary in MEGEX can steal highly accurate models - 0.98\u00d7, 0.91\u00d7, and 0.96\u00d7 the victim model accuracy on SVHN, Fashion-MNIST, and CIFAR-10 datasets given 1.5M, 5M, 20M queries, respectively. In addition, we also apply sophisticated gradient-based explanations, i.e., SmoothGrad and Integrated Gradients, to MEGEX. The experimental results indicate that these explanations are potential countermeasures to MEGEX. We also found that the accuracy of the model stolen by the adversary depends on the diversity of query inputs by the generative model.",
      "year": 2021,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "T. Miura",
        "Toshiki Shibahara",
        "Naoto Yanai"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "pdf_url": "https://arxiv.org/pdf/2107.08909",
      "publication_date": "2021-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "title": "InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion",
      "abstract": "Cloud service providers, including Google, Amazon, and Alibaba, have now launched machine-learning-as-a-service (MLaaS) platforms, allowing clients to access sophisticated cloud-based machine learning models via APIs. Unfortunately, however, the commercial value of these models makes them alluring targets for theft, and their strategic position as part of the IT infrastructure of many companies makes them an enticing springboard for conducting further adversarial attacks. In this paper, we put forth a novel and effective attack strategy, dubbed InverseNet, that steals the functionality of black-box cloud-based models with only a small number of queries. The crux of the innovation is that, unlike existing model extraction attacks that rely on public datasets or adversarial samples, InverseNet constructs inversed training samples to increase the similarity between the extracted substitute model and the victim model. Further, only a small number of data samples with high confidence scores (rather than an entire dataset) are used to reconstruct the inversed dataset, which substantially reduces the attack cost. Extensive experiments conducted on three simulated victim models and Alibaba Cloud's commercially-available API demonstrate that InverseNet yields a model with significantly greater functional similarity to the victim model than the current state-of-the-art attacks at a substantially lower query budget.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Guanghao Mei",
        "Qian Wang"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0336.pdf",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "title": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey",
      "abstract": "During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "M. M. Real",
        "R. Salvador"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "pdf_url": "https://www.mdpi.com/2076-3417/11/15/6790/pdf?version=1627954373",
      "publication_date": "2021-07-23",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3b357acb6c807bc694372c773e507cdcef27d974",
      "title": "Leaky Nets: Recovering Embedded Neural Network Models and Inputs Through Simple Power and Timing Side-Channels\u2014Attacks and Defenses",
      "abstract": "With the recent advancements in machine learning theory, many commercial embedded microprocessors use neural network (NN) models for a variety of signal processing applications. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. But, many of these attacks involve specific assumptions, have limited applicability, or pose huge overheads to the attacker. In this work, we study the side-channel vulnerabilities of embedded NN implementations by recovering their parameters using timing-based information leakage and simple power analysis side-channel attacks. We demonstrate our attacks on popular microcontroller platforms over networks of different precisions, such as floating point, fixed point, and binary networks. We are able to successfully recover not only the model parameters but also the inputs for the above networks. Countermeasures against timing-based attacks are implemented and their overheads are analyzed.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "A. Chandrakasan"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/3b357acb6c807bc694372c773e507cdcef27d974",
      "pdf_url": "",
      "publication_date": "2021-02-23",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "20a6a74e26f41d62fc85733e3fe07897d7e9eb90",
      "title": "Convolutional neural network inversion of airborne transient electromagnetic data",
      "abstract": "As an efficient geophysical exploration technique, airborne transient electromagnetics shows strong adaptability to complex terrains and can provide subsurface resistivity information rapidly with a dense spatial coverage. However, the huge volume of airborne transient electromagnetic data obtained from a large number of spatial locations presents a great challenge to real\u2010time airborne transient electromagnetic interpretation due to the high computational cost. Moreover, the inherent non\u2010uniqueness of the inverse problem also limits our ability to constrain the underground resistivity structure. In this study, we develop an entirely data\u2010driven convolutional neural network to solve the airborne transient electromagnetic inverse problem. Synthetic tests show that the convolutional neural network is computationally efficient and yields robust results. Compared with the Gauss\u2013Newton method, convolutional neural network inversion does not depend on the choices of an initial model and the regularization parameters and is less prone to getting trapped in a local minimum. We also demonstrate the general applicability of the convolutional neural network to three\u2010dimensional synthetic airborne transient electromagnetic responses and the field observations acquired from Leach Lake Basin, Fort Irwin, California. The efficient convolutional neural network inversion framework can support real\u2010time resistivity imaging of subsurface structures from airborne transient electromagnetic observations, providing a powerful tool for field explorations.",
      "year": 2021,
      "venue": "Geophysical Prospecting",
      "authors": [
        "Sihong Wu",
        "Qinghua Huang",
        "Li Zhao"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/20a6a74e26f41d62fc85733e3fe07897d7e9eb90",
      "pdf_url": "",
      "publication_date": "2021-08-19",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "pdf_url": "",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
      "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals\u2019 sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users\u2019 privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
      "year": 2021,
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "authors": [
        "A. R. Javed",
        "S. Rehman",
        "M. Khan",
        "M. Alazab",
        "H. Khan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "pdf_url": "http://qspace.qu.edu.qa/bitstream/10576/37656/2/3460392.pdf",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "title": "Imaging subsurface orebodies with airborne electromagnetic data using a recurrent neural network",
      "abstract": "Conventional interpretation of airborne electromagnetic data has been conducted by solving the inverse problem. However, with recent advances in machine learning (ML) techniques, a one-dimensional (1D) deep neural network inversion that predicts a 1D resistivity model using multi-frequency vertical magnetic fields and sensor height information at one location has been applied. Nevertheless, bacause the final interpretation of this 1D approach relies on connecting 1D resistivity models, 1D ML interpretation has low accuracy for the estimation of an isolated anomaly, as in conventional 1D inversion. Thus, we propose a two-dimensional (2D) interpretation technique that can overcome the limitations of 1D interpretation, and consider spatial continuity by using a recurrent neural network (RNN). We generated various 2D resistivity models, calculated the ratio of primary and induced secondary magnetic fields of vertical direction in ppm scale using vertical magnetic dipole source, and then trained the RNN using the resistivity models and the corresponding electromagnetic (EM) responses. To verify the validity of 2D RNN inversion, we applied the trained RNN to synthetic and field data. Through application of the field data, we demonstrated that the design of the training dataset is crucial to improve prediction performance in a 2D RNN inversion. In addition, we investigated changes in the RNN inversion results of field data dependent on the data preprocessing. We demonstrated that using two types of data, logarithmic transformed data and linear scale data, which having different patterns of input information can enhance the prediction performance of the EM inversion results.",
      "year": 2021,
      "venue": "Geophysics",
      "authors": [
        "M. Bang",
        "Seokmin Oh",
        "K. Noh",
        "S. Seol",
        "J. Byun"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/beec2b61ce9e50d81ff10a0d5e4e3c2a7e11f079",
      "pdf_url": "",
      "publication_date": "2021-08-24",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "13d626a08050930fbf1cb072938d9de87e742fee",
      "title": "Extraction of Binarized Neural Network Architecture and Secret Parameters Using Side-Channel Information",
      "abstract": "In recent years, neural networks have been applied to various applications. To speed up the evaluation, a method using binarized network weights has been introduced, facilitating extremely efficient hardware implementation. Using electromagnetic (EM) side-channel analysis techniques, this study presents a framework of model extraction from practical binarized neural network (BNN) hardware. The target BNN hardware is generated and synthesized using open-source and commercial high-level synthesis tools GUINNESS and Xilinx SDSoC, respectively. With the hardware implemented on an up-to-date FPGA chip, we demonstrate how the layers can be identified from a single EM trace measured during the network evaluation, and we also demonstrate how an attacker may use side-channel attacks to recover secret weights used in the network.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "N. Homma",
        "S. Bhasin",
        "Dirmanto Jap"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/13d626a08050930fbf1cb072938d9de87e742fee",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b2212da1f8c968a60f70227623678a41fb63c3cf",
      "title": "Toward Invisible Adversarial Examples Against DNN-Based Privacy Leakage for Internet of Things",
      "abstract": "Deep neural networks (DNNs) can be utilized maliciously for compromising the privacy stored in electronic devices, e.g., identifying the images stored in a mobile phone connected to the Internet of Things (IoT). However, recent studies demonstrated that DNNs are vulnerable to adversarial examples, which are artificially designed perturbations in the original samples for misleading DNNs. Adversarial examples can be used to protect the DNN-based privacy leakage in mobile phones by replacing the photos with adversarial examples. To avoid affecting the normal use of photos, the adversarial examples need to be highly similar to original images. To handle a large number of photos stored in the devices at a proper time, the time efficiency of a method needs to be high enough. Previous methods cannot do well on both sides. In this article, we propose a broad class of selective gradient sign iterative algorithms to make adversarial examples useful in protecting the privacy of photos in IoT devices. By neglecting the unimportant image pixels in the iterative process of attacks according to the sort of first-order partial derivative, we control the optimization direction meticulously to reduce image distortions of adversarial examples without leveraging high time-consuming tricks. Extensive experimental results show that the proposed methods successfully fool the neural network classifiers for the image classification task with a small change in the visual effects and consume little calculating time simultaneously.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuyang Ding",
        "Shuai Zhang",
        "Mengkai Song",
        "Xiaocong Ding",
        "Fagen Li"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b2212da1f8c968a60f70227623678a41fb63c3cf",
      "pdf_url": "",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "title": "Stealing Machine Learning Parameters via Side Channel Power Attacks",
      "abstract": "Side-channel attacks target system implementation statistics, such as the electricity required to run a cryptographic function. Deriving cryptographic keys, such as AES keys, has become such a simplified process that extracting sensitive information from an otherwise secure algorithm requires only a $35USD microcontroller. While cryptographic algorithms indicate the presence of sensitive data, making them a preferable target, other systems hold valuable data with significantly less protection. Due to the ubiquity and rigidity of machine learning algorithms, the ability to infer model parameters has drastic security implications. This investigation extracted information from machine learning models through the use of traditional side-channel techniques. Specifically, a side-channel power analysis was performed using a ChipWhisperer Lite to extract information from Neural Networks and Linear Regression models running on a target microcontroller. Then, time series classification tasks were performed on the resultant power traces to determine the differences between the two models and their varied hyperparameters. Three such classification tasks were tested. In the first, a neural network was differentiated from a linear regression model with 100% accuracy. In the second, two neural networks with different sized hidden layers are classified with 97.92% accuracy. In the third, two virtually identical linear regression models are compared that differ only in the initial value of one hyperparameter. These models were only classified with 67.92% accuracy. Although the accuracy decreases as the models become more alike, these results indicate that machine learning model parameters can be inferred from power-based side-channel attacks.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Shaya Wolf",
        "Hui Hu",
        "Rafer Cooley",
        "Mike Borowczak"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "stealing machine learning"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "43c6ffef36a8acda9da688a0fca324abdb64498b",
      "title": "Model Reverse-Engineering Attack against Systolic-Array-Based DNN Accelerator Using Correlation Power Analysis",
      "abstract": "SUMMARY A model extraction attack is a security issue in deep neural networks (DNNs). Information on a trained DNN model is an attractive target for an adversary not only in terms of intellectual property but also of security. Thus, an adversary tries to reveal the sensitive information contained in the trained DNN model from machine-learning services. Previous studies on model extraction attacks assumed that the victim provides a machine-learning cloud service and the adversary accesses the service through formal queries. However, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the sensitive information contained in the implemented DNN model. We call these physical model extraction attacks model reverse-engineering (MRE) attacks to distinguish them from attacks on cloud services. Power side-channel analyses are often used in MRE attacks to reveal the internal operation from power consumption or electromagnetic leakage. Previous studies, including ours, evaluated MRE attacks against several types of DNN processors with power side-channel analyses. In this paper, information leakage from a systolic array which is used for the matrix multiplication unit in the DNN processors is evaluated. We utilized correlation power analysis (CPA) for the MRE attack and reveal weight parameters of a DNN model from the systolic array. Two types of the systolic array were implemented on \ufb01eld-programmable gate array (FPGA) to demonstrate that CPA reveals weight parameters from those systolic arrays. In addition, we applied an extended analysis approach called \u201cchain CPA\u201d for robust CPA analysis against the systolic arrays. Our experimental results indicate that an adversary can reveal trained model parameters from a DNN accelerator even if the DNN model parameters in the o \ufb00 -chip bus are protected with data encryption. Countermeasures against side-channel leaks will be important for implementing a DNN accelerator on a FPGA or application-speci\ufb01c integrated circuit (ASIC).",
      "year": 2021,
      "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences",
      "authors": [
        "Kota Yoshida",
        "M. Shiozaki",
        "S. Okura",
        "Takaya Kubota",
        "T. Fujino"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/43c6ffef36a8acda9da688a0fca324abdb64498b",
      "pdf_url": "https://doi.org/10.1587/transfun.2020cip0024",
      "publication_date": "2021-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "373936d00c4a357579c4d375de0ce439e4e54d5f",
      "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Chen Chen",
        "Xuanli He",
        "Lingjuan Lyu",
        "Fangzhao Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "title": "Power-based Attacks on Spatial DNN Accelerators",
      "abstract": "With proliferation of DNN-based applications, the confidentiality of DNN model is an important commercial goal. Spatial accelerators, which parallelize matrix/vector operations, are utilized for enhancing energy efficiency of DNN computation. Recently, model extraction attacks on simple accelerators, either with a single processing element or running a binarized network, were demonstrated using the methodology derived from differential power analysis (DPA) attack on cryptographic devices. This article investigates the vulnerability of realistic spatial accelerators using general, 8-bit, number representation. We investigate two systolic array architectures with weight-stationary dataflow: (1) a 3 \u00d7 1 array for a dot-product operation and (2) a 3 \u00d7 3 array for matrix-vector multiplication. Both are implemented on the SAKURA-G FPGA board. We show that both architectures are ultimately vulnerable. A conventional DPA succeeds fully on the 1D array, requiring 20K power measurements. However, the 2D array exhibits higher security even with 460K traces. We show that this is because the 2D array intrinsically entails multiple MACs simultaneously dependent on the same input. However, we find that a novel template-based DPA with multiple profiling phases is able to fully break the 2D array with only 40K traces. Corresponding countermeasures need to be investigated for spatial DNN accelerators.",
      "year": 2021,
      "venue": "ACM Journal on Emerging Technologies in Computing Systems",
      "authors": [
        "Ge Li",
        "Mohit Tiwari",
        "M. Orshansky"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "pdf_url": "https://arxiv.org/pdf/2108.12579",
      "publication_date": "2021-08-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16d98de120fc1e55000e65a285684c4dc7a04807",
      "title": "Efficiently Learning Any One Hidden Layer ReLU Network From Queries",
      "abstract": "Model extraction attacks have renewed interest in the classic problem of learning neural networks from queries. In this work we give the first polynomial-time algorithm for learning arbitrary one hidden layer neural networks activations provided black-box access to the network. Formally, we show that if $F$ is an arbitrary one hidden layer neural network with ReLU activations, there is an algorithm with query complexity and running time that is polynomial in all parameters that outputs a network $F'$ achieving low square loss relative to $F$ with respect to the Gaussian measure. While a number of works in the security literature have proposed and empirically demonstrated the effectiveness of certain algorithms for this problem, ours is the first with fully polynomial-time guarantees of efficiency even for worst-case networks (in particular our algorithm succeeds in the overparameterized setting).",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sitan Chen",
        "Adam R. Klivans",
        "Raghu Meka"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/16d98de120fc1e55000e65a285684c4dc7a04807",
      "pdf_url": "",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "title": "Side-Channel based Disassembler for AVR Micro-Controllers using Convolutional Neural Networks",
      "abstract": "Reverse engineering using Side Channel Attacks (SCA) have been known as a serious menace against embedded devices. The attacker could employ side channel data to retrieve some sensitive information from the device, security analysis, existence of a library in the device or execution of a special stream of codes. Side channel data could be gathered from the power consumption or electromagnetic radiations by the device. In this paper, we propose a disassembler to extract the instructions of the device under attack. A deep convolutional neural network is employed to make templates of the target to use it for real-time scenarios. Short Time Fourier Transform (STFT), and Mel-Frequency Cepstrum Coefficients (MFCC) are utilized as feature extractors. The proposed method consists of two different parts: 1) Hierarchical scenario and 2) Sole model. Atmel 8-bit AVR micro-controller is employed as the target device under attack. Our results indicate that, even with an experimental and low cost setup a vast number of instructions are detectable. The proposed method reaches 98.21% accuracy on the real code, outperforms state-of-the-art methods on the proposed dataset.",
      "year": 2021,
      "venue": "ISC Conference on Information Security and Cryptology",
      "authors": [
        "Pouya Narimani",
        "M. Akhaee",
        "Seyedamin Habibi"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ce9109dd491603364f6e6059d28757b52fafd5d",
      "title": "A Review of Confidentiality Threats Against Embedded Neural Network Models",
      "abstract": "Utilization of Machine Learning (ML) algorithms, especially Deep Neural Network (DNN) models, becomes a widely accepted standard in many domains more particularly IoT-based systems. DNN models reach impressive performances in several sensitive fields such as medical diagnosis, smart transport or security threat detection, and represent a valuable piece of Intellectual Property. Over the last few years, a major trend is the large-scale deployment of models in a wide variety of devices. However, this migration to embedded systems is slowed down because of the broad spectrum of attacks threatening the integrity, confidentiality and availability of embedded models. In this review, we cover the landscape of attacks targeting the confidentiality of embedded DNN models that may have a major impact on critical IoT systems, with a particular focus on model extraction and data leakage. We highlight the fact that Side-Channel Analysis (SCA) is a relatively unexplored bias by which model\u2019s confidentiality can be compromised. Input data, architecture or parameters of a model can be extracted from power or electromagnetic observations, testifying a real need from a security point of view.",
      "year": 2021,
      "venue": "World Forum on Internet of Things",
      "authors": [
        "Raphael Joud",
        "P. Moellic",
        "R\u00e9mi Bernhard",
        "J. Rigaud"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0ce9109dd491603364f6e6059d28757b52fafd5d",
      "pdf_url": "https://cea.hal.science/cea-04176698/file/A_Review_of_Confidentiality_Threats_Against_Embedded_Neural_Network_Models.pdf",
      "publication_date": "2021-05-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "beaa48a90464b1d9df6d42566a42c7326cc75826",
      "title": "Shuffling Countermeasure against Power Side-Channel Attack for MLP with Software Implementation",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Electrical and Control Engineering",
      "authors": [
        "Y. Nozaki",
        "M. Yoshikawa"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/beaa48a90464b1d9df6d42566a42c7326cc75826",
      "pdf_url": "",
      "publication_date": "2021-12-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "title": "First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data",
      "abstract": "Model extraction attacks are a kind of attacks where an adversary obtains a machine learning model whose performance is comparable with one of the victim model through queries and their results. This paper presents a novel model extraction attack, named TEMPEST, applicable on tabular data under a practical data-free setting. Whereas model extraction is more challenging on tabular data due to normalization, TEMPEST no longer needs initial samples that previous attacks require; instead, it makes use of publicly available statistics to generate query samples. Experiments show that our attack can achieve the same level of performance as the previous attacks. Moreover, we identify that the use of mean and variance as statistics for query generation and the use of the same normalization process as the victim model can improve the performance of our attack. We also discuss a possibility whereby TEMPEST is executed in the real world through an experiment with a medical diagnosis dataset. We plan to release the source code for reproducibility and a reference to subsequent works.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Masataka Tasumi",
        "Kazuki Iwahana",
        "Naoto Yanai",
        "Katsunari Shishido",
        "Toshiya Shimizu",
        "Yuji Higuchi",
        "I. Morikawa",
        "Jun Yajima"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "pdf_url": "",
      "publication_date": "2021-09-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
      "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
      "year": 2021,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Chengsi Gao",
        "Bing Li",
        "Ying Wang",
        "Weiwei Chen",
        "Lei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3453688.3461512",
      "publication_date": "2021-06-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "title": "BODAME: Bilevel Optimization for Defense Against Model Extraction",
      "abstract": "Model extraction attacks have become serious issues for service providers using machine learning. We consider an adversarial setting to prevent model extraction under the assumption that attackers will make their best guess on the service provider's model using query accesses, and propose to build a surrogate model that significantly keeps away the predictions of the attacker's model from those of the true model. We formulate the problem as a non-convex constrained bilevel optimization problem and show that for kernel models, it can be transformed into a non-convex 1-quadratically constrained quadratic program with a polynomial-time algorithm to find the global optimum. Moreover, we give a tractable transformation and an algorithm for more complicated models that are learned by using stochastic gradient descent-based algorithms. Numerical experiments show that the surrogate model performs well compared with existing defense models when the difference between the attacker's and service provider's distributions is large. We also empirically confirm the generalization ability of the surrogate model.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Y. Mori",
        "Atsushi Nitanda",
        "A. Takeda"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "pdf_url": "",
      "publication_date": "2021-03-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "title": "A Backpropagation Extreme Learning Machine Approach to Fast Training Neural Network-Based Side-Channel Attack",
      "abstract": "This work presented new Deep learning Side-channel Attack (DL-SCA) models that are based on Extreme Learning Machine (ELM). Unlike the conventional iterative backpropagation method, ELM is a fast learning algorithm that computes the trainable weights within a single iteration. Two models (Ensemble bpELM and CAE-ebpELM) are designed to perform SCA on AES with Boolean masking and desynchronization/jittering. The best models for both attack tasks can be trained 27x faster than MLP and 5x faster than CNN respectively. Verified and validated using ASCAD dataset, our models successfully recover all 16 subkeys using approximately 3K traces in the worst case scenario.",
      "year": 2021,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Xuyang Huang",
        "M. Wong",
        "A. Do",
        "W. Goh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "pdf_url": "",
      "publication_date": "2021-12-16",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d31062435210a9ccca679114ffc0e089744ae683",
      "title": "Bandwidth Utilization Side-Channel on ML Inference Accelerators",
      "abstract": "Accelerators used for machine learning (ML) inference provide great performance benefits over CPUs. Securing confidential model in inference against off-chip side-channel attacks is critical in harnessing the performance advantage in practice. Data and memory address encryption has been recently proposed to defend against off-chip attacks. In this paper, we demonstrate that bandwidth utilization on the interface between accelerators and the weight storage can serve a side-channel for leaking confidential ML model architecture. This side channel is independent of the type of interface, leaks even in the presence of data and memory address encryption and can be monitored through performance counters or through bus contention from an on-chip unprivileged process.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sarbartha Banerjee",
        "Shijia Wei",
        "Prakash Ramrakhyani",
        "Mohit Tiwari"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d31062435210a9ccca679114ffc0e089744ae683",
      "pdf_url": "",
      "publication_date": "2021-10-14",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8283479256ea1d54d98ed00bb53caba38f2f688e",
      "title": "Side-Channel Analysis-Based Model Extraction on Intelligent CPS: An Information Theory Perspective",
      "abstract": "The intelligent cyber-physical system (CPS) has been applied in various fields, covering multiple critical infras-tructures and human daily life support areas. CPS Security is a major concern and of critical importance, especially the security of the intelligent control component. Side-channel analysis (SCA) is the common threat exploiting the weaknesses in system operation to extract information of the intelligent CPS. However, existing literature lacks the systematic theo-retical analysis of the side-channel attacks on the intelligent CPS, without the ability to quantify and measure the leaked information. To address these issues, we propose the SCA-based model extraction attack on intelligent CPS. First, we design an efficient and novel SCA-based model extraction framework, including the threat model, hierarchical attack process, and the multiple micro-space parallel search enabled weight extraction algorithm. Secondly, an information theory-empowered analy-sis model for side-channel attacks on intelligent CPS is built. We propose a mutual information-based quantification method and derive the capacity of side-channel attacks on intelligent CPS, formulating the amount of information leakage through side channels. Thirdly, we develop the theoretical bounds of the leaked information over multiple attack queries based on the data processing inequality and properties of entropy. These convergence bounds provide theoretical means to estimate the amount of information leaked. Finally, experimental evaluation, including real-world experiments, demonstrates the effective-ness of the proposed SCA-based model extraction algorithm and the information theory-based analysis method in intelligent CPS.",
      "year": 2021,
      "venue": "2021 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "Xi Lin",
        "Jianhua Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8283479256ea1d54d98ed00bb53caba38f2f688e",
      "pdf_url": "",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08b2808afb0b2b81325a0b77e2617b180b8a684e",
      "title": "Quantitative estimation of side-channel leaks with neural networks",
      "abstract": null,
      "year": 2021,
      "venue": "International Journal on Software Tools for Technology Transfer (STTT)",
      "authors": [
        "Saeid Tizpaz-Niari",
        "Pavol Cern\u00fd",
        "S. Sankaranarayanan",
        "Ashutosh Trivedi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/08b2808afb0b2b81325a0b77e2617b180b8a684e",
      "pdf_url": "",
      "publication_date": "2021-05-26",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "title": "Neural Networks as a Side-Channel Countermeasure: Challenges and Opportunities",
      "abstract": "Specialized acceleration hardware for artificial deep neural network inference is available from the cloud to the edge. FPGAs in particular are heavily advertised for the acceleration of neural network-based applications. Traditionally, those applications are classification or nonlinear regression tasks with the goal to approximate an unknown function. However, they can be trained to replicate a fully known deterministic function - a classical example being the boolean XOR - as well. On the other hand, side-channel attacks remain a concern from the cloud to the edge, where attackers are often able to extract secret information through direct or indirect measurements of observables like power, voltage, electromagnetic emanation or timing. In this work, we show how an FPGA-mapped neural network implementation of the AES S-Box can improve side-channel resistance against Correlation Power Analysis (CPA) attacks. Although the implementation of a hardware-optimized algorithm such as the AES as a neural network introduces significant overhead, the generality of the representation allows to mitigate leakage in a manner agnostic to the overlying cryptographic primitive. We demonstrate the benefits of a generic representation, by optimizing an initially vulnerable neural network implementation towards side-channel resilience, through careful choice of activation function and input representation. The implementation is evaluated both against an external attacker measuring power with an oscilloscope, as well as a remote, internal adversary, who is able to capture voltage traces through FPGA-internal sensors in multi-tenant FPGAs. Our results show, how external attacks on the optimized neural network are no longer possible with up to one million traces, whereas an internal attacker is still able to recover the secret key. The latter result also exposes that in some cases measurement through internal sensors can be even more beneficial for an attacker than physical access with measurement equipment.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Jonas Krautter",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ef2325d326986f6e9958332100fce2f00ae65a29",
      "title": "Emerging AI Security Threats for Autonomous Cars - Case Studies",
      "abstract": "Artificial Intelligence has made a significant contribution to autonomous vehicles, from object detection to path planning. However, AI models require a large amount of sensitive training data and are usually computationally intensive to build. The commercial value of such models motivates attackers to mount various attacks. Adversaries can launch model extraction attacks for monetization purposes or step-ping-stone towards other attacks like model evasion. In specific cases, it even results in destroying brand reputation, differentiation, and value proposition. In addition, IP laws and AI-related legalities are still evolving and are not uniform across countries. We discuss model extraction attacks in detail with two use-cases and a generic kill-chain that can compromise autonomous cars. It is essential to investigate strategies to manage and mitigate the risk of model theft.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Shanthi Lekkala",
        "Tanya Motwani",
        "Manojkumar Somabhai Parmar",
        "A. Phadke"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ef2325d326986f6e9958332100fce2f00ae65a29",
      "pdf_url": "",
      "publication_date": "2021-09-10",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aa660c696749254dceee53270745d642779aa762",
      "title": "Machine learning privacy : analysis and implementation of model extraction attacks",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Bc. V\u00edt Karafi\u00e1t"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/aa660c696749254dceee53270745d642779aa762",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot"
      ],
      "citation_count": 261,
      "url": "https://www.semanticscholar.org/paper/abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "pdf_url": "",
      "publication_date": "2020-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "title": "Cryptanalytic Extraction of Neural Network Models",
      "abstract": "We argue that the machine learning problem of model extraction is actually a cryptanalytic problem in disguise, and should be studied as such. Given oracle access to a neural network, we introduce a differential attack that can efficiently steal the parameters of the remote model up to floating point precision. Our attack relies on the fact that ReLU neural networks are piecewise linear functions, and thus queries at the critical points reveal information about the model parameters. \nWe evaluate our attack on multiple neural network models and extract models that are 2^20 times more precise and require 100x fewer queries than prior work. For example, we extract a 100,000 parameter neural network trained on the MNIST digit recognition task with 2^21.5 queries in under an hour, such that the extracted model agrees with the oracle on all inputs up to a worst-case error of 2^-25, or a model with 4,000 parameters in 2^18.5 queries with worst-case error of 2^-40.4. Code is available at this https URL.",
      "year": 2020,
      "venue": "Annual International Cryptology Conference",
      "authors": [
        "Nicholas Carlini",
        "Matthew Jagielski",
        "Ilya Mironov"
      ],
      "citation_count": 151,
      "url": "https://www.semanticscholar.org/paper/186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "pdf_url": "",
      "publication_date": "2020-03-10",
      "keywords_matched": [
        "model extraction",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3136dd4036331b4559b341560712942ca2e765e3",
      "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
      "abstract": "Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Mingfu Xue",
        "Chengxiang Yuan",
        "Heyi Wu",
        "Yushu Zhang",
        "Weiqiang Liu"
      ],
      "citation_count": 141,
      "url": "https://www.semanticscholar.org/paper/3136dd4036331b4559b341560712942ca2e765e3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09064510.pdf",
      "publication_date": "2020-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d0d8c912b20d2d081672f07b6926b620950a39c8",
      "title": "Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel",
      "abstract": "Machine learning has been attracting strong interests in recent years. Numerous companies have invested great efforts and resources to develop customized deep-learning models, which are their key intellectual properties. In this work, we investigate to what extent the secret of deep-learning models can be inferred by attackers. In particular, we focus on the scenario that a model developer and an adversary share the same GPU when training a Deep Neural Network (DNN) model. We exploit the GPU side-channel based on context-switching penalties. This side-channel allows us to extract the fine-grained structural secret of a DNN model, including its layer composition and hyper-parameters. Leveraging this side-channel, we developed an attack prototype named MosConS, which applies LSTM-based inference models to identify the structural secret. Our evaluation of MosConS shows the structural information can be accurately recovered. Therefore, we believe new defense mechanisms should be developed to protect training against the GPU side-channel.",
      "year": 2020,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Junyin Wei",
        "Yicheng Zhang",
        "Zhe Zhou",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/d0d8c912b20d2d081672f07b6926b620950a39c8",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9419ceaf7a62bc1c1d9085f37632a2ecc884394c",
      "title": "One-dimensional deep learning inversion of electromagnetic induction data using convolutional neural network",
      "abstract": "\n Conventional geophysical inversion techniques suffer from several limitations including computational cost, nonlinearity, non-uniqueness and dimensionality of the inverse problem. Successful inversion of geophysical data has been a major challenge for decades. Here, a novel approach based on deep learning (DL) inversion via convolutional neural network (CNN) is proposed to instantaneously estimate subsurface electrical conductivity (\u03c3) layering from electromagnetic induction (EMI) data. In this respect, a fully convolutional network was trained on a large synthetic data set generated based on 1-D EMI forward model. The accuracy of the proposed approach was examined using several synthetic scenarios. Moreover, the trained network was used to find subsurface electromagnetic conductivity images (EMCIs) from EMI data measured along two transects from Chicken Creek catchment (Brandenburg, Germany). Dipole\u2013dipole electrical resistivity tomography data were measured as well to obtain reference subsurface \u03c3 distributions down to a 6\u00a0m depth. The inversely estimated models were juxtaposed and compared with their counterparts obtained from a spatially constrained deterministic algorithm as a standard code. Theoretical simulations demonstrated a well performance of the algorithm even in the presence of noise in data. Moreover, application of the DL inversion for subsurface imaging from Chicken Creek catchment manifested the accuracy and robustness of the proposed approach for EMI inversion. This approach returns subsurface \u03c3 distribution directly from EMI data in a single step without any iterations. The proposed strategy simplifies considerably EMI inversion and allows for rapid and accurate estimation of subsurface EMCI from multiconfiguration EMI data.",
      "year": 2020,
      "venue": "Geophysical Journal International",
      "authors": [
        "D. Moghadas"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/9419ceaf7a62bc1c1d9085f37632a2ecc884394c",
      "pdf_url": "",
      "publication_date": "2020-04-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "title": "SNIFF: Reverse Engineering of Neural Networks With Fault Attacks",
      "abstract": "Neural networks have been shown to be vulnerable against fault injection attacks. These attacks change the physical behavior of the device during the computation, resulting in a change of value that is currently being computed. They can be realized by various techniques, ranging from clock/voltage glitching to application of lasers to rowhammer. Previous works have mostly explored fault attacks for output misclassification, thus affecting the reliability of neural networks. In this article, we investigate the possibility to reverse engineer neural networks with fault attacks. Sign bit flip fault attack enables the reverse engineering by changing the sign of intermediate values. We develop the first exact extraction method on deep-layer feature extractor networks that provably allows the recovery of proprietary model parameters. Our experiments with Keras library show that the precision error for the parameter recovery for the tested networks is less than $10^{-13}$ with the usage of 64-bit floats, which improves the current state of the art by six orders of magnitude.",
      "year": 2020,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin",
        "Yang Liu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/155678/2/_IEEE_TREL__Reverse_Engineering_of_Neural_Networks_with_Fault_Attacks.pdf",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "title": "Model extraction from counterfactual explanations",
      "abstract": "Post-hoc explanation techniques refer to a posteriori methods that can be used to explain how black-box machine learning models produce their outcomes. Among post-hoc explanation techniques, counterfactual explanations are becoming one of the most popular methods to achieve this objective. In particular, in addition to highlighting the most important features used by the black-box model, they provide users with actionable explanations in the form of data instances that would have received a different outcome. Nonetheless, by doing so, they also leak non-trivial information about the model itself, which raises privacy issues. In this work, we demonstrate how an adversary can leverage the information provided by counterfactual explanations to build high-fidelity and high-accuracy model extraction attacks. More precisely, our attack enables the adversary to build a faithful copy of a target model by accessing its counterfactual explanations. The empirical evaluation of the proposed attack on black-box models trained on real-world datasets demonstrates that they can achieve high-fidelity and high-accuracy extraction even under low query budgets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "U. A\u00efvodji",
        "Alexandre Bolot",
        "S. Gambs"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "pdf_url": "",
      "publication_date": "2020-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "title": "BoMaNet: Boolean Masking of an Entire Neural Network",
      "abstract": "Recent work on stealing machine learning (ML) models from inference engines with physical side-channel attacks warrant an urgent need for effective side-channel defenses. This work proposes the first fully-masked neural network inference engine design. Masking uses secure multi-party computation to split the secrets into random shares and to decorrelate the statistical relation of secret-dependent computations to side-channels (e.g., the power draw). In this work, we construct secure hardware primitives to mask all the linear and non-linear operations in a neural network. We address the challenge of masking integer addition by converting each addition into a sequence of XOR and AND gates and by augmenting Trichina's secure Boolean masking style. We improve the traditional Trichina's AND gates by adding pipelining elements for better glitch-resistance and we architect the whole design to sustain a throughput of 1 masked addition per cycle. We implement the proposed secure inference engine on a Xilinx Spartan-6 (XC6SLX75) FPGA. The results show that masking incurs an overhead of 3.5% in latency and 5.9\u00d7 in area. Finally, we demonstrate the security of the masked design with 2M traces.",
      "year": 2020,
      "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "pdf_url": "",
      "publication_date": "2020-06-16",
      "keywords_matched": [
        "stealing machine learning",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0d43d80c40baec445bc39a8dfa4789d070786319",
      "title": "Reverse-Engineering Deep Neural Networks Using Floating-Point Timing Side-Channels",
      "abstract": "Trained Deep Neural Network (DNN) models have become valuable intellectual property. A new attack surface has emerged for DNNs: model reverse engineering. Several recent attempts have utilized various common side channels. However, recovering DNN parameters, weights and biases, remains a challenge. In this paper, we present a novel attack that utilizes a floating-point timing side channel to reverse-engineer parameters of multi-layer perceptron (MLP) models in software implementation, entirely and precisely. To the best of our knowledge, this is the first work that leverages a floating-point timing side-channel for effective DNN model recovery.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Gongye Cheng",
        "Yunsi Fei",
        "T. Wahl"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/0d43d80c40baec445bc39a8dfa4789d070786319",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "70c1f8f6c079b5a3782fa73eebc3255aada58dd8",
      "title": "(Quasi-)Real-Time Inversion of Airborne Time-Domain Electromagnetic Data via Artificial Neural Network",
      "abstract": "The possibility to have results very quickly after, or even during, the collection of electromagnetic data would be important, not only for quality check purposes, but also for adjusting the location of the proposed flight lines during an airborne time-domain acquisition. This kind of readiness could have a large impact in terms of optimization of the Value of Information of the measurements to be acquired. In addition, the importance of having fast tools for retrieving resistivity models from airborne time-domain data is demonstrated by the fact that Conductivity-Depth Imaging methodologies are still the standard in mineral exploration. In fact, they are extremely computationally efficient, and, at the same time, they preserve a very high lateral resolution. For these reasons, they are often preferred to inversion strategies even if the latter approaches are generally more accurate in terms of proper reconstruction of the depth of the targets and of reliable retrieval of true resistivity values of the subsurface. In this research, we discuss a novel approach, based on neural network techniques, capable of retrieving resistivity models with a quality comparable with the inversion strategy, but in a fraction of the time. We demonstrate the advantages of the proposed novel approach on synthetic and field datasets.",
      "year": 2020,
      "venue": "Remote Sensing",
      "authors": [
        "P. Bai",
        "G. Vignoli",
        "A. Viezzoli",
        "J. Nevalainen",
        "G. Vacca"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/70c1f8f6c079b5a3782fa73eebc3255aada58dd8",
      "pdf_url": "https://www.mdpi.com/2072-4292/12/20/3440/pdf?version=1603361484",
      "publication_date": "2020-10-20",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "title": "Model Reverse-Engineering Attack using Correlation Power Analysis against Systolic Array Based Neural Network Accelerator",
      "abstract": "Various deep neural network (DNN) accelerators have been proposed for artificial intelligence (AI) inference on edge devices. On the other hand, hardware security issues of the DNN accelerator have not been discussed well. Trained DNN models are important intellectual property and a valuable target for adversaries. In particular, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the implemented DNN model. Therefore, the DNN execution environment on an edge device requires countermeasures such as data encryption on off-chip memory against various reverse-engineering attacks. In this paper, we reveal DNN model parameters by utilizing correlation power analysis (CPA) against a systolic array circuit that is widely used in DNN accelerator hardware. Our experimental results show that the adversary can extract trained model parameters from a DNN accelerator even if the DNN model parameters are protected with data encryption. The results suggest that countermeasures against side-channel leaks are important for implementing a DNN accelerator on FPGA or ASIC.",
      "year": 2020,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Kota Yoshida",
        "Takaya Kubota",
        "S. Okura",
        "M. Shiozaki",
        "T. Fujino"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "pdf_url": "",
      "publication_date": "2020-10-01",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "729fbe386e31e36ed5108ee276a6db223176a33d",
      "title": "Information Leakage by Model Weights on Federated Learning",
      "abstract": "Federated learning aggregates data from multiple sources while protecting privacy, which makes it possible to train efficient models in real scenes. However, although federated learning uses encrypted security aggregation, its decentralised nature makes it vulnerable to malicious attackers. A deliberate attacker can subtly control one or more participants and upload malicious model parameter updates, but the aggregation server cannot detect it due to encrypted privacy protection. Based on these problems, we find a practical and novel security risk in the design of federal learning. We propose an attack for conspired malicious participants to adjust the training data strategically so that the weight of a certain dimension in the aggregation model will rise or fall with a pattern. The trend of weights or parameters in the aggregation model forms meaningful signals, which is the risk of information leakage. The leakage is exposed to other participants in this federation but only available for participants who reach an agreement with the malicious participant, i.e., the receiver must be able to understand patterns of changes in weights. The attack effect is evaluated and verified on open-source code and data sets.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Xiaoyun Xu",
        "Jingzheng Wu",
        "Mutian Yang",
        "Tianyue Luo",
        "Xu Duan",
        "Weiheng Li",
        "Yanjun Wu",
        "Bin Wu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/729fbe386e31e36ed5108ee276a6db223176a33d",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "title": "Model Extraction Attacks on Recurrent Neural Networks",
      "abstract": ": Model extraction attacks are an attack in which an adversary utilizes a query access to the target model to obtain a new model whose performance is equivalent to the target model e \ufb03 ciently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the e \ufb00 ectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks on RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful type of RNN. Speci\ufb01cally, we tackle the following problems. First, in case of a classi\ufb01cation task, such as image recognition, extraction of an RNN model without \ufb01nal outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in case of a regression task such as weather forecasting, a new attack by newly con\ufb01guring a loss function is presented. We conduct experiments on our model extraction attacks on an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted e \ufb03 ciently, especially through con\ufb01guring a loss function and a more complex architecture di \ufb00 erent from the target model.",
      "year": 2020,
      "venue": "Journal of Information Processing",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "pdf_url": "https://doi.org/10.2197/ipsjjip.28.1010",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "title": "Model Extraction Attacks against Recurrent Neural Networks",
      "abstract": "Model extraction attacks are a kind of attacks in which an adversary obtains a new model, whose performance is equivalent to that of a target model, via query access to the target model efficiently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the effectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks against RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful RNN. Specifically, we tackle the following problems. First, in a case of a classification problem, such as image recognition, extraction of an RNN model without final outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in a case of a regression problem. such as in weather forecasting, a new attack by newly configuring a loss function is presented. We conduct experiments on our model extraction attacks against an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted efficiently, especially through configuring a loss function and a more complex architecture different from the target model.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "pdf_url": "",
      "publication_date": "2020-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "12393a8e6e7c750eae30c0c538663fa7b426f304",
      "title": "A New Privacy-Preserving Framework based on Edge-Fog-Cloud Continuum for Load Forecasting",
      "abstract": "As an essential part to intelligently fine-grained scheduling, planning and maintenance in smart grid and energy internet, short-term load forecasting makes great progress recently owing to the big data collected from smart meters and the leap forward in machine learning technologies. However, the centralized computing topology of classical electric information system, where individual electricity consumption data are frequently transmitted to the cloud center for load forecasting, tends to violate electric consumers\u2019 privacy as well as to increase the pressure on network bandwidth. To tackle the tricky issues, we propose a privacy-preserving framework based on the edge-fog-cloud continuum for smart grid. Specifically, 1) we gravitate the training of load forecasting models and forecasting workloads to distributed smart meters so that consumers\u2019 raw data are handled locally, and only the forecasting outputs that have been protected are reported to the cloud center via fog nodes; 2) we protect the local forecasting models that imply electricity features from model extraction attacks by model randomization; 3) we exploit a shuffle scheme among smart meters to protect the data ownership privacy, and utilize a re-encryption scheme to guarantee the forecasting data privacy. Finally, through comprehensive simulation and analysis, we validate our proposed privacy-preserving framework in terms of privacy protection, and computation and communication efficiency.",
      "year": 2020,
      "venue": "IEEE Wireless Communications and Networking Conference",
      "authors": [
        "S. Hou",
        "Hongjia Li",
        "Chang Yang",
        "Liming Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/12393a8e6e7c750eae30c0c538663fa7b426f304",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7282e208d9063a460ad0306103a76557387322f6",
      "title": "Practical Side-Channel Based Model Extraction Attack on Tree-Based Machine Learning Algorithm",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Dirmanto Jap",
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "Rei Ueno",
        "S. Bhasin",
        "N. Homma"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/7282e208d9063a460ad0306103a76557387322f6",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06166afe614094369c5633957feb7bcdf6c98d1a",
      "title": "Preventing Neural Network Weight Stealing via Network Obfuscation",
      "abstract": null,
      "year": 2020,
      "venue": "Sai",
      "authors": [
        "K\u00e1lm\u00e1n Szentannai",
        "Jalal Al-Afandi",
        "A. Horv\u00e1th"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/06166afe614094369c5633957feb7bcdf6c98d1a",
      "pdf_url": "",
      "publication_date": "2020-07-16",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f9f49997c404e386610289202a7c3812c6cbfe34",
      "title": "Differentially Private Machine Learning Model against Model Extraction Attack",
      "abstract": "Machine learning model is vulnerable to model extraction attacks since the attackers can send plenty of queries to infer the hyperparameters of the machine learning model thus stealing confidential information of the learning models. Therefore, there is a urgent need to defend against such an attack. Differential privacy is a promising technique to protect the valuable information. We propose a differential privacy-based method applied in the linear neural network to obfuscate the output of the machine learning model. The security and utility issue of injecting a noise layer to the linear neural network is mathematically analyzed. The experiment results show that our proposed method can lower the attacker's extraction rate while keeping high utility.",
      "year": 2020,
      "venue": "2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Zelei Cheng",
        "Zuotian Li",
        "Jiwei Zhang",
        "Shuhan Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f9f49997c404e386610289202a7c3812c6cbfe34",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "title": "Stealing Your Data from Compressed Machine Learning Models",
      "abstract": "Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Nuo Xu",
        "Qi Liu",
        "Tao Liu",
        "Zihao Liu",
        "Xiaochen Guo",
        "Wujie Wen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "title": "Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment",
      "abstract": "Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Gupta",
        "Erick Galinkin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/9ff6c3660fbb535d72fa100758e59df0f71945f8",
      "pdf_url": "",
      "publication_date": "2020-07-09",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "05b8495df20c2539d25448b278660e776175fccb",
      "title": "Research on Side-Channel Attack Based on the Synergy between SNR and Convolutional Neural Networks",
      "abstract": "The data encryption process in the encryption chip will be leaked by means of timing operation time, probing data collecting operation power and electromagnetic signal interception, which makes side channel attack possible. Nowadays, the research on template creation of template attacks has shifted from Gaussian distribution to the use of machine learning algorithms to create templates. With many parameters, it is difficult to find a suitable network structure. Based on many experimental studies, the experience of a convolutional neural network structure suitable for side channel analysis is summarized and proposed.",
      "year": 2020,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Zixin Liu",
        "Ting Zhu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/05b8495df20c2539d25448b278660e776175fccb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/1575/1/012026",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0d3386725f66c8436f2b37a419fddcea3e0f302",
      "title": "Model Extraction Oriented Data Publishing with k-anonymity",
      "abstract": null,
      "year": 2020,
      "venue": "International Workshop on Security",
      "authors": [
        "Takeru Fukuoka",
        "Yuji Yamaoka",
        "Takeaki Terada"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f0d3386725f66c8436f2b37a419fddcea3e0f302",
      "pdf_url": "",
      "publication_date": "2020-09-02",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "title": "Bident Structure for Neural Network Model Protection",
      "abstract": ": Deep neural networks are widely deployed in a variety of application areas to provide real-time inference services, such as mobile phones, autonomous vehicles and industrial automation. Deploying trained models in end-user devices rises high demands on protecting models against model stealing attacks. To tackle this concern, applying cryptography algorithms and using trusted execution environments have been proposed. However, both approaches cause significant overhead on inference time. With the support of trusted execution environment, we propose bident-structure networks to protect the neural networks while maintaining inference efficiency. Our main idea is inspired by the secret-sharing concept from cryptography community, where we treat the neural network as the secret to be protected. We prove the feasibility of bident-structure methods by empirical experiments on MNIST. Experimental results also demonstrate that efficiency overhead can be reduced by compressing sub-networks running in trusted execution environments.",
      "year": 2020,
      "venue": "International Conference on Information Systems Security and Privacy",
      "authors": [
        "Hsiao-Ying Lin",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "pdf_url": "https://doi.org/10.5220/0008923403770384",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "title": "Model Extraction Defense using Modified Variational Autoencoder",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Yash Gupta"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "pdf_url": "",
      "publication_date": "2020-06-04",
      "keywords_matched": [
        "model extraction",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d51515601024e2b0cd12abf878b5d7db8adda6ff",
      "title": "An Electromagnetic Neural Network for Inverse Source Modeling of Wire-Like Objects",
      "abstract": "We propose a new electromagnetic neural network to extract the spatial features of wire objects via a novel spatial singularity expansion method (S-SEM). The proposed approach utilizes a recently-found radiation function that holds the spatial parameters of targets defined as the surface current and the geometrical details. Through EM machine learning, an estimation of these parameters is performed in a form of inverse problems for a single wire system. The estimated parameters, which are the S-SEM's poles and strengths, are validated and compared with numerical results obtained from the EM solver where an excellent agreement is observed.",
      "year": 2020,
      "venue": "2020 IEEE International Symposium on Antennas and Propagation and North American Radio Science Meeting",
      "authors": [
        "Abdelelah M. Alzahed",
        "Y. Antar",
        "S. Mikki"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d51515601024e2b0cd12abf878b5d7db8adda6ff",
      "pdf_url": "",
      "publication_date": "2020-07-05",
      "keywords_matched": [
        "electromagnetic neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "52a222d38a8640499010d470d5589a81882bc425",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input. \nTo extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights. \nWe perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alexey Kurakin",
        "Nicolas Papernot"
      ],
      "citation_count": 424,
      "url": "https://www.semanticscholar.org/paper/52a222d38a8640499010d470d5589a81882bc425",
      "pdf_url": "",
      "publication_date": "2019-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "title": "Model inversion attacks against collaborative inference",
      "abstract": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and datasets, to show their effectiveness and generalization. We also study the characteristics of deep learning models that make them susceptible to such inference privacy threats. This provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.",
      "year": 2019,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "R. Lee"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "pdf_url": "",
      "publication_date": "2019-12-09",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b12c15503435cffee16bdd156766dcf390eeb8f8",
      "title": "CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel",
      "abstract": null,
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 309,
      "url": "https://www.semanticscholar.org/paper/b12c15503435cffee16bdd156766dcf390eeb8f8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "title": "MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel Protection",
      "abstract": "Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. The research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored. This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the first countermeasures against these attacks by augmenting masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X FPGA board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by $ 2.8\\times$ and $2.3\\times$.",
      "year": 2019,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "pdf_url": "https://arxiv.org/pdf/1910.13063",
      "publication_date": "2019-10-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8bedaca9ebb1d055046643802683d013fd1b2da9",
      "title": "BDPL: A Boundary Differentially Private Layer Against Machine Learning Model Extraction Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Huadi Zheng",
        "Qingqing Ye",
        "Haibo Hu",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/8bedaca9ebb1d055046643802683d013fd1b2da9",
      "pdf_url": "",
      "publication_date": "2019-09-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "304ac5d62f2888e94078715413c40cf00b58ac4f",
      "title": "Efficiently Stealing your Machine Learning Models",
      "abstract": "Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.",
      "year": 2019,
      "venue": "WPES@CCS",
      "authors": [
        "R. Reith",
        "T. Schneider",
        "Oleksandr Tkachenko"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/304ac5d62f2888e94078715413c40cf00b58ac4f",
      "pdf_url": "https://encrypto.de/papers/RST19.pdf",
      "publication_date": "2019-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "title": "A framework for the extraction of Deep Neural Networks by leveraging public data",
      "abstract": "Machine learning models trained on confidential datasets are increasingly being deployed for profit. Machine Learning as a Service (MLaaS) has made such models easily accessible to end-users. Prior work has developed model extraction attacks, in which an adversary extracts an approximation of MLaaS models by making black-box queries to it. However, none of these works is able to satisfy all the three essential criteria for practical model extraction: (1) the ability to work on deep learning models, (2) the non-requirement of domain knowledge and (3) the ability to work with a limited query budget. We design a model extraction framework that makes use of active learning and large public datasets to satisfy them. We demonstrate that it is possible to use this framework to steal deep classifiers trained on a variety of datasets from image and text domains. By querying a model via black-box access for its top prediction, our framework improves performance on an average over a uniform noise baseline by 4.70x for image tasks and 2.11x for text tasks respectively, while using only 30% (30,000 samples) of the public dataset at its disposal.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "pdf_url": "",
      "publication_date": "2019-05-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "title": "Deep Neural Network Attribution Methods for Leakage Analysis and Symmetric Key Recovery",
      "abstract": null,
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Benjamin Hettwer",
        "Stefan Gehrer",
        "Tim G\u00fcneysu"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/7542385f3672cfee43adc964a90efb0b19f6b9fa",
      "pdf_url": "",
      "publication_date": "2019-08-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fa30abd5d9233ccde88502d5bae300e6666d3417",
      "title": "Make Some Noise. Unleashing the Power of Convolutional Neural Networks for Profiled Side-channel Analysis",
      "abstract": "Profiled side-channel analysis based on deep learning, and more precisely Convolutional Neural Networks, is a paradigm showing significant potential. The results, although scarce for now, suggest that such techniques are even able to break cryptographic implementations protected with countermeasures. In this paper, we start by proposing a new Convolutional Neural Network instance able to reach high performance for a number of considered datasets. We compare our neural network with the one designed for a particular dataset with masking countermeasure and we show that both are good designs but also that neither can be considered as a superior to the other one.Next, we address how the addition of artificial noise to the input signal can be actually beneficial to the performance of the neural network. Such noise addition is equivalent to the regularization term in the objective function. By using this technique, we are able to reduce the number of measurements needed to reveal the secret key by orders of magnitude for both neural networks. Our new convolutional neural network instance with added noise is able to break the implementation protected with the random delay countermeasure by using only 3 traces in the attack phase. To further strengthen our experimental results, we investigate the performance with a varying number of training samples, noise levels, and epochs. Our findings show that adding noise is beneficial throughout all training set sizes and epochs.",
      "year": 2019,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Jaehun Kim",
        "S. Picek",
        "Annelie Heuser",
        "S. Bhasin",
        "A. Hanjalic"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/fa30abd5d9233ccde88502d5bae300e6666d3417",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/8292/7642",
      "publication_date": "2019-05-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "697cbfa1a3ea202a5710138b089ecefca3f5d935",
      "title": "Imaging subsurface resistivity structure from airborne electromagnetic induction data using deep neural network",
      "abstract": "ABSTRACT Due to the rapid development and spread of deep learning technologies, potential applications of artificial intelligence technology in the field of geophysical inversion are being explored. In this study, we applied a deep neural network (DNN) to reconstruct one-dimensional electrical resistivity structures from airborne electromagnetic (AEM) data for varying sensor heights. We used numerical models and their simulated AEM responses to train the DNN to be an inversion operator, and determined that it was possible to train the DNN without the use of stabilisers on the subsurface structures. We compared the quantitative performance of DNN and Gauss\u2013Newton inversion of synthetic datasets, and demonstrated that DNN inversion reconstructed the subsurface structure more accurately, and within a significantly shorter period. We subsequently applied DNN inversion to a field dataset to quantify the effectiveness and applicability of the proposed method for real data. The results of the current study will open new avenues for real-time imaging of subsurface structures from AEM data.",
      "year": 2019,
      "venue": "Exploration Geophysics",
      "authors": [
        "K. Noh",
        "D. Yoon",
        "J. Byun"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/697cbfa1a3ea202a5710138b089ecefca3f5d935",
      "pdf_url": "",
      "publication_date": "2019-10-22",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e36c9f2c5c4791cf504760988de33b39a013373f",
      "title": "Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints",
      "abstract": "As neural networks continue their reach into nearly every aspect of software operations, the details of those networks become an increasingly sensitive subject. Even those that deploy neural networks embedded in physical devices may wish to keep the inner working of their designs hidden -- either to protect their intellectual property or as a form of protection from adversarial inputs. The specific problem we address is how, through heavy system stack, given noisy and imperfect memory traces, one might reconstruct the neural network architecture including the set of layers employed, their connectivity, and their respective dimension sizes. Considering both the intra-layer architecture features and the inter-layer temporal association information introduced by the DNN design empirical experience, we draw upon ideas from speech recognition to solve this problem. We show that off-chip memory address traces and PCIe events provide ample information to reconstruct such neural network architectures accurately. We are the first to propose such accurate model extraction techniques and demonstrate an end-to-end attack experimentally in the context of an off-the-shelf Nvidia GPU platform with full system stack. Results show that the proposed techniques achieve a high reverse engineering accuracy and improve the one's ability to conduct targeted adversarial attack with success rate from 14.6\\%$\\sim$25.5\\% (without network architecture knowledge) to 75.9\\% (with extracted network architecture).",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Lei Deng",
        "Shuangchen Li",
        "Xinfeng Xie",
        "Yu Ji",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e36c9f2c5c4791cf504760988de33b39a013373f",
      "pdf_url": "",
      "publication_date": "2019-03-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b82d52b95040c0682b7a8baa9254eae79b068505",
      "title": "Adversarial Model Extraction on Graph Neural Networks",
      "abstract": "Along with the advent of deep neural networks came various methods of exploitation, such as fooling the classifier or contaminating its training data. Another such attack is known as model extraction, where provided API access to some black box neural network, the adversary extracts the underlying model. This is done by querying the model in such a way that the underlying neural network provides enough information to the adversary to be reconstructed. While several works have achieved impressive results with neural network extraction in the propositional domain, this problem has not yet been considered over the relational domain, where data samples are no longer considered to be independent and identically distributed (iid). Graph Neural Networks (GNNs) are a popular deep learning framework to perform machine learning tasks over relational data. In this work, we formalize an instance of GNN extraction, present a solution with preliminary results, and discuss our assumptions and future directions.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "David DeFazio",
        "Arti Ramesh"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/b82d52b95040c0682b7a8baa9254eae79b068505",
      "pdf_url": "",
      "publication_date": "2019-12-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "title": "Towards Privacy and Security of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, \\eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "pdf_url": "",
      "publication_date": "2019-11-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "title": "Quantifying (Hyper) Parameter Leakage in Machine Learning",
      "abstract": "Machine Learning models are extensively used for various multimedia applications and are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks that reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Extraction attacks proposed in the literature are empirically evaluated and lack a theoretical framework to measure the information leaked under such attacks. In this work, we propose a novel model-agnostic probabilistic framework, AIRAVATA, to quantify information leakage using partial knowledge and limited evidences from model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training for stealing the target model functionality. We use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in the literature to reason about the attack efficacy. This provides a practical tool to identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model and estimate the relative threats from different attacks.",
      "year": 2019,
      "venue": "IEEE International Conference on Multimedia Big Data",
      "authors": [
        "Vasisht Duddu",
        "D. V. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "pdf_url": "https://arxiv.org/pdf/1910.14409",
      "publication_date": "2019-10-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4f95e417a5aeeaf01635a17d58e512d1c9527d86",
      "title": "Improved Study of Side-Channel Attacks Using Recurrent Neural Networks",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Rony Chowdhury",
        "M. Naser"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4f95e417a5aeeaf01635a17d58e512d1c9527d86",
      "pdf_url": "https://doi.org/10.18122/td/1611/boisestate",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a391a74b82dbb86b120c78976f21546c08d7116e",
      "title": "Analysis of Information Leakage from MCU using Neural Network",
      "abstract": "Electromagnetic leakage from an operating microcontroller unit (MCU) can be intercepted and analyzed to deduce the instructions being processed. This is helpful to understanding eavesdropping and to later protecting against it. In this work, harnessing a deep neural network, we analyze the massive electromagnetic (EM) leakage information to extract the instructions run in a microcontroller unit (MCU). An EM leakage acquisition environment is built, and the leakage signal of an MCU is collected. A multi-layer convolutional neural network is constructed for the side channel analysis and identification. The recognition accuracy is over 95% in the training phase, and more than 75% in the prediction phase. This experiment proves that the electromagnetic leakage emitted can be collected by appropriate methods and analyzed by deep learning technology. It can effectively deduce the instructions running in an MCU, which lays a foundation for the protection against eavesdropping.",
      "year": 2019,
      "venue": "International Workshop on Electromagnetic Compatibility of Integrated Circuits",
      "authors": [
        "Siping Gao",
        "Yingkai Guo",
        "Z. Aung",
        "Yongxin Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a391a74b82dbb86b120c78976f21546c08d7116e",
      "pdf_url": "",
      "publication_date": "2019-10-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "title": "Reverse Engineering Convolutional Neural Networks Through Side-channel Information Leaks",
      "abstract": "A convolutional neural network (CNN) model represents a crucial piece of intellectual property in many applications. Revealing its structure or weights would leak confidential information. In this paper we present novel reverse-engineering attacks on CNNs running on a hardware accelerator, where an adversary can feed inputs to the accelerator and observe the resulting off-chip memory accesses. Our study shows that even with data encryption, the adversary can infer the underlying network structure by exploiting the memory and timing side-channels. We further identify the information leakage on the values of weights when a CNN accelerator performs dynamic zero pruning for off-chip memory accesses. Overall, this work reveals the importance of hiding off-chip memory access pattern to truly protect confidential CNN models.",
      "year": 2018,
      "venue": "Design Automation Conference",
      "authors": [
        "Weizhe Hua",
        "Zhiru Zhang",
        "G. Suh"
      ],
      "citation_count": 274,
      "url": "https://www.semanticscholar.org/paper/6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators",
      "abstract": "Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.",
      "year": 2018,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Lingxiao Wei",
        "Yannan Liu",
        "Bo Luo",
        "Yu LI",
        "Qiang Xu"
      ],
      "citation_count": 220,
      "url": "https://www.semanticscholar.org/paper/669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "pdf_url": "https://arxiv.org/pdf/1803.05847",
      "publication_date": "2018-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 177,
      "url": "https://www.semanticscholar.org/paper/35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "905ad646e5745afe6a3b02617cd8452655232c0d",
      "title": "CSI Neural Network: Using Side-channels to Recover Your Artificial Neural Network Information",
      "abstract": "Machine learning has become mainstream across industries. Numerous examples proved the validity of it for security applications. In this work, we investigate how to reverse engineer a neural network by using only power side-channel information. To this end, we consider a multilayer perceptron as the machine learning architecture of choice and assume a non-invasive and eavesdropping attacker capable of measuring only passive side-channel leakages like power consumption, electromagnetic radiation, and reaction time. \nWe conduct all experiments on real data and common neural net architectures in order to properly assess the applicability and extendability of those attacks. Practical results are shown on an ARM CORTEX-M3 microcontroller. Our experiments show that the side-channel attacker is capable of obtaining the following information: the activation functions used in the architecture, the number of layers and neurons in the layers, the number of output classes, and weights in the neural network. Thus, the attacker can effectively reverse engineer the network using side-channel information. \nNext, we show that once the attacker has the knowledge about the neural network architecture, he/she could also recover the inputs to the network with only a single-shot measurement. Finally, we discuss several mitigations one could use to thwart such attacks.",
      "year": 2018,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/905ad646e5745afe6a3b02617cd8452655232c0d",
      "pdf_url": "",
      "publication_date": "2018-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "title": "Law and Adversarial Machine Learning",
      "abstract": "When machine learning systems fail because of adversarial manipulation, how should society expect the law to respond? Through scenarios grounded in adversarial ML literature, we explore how some aspects of computer crime, copyright, and tort law interface with perturbation, poisoning, model stealing and model inversion attacks to show how some attacks are more likely to result in liability than others. We end with a call for action to ML researchers to invest in transparent benchmarks of attacks and defenses; architect ML systems with forensics in mind and finally, think more about adversarial machine learning in the context of civil liberties. The paper is targeted towards ML researchers who have no legal background.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "R. Kumar",
        "David R. O'Brien",
        "Kendra Albert",
        "Salome Vilojen"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "pdf_url": "",
      "publication_date": "2018-10-25",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "08c1a578af25fbca2f5bf86d8a9752503b5805e7",
      "title": "DNN model extraction attacks using prediction interfaces",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "A. Dmitrenko"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/08c1a578af25fbca2f5bf86d8a9752503b5805e7",
      "pdf_url": "",
      "publication_date": "2018-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "479779a8de7147f09ffece0c0894e7b2858f0c42",
      "title": "The Impact of Message Length and Medium on Imitation Attack Creation and Detection by Humans",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Sarah Lazbin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/479779a8de7147f09ffece0c0894e7b2858f0c42",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}