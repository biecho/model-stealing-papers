{
  "owasp_id": "ML03",
  "owasp_name": "Model Inversion Attack",
  "total": 45,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "2111.07380",
      "title": "Eluding Secure Aggregation in Federated Learning via Model Inconsistency",
      "abstract": "Secure aggregation is a cryptographic protocol that securely computes the aggregation of its inputs. It is pivotal in keeping model updates private in federated learning. Indeed, the use of secure aggregation prevents the server from learning the value and the source of the individual model updates provided by the users, hampering inference and data attribution attacks. In this work, we show that a malicious server can easily elude secure aggregation as if the latter were not in place. We devise two different attacks capable of inferring information on individual private training datasets, independently of the number of users participating in the secure aggregation. This makes them concrete threats in large-scale, real-world federated learning applications. The attacks are generic and equally effective regardless of the secure aggregation protocol used. They exploit a vulnerability of the federated learning protocol caused by incorrect usage of secure aggregation and lack of parameter validation. Our work demonstrates that current implementations of federated learning with secure aggregation offer only a \"false sense of security\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Danilo Francati",
        "Giuseppe Ateniese"
      ],
      "url": "https://arxiv.org/abs/2111.07380",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W7104672862",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "CISPA Helmholtz Center",
      "authors": [
        "Ye, Dayong",
        "Zhu, Tianqing",
        "Wang Shang",
        "Liu Bo",
        "Yu Zhang, Leo",
        "Zhou, Wanlei",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W7104672862",
      "pdf_url": "https://doi.org/10.60882/cispa.30581237.v1",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4416075237",
      "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
      "abstract": "Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yanting Wang",
        "Wei Zou",
        "Rongli Geng",
        "Jinyuan Jia"
      ],
      "url": "https://openalex.org/W4416075237",
      "pdf_url": "https://arxiv.org/pdf/2506.04202",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2926319231",
      "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
      "abstract": "Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. In this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model ({\\UGAN}) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ahmed Salem",
        "Apratim Bhattacharya",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W2926319231",
      "pdf_url": "https://arxiv.org/pdf/1904.01067",
      "cited_by_count": 43,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3112689365",
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss",
        "Katherine Lee",
        "Adam P. Roberts",
        "T. B. Brown",
        "Dawn Song",
        "\u00dalfar Erlingsson",
        "Alina Oprea",
        "Colin Raffel"
      ],
      "url": "https://openalex.org/W3112689365",
      "pdf_url": "https://arxiv.org/pdf/2012.07805",
      "cited_by_count": 274,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3027379683",
      "title": "Analyzing Information Leakage of Updates to Natural Language Models",
      "abstract": "To continuously improve quality and reflect changes in data, machine learning\\napplications have to regularly retrain and update their core models. We show\\nthat a differential analysis of language model snapshots before and after an\\nupdate can reveal a surprising amount of detailed information about changes in\\nthe training data. We propose two new metrics---\\\\emph{differential score} and\\n\\\\emph{differential rank}---for analyzing the leakage due to updates of natural\\nlanguage models. We perform leakage analysis using these metrics across models\\ntrained on several different datasets using different methods and\\nconfigurations. We discuss the privacy implications of our findings, propose\\nmitigation strategies and evaluate their effect.\\n",
      "year": 2020,
      "venue": null,
      "authors": [
        "Santiago Zanella-B\u00e9guelin",
        "Lukas Wutschitz",
        "Shruti Tople",
        "Victor R\u00fchle",
        "Andrew Paverd",
        "Olga Ohrimenko",
        "Boris K\u00f6pf",
        "Marc Brockschmidt"
      ],
      "url": "https://openalex.org/W3027379683",
      "pdf_url": "https://arxiv.org/pdf/1912.07942",
      "cited_by_count": 85,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3213807399",
      "title": "Property Inference Attacks Against GANs",
      "abstract": "While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN\u2019s training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Junhao Zhou",
        "Yufei Chen",
        "Chao Shen",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3213807399",
      "pdf_url": "https://doi.org/10.14722/ndss.2022.23019",
      "cited_by_count": 39,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4226117300",
      "title": "MIRROR: Model Inversion for Deep Learning Network with High Fidelity",
      "abstract": "Model inversion reverse-engineers input samples from a given model, and hence poses serious threats to information confidentiality.We propose a novel inversion technique based on StyleGAN, whose generator has a special architecture that forces the decomposition of an input to styles of various granularities such that the model can learn them separately in training.During sample generation, the generator transforms a latent value to parameters controlling these styles to compose a sample.In our inversion, given a target label of some subject model to invert (e.g., a private face based identity recognition model), our technique leverages a StyleGAN trained on public data from the same domain (e.g., a public human face dataset), uses the gradient descent or genetic search algorithm, together with distribution based clipping, to find a proper parameterization of the styles such that the generated sample is correctly classified to the target label (by the subject model) and recognized by humans.The results show that our inverted samples have high fidelity, substantially better than those by existing state-of-the-art techniques.",
      "year": 2022,
      "venue": null,
      "authors": [
        "Shengwei An",
        "Guanhong Tao",
        "Qiuling Xu",
        "Yingqi Liu",
        "Guangyu Shen",
        "Yuan Yao",
        "Jingwei Xu",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W4226117300",
      "pdf_url": "https://doi.org/10.14722/ndss.2022.24335",
      "cited_by_count": 38,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2302.00539",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Ahmed Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "url": "https://arxiv.org/abs/2302.00539",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391724747",
      "title": "Timing Channels in Adaptive Neural Networks",
      "abstract": "Current machine learning systems offer great predictive power but also require significant computational resources.As a result, the promise of a class of optimized machine learning models, called adaptive neural networks (ADNNs), has seen recent wide appeal.These models make dynamic decisions about the amount of computation to perform based on the given input, allowing for fast predictions on \"easy\" input.While various considerations of ADNNs have been extensively researched, how these input-dependent optimizations might introduce vulnerabilities has been hitherto under-explored.Our work is the first to demonstrate and evaluate timing channels due to the optimizations of ADNNs with the capacity to leak sensitive attributes about a user's input.We empirically study six ADNNs types and demonstrate how an attacker can significantly improve their ability to infer sensitive attributes, such as class label, of another user's input from an observed timing measurement.Our results show that timing information can increase an attacker's probability of correctly inferring the attribute of the user's input by up to a factor of 9.89x.Our empirical evaluation uses four different datasets, including those containing sensitive medical and demographic information, and considers leakage across a variety of sensitive attributes of the user's input.We conclude by demonstrating how timing channels can be exploited across the public internet in two fictitious web applications -Fictitious Health Company and Fictitious HR -that make use of ADNNs for serving predictions to their clients.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Ayomide Akinsanya",
        "Tegan Brennan"
      ],
      "url": "https://openalex.org/W4391724747",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.24125",
      "cited_by_count": 1,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2401.07205",
      "title": "Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models",
      "abstract": "With the increased capabilities at the edge (e.g., mobile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Shiming Wang",
        "Zhe Ji",
        "Liyao Xiang",
        "Hao Zhang",
        "Xinbing Wang",
        "Chenghu Zhou",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2401.07205",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417136254",
      "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
      "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a target model with limited access, have gained increasing attention in recent years. However, there is currently no consensus on a formal definition of data reconstruction attacks or appropriate evaluation metrics for measuring their quality. This lack of rigorous definitions and universal metrics has hindered further advancement in this field. In this paper, we address this issue in the vision domain by proposing a unified attack taxonomy and formal definitions of data reconstruction attacks. We first propose a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity. Additionally, we leverage large language models (LLMs) as a substitute for human judgment, enabling visual evaluation with an emphasis on high-quality reconstructions. Using our proposed taxonomy and metrics, we present a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research. Empirical results, primarily from a memorization perspective, not only validate the effectiveness of our metrics but also offer valuable insights for designing new attacks.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wen Rui",
        "Liu, Yiyong",
        "Backes, Michael",
        "Zhang Yang"
      ],
      "url": "https://openalex.org/W4417136254",
      "pdf_url": "https://arxiv.org/pdf/2506.07888",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3171497996",
      "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
      "abstract": "Secure multi-party machine learning allows several parties to build a model on their pooled data to increase utility while not explicitly sharing data with each other. We show that such multi-party computation can cause leakage of global dataset properties between the parties even when parties obtain only black-box access to the final model. In particular, a ``curious'' party can infer the distribution of sensitive attributes in other parties' data with high accuracy. This raises concerns regarding the confidentiality of properties pertaining to the whole dataset as opposed to individual data records. We show that our attack can leak population-level properties in datasets of different types, including tabular, text, and graph data. To understand and measure the source of leakage, we consider several models of correlation between a sensitive attribute and the rest of the data. Using multiple machine learning models, we show that leakage occurs even if the sensitive attribute is not included in the training data and has a low correlation with other attributes or the target variable.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wanrong Zhang",
        "Shruti Tople",
        "Olga Ohrimenko"
      ],
      "url": "https://openalex.org/W3171497996",
      "pdf_url": "https://arxiv.org/pdf/2006.07267",
      "cited_by_count": 28,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2012.02670",
      "title": "Unleashing the Tiger: Inference Attacks on Split Learning",
      "abstract": "We investigate the security of Split Learning -- a novel collaborative machine learning framework that enables peak performance by requiring minimal resources consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack is able to overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning. To make our results reproducible, we made our code available at https://github.com/pasquini-dario/SplitNN_FSHA.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Dario Pasquini",
        "Giuseppe Ateniese",
        "Massimo Bernaschi"
      ],
      "url": "https://arxiv.org/abs/2012.02670",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4281609047",
      "title": "Gradient Obfuscation Gives a False Sense of Security in Federated Learning",
      "abstract": "Federated learning has been proposed as a privacy-preserving machine learning framework that enables multiple clients to collaborate without sharing raw data. However, client privacy protection is not guaranteed by design in this framework. Prior work has shown that the gradient sharing strategies in federated learning can be vulnerable to data reconstruction attacks. In practice, though, clients may not transmit raw gradients considering the high communication cost or due to privacy enhancement requirements. Empirical studies have demonstrated that gradient obfuscation, including intentional obfuscation via gradient noise injection and unintentional obfuscation via gradient compression, can provide more privacy protection against reconstruction attacks. In this work, we present a new data reconstruction attack framework targeting the image classification task in federated learning. We show that commonly adopted gradient postprocessing procedures, such as gradient quantization, gradient sparsification, and gradient perturbation, may give a false sense of security in federated learning. Contrary to prior studies, we argue that privacy enhancement should not be treated as a byproduct of gradient compression. Additionally, we design a new method under the proposed framework to reconstruct the image at the semantic level. We quantify the semantic privacy leakage and compare with conventional based on image similarity scores. Our comparisons challenge the image data leakage evaluation schemes in the literature. The results emphasize the importance of revisiting and redesigning the privacy protection mechanisms for client data in existing federated learning algorithms.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Kai Yue",
        "Richeng Jin",
        "Chau-Wai Wong",
        "Dror Baron",
        "Huaiyu Dai"
      ],
      "url": "https://openalex.org/W4281609047",
      "pdf_url": "https://arxiv.org/pdf/2206.04055",
      "cited_by_count": 13,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2205.08443",
      "title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
      "abstract": "In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Dario Pasquini",
        "Mathilde Raynal",
        "Carmela Troncoso"
      ],
      "url": "https://arxiv.org/abs/2205.08443",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2303.12233",
      "title": "LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation",
      "abstract": "Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting. In this work we introduce LOKI, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters, and the weight gradients of data points between clients remain separate even through aggregation. With FedAVG and aggregation across 100 clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, and Tiny ImageNet. Using only a single training round, LOKI is able to leak 76-86% of all data samples.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Joshua C. Zhao",
        "Atul Sharma",
        "Ahmed Roushdy Elkordy",
        "Yahya H. Ezzeldin",
        "Salman Avestimehr",
        "Saurabh Bagchi"
      ],
      "url": "https://arxiv.org/abs/2303.12233",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2408.16913",
      "title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning",
      "abstract": "In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Zhuohang Li",
        "Andrew Lowy",
        "Jing Liu",
        "Toshiaki Koike-Akino",
        "Kieran Parsons",
        "Bradley Malin",
        "Ye Wang"
      ],
      "url": "https://arxiv.org/abs/2408.16913",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417256413",
      "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
      "abstract": "Federated learning (FL) enables collaborative model training among multiple clients without the need to expose raw data. Its ability to safeguard privacy, at the heart of FL, has recently been a hot-button debate topic. To elaborate, several studies have introduced a type of attacks known as gradient leakage attacks (GLAs), which exploit the gradients shared during training to reconstruct clients' raw data. On the flip side, some literature, however, contends no substantial privacy risk in practical FL environments due to the effectiveness of such GLAs being limited to overly relaxed conditions, such as small batch sizes and knowledge of clients' data distributions. This paper bridges this critical gap by empirically demonstrating that clients' data can still be effectively reconstructed, even within realistic FL environments. Upon revisiting GLAs, we recognize that their performance failures stem from their inability to handle the gradient matching problem. To alleviate the performance bottlenecks identified above, we develop FedLeak, which introduces two novel techniques, partial gradient matching and gradient regularization. Moreover, to evaluate the performance of FedLeak in real-world FL environments, we formulate a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices. Under this protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby underscoring the significant vulnerability in FL systems and the urgent need for more effective defense methods.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mingyuan Fan",
        "Fuyi Wang",
        "Cen Chen",
        "Jianying Zhou"
      ],
      "url": "https://openalex.org/W4417256413",
      "pdf_url": "https://arxiv.org/pdf/2506.08435",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4310827164",
      "title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning",
      "abstract": "Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and performance maintenance. The utility metric is designed to promote consistency between the gradients of key parameters associated with robust data and those derived from clients' data, thus maintaining model performance. Furthermore, the privacy metric guides the generation of robust data towards enlarging the semantic gap with clients' data. Theoretical analysis supports the effectiveness of Refiner, and empirical evaluations on multiple benchmark datasets demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mingyuan Fan",
        "Cen Chen",
        "Chengyu Wang",
        "Wenmeng Zhou",
        "Jun Huang",
        "Ximeng Liu",
        "Wenzhong Guo"
      ],
      "url": "https://openalex.org/W4310827164",
      "pdf_url": "https://arxiv.org/pdf/2212.02042",
      "cited_by_count": 1,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4394642568",
      "title": "SoK: On Gradient Leakage in Federated Learning",
      "abstract": "Federated learning (FL) facilitates collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from shared gradients in FL, a vulnerability known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \\emph{ideal settings and auxiliary assumptions}, their actual efficacy against \\emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a timeline to trace their evolution and develops a systematization to uncover their inherent threats. By rethinking GIA in practical FL systems, three fundamental aspects influencing GIA's effectiveness are identified: \\textit{training setup}, \\textit{model}, and \\textit{post-processing}. Guided by these aspects, we perform extensive theoretical and empirical evaluations of SOTA GIAs across diverse settings. Our findings highlight that GIA is notably \\textit{constrained}, \\textit{fragile}, and \\textit{easily defensible}. Specifically, GIAs exhibit inherent limitations against practical local training settings. Additionally, their effectiveness is highly sensitive to the trained model, and even simple post-processing techniques applied to gradients can serve as effective defenses. Our work provides crucial insights into the limited threats of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiacheng Du",
        "Jiahui Hu",
        "Zhibo Wang",
        "Peng Sun",
        "Neil Zhenqiang Gong",
        "Kui Ren"
      ],
      "url": "https://openalex.org/W4394642568",
      "pdf_url": "https://arxiv.org/pdf/2404.05403",
      "cited_by_count": 1,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W7116111594",
      "title": "From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning",
      "abstract": "Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xu, Xiangrui",
        "Li, Zhize",
        "Han, Yufei",
        "Wang, Bin",
        "Liu, Jiqiang",
        "Wang, Wei"
      ],
      "url": "https://openalex.org/W7116111594",
      "pdf_url": "https://doi.org/10.48550/arxiv.2512.15460",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3046764764",
      "title": "Privacy Risks of General-Purpose Language Models",
      "abstract": "Recently, a new paradigm of building general-purpose language models (e.g., Google's Bert and OpenAI's GPT-2) in Natural Language Processing (NLP) for text feature extraction, a standard procedure in NLP systems that converts texts to vectors (i.e., embeddings) for downstream modeling, has arisen and starts to find its application in various downstream NLP tasks and real world systems (e.g., Google's search engine [6]). To obtain general-purpose text embeddings, these language models have highly complicated architectures with millions of learnable parameters and are usually pretrained on billions of sentences before being utilized. As is widely recognized, such a practice indeed improves the state-of-the-art performance of many downstream NLP tasks. However, the improved utility is not for free. We find the text embeddings from general-purpose language models would capture much sensitive information from the plain text. Once being accessed by the adversary, the embeddings can be reverse-engineered to disclose sensitive information of the victims for further harassment. Although such a privacy risk can impose a real threat to the future leverage of these promising NLP tools, there are neither published attacks nor systematic evaluations by far for the mainstream industry-level language models. To bridge this gap, we present the first systematic study on the privacy risks of 8 state-of-the-art language models with 4 diverse case studies. By constructing 2 novel attack classes, our study demonstrates the aforementioned privacy risks do exist and can impose practical threats to the application of general-purpose language models on sensitive data covering identity, genome, healthcare and location. For example, we show the adversary with nearly no prior knowledge can achieve about 75% accuracy when inferring the precise disease site from Bert embeddings of patients' medical descriptions. As possible countermeasures, we propose 4 different defenses (via rounding, differential privacy, adversarial training and subspace projection) to obfuscate the unprotected embeddings for mitigation purpose. With extensive evaluations, we also provide a preliminary analysis on the utility-privacy trade-off brought by each defense, which we hope may foster future mitigation researches.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Xudong Pan",
        "Mi Zhang",
        "Shouling Ji",
        "Min Yang"
      ],
      "url": "https://openalex.org/W3046764764",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152761.pdf",
      "cited_by_count": 154,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2004.00053",
      "title": "Information Leakage in Embedding Models",
      "abstract": "Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains.   We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50\\%--70\\% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Congzheng Song",
        "Ananth Raghunathan"
      ],
      "url": "https://arxiv.org/abs/2004.00053",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2105.12049",
      "title": "Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "url": "https://arxiv.org/abs/2105.12049",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_e8f3ac3f",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.",
      "year": 2021,
      "venue": "USENIX Security",
      "authors": [
        "Mohammad Malekzadeh",
        "Anastasia Borovykh",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "url": "https://arxiv.org/abs/2105.12049",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2108.06504",
      "title": "LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis",
      "abstract": "Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Fan Wu",
        "Yunhui Long",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2108.06504",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2204.06963",
      "title": "Finding MNEMON: Reviving Memories of Node Embeddings",
      "abstract": "Previous security research efforts orbiting around graphs have been exclusively focusing on either (de-)anonymizing the graphs or understanding the security and privacy issues of graph neural networks. Little attention has been paid to understand the privacy risks of integrating the output from graph embedding models (e.g., node embeddings) with complex downstream machine learning pipelines. In this paper, we fill this gap and propose a novel model-agnostic graph recovery attack that exploits the implicit graph structural information preserved in the embeddings of graph nodes. We show that an adversary can recover edges with decent accuracy by only gaining access to the node embedding matrix of the original graph without interactions with the node embedding models. We demonstrate the effectiveness and applicability of our graph recovery attack through extensive experiments.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yun Shen",
        "Yufei Han",
        "Zhikun Zhang",
        "Min Chen",
        "Ting Yu",
        "Michael Backes",
        "Yang Zhang",
        "Gianluca Stringhini"
      ],
      "url": "https://arxiv.org/abs/2204.06963",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2209.01100",
      "title": "Group Property Inference Attacks Against Graph Neural Networks",
      "abstract": "With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet.   In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA's success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Xiuling Wang",
        "Wendy Hui Wang"
      ],
      "url": "https://arxiv.org/abs/2209.01100",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2205.03105",
      "title": "LPGNet: Link Private Graph Networks for Node Classification",
      "abstract": "Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than \"trivially\" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Aashish Kolluri",
        "Teodora Baluta",
        "Bryan Hooi",
        "Prateek Saxena"
      ],
      "url": "https://arxiv.org/abs/2205.03105",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2501.10985",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nian-Feng Tzeng"
      ],
      "url": "https://arxiv.org/abs/2501.10985",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2209.01292",
      "title": "Are Attribute Inference Attacks Just Imputation?",
      "abstract": "Models can expose sensitive information about their training data. In an attribute inference attack, an adversary has partial knowledge of some training records and access to a model trained on those records, and infers the unknown values of a sensitive feature of those records. We study a fine-grained variant of attribute inference we call \\emph{sensitive value inference}, where the adversary's goal is to identify with high confidence some records from a candidate set where the unknown attribute has a particular sensitive value. We explicitly compare attribute inference with data imputation that captures the training distribution statistics, under various assumptions about the training data available to the adversary. Our main conclusions are: (1) previous attribute inference methods do not reveal more about the training data from the model than can be inferred by an adversary without access to the trained model, but with the same knowledge of the underlying distribution as needed to train the attribute inference attack; (2) black-box attribute inference attacks rarely learn anything that cannot be learned without the model; but (3) white-box attacks, which we introduce and evaluate in the paper, can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model. Furthermore, we show that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk. The code for our experiments is available at \\url{https://github.com/bargavj/EvaluatingDPML}.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "url": "https://arxiv.org/abs/2209.01292",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4308361263",
      "title": "Feature Inference Attack on Shapley Values",
      "abstract": "As a solution concept in cooperative game theory, Shapley value is highly\\nrecognized in model interpretability studies and widely adopted by the leading\\nMachine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and\\nIBM. However, as the Shapley value-based model interpretability methods have\\nbeen thoroughly studied, few researchers consider the privacy risks incurred by\\nShapley values, despite that interpretability and privacy are two foundations\\nof machine learning (ML) models.\\n In this paper, we investigate the privacy risks of Shapley value-based model\\ninterpretability methods using feature inference attacks: reconstructing the\\nprivate model inputs based on their Shapley value explanations. Specifically,\\nwe present two adversaries. The first adversary can reconstruct the private\\ninputs by training an attack model based on an auxiliary dataset and black-box\\naccess to the model interpretability services. The second adversary, even\\nwithout any background knowledge, can successfully reconstruct most of the\\nprivate features by exploiting the local linear correlations between the model\\ninputs and outputs. We perform the proposed attacks on the leading MLaaS\\nplatforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The\\nexperimental results demonstrate the vulnerability of the state-of-the-art\\nShapley value-based model interpretability methods used in the leading MLaaS\\nplatforms and highlight the significance and necessity of designing\\nprivacy-preserving model interpretability methods in future studies. To our\\nbest knowledge, this is also the first work that investigates the privacy risks\\nof Shapley values.\\n",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Xinjian Luo",
        "Yang-Fan Jiang",
        "Xiaokui Xiao"
      ],
      "url": "https://openalex.org/W4308361263",
      "pdf_url": "https://doi.org/10.1145/3548606.3560573",
      "cited_by_count": 18,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2211.05249",
      "title": "QuerySnout: Automating the Discovery of Attribute Inference Attacks against Query-Based Systems",
      "abstract": "Although query-based systems (QBS) have become one of the main solutions to share data anonymously, building QBSes that robustly protect the privacy of individuals contributing to the dataset is a hard problem. Theoretical solutions relying on differential privacy guarantees are difficult to implement correctly with reasonable accuracy, while ad-hoc solutions might contain unknown vulnerabilities. Evaluating the privacy provided by QBSes must thus be done by evaluating the accuracy of a wide range of privacy attacks. However, existing attacks require time and expertise to develop, need to be manually tailored to the specific systems attacked, and are limited in scope. In this paper, we develop QuerySnout (QS), the first method to automatically discover vulnerabilities in QBSes. QS takes as input a target record and the QBS as a black box, analyzes its behavior on one or more datasets, and outputs a multiset of queries together with a rule to combine answers to them in order to reveal the sensitive attribute of the target record. QS uses evolutionary search techniques based on a novel mutation operator to find a multiset of queries susceptible to lead to an attack, and a machine learning classifier to infer the sensitive attribute from answers to the queries selected. We showcase the versatility of QS by applying it to two attack scenarios, three real-world datasets, and a variety of protection mechanisms. We show the attacks found by QS to consistently equate or outperform, sometimes by a large margin, the best attacks from the literature. We finally show how QS can be extended to QBSes that require a budget, and apply QS to a simple QBS based on the Laplace mechanism. Taken together, our results show how powerful and accurate attacks against QBSes can already be found by an automated system, allowing for highly complex QBSes to be automatically tested \"at the pressing of a button\".",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Ana-Maria Cretu",
        "Florimond Houssiau",
        "Antoine Cully",
        "Yves-Alexandre de Montjoye"
      ],
      "url": "https://arxiv.org/abs/2211.05249",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4415981201",
      "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
      "abstract": "As machine learning (ML) technologies become more prevalent in privacy-sensitive areas like healthcare and finance, eventually incorporating sensitive information in building data-driven algorithms, it is vital to scrutinize whether these data face any privacy leakage risks. One potential threat arises from an adversary querying trained models using the public, non-sensitive attributes of entities in the training data to infer their private, sensitive attributes, a technique known as the attribute inference attack. This attack is particularly deceptive because, while it may perform poorly in predicting sensitive attributes across the entire dataset, it excels at predicting the sensitive attributes of records from a few vulnerable groups, a phenomenon known as disparate vulnerability. This paper illustrates that an adversary can take advantage of this disparity to carry out a series of new attacks, showcasing a threat level beyond previous imagination. We first develop a novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset. We then introduce two targeted variations of the attribute inference attack that can identify and exploit a vulnerable subset of the training data, marking the first instances of targeted attacks in this category, achieving significantly higher accuracy than untargeted versions. We are also the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ehsanul Kabir",
        "Leone Craig",
        "Shagufta Mehnaz"
      ],
      "url": "https://openalex.org/W4415981201",
      "pdf_url": "https://arxiv.org/pdf/2504.04033",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2208.12348",
      "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
      "abstract": "Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners sharing their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed, but they all rely on the attacker training a large number of shadow models, which induces a large computational overhead.   In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model confidences under poisoning, we design an efficient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. For example, on the Census dataset, SNAP achieves 34% higher success rate than Mahloujifar et al. while being 56.5x faster. We also extend our attack to infer whether a certain property was present at all during training and estimate the exact proportion of a property of interest efficiently. We evaluate our attack on several properties of varying proportions from four datasets and demonstrate SNAP's generality and effectiveness. An open-source implementation of SNAP can be found at https://github.com/johnmath/snap-sp23.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Harsh Chaudhari",
        "John Abascal",
        "Alina Oprea",
        "Matthew Jagielski",
        "Florian Tram\u00e8r",
        "Jonathan Ullman"
      ],
      "url": "https://arxiv.org/abs/2208.12348",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3176305602",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3176305602",
      "pdf_url": "https://arxiv.org/pdf/2106.12478",
      "cited_by_count": 3,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2503.09022",
      "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
      "abstract": "Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity.   In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Wenjie Qu",
        "Yuguang Zhou",
        "Yongji Wu",
        "Tingsong Xiao",
        "Binhang Yuan",
        "Yiming Li",
        "Jiaheng Zhang"
      ],
      "url": "https://arxiv.org/abs/2503.09022",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2503.09291",
      "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
      "abstract": "The inference process of modern large language models (LLMs) demands prohibitive computational resources, rendering them infeasible for deployment on consumer-grade devices. To address this limitation, recent studies propose distributed LLM inference frameworks, which employ split learning principles to enable collaborative LLM inference on resource-constrained hardware. However, distributing LLM layers across participants requires the transmission of intermediate outputs, which may introduce privacy risks to the original input prompts - a critical issue that has yet to be thoroughly explored in the literature.   In this paper, we rigorously examine the privacy vulnerabilities of distributed LLM inference frameworks by designing and evaluating three prompt inference attacks aimed at reconstructing input prompts from intermediate LLM outputs. These attacks are developed under various query and data constraints to reflect diverse real-world LLM service scenarios. Specifically, the first attack assumes an unlimited query budget and access to an auxiliary dataset sharing the same distribution as the target prompts. The second attack also leverages unlimited queries but uses an auxiliary dataset with a distribution differing from the target prompts. The third attack operates under the most restrictive scenario, with limited query budgets and no auxiliary dataset available. We evaluate these attacks on a range of LLMs, including state-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used models like GPT-2 and BERT for comparative analysis. Our experiments show that the first two attacks achieve reconstruction accuracies exceeding 90%, while the third achieves accuracies typically above 50%, even under stringent constraints. These findings highlight privacy risks in distributed LLM inference frameworks, issuing a strong alert on their deployment in real-world applications.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Xinjian Luo",
        "Ting Yu",
        "Xiaokui Xiao"
      ],
      "url": "https://arxiv.org/abs/2503.09291",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4414421755",
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into daily routines, yet they raise significant privacy and safety concerns. Recent research proposes collaborative inference, which outsources the early-layer inference to ensure data locality, and introduces model safety auditing based on inner neuron patterns. Both techniques expose the LLM's Internal States (ISs), which are traditionally considered irreversible to inputs due to optimization challenges and the highly abstract representations in deep layers. In this work, we challenge this assumption by proposing four inversion attacks that significantly improve the semantic similarity and token matching rate of inverted inputs. Specifically, we first develop two white-box optimization-based attacks tailored for low-depth and high-depth ISs. These attacks avoid local minima convergence, a limitation observed in prior work, through a two-phase inversion process. Then, we extend our optimization attack under more practical black-box weight access by leveraging the transferability between the source and the derived LLMs. Additionally, we introduce a generation-based attack that treats inversion as a translation task, employing an inversion model to reconstruct inputs. Extensive evaluation of short and long prompts from medical consulting and coding assistance datasets and 6 LLMs validates the effectiveness of our inversion attacks. Notably, a 4,112-token long medical consulting prompt can be nearly perfectly inverted with 86.88 F1 token matching from the middle layer of Llama-3 model. Finally, we evaluate four practical defenses that we found cannot perfectly prevent ISs inversion and draw conclusions for future mitigation design.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Tian Dong",
        "Yan Meng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Zhen Liu",
        "Haojin Zhu"
      ],
      "url": "https://openalex.org/W4414421755",
      "pdf_url": "https://arxiv.org/pdf/2507.16372",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4402427674",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "abstract": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yingzhen Liu",
        "Y. Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W4402427674",
      "pdf_url": "https://arxiv.org/pdf/2408.07291",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4414684146",
      "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
      "abstract": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jinwen He",
        "Yi Lu",
        "Zijin Lin",
        "Kai Chen",
        "Yue Zhao"
      ],
      "url": "https://openalex.org/W4414684146",
      "pdf_url": "https://arxiv.org/pdf/2506.19563",
      "cited_by_count": 0,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3126152116",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "Ahmed Salem",
        "Zhikun Zhang",
        "Michael Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W3126152116",
      "pdf_url": "https://arxiv.org/pdf/2102.02551",
      "cited_by_count": 46,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391724770",
      "title": "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks",
      "abstract": "Image data have been extensively used in Deep Neural Network (DNN) tasks in various scenarios, e.g., autonomous driving and medical image analysis, which incurs significant privacy concerns.Existing privacy protection techniques are unable to efficiently protect such data.For example, Differential Privacy (DP) that is an emerging technique protects data with strong privacy guarantee cannot effectively protect visual features of exposed image dataset.In this paper, we propose a novel privacy-preserving framework VisualMixer that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises.VisualMixer utilizes a new privacy metric called Visual Feature Entropy (VFE) to effectively quantify the visual features of an image from both biological and machine vision aspects.In VisualMixer, we devise a task-agnostic image obfuscation method to protect the visual privacy of data for DNN training and inference.For each image, it determines regions for pixel shuffling in the image and the sizes of these regions according to the desired VFE.It shuffles pixels both in the spatial domain and in the chromatic channel space in the regions without injecting noises so that it can prevent visual features from being discerned and recognized, while incurring negligible accuracy loss.Extensive experiments on real-world datasets demonstrate that VisualMixer can effectively preserve the visual privacy with negligible accuracy loss, i.e., at average 2.35 percentage points of model accuracy loss, and almost no performance degradation on model training.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Qiushi Li",
        "Yan Zhang",
        "Ju Ren",
        "Qi Li",
        "Yaoxue Zhang"
      ],
      "url": "https://openalex.org/W4391724770",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.241361",
      "cited_by_count": 25,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2405.06823",
      "title": "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
      "abstract": "Large Language Models (LLMs) enable a new ecosystem with many downstream applications, called LLM applications, with different natural language processing tasks. The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform. Therefore, an LLM application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an LLM application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness.   In this paper, we design a novel, closed-box prompt leaking attack framework, called PLeak, to optimize an adversarial query such that when the attacker sends it to a target LLM application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt.   We evaluate PLeak in both offline settings and for real-world LLM applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that PLeak can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/BHui97/PLeak.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Philippe Burlina",
        "Yinzhi Cao"
      ],
      "url": "https://arxiv.org/abs/2405.06823",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    }
  ]
}