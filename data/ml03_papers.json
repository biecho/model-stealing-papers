{
  "updated": "2026-01-06",
  "total": 196,
  "owasp_id": "ML03",
  "owasp_name": "Model Inversion Attack",
  "description": "Attacks that reconstruct or recover private training data from machine learning\n        models. This includes model inversion attacks that recover input features,\n        gradient leakage attacks, training data reconstruction, attribute inference,\n        and techniques to extract sensitive information about training samples from\n        model outputs or gradients. Focus is on privacy of TRAINING DATA, not the model.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "6a5c8b5f4fde3590812b9d8a53e9dbe1e43c9de5",
      "title": "EVAA\u2014Exchange Vanishing Adversarial Attack on LiDAR Point Clouds in Autonomous Vehicles",
      "abstract": "In addition to red-green-blue (RGB) camera sensors, light detection and ranging (LiDAR) plays an important role in autonomous vehicles (AVs) to perceive their surroundings. Deep neural networks (DNNs) are able to achieve cutting-edge 3-D object detection and segmentation performance using LiDAR point clouds. LiDAR-enabled AVs provide human perception by segmenting LiDAR point clouds into meaningful regions and providing semantic context to the AV user. However, the generation of point clouds to provide semantic segmentation in AVs is not reliable and secure, which may result in traffic accidents. We propose a novel adversarial attack against LiDAR point clouds in AVs in this article. We devised an exchange vanishing adversarial attack (EVAA) to deceive LiDAR point clouds by introducing targeted noise on specific objects (e.g., vehicles and driveways). In two autonomous driving datasets with 3-D object annotations, NuScenes and PandaSet, we evaluate the performance of our proposed attack framework. We achieve an attack success rate (ASR) of <inline-formula> <tex-math notation=\"LaTeX\">$\\approx 63$ </tex-math></inline-formula>% and ASR of <inline-formula> <tex-math notation=\"LaTeX\">$\\approx 29$ </tex-math></inline-formula>% on both NuScenes and PandaSet datasets, respectively.",
      "year": 2023,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Vishnu Chalavadi",
        "Jayesh Khandelwal",
        "C. Krishna Mohan",
        "Cenkeramaddi Linga Reddy"
      ],
      "citation_count": 81,
      "url": "https://www.semanticscholar.org/paper/6a5c8b5f4fde3590812b9d8a53e9dbe1e43c9de5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6a662cec0ad532371099a1397fbd4196ab8d8a1",
      "title": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
      "abstract": "Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\\times$64, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at https://github.com/forever208/DDPM-IP",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mang Ning",
        "E. Sangineto",
        "Angelo Porrello",
        "S. Calderara",
        "R. Cucchiara"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/e6a662cec0ad532371099a1397fbd4196ab8d8a1",
      "pdf_url": "http://arxiv.org/pdf/2301.11706",
      "publication_date": "2023-01-27",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "15fef7a8b6468e924332ac99da342fefdf0fd695",
      "title": "Input Perturbation: A New Paradigm between Central and Local Differential Privacy",
      "abstract": "Traditionally, there are two models on differential privacy: the central model and the local model. The central model focuses on the machine learning model and the local model focuses on the training data. In this paper, we study the \\textit{input perturbation} method in differentially private empirical risk minimization (DP-ERM), preserving privacy of the central model. By adding noise to the original training data and training with the `perturbed data', we achieve ($\\epsilon$,$\\delta$)-differential privacy on the final model, along with some kind of privacy on the original data. We observe that there is an interesting connection between the local model and the central model: the perturbation on the original data causes the perturbation on the gradient, and finally the model parameters. This observation means that our method builds a bridge between local and central model, protecting the data, the gradient and the model simultaneously, which is more superior than previous central methods. Detailed theoretical analysis and experiments show that our method achieves almost the same (or even better) performance as some of the best previous central methods with more protections on privacy, which is an attractive result. Moreover, we extend our method to a more general case: the loss function satisfies the Polyak-Lojasiewicz condition, which is more general than strong convexity, the constraint on the loss function in most previous work.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Yilin Kang",
        "Yong Liu",
        "Ben Niu",
        "Xin-Yi Tong",
        "Likun Zhang",
        "Weiping Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/15fef7a8b6468e924332ac99da342fefdf0fd695",
      "pdf_url": "",
      "publication_date": "2020-02-20",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a0f2d2a261776cdf057fc0dce00784d215557e47",
      "title": "Differentially Private Empirical Risk Minimization with Input Perturbation",
      "abstract": "We propose a novel framework for the differentially private ERM, input perturbation. Existing differentially private ERM implicitly assumed that the data contributors submit their private data to a database expecting that the database invokes a differentially private mechanism for publication of the learned model. In input perturbation, each data contributor independently randomizes her/his data by itself and submits the perturbed data to the database. We show that the input perturbation framework theoretically guarantees that the model learned with the randomized data eventually satisfies differential privacy with the prescribed privacy parameters. At the same time, input perturbation guarantees that local differential privacy is guaranteed to the server. We also show that the excess risk bound of the model learned with input perturbation is $O(1/n)$ under a certain condition, where $n$ is the sample size. This is the same as the excess risk bound of the state-of-the-art.",
      "year": 2017,
      "venue": "IFIP Working Conference on Database Semantics",
      "authors": [
        "Kazuto Fukuchi",
        "Quang Khai Tran",
        "Jun Sakuma"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/a0f2d2a261776cdf057fc0dce00784d215557e47",
      "pdf_url": "",
      "publication_date": "2017-10-15",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f47c57a8a5c44b80927bf8d49725e7e98090a61f",
      "title": "Differentially private multivariate time series forecasting of aggregated human mobility with deep learning: Input or gradient perturbation?",
      "abstract": "This paper investigates the problem of forecasting multivariate aggregated human mobility while preserving the privacy of the individuals concerned. Differential privacy, a state-of-the-art formal notion, has been used as the privacy guarantee in two different and independent steps when training deep learning models. On one hand, we considered gradient perturbation, which uses the differentially private stochastic gradient descent algorithm to guarantee the privacy of each time series sample in the learning stage. On the other hand, we considered input perturbation, which adds differential privacy guarantees in each sample of the series before applying any learning. We compared four state-of-the-art recurrent neural networks: Long Short-Term Memory, Gated Recurrent Unit, and their Bidirectional architectures, i.e., Bidirectional-LSTM and Bidirectional-GRU. Extensive experiments were conducted with a real-world multivariate mobility dataset, which we published openly along with this paper. As shown in the results, differentially private deep learning models trained under gradient or input perturbation achieve nearly the same performance as non-private deep learning models, with loss in performance varying between 0.57\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.57$$\\end{document} and 2.8%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2.8\\%$$\\end{document}. The contribution of this paper is significant for those involved in urban planning and decision-making, providing a solution to the human mobility multivariate forecast problem through differentially private deep learning models.",
      "year": 2022,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "H\u00e9ber H. Arcolezi",
        "Jean-Fran\u00e7ois Couchot",
        "Denis Renaud",
        "Bechara al Bouna",
        "X. Xiao"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/f47c57a8a5c44b80927bf8d49725e7e98090a61f",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s00521-022-07393-0.pdf",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "44e2dfde8884f1015c2de09c21a70e6e5e5935f9",
      "title": "Laplace Input and Output Perturbation for Differentially Private Principal Components Analysis",
      "abstract": "With the widespread application of big data, privacy-preserving data analysis has become a topic of increasing significance. The current research studies mainly focus on privacy-preserving classification and regression. However, principal component analysis (PCA) is also an effective data analysis method which can be used to reduce the data dimensionality, commonly used in data processing, machine learning, and data mining. In order to implement approximate PCA while preserving data privacy, we apply the Laplace mechanism to propose two differential privacy principal component analysis algorithms: Laplace input perturbation (LIP) and Laplace output perturbation (LOP). We evaluate the performance of LIP and LOP in terms of noise magnitude and approximation error theoretically and experimentally. In addition, we explore the variation of performance of the two algorithms with different parameters such as number of samples, target dimension, and privacy parameter. Theoretical and experimental results show that algorithm LIP adds less noise and has lower approximation error than LOP. To verify the effectiveness of algorithm LIP, we compare our LIP with other algorithms. The experimental results show that algorithm LIP can provide strong privacy guarantee and good data utility.",
      "year": 2019,
      "venue": "Secur. Commun. Networks",
      "authors": [
        "Yahong Xu",
        "Geng Yang",
        "Shuangjie Bai"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/44e2dfde8884f1015c2de09c21a70e6e5e5935f9",
      "pdf_url": "https://downloads.hindawi.com/journals/scn/2019/9169802.pdf",
      "publication_date": "2019-11-03",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d434cf4df3a2c9435acb7d430363a892087278c4",
      "title": "Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models",
      "abstract": "In recent years, deep neural networks (DNNs) have been successfully applied in various tasks, and various third-party models are available to data holders. However, data holders who blindly use third-party models to train on their data may lead to data leakage, resulting in serious data privacy problems. The Capacity Abuse Attack (CAA) is the state-of-the-art black-box attack method which uses the labels of the augmented malicious dataset to encode the information of the training data. However, the expanded malicious dataset in CAA are artificially synthesized, not natural images, and significantly different from the original training data. Thus these malicious images are easy to be detected. In our attack, we use a technique similar to generating poisoned datasets in backdoor attacks, make malicious data generated similar to real and natural images, and make our attack more concealed. Extensive experiments are conducted, and the results demonstrate that our attack can effectively obtain the private training data of data holders without significantly impacting the model's original task.",
      "year": 2022,
      "venue": "International Conference on Data Intelligence and Security",
      "authors": [
        "Xiangkai Yang",
        "Wenjian Luo",
        "Licai Zhang",
        "Zhijian Chen",
        "Jiahai Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d434cf4df3a2c9435acb7d430363a892087278c4",
      "pdf_url": "",
      "publication_date": "2022-08-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d1127180b22d9796ecc31b3ad8d2c2fd771b28d",
      "title": "Reversible attack based on local visible adversarial perturbation",
      "abstract": "Adding perturbation to images can mislead classification models to produce incorrect results. Based on this, research has exploited adversarial perturbation to protect private images from retrieval by malicious intelligent models. However, adding adversarial perturbation to images destroys the original data, making images useless in digital forensics and other fields. To prevent illegal or unauthorized access to sensitive image data such as human faces without impeding legitimate users, the use of reversible adversarial attack techniques is becoming more widely investigated, where the original image can be recovered from its reversible adversarial examples. However, existing reversible adversarial attack methods are designed for traditional imperceptible adversarial perturbation and ignore the local visible adversarial perturbation. In this paper, we propose a new method for generating reversible adversarial examples based on local visible adversarial perturbation. The information needed for image recovery is embedded into the area beyond the adversarial patch by the reversible data hiding technique. To reduce image distortion, lossless compression and the B-R-G (blue-red-green) embedding principle are adopted. Experiments on CIFAR-10 and ImageNet datasets show that the proposed method can restore the original images error-free while ensuring good attack performance.",
      "year": 2021,
      "venue": "Multimedia tools and applications",
      "authors": [
        "Li Chen",
        "Shaowei Zhu",
        "Abel Andrew",
        "Z. Yin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/7d1127180b22d9796ecc31b3ad8d2c2fd771b28d",
      "pdf_url": "",
      "publication_date": "2021-10-06",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d216fd33d3151c0781d3b1732f0e7d0637ad2b6",
      "title": "Face image de\u2010identification by feature space adversarial perturbation",
      "abstract": "Privacy leakage in images attracts increasing concerns these days, as photos uploaded to large social platforms are usually not processed by proper privacy protection mechanisms. Moreover, with advanced artificial intelligence (AI) tools such as deep neural network (DNN), an adversary can detect people's identities and collect other sensitive personal information from images at an unprecedented scale. In this paper, we introduce a novel face image de\u2010identification framework using adversarial perturbations in the feature space. Manipulating the feature space vector ensures the good transferability of our framework. Moreover, the proposed feature space adversarial perturbation generation algorithm can successfully protect the identity\u2010related information while ensuring the other attributes remain similar. Finally, we conduct extensive experiments on two face image datasets to evaluate the performance of the proposed method. Our results show that the proposed method can generate real\u2010looking privacy\u2010preserving images efficiently. Although our framework has only been tested on two real\u2010life face image datasets, it can be easily extended to other types of images.",
      "year": 2022,
      "venue": "Concurrency and Computation",
      "authors": [
        "Hanyu Xue",
        "Bo Liu",
        "Xinrui Yuan",
        "Ming Ding",
        "Tianqing Zhu"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/9d216fd33d3151c0781d3b1732f0e7d0637ad2b6",
      "pdf_url": "https://doi.org/10.1002/cpe.7554",
      "publication_date": "2022-12-13",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3f6d6eb3a427832143b130e7410f4c7bf18cbb00",
      "title": "Image to Perturbation: An Image Transformation Network for Generating Visually Protected Images for Privacy-Preserving Deep Neural Networks",
      "abstract": "We propose a novel image transformation network for generating visually protected images for privacy-preserving deep neural networks (DNNs). The proposed transformation network is trained by using a plain image dataset so that plain images are converted into visually protected ones. Conventional perceptual encryption methods cause some accuracy degradation in image classification and are not robust enough against state-of-the-art attacks. In contrast, the proposed network not only enables us to maintain the image classification accuracy that using plain images achieves but is also strongly robust against attacks including DNN-based ones. Furthermore, there is no need to manage any security keys as the conventional methods require. In an image classification experiment, the proposed network is demonstrated to strongly protect the visual information of plain images while maintaining a high classification accuracy under the use of two typical classification networks: ResNet and VGG. In addition, it is shown that the visually protected images are robust enough against various attacks in an experiment in which we tried to restore the visual information of plain images.",
      "year": 2021,
      "venue": "IEEE Access",
      "authors": [
        "Hiroki Ito",
        "Yuma Kinoshita",
        "Maungmaung Aprilpyone",
        "H. Kiya"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/3f6d6eb3a427832143b130e7410f4c7bf18cbb00",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09410466.pdf",
      "publication_date": null,
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e1157ad05e3c9320e6c18b3887ff67f86741864",
      "title": "A Privacy-Aware and Incremental Defense Method Against GAN-Based Poisoning Attack",
      "abstract": "Federated learning is usually utilized as a fraud detection framework in the domain of financial risk management, which promotes the model accuracy without training data exchange. One of the challenges in federated learning is the GAN-based poisoning attack. The GAN-based poisoning attack is a type of intractable poisoning attack that causes global model accuracy degradation and privacy leak. Most of the existing defenses for GAN-based poisoning attack have the three problems: 1) dependence on validation datasets; 2) incompetence of dealing with incremental poisoning attack; and 3) privacy leak. To address the above problems, we present a privacy-aware and incremental defense (PID) method to detect malicious participants and protect privacy. In PID, we design a method to accumulate the offset of model parameters from participants in all current epochs to represent the moving tendency for model parameters. Thus, we can distinguish the adversaries from normal participants based on the accumulations in this incremental poisoning attack. We also use multiple trust domains to reduce the rate of misjudging benign participants as adversaries. Moreover, a differentiated differential privacy is utilized before the global model sending to protect the privacy of participants\u2019 training datasets in PID. The experiments conducted on two real-world datasets under financial fraud detection scenario demonstrate that the PID reduces the fallout of adversaries detection (the rate of misjudging benign participants as adversaries) by at least 51.1% and improve the speed of detecting all malicious participants by at least 33.4% compared with two popular defense methods. Besides, the privacy preserving of PID is also effective.",
      "year": 2024,
      "venue": "IEEE Transactions on Computational Social Systems",
      "authors": [
        "Feifei Qiao",
        "Zhong Li",
        "Yubo Kong"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/0e1157ad05e3c9320e6c18b3887ff67f86741864",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "data poisoning attack",
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ce5c6891853a6329a4bd35a165315159d855165",
      "title": "Data Poisoning Attacks and Defenses to LDP-Based Privacy-Preserving Crowdsensing",
      "abstract": "In this article, we explore data poisoning attacks and their defenses in local differential privacy (LDP)-based crowdsensing systems. First, we construct data poisoning attacks launched by corrupted workers to subvert crowdsensing results by tampering information reported. Specifically, the attacks are formulated as a bi-level optimization problem where attackers strive to conceal their malicious behavior by delicately exploiting noise perturbation introduced by LDP protocols. In this way, the attacks can not be detected, even with the weight-based truth discovery methods. Due to the NP-hard nature of the bi-level problem, we decompose it into upper-level and lower-level sub-problems and employ the augmented Lagrangian method to iteratively solve them, ultimately identifying optimal attack strategies. Second, we propose corresponding countermeasures to defend against the attacks. The countermeasures are formulated as a minimization problem, with the objective of minimizing disruptions caused by attacks through the identification and removal of corrupted workers from crowdsensing systems. To solve the problem, we utilize a differential evolution algorithm instead of gradient-based methods since the objective function of the problem is not differentiable. Extensive experiments on real-world datasets are conducted to evaluate the performance of the proposed attacks and defenses. The evaluation results demonstrate that LDP perturbation indeed facilitates the success of data poisoning attacks, and the proposed defenses can accurately distinguish malicious behaviors disguised.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhirun Zheng",
        "Zhetao Li",
        "Cheng Huang",
        "Saiqin Long",
        "Mushu Li",
        "X. Shen"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/6ce5c6891853a6329a4bd35a165315159d855165",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b01288396df406ecf8c9dfb68349903bb3210ecb",
      "title": "Collusive Model Poisoning Attack in Decentralized Federated Learning",
      "abstract": "As a privacy-preserving machine learning paradigm, federated learning (FL) has attracted widespread attention from both academia and industry. Decentralized FL (DFL) overcomes the problems of untrusted aggregation server, single point of failure and poor scalability in traditional FL, making it suitable for industrial Internet of Things (IIoT). However, DFL provides more convenient conditions for malicious participants to launch attacks. This article focuses on the model poisoning attack in DFL for the first time, and proposes a novel attack method called collusive model poisoning attack (CMPA). To implement CMPA, we propose the dynamic adaptive construction mechanism, in which malicious participants can dynamically and adaptively construct malicious local models that meet distance constraints, reducing the convergence speed and accuracy of consensus models. Furthermore, we design the collusion-based attack enhancement strategies, where multiple participants can collude in the process of constructing malicious local models to improve the strength of attack. Empirical experiments conducted on MNIST and CIFAR-10 datasets reveal that CMPA significantly impacts the training process and results of DFL. Attack tests against representative defense methods show that CMPA not only invalidates statistical-based defenses but also skillfully overcomes performance-based methods, further proving its effectiveness and stealthiness. In addition, experiments based on practical IIoT scenario have also shown that CMPA can effectively disrupt system functionality.",
      "year": 2024,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Shouhong Tan",
        "Fengrui Hao",
        "Tianlong Gu",
        "Long Li",
        "Ming Liu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/b01288396df406ecf8c9dfb68349903bb3210ecb",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bc30de4c93fc9ff6c598c815bfefe7112911dbb5",
      "title": "Model poisoning attack in differential privacy-based federated learning",
      "abstract": null,
      "year": 2023,
      "venue": "Information Sciences",
      "authors": [
        "Ming-How Yang",
        "Hang Cheng",
        "Fei Chen",
        "Ximeng Liu",
        "Mei Wang",
        "Xibin Li"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/bc30de4c93fc9ff6c598c815bfefe7112911dbb5",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1bdb7429302500aa559c4e1653543802365abc47",
      "title": "Deep Model Poisoning Attack on Federated Learning",
      "abstract": "Federated learning is a novel distributed learning framework, which enables thousands of participants to collaboratively construct a deep learning model. In order to protect confidentiality of the training data, the shared information between server and participants are only limited to model parameters. However, this setting is vulnerable to model poisoning attack, since the participants have permission to modify the model parameters. In this paper, we perform systematic investigation for such threats in federated learning and propose a novel optimization-based model poisoning attack. Different from existing methods, we primarily focus on the effectiveness, persistence and stealth of attacks. Numerical experiments demonstrate that the proposed method can not only achieve high attack success rate, but it is also stealthy enough to bypass two existing defense methods.",
      "year": 2021,
      "venue": "Future Internet",
      "authors": [
        "Xin-Lun Zhou",
        "Ming Xu",
        "Yiming Wu",
        "Ning Zheng"
      ],
      "citation_count": 152,
      "url": "https://www.semanticscholar.org/paper/1bdb7429302500aa559c4e1653543802365abc47",
      "pdf_url": "https://www.mdpi.com/1999-5903/13/3/73/pdf?version=1615975400",
      "publication_date": null,
      "keywords_matched": [
        "poisoning attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "970b7a1971e5a9d53bd140c599c3706c3bc8b6c0",
      "title": "Federated Anomaly Analytics for Local Model Poisoning Attack",
      "abstract": "The local model poisoning attack is an attack to manipulate the shared local models during the process of distributed learning. Existing defense methods are passive in the sense that they try to mitigate the negative impact of the poisoned local models instead of eliminating them. In this paper, we leverage the new federated analytics paradigm, to develop a proactive defense method. More specifically, federated analytics is to collectively carry out analytics tasks without disclosing local data of the edge devices. We propose a Federated Anomaly Analytics enhanced Distributed Learning (FAA-DL) framework, where the clients and the server collaboratively analyze the anomalies. FAA-DL firstly detects all the uploaded local models and splits out the potential malicious ones. Then, it verifies each potential malicious local model with functional encryption. Finally, it removes the verified anomalies and aggregates the remaining to produce the global model. We analyze the FAA-DL framework and show that it is accurate, robust, and efficient. We evaluate FAA-DL by training classifiers on MNIST and Fashion-MNIST under various local model poisoning attacks. Our experiment results show FAA-DL improves the accuracy of the learned global model under strong attacks up to 6.90 times and outperforms the state-of-the-art defense methods with a robustness guarantee.",
      "year": 2022,
      "venue": "IEEE Journal on Selected Areas in Communications",
      "authors": [
        "Siping Shi",
        "Chuang Hu",
        "Dan Wang",
        "Yifei Zhu",
        "Zhu Han"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/970b7a1971e5a9d53bd140c599c3706c3bc8b6c0",
      "pdf_url": "",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ce75872fa42fdd1afa5b5ee02334cbbfc4472c9b",
      "title": "Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation",
      "abstract": "Although local differential privacy (LDP) protects individual users' data from inference by an untrusted data curator, recent studies show that an attacker can launch a data poisoning attack from the user side to inject carefully-crafted bogus data into the LDP protocols in order to maximally skew the final estimate by the data curator. In this work, we further advance this knowledge by proposing a new fine-grained attack, which allows the attacker to fine-tune and simultaneously manipulate mean and variance estimations that are popular analytical tasks for many real-world applications. To accomplish this goal, the attack leverages the characteristics of LDP to inject fake data into the output domain of the local LDP instance. We call our attack the output poisoning attack (OPA). We observe a security-privacy consistency where a small privacy loss enhances the security of LDP, which contradicts the known security-privacy trade-off from prior work. We further study the consistency and reveal a more holistic view of the threat landscape of data poisoning attacks on LDP. We comprehensively evaluate our attack against a baseline attack that intuitively provides false input to LDP. The experimental results show that OPA outperforms the baseline on three real-world datasets. We also propose a novel defense method that can recover the result accuracy from polluted data collection and offer insight into the secure LDP design.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xiaoguang Li",
        "Ninghui Li",
        "Wenhai Sun",
        "N. Gong",
        "Hui Li"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/ce75872fa42fdd1afa5b5ee02334cbbfc4472c9b",
      "pdf_url": "",
      "publication_date": "2022-05-24",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f8036a0dbeb67c3523a59173a8c3feda12702d66",
      "title": "Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Xiong Xu",
        "Kunzhe Huang",
        "Yiming Li",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/f8036a0dbeb67c3523a59173a8c3feda12702d66",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "trigger pattern",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54d8c31015b46977f1fc422a66f05728ff571f60",
      "title": "Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers",
      "abstract": "Machine unlearning as a fundamental requirement in Machine-Learning-as-a-Service (MLaaS) has been extensively studied with increasing concerns about data privacy. It requires MLaaS providers should delete training data upon user requests. Unfortunately, none of the existing studies can efficiently achieve machine unlearning validation while preserving the retraining efficiency and the service quality after data deletion. Besides, how to craft the validation scheme to prevent providers from spoofing validation by forging proofs remains under-explored. In this paper, we introduce a backdoor-assisted validation scheme for machine unlearning. The proposed design is built from the ingenious combination of backdoor triggers and incremental learning to assist users in verifying proofs of machine unlearning without compromising performance and service quality. We propose to embed invisible markers based on backdoor triggers into privacy-sensitive data to prevent MLaaS providers from distinguishing poisoned data for validation spoofing. Users can use prediction results to determine whether providers comply with data deletion requests. Besides, we incorporate our validation scheme into an efficient incremental learning approach via our index structure to further facilitate the performance of retraining after data deletion. Evaluation results on real-world datasets confirm the efficiency and effectiveness of our proposed verifiable machine unlearning scheme.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yu Guo",
        "Yu Zhao",
        "Saihui Hou",
        "Cong Wang",
        "Xiaohua Jia"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/54d8c31015b46977f1fc422a66f05728ff571f60",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "67606bec9d7a58ef61befff3230a3616887828df",
      "title": "Privacy-Preserving Federated Learning Against Label-Flipping Attacks on Non-IID Data",
      "abstract": "Federated learning (FL) has attracted widespread attention in the Internet of Things domain recently. With FL, multiple distributed devices can cooperatively train a global model by transmitting model updates without disclosing the original data. However, the distributed nature of FL makes it vulnerable to data poisoning attacks. In practice, malicious clients can launch the label-flipping attack (LFA) by simply tampering with the labels of local data, thus causing the global model to misclassify the samples of a selected class as the target class. Although some defense mechanisms have been proposed, they rely on specific assumptions about data distribution, and their performance degrades significantly when the data on clients are non-IID. Besides, most existing methods require clients to upload model updates in plaintext so that the server can identify and remove the malicious updates. But, direct transmission of model updates may still reveal private information. Considering these issues, we develop a label-flipping-robust and privacy-preserving FL (LFR-PPFL) algorithm, which is applicable to both independent and identically distributed (IID) and non-IID data. We first propose a detection method based on temporal analysis on cosine similarity to distinguish malicious clients from benign clients. Then, we propose a privacy-preserving computation protocol based on homomorphic encryption to implement this detection method and perform federated aggregation while protecting the privacy of clients. Besides, a detailed theoretical analysis is given to demonstrate the privacy guarantee of the proposed protocol. Experimental results on real-world data sets show that the proposed algorithm can effectively defend against LFAs under various data distributions.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xicong Shen",
        "Ying Liu",
        "Fu Li",
        "Chunguang Li"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/67606bec9d7a58ef61befff3230a3616887828df",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1ebd8d6ce23deb2f40187c0cd8aad88cac383ee7",
      "title": "Detection and Mitigation of Label-Flipping Attacks in FL Systems With KL Divergence",
      "abstract": "The application of federated learning (FL) in the Internet of Things (IoT) is experiencing rapid growth. FL becomes vulnerable to data poisoning attacks in the IoT environment, characterized by distributed devices, limited resources, and open networks. Since IoT systems rely on measurement data to make intelligent decisions, attackers can deceive the system by flipping the labels of local data, causing it to misclassify source class samples as target labels at a low cost. Existing defense mechanisms primarily rely on similarity analysis to detect malicious behaviors. However, these methods often lack the sensitivity to handle high-dimensional or complex network models, struggling to identify subtle but significant changes in model parameters or updates. Considering this issue, we propose a label-flipping-robust approach to distinguish malicious clients from benign ones. In brief, we adopt Kullback-Leibler (KL) divergence to quantify the difference between client models, which offers an improved ability to capture the subtle variations within the model. Furthermore, we introduce privacy-protecting FL (GAN in FL) to protect users\u2019 privacy while utilizing the shared knowledge in the global generator to support detecting malicious clients. Experimental results on real-world data sets show that our proposed algorithm can effectively defend against label-flipping attacks (LFAs) under various data distributions.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Liguang Zang",
        "Yuancheng Li"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/1ebd8d6ce23deb2f40187c0cd8aad88cac383ee7",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2fd6e22d449499b7045838d7432b67500bf9731",
      "title": "Label Flipping Attacks on Federated Learning: GAN-Based Poisoning and Countermeasures",
      "abstract": "Federated Learning (FL) is a promising approach for training machine learning models in a decentralized manner to preserve data privacy. However, FL is vulnerable to various attacks, including label-flipping attacks where malicious clients manipulate labels to degrade global model accuracy. This research investigates the efficacy of label-flipping attacks using synthetic images generated by a Generative Adversarial Network (GAN). The GAN architecture and learning procedure are adapted to address challenges related to dataset limitations and ensure realistic data generation for the attacks. The study analyzes the impact of label-flipping attacks on FL model performance. The results demonstrate that these attacks can evade conventional defenses and significantly reduce model accuracy. To mitigate this threat, a novel defense mechanism is proposed to identify and isolate poisoned clients based on abnormally high gradient values. This research highlights the security weaknesses of FL and proposes a defense mechanism against label-flipping attacks. This paves the way for more secure and trustworthy FL applications while preserving data privacy.",
      "year": 2024,
      "venue": "2024 International Conference on Data Science and Network Security (ICDSNS)",
      "authors": [
        "Muhammad Abdullah",
        "Vidhatri Bapu",
        "Akash Ka",
        "Abdul Mannan Khan",
        "M. S. Bhargavi"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c2fd6e22d449499b7045838d7432b67500bf9731",
      "pdf_url": "",
      "publication_date": "2024-07-26",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a7ec731e4a32c0292e303d58ca595882ec6c6170",
      "title": "Investigating the Label-flipping Attacks Impact in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative model training approach that protects data privacy while allowing for model updates and optimization. However, FL is vulnerable to poisoning attacks due to its distributed nature. Among these attacks, label-flipping stands out for being straightforward to implement yet challenging to detect. Additionally, current research used to treat it as a black box, which could hinder the development of defense strategies. To fill this gap, this work investigates the impact of label-flipping attacks from multiple perspectives. Experiments are conducted using the MNIST and Fashion-MNIST dataset and an MLP (MultiLayer Perceptron) model. Results show that in label-flipping attacks, malicious clients primarily alter parameters in specific model components. Different flipping strategies also have varying effects on model parameters. Finally, this study compares label-flipping attacks with model poisoning attacks to better understand their differences. We\u2019re looking forward to this work to offer insights for the FL community regarding label-flipping understanding and defense strategies.",
      "year": 2024,
      "venue": "2024 5th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)",
      "authors": [
        "Jiaxin Ji"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a7ec731e4a32c0292e303d58ca595882ec6c6170",
      "pdf_url": "",
      "publication_date": "2024-05-31",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "808380e10790ad809cffdc6dbf858b5deb2e2634",
      "title": "An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack",
      "abstract": "In this paper, we empirically analyze adversarial attacks on selected federated learning models. The specific learning models considered are Multinominal Logistic Regression (MLR), Support Vector Classifier (SVC), Multilayer Perceptron (MLP), Convolution Neural Network (CNN), %Recurrent Neural Network (RNN), Random Forest, XGBoost, and Long Short-Term Memory (LSTM). For each model, we simulate label-flipping attacks, experimenting extensively with 10 federated clients and 100 federated clients. We vary the percentage of adversarial clients from 10% to 100% and, simultaneously, the percentage of labels flipped by each adversarial client is also varied from 10% to 100%. Among other results, we find that models differ in their inherent robustness to the two vectors in our label-flipping attack, i.e., the percentage of adversarial clients, and the percentage of labels flipped by each adversarial client. We discuss the potential practical implications of our results.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Kunal Bhatnagar",
        "Sagana Chattanathan",
        "Angela Dang",
        "Bhargavi Eranki",
        "Ronnit Rana",
        "Charan Sridhar",
        "Siddharth Vedam",
        "Angie Yao",
        "Mark Stamp"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/808380e10790ad809cffdc6dbf858b5deb2e2634",
      "pdf_url": "",
      "publication_date": "2024-12-24",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f61fe83bed7493aa9aa0cbbf381caf1f89838d62",
      "title": "LFighter: Defending against the label-flipping attack in federated learning",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Networks",
      "authors": [
        "N. Jebreel",
        "J. Domingo-Ferrer",
        "David S\u00e1nchez",
        "Alberto Blanco-Justicia"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/f61fe83bed7493aa9aa0cbbf381caf1f89838d62",
      "pdf_url": "",
      "publication_date": "2023-11-11",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e731e4f6d14bad6172603e765d6e74dbd5446315",
      "title": "LFGurad: A Defense against Label Flipping Attack in Federated Learning for Vehicular Network",
      "abstract": null,
      "year": 2024,
      "venue": "Comput. Networks",
      "authors": [
        "Sameera K.M.",
        "V. P.",
        "Rafidha Rehiman K.A.",
        "Mauro Conti"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e731e4f6d14bad6172603e765d6e74dbd5446315",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be791235880e0e3f7fb163472d202f4eec5f5061",
      "title": "Smart Lexical Search for Label Flipping Adversial Attack",
      "abstract": "Language models are susceptible to vulnerability through adversarial attacks, using manipulations of the input data to disrupt their performance. Accordingly, it represents a cibersecurity leak. Data manipulations are intended to be unidentifiable by the learning model and by humans, small changes can disturb the final label of a classification task. Hence, we propose a novel attack built upon explainability methods to identify the salient lexical units to alter in order to flip the classification label. We asses our proposal on a disinformation dataset, and we show that our attack reaches high balance among stealthiness and efficiency.",
      "year": 2024,
      "venue": "PRIVATENLP",
      "authors": [
        "A. Guti\u00e9rrez-Meg\u00edas",
        "Salud Mar\u00eda Jim\u00e9nez-Zafra",
        "L. Alfonso Ure\u00f1a",
        "Eugenio Mart\u00ednez-C\u00e1mara"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/be791235880e0e3f7fb163472d202f4eec5f5061",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b88167c2b601ad725b6387a857a98def82f7f0a0",
      "title": "Federated Learning: Countering Label Flipping Attacks in Retinal OCT",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on the Digital Society",
      "authors": [
        "K. P. Anunita",
        "S. R. Devisri",
        "C. Arumugam"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b88167c2b601ad725b6387a857a98def82f7f0a0",
      "pdf_url": "https://doi.org/10.1016/j.procs.2025.02.064",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50c9146610cd49599c17a7a43034b0fba3319603",
      "title": "Rethinking Label Flipping Attack: From Sample Masking to Sample Thresholding",
      "abstract": null,
      "year": 2023,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Qianqian Xu",
        "Zhiyong Yang",
        "Yunrui Zhao",
        "Xiaochun Cao",
        "Qingming Huang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/50c9146610cd49599c17a7a43034b0fba3319603",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "61b54a6bf561256b7c80e33a079e7eaeb4e210da",
      "title": "Federated Learning Defence Based on Label-Flipping Attacks",
      "abstract": "Against the backdrop of the era of artificial intelligence, federated learning has effectively addressed the issues of data silos and privacy protection, but the particularity of its system architecture makes user nodes susceptible to label flipping attacks, which reduce the accuracy of model training. In response to this issue, this paper proposes a defense strategy for federated learning based on label flipping attacks. By setting thresholds through theoretical analysis to extract outliers in the gradients of output layer neurons, and using unsupervised learning algorithms to detect malicious node updates. The experimental results show that the algorithm proposed in this paper has good performance in defending against label flipping attacks.",
      "year": 2024,
      "venue": "International Conference on Artificial Intelligence, Automation and High Performance Computing",
      "authors": [
        "Aiwang Chen",
        "Yangcheng Mou",
        "Guirong Chen",
        "Jiming Xu",
        "Xiaomei Yan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/61b54a6bf561256b7c80e33a079e7eaeb4e210da",
      "pdf_url": "",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9546398066744f71a60e3a961be5022cff5fb19f",
      "title": "Resilient Distributed Classification Learning Against Label Flipping Attack: An ADMM-Based Approach",
      "abstract": "Distributed classification learning (DCL) is a promising solution to establish Internet of Things-based smart applications, especially due to its strong ability in dealing with large-scale and high-concurrency data. However, the performance of DCL may be seriously affected by the label flipping attack (LFA). Regarding the LFA-resilient learning problem, most existing works are built in more centralized settings. The work addressing the secure DCL issue makes an assumption that the label flipping rates are symmetric and available for scheme design. In this article, we remove this assumption and propose an LFA-resilient DCL scheme, named FENDER, without knowing the asymmetric flipping rates. The challenge is to guarantee both attack resilience and algorithm convergence. We carefully integrate a resilient loss and the alternating direction method of the multiplier scheme, making FENDER resilient to LFA. Further, we systematically analyze the performance of FENDER according to a metric reflecting the models obtained by all the servers at different iterations. In addition, we discuss and compare FENDER with some existing methods from the aspects of algorithm establishment and performance guarantee. Finally, extensive experiments with multiple real-world data sets are performed to validate the developed theory and evaluate the performance of the trained models.",
      "year": 2023,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xin Wang",
        "Chongrong Fang",
        "Ming Yang",
        "Xiaoming Wu",
        "Heng Zhang",
        "Peng Cheng"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9546398066744f71a60e3a961be5022cff5fb19f",
      "pdf_url": "",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e8e714fdf1dbb618686c8b9696fb250a9c649625",
      "title": "Defending against the Label-flipping Attack in Federated Learning",
      "abstract": "Federated learning (FL) provides autonomy and privacy by design to participating peers, who cooperatively build a machine learning (ML) model while keeping their private data in their devices. However, that same autonomy opens the door for malicious peers to poison the model by conducting either untargeted or targeted poisoning attacks. The label-flipping (LF) attack is a targeted poisoning attack where the attackers poison their training data by flipping the labels of some examples from one class (i.e., the source class) to another (i.e., the target class). Unfortunately, this attack is easy to perform and hard to detect and it negatively impacts on the performance of the global model. Existing defenses against LF are limited by assumptions on the distribution of the peers' data and/or do not perform well with high-dimensional models. In this paper, we deeply investigate the LF attack behavior and find that the contradicting objectives of attackers and honest peers on the source class examples are reflected in the parameter gradients corresponding to the neurons of the source and target classes in the output layer, making those gradients good discriminative features for the attack detection. Accordingly, we propose a novel defense that first dynamically extracts those gradients from the peers' local updates, and then clusters the extracted gradients, analyzes the resulting clusters and filters out potential bad updates before model aggregation. Extensive empirical analysis on three data sets shows the proposed defense's effectiveness against the LF attack regardless of the data distribution or model dimensionality. Also, the proposed defense outperforms several state-of-the-art defenses by offering lower test error, higher overall accuracy, higher source class accuracy, lower attack success rate, and higher stability of the source class accuracy.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "N. Jebreel",
        "J. Domingo-Ferrer",
        "David S\u00e1nchez",
        "Alberto Blanco-Justicia"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/e8e714fdf1dbb618686c8b9696fb250a9c649625",
      "pdf_url": "http://arxiv.org/pdf/2207.01982",
      "publication_date": "2022-07-05",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9441d1a845a3a1a71326a60a4a4d32882e1b5834",
      "title": "Forgetting User Preference in Recommendation Systems with Label-Flipping",
      "abstract": "Recommendation systems play a crucial role in identifying users\u2019 preferences based on their historical interaction records and those of other users. However, the ability to \u201cforget\u201d certain users\u2019 preferences is indispensable for ensuring user privacy and maintaining recommendation accuracy. It is essential to accommodate a user\u2019s request to exclude their behavioral data from the recommendation system when necessary. Likewise, if certain data corrupts the system, its impact should be removed to restore system performance. In this paper, we propose FlipRec, a general and efficient framework for recommendation models to \u201cforget\u201d the preferences of specific users while retaining the model\u2019s performance for all other users. Our concept of forgetting user preferences is inspired by the label-flipping attack, a technique where the labels of some training samples are inverted to adversarially manipulate the weights of the trained model. FlipRec adjusts the recommendation model weights to forget the targeted users by flipping their interaction records $y \\in \\{ 0,1\\}$. To preserve the model\u2019s performance for the remaining users, we augment the fine-tuning data with samples from users who have interacted with the same items as the targeted users. This ensures minimal impact on these users during the \u201cforgetting\u201d process. FlipRec has been validated on both contentbased recommendation models and collaborative filtering models. The experimental results show that FlipRec outperforms stateof-the-art unlearning methods in terms of efficiency, the ability to forget targeted users, and the preservation of performance for the remaining users.",
      "year": 2023,
      "venue": "BigData Congress [Services Society]",
      "authors": [
        "Manal A. Alshehri",
        "Xiangliang Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9441d1a845a3a1a71326a60a4a4d32882e1b5834",
      "pdf_url": "",
      "publication_date": "2023-12-15",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1498429ebbc2b6d2c222bc99efe2f2ac9e3eb9cd",
      "title": "The Effect of Label-Flipping attack on Different Mobile Machine Learning Classifiers",
      "abstract": "AI technology is widely used in different fields due to the effectiveness and accurate results that have been achieved. The diversity of usage attracts many attackers to attack AI systems to reach their goals. One of the most important and powerful attacks launched against AI models is the label-flipping attack. This attack allows the attacker to compromise the integrity of the dataset, where the attacker is capable of degrading the accuracy of ML models or generating specific output that is targeted by the attacker. Therefore, this paper studies the robustness of several Machine Learning models against targeted and non-targeted label-flipping attacks against the dataset during the training phase. Also, it checks the repeatability of the results obtained in the existing literature. The results are observed and explained in the domain of the cyber security paradigm.",
      "year": 2023,
      "venue": "2023 International Conference on Business Analytics for Technology and Security (ICBATS)",
      "authors": [
        "Alanoud Almemari",
        "Raviha Khan",
        "C. Yeun"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1498429ebbc2b6d2c222bc99efe2f2ac9e3eb9cd",
      "pdf_url": "",
      "publication_date": "2023-03-07",
      "keywords_matched": [
        "label flipping",
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cec90dd253eba889e3b88bcea544cfe3fe3472d2",
      "title": "Blockchain-based fairness-enhanced federated learning scheme against label flipping attack",
      "abstract": null,
      "year": 2023,
      "venue": "Journal of Information Security and Applications",
      "authors": [
        "Shan Jin",
        "Yong Li",
        "Xi Chen",
        "Ruxian Li",
        "Zhibin Shen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/cec90dd253eba889e3b88bcea544cfe3fe3472d2",
      "pdf_url": "",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54c58483885be202ed63102d05b7c65cf3e71420",
      "title": "iFlipper: Label Flipping for Individual Fairness",
      "abstract": "As machine learning becomes prevalent, mitigating any unfairness present in the training data becomes critical. Among the various notions of fairness, this paper focuses on the well-known individual fairness, which states that similar individuals should be treated similarly. While individual fairness can be improved when training a model (in-processing), we contend that fixing the data before model training (pre-processing) is a more fundamental solution. In particular, we show that label flipping is an effective pre-processing technique for improving individual fairness. Our system iFlipper solves the optimization problem of minimally flipping labels given a limit to the individual fairness violations, where a violation occurs when two similar examples in the training data have different labels. We first prove that the problem is NP-hard. We then propose an approximate linear programming algorithm and provide theoretical guarantees on how close its result is to the optimal solution in terms of the number of label flips. We also propose techniques for making the linear programming solution more optimal without exceeding the violations limit. Experiments on real datasets show that iFlipper significantly outperforms other pre-processing baselines in terms of individual fairness and accuracy on unseen test sets. In addition, iFlipper can be combined with in-processing techniques for even better results.",
      "year": 2022,
      "venue": "Proc. ACM Manag. Data",
      "authors": [
        "Hantian Zhang",
        "Ki Hyun Tae",
        "Jaeyoung Park",
        "Xu Chu",
        "Steven Euijong Whang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/54c58483885be202ed63102d05b7c65cf3e71420",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3588688",
      "publication_date": "2022-09-15",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "983abd6da77712adf4a7908f8034137829232f1e",
      "title": "Classification and Analysis of Adversarial Machine Learning Attacks in IoT: a Label Flipping Attack Case Study",
      "abstract": "With the increased usage of Internet of Things (IoT) devices in recent years, various Machine Learning (ML) algorithms have also developed dramatically for attack detection in this domain. However, the ML models are exposed to different classes of adversarial attacks that aim to fool a model into making an incorrect prediction. For instance, label manipulation or label flipping is an adversarial attack where the adversary attempts to manipulate the label of training data that causes the trained model biased and/or with decreased performance. However, the number of samples to be flipped in this category of attack can be restricted, giving the attacker a limited target selection. Due to the great significance of securing ML models against Adversarial Machine Learning (AML) attacks particularly in the IoT domain, this research presents an extensive review of AML in IoT. Then, a classification of AML attacks is presented based on the literature which sheds light on the future research in this domain. Next, this paper investigates the negative impact levels of applying the malicious label-flipping attacks on IoT data. We devise label-flipping scenarios for training a Support Vector Machine (SVM) model. The experiments demonstrate that the label flipping attacks impact the performance of ML models. These results can lead to designing more effective and powerful attack and defense mechanisms in adversarial settings. Finally, we show the weaknesses of the K-NN defense method against the random label flipping attack.",
      "year": 2022,
      "venue": "Conference of the Open Innovations Association",
      "authors": [
        "M. Abrishami",
        "Sajjad Dadkhah",
        "E. P. Neto",
        "Pulei Xiong",
        "Shahrear Iqbal",
        "S. Ray",
        "A. Ghorbani"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/983abd6da77712adf4a7908f8034137829232f1e",
      "pdf_url": "",
      "publication_date": "2022-11-09",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1943b862598fe27f302f05fb0a690b3a4a9fcbca",
      "title": "Adversarial Label-Flipping Attack and Defense for Graph Neural Networks",
      "abstract": "With the great popularity of Graph Neural Networks (GNNs), the robustness of GNNs to adversarial attacks has received increasing attention. However, existing works neglect adversarial label-flipping attacks, where the attacker can manipulate an unnoticeable fraction of training labels. Exploring the robustness of GNNs to label-flipping attacks is highly critical, especially when labels are collected from external sources and false labels are easy to inject (e.g., recommendation systems). In this work, we introduce the first study of adversarial label-flipping attacks on GNNs. We propose an effective attack model LafAK based on approximated closed form of GNNs and continuous surrogate of non-differentiable objective, efficiently generating attacks via gradient-based optimizers. Furthermore, we show that one key reason for the vulnerability of GNNs to label-flipping attack is overfitting to flipped nodes. Based on this observation, we propose a defense framework which introduces a community-preserving self-supervised task as regularization to avoid overfitting. We demonstrate the effectiveness of our proposed attack model to GNNs on four real-world datasets. The effectiveness of our defense framework is also well validated by the substantial improvements of defense based GNN and its variants under label-flipping attacks.",
      "year": 2020,
      "venue": "Industrial Conference on Data Mining",
      "authors": [
        "Mengmei Zhang",
        "Linmei Hu",
        "C. Shi",
        "Xiao Wang"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/1943b862598fe27f302f05fb0a690b3a4a9fcbca",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "767c875a9e8c4e527b4919579ddba85bc0653447",
      "title": "AWFC: Preventing Label Flipping Attacks Towards Federated Learning for Intelligent IoT",
      "abstract": null,
      "year": 2022,
      "venue": "Computer/law journal",
      "authors": [
        "Zhuo Lv",
        "Hongbo Cao",
        "Fengchen Zhang",
        "Yuange Ren",
        "Bin Wang",
        "Cen Chen",
        "Nuannuan Li",
        "Hao Chang",
        "Wen Wang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/767c875a9e8c4e527b4919579ddba85bc0653447",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3323aa232994c4e1900d4cebc2ed1046810b9f88",
      "title": "A Label Flipping Attack on Machine Learning Model and Its Defense Mechanism",
      "abstract": null,
      "year": 2022,
      "venue": "International Conference on Algorithms and Architectures for Parallel Processing",
      "authors": [
        "Qingru Li",
        "Xinru Wang",
        "Fangwei Wang",
        "Changguang Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/3323aa232994c4e1900d4cebc2ed1046810b9f88",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aa2bedef873635fdf5e052193c42fd54e8c3c114",
      "title": "The Empirical Impact of Data Sanitization on Language Models",
      "abstract": "Data sanitization in the context of language modeling involves identifying sensitive content, such as personally identifiable information (PII), and redacting them from a dataset corpus. It is a common practice used in natural language processing (NLP) to maintain privacy. Nevertheless, the impact of data sanitization on the language understanding capability of a language model remains less studied. This paper empirically analyzes the effects of data sanitization across several benchmark language-modeling tasks including comprehension question answering (Q&A), entailment, sentiment analysis, and text classification. Our experiments cover a wide spectrum comprising finetuning small-scale language models, to prompting large language models (LLMs), on both original and sanitized datasets, and comparing their performance across the tasks. Interestingly, our results suggest that for some tasks such as sentiment analysis or entailment, the impact of redaction is quite low, typically around 1-5%, while for tasks such as comprehension Q&A there is a big drop of>25% in performance observed in redacted queries as compared to the original. For tasks that have a higher impact, we perform a deeper dive to inspect the presence of task-critical entities. Finally, we investigate correlation between performance and number of redacted entities, and also suggest a strategy to repair an already redacted dataset by means of content-based subsampling. Additional details are available at https://sites.google.com/view/datasan.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Anwesan Pal",
        "Radhika Bhargava",
        "Kyle Hinsz",
        "Jacques Esterhuizen",
        "Sudipta Bhattacharya"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/aa2bedef873635fdf5e052193c42fd54e8c3c114",
      "pdf_url": "",
      "publication_date": "2024-11-08",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20a8865f5dddf6db7cb8ca3b5f96c04d770638aa",
      "title": "Privacy preservation of data using modified rider optimization algorithm: Optimal data sanitization and restoration model",
      "abstract": "Data preservation is the mechanism of protecting and safeguarding the confidentiality and integrity of data. Data stored in huge databases may contain metadata, elements that may be imprecise and unstable, It may include sensitive data, personal profiles and so on, which is vulnerable to third parties such as hackers or attackers. They may misuse the data and as a consequence of this the confidentiality and privacy of the data gets lost. There is a need to conserve the data and make it available for reuse when needed. Hence, it needs a proficient method to maintain and protect individuals' data privacy regarding confidentiality and reliability. This paper intends to develop an advanced model for privacy preservation of huge data with the accomplishment of two stages, namely data sanitization and data restoration. Data sanitization process preserves the safety of sensitive data stored in huge databases, by means of hiding those sensitive data from unauthorized users. Data restoration is the process of recovering or restoring of data that is sanitized at the sender side. Concerning the secrecy, there is a need for an optimal key to hide the sensitive data at sender as well as receiver side. Subsequent to the data sanitization, it requires the same key to restore the sanitized data. Thus, the optimal key generation plays a vital role to maintain privacy preservation. In order to choose an optimal key, a modified Rider optimization Algorithm (ROA) named as Randomized ROA (RROA) model is implemented in this work. Furthermore, the efficiency of the proposed work is compared over the state\u2010of\u2010the\u2010arts models by concerning the sanitization as well as restoration efficiency.",
      "year": 2021,
      "venue": "Expert Syst. J. Knowl. Eng.",
      "authors": [
        "Mohana Shivashankar",
        "S. A. Mary"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/20a8865f5dddf6db7cb8ca3b5f96c04d770638aa",
      "pdf_url": "",
      "publication_date": "2021-01-22",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa59fc5841a9f8615b97b7e5e3a30df0983ca511",
      "title": "Data Sanitization to Reduce Private Information Leakage from Functional Genomics",
      "abstract": null,
      "year": 2020,
      "venue": "Cell",
      "authors": [
        "Gamze G\u00fcrsoy",
        "Prashant S. Emani",
        "Charlotte M. Brannon",
        "Otto Jolanki",
        "A. Harmanci",
        "J. Strattan",
        "J. Cherry",
        "A. Miranker",
        "M. Gerstein"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/fa59fc5841a9f8615b97b7e5e3a30df0983ca511",
      "pdf_url": "http://www.cell.com/article/S0092867420312332/pdf",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1373fcdd33014cb44bd1d49ee5a9dca0ea72dbcf",
      "title": "Robust and lossless data privacy preservation: optimal key based data sanitization",
      "abstract": null,
      "year": 2019,
      "venue": "Evolutionary Intelligence",
      "authors": [
        "G. K. Shailaja",
        "C. Rao"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/1373fcdd33014cb44bd1d49ee5a9dca0ea72dbcf",
      "pdf_url": "",
      "publication_date": "2019-12-12",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4a7093cfd372b6f388aa50bc73946e0ca646b7c5",
      "title": "Private information leakage from functional genomics data: Quantification with calibration experiments and reduction via data sanitization protocols",
      "abstract": "The generation of functional genomics datasets is surging, as they provide insight into gene regulation and organismal phenotypes (e.g., genes upregulated in cancer). The intention of functional genomics experiments is not necessarily to study genetic variants, yet they pose privacy concerns due to their use of next-generation sequencing. Moreover, there is a great incentive to share raw reads for better analyses and general research reproducibility. Thus, we need new modes of sharing beyond traditional controlled-access models. Here, we develop a data-sanitization procedure allowing raw functional genomics reads to be shared while minimizing privacy leakage, thus enabling principled privacy-utility trade-offs. It works with traditional Illumina-based assays and newer technologies such as 10x single-cell RNA-sequencing. The procedure depends on quantifying the privacy leakage in reads by statistically linking study participants to known individuals. We carried out these linkages using data from highly accurate reference genomes and more realistic environmental samples.",
      "year": 2018,
      "venue": "bioRxiv",
      "authors": [
        "Gamze G\u00fcrsoy",
        "Prashant S. Emani",
        "Charlotte M. Brannon",
        "Otto Jolanki",
        "A. Harmanci",
        "J. Strattan",
        "Andrew Miranker",
        "M. Gerstein"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/4a7093cfd372b6f388aa50bc73946e0ca646b7c5",
      "pdf_url": "http://www.cell.com/article/S0092867420312332/pdf",
      "publication_date": "2018-06-12",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cffcfc8d317448d902a98f703f2ef56433c4aa0d",
      "title": "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage",
      "abstract": "Generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve users' PII, which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites, a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing users' specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user request. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected via human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackers' efforts can make EIA seamlessly adapted, rendering such supervision ineffective. Thus, we further discuss the defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Zeyi Liao",
        "Lingbo Mo",
        "Chejian Xu",
        "Mintong Kang",
        "Jiawei Zhang",
        "Chaowei Xiao",
        "Yuan Tian",
        "Bo Li",
        "Huan Sun"
      ],
      "citation_count": 96,
      "url": "https://www.semanticscholar.org/paper/cffcfc8d317448d902a98f703f2ef56433c4aa0d",
      "pdf_url": "",
      "publication_date": "2024-09-17",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a54306a5b29ab8b6675a45d62c8ee5e43942b988",
      "title": "Privacy Leakage in Wireless Charging",
      "abstract": "Wireless charging is becoming an essential power supply pattern for electronic devices. Currently, mainstream smartphones are almost compatible with wireless charging. However, when the charging efficiency is continuously improved, its security challenge still remains open yet overlooked. In this paper, we reveal that severe security flaws exist in the wireless charging procedure of off-the-shelf commodity smartphones. Specifically, we find that an attacker can utilize the electromagnetic induction effect between the wireless charger and the smartphone to detect the activities and operations performed on the smartphone. We term such attack as EM-Surfing side-channel attack and build a theoretical model to show its feasibility. To explore the hazard of EM-Surfing, we propose a three-module attack method, with which we conduct real-world experiments over three mainstream models of smartphones. The results show that the attacker can achieve over 99%, 96%, 94%, and 97% accuracy when inferring the passcode, keystroke, App information, and speech content, respectively. We also design an App named SecCharging to prevent smartphones from EM-Surfing attacks. The defense experiment results demonstrate that SecCharging can mitigate the threats posed by EM-Surfing effectively.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Jianwei Liu",
        "Xiang Zou",
        "Leqi Zhao",
        "Yusheng Tao",
        "Sideng Hu",
        "Jinsong Han",
        "Kui Ren"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/a54306a5b29ab8b6675a45d62c8ee5e43942b988",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f01dc983b6bf4218d602c8bc693636dce71825aa",
      "title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent",
      "abstract": "Language model (LM) agents that act on users'behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents'actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Zhiping Zhang",
        "Bingcan Guo",
        "Tianshi Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/f01dc983b6bf4218d602c8bc693636dce71825aa",
      "pdf_url": "",
      "publication_date": "2024-11-02",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1625958e92e307d13fe7eb4a74fce63d13079f52",
      "title": "Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving Federated Learning",
      "abstract": "Federated learning (FL) requires frequent uploading and updating of model parameters, which is naturally vulnerable to gradient leakage attacks (GLAs) that reconstruct private training data through gradients. Although some works incorporate differential privacy (DP) into FL to mitigate such privacy issues, their performance is not satisfactory since they did not notice that GLA incurs heterogeneous risks of privacy leakage (RoPL) with respect to gradients from different communication rounds and clients. In this paper, we propose an Adaptive Privacy-Preserving Federated Learning (Adp-PPFL) framework to achieve satisfactory privacy protection against GLA, while ensuring good performance in terms of model accuracy and convergence speed. Specifically, a leakage risk-aware privacy decomposition mechanism is proposed to provide adaptive privacy protection to different communication rounds and clients by dynamically allocating the privacy budget according to the quantified RoPL. In particular, we exploratively design a round-level and a client-level RoPL quantification method to measure the possible risks of GLA breaking privacy from gradients in different communication rounds and clients respectively, which only employ the limited information in general FL settings. Furthermore, to improve the FL model training performance (i.e., convergence speed and global model accuracy), we propose an adaptive privacy-preserving local training mechanism that dynamically clips the gradients and decays the noises added to the clipped gradients during the local training process. Extensive experiments show that our framework outperforms the existing differentially private FL schemes on model accuracy, convergence, and attack resistance.",
      "year": 2024,
      "venue": "IEEE/ACM Transactions on Networking",
      "authors": [
        "Jiahui Hu",
        "Zhibo Wang",
        "Yongsheng Shen",
        "Bohan Lin",
        "Peng Sun",
        "Xiaoyi Pang",
        "Jian Liu",
        "Kui Ren"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/1625958e92e307d13fe7eb4a74fce63d13079f52",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "516eaa8490537f63bd375d5ee0899947ef93b502",
      "title": "Model architecture level privacy leakage in neural networks",
      "abstract": null,
      "year": 2023,
      "venue": "Science China Information Sciences",
      "authors": [
        "Yan Li",
        "Hongyang Yan",
        "Teng Huang",
        "Zijie Pan",
        "Jiewei Lai",
        "Xiaoxue Zhang",
        "Kongyang Chen",
        "Jin Li"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/516eaa8490537f63bd375d5ee0899947ef93b502",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e22dd9a127b27d82e7e3ab8f5876405df85c2cd",
      "title": "Does Differential Privacy Really Protect Federated Learning From Gradient Leakage Attacks?",
      "abstract": "Federated Learning (FL) is susceptible to the gradient leakage attack (GLA), which can recover local private training data from the shared gradients or model updates. To ensure privacy, differential privacy is applied in FL by clipping and adding noise to local gradients (i.e., Local Differential Privacy (LDP)) or the global model update (i.e., Central Differential Privacy (CDP)). However, the effectiveness of DP in defending GLAs needs to be thoroughly investigated since some works briefly verify that DP can guard FL against GLAs while others question its defense capability. In this paper, we empirically evaluate CDP and LDP on the resistance of GLAs, and pay close attention to the trade-offs between privacy and utility in FL. Our findings reveal that: 1) existing GLAs can be defended by CDP using a per-layer clipping strategy and LDP with a reasonable privacy guarantee and 2) both CDP and LDP ensure the trade-off between privacy and utility in training shallow model, but cannot guarantee this trade-off in deeper model training (e.g., ResNets). Triggered by the crucial role of clipping operation for DP, we propose an improved attack that incorporates the clipping operation into existing GLAs without requiring additional information. The experimental results show our attack can destruct the protection of CDP and weaken the effectiveness of LDP. Overall, our work validates the effectiveness as well as reveals the vulnerability of DP under GLAs. We hope this work can provide guidance on utilizing DP for defending against GLA in FL and inspire the design of future privacy-preserving FL.",
      "year": 2024,
      "venue": "IEEE Transactions on Mobile Computing",
      "authors": [
        "Jiahui Hu",
        "Jiacheng Du",
        "Zhibo Wang",
        "Xiaoyi Pang",
        "Yajie Zhou",
        "Peng Sun",
        "Kui Ren"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/0e22dd9a127b27d82e7e3ab8f5876405df85c2cd",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "743b6dc485e2ea088e739b52aad3e770e5a88a01",
      "title": "Gradient-Free Privacy Leakage in Federated Language Models through Selective Weight Tampering",
      "abstract": "Federated learning (FL) has become a key component in various language modeling applications such as machine translation, next-word prediction, and medical record analysis. These applications are trained on datasets from many FL participants that often include privacy-sensitive data, such as healthcare records, phone/credit card numbers, login credentials, etc. Although FL enables computation without necessitating clients to share their raw data, existing works show that privacy leakage is still probable in federated language models. In this paper, we present two novel findings on the leakage of privacy-sensitive user data from federated large language models without requiring access to gradients. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that a malicious FL participant can aggravate the leakage by tampering with the model's selective weights that are responsible for memorizing the sensitive training data of some other clients, even without any cooperation from the server. Our best-performing method increases the membership inference recall by 29% and achieves up to 71% private data reconstruction, evidently outperforming existing attacks that consider much stronger adversary capabilities. Lastly, we recommend a balanced suite of techniques for an FL client to defend against such privacy risk.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Md. Rafi Ur Rashid",
        "Vishnu Asutosh Dasu",
        "Kang Gu",
        "Najrin Sultana",
        "Shagufta Mehnaz"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/743b6dc485e2ea088e739b52aad3e770e5a88a01",
      "pdf_url": "",
      "publication_date": "2023-10-24",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33c3f816bde8ee63ee9f2e60d4387b9390696371",
      "title": "Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning",
      "abstract": "Federated learning, i.e., a mobile edge computing framework for deep learning, is a recent advance in privacy-preserving machine learning, where the model is trained in a decentralized manner by the clients, i.e., data curators, preventing the server from directly accessing those private data from the clients. This learning mechanism significantly challenges the attack from the server side. Although the state-of-the-art attacking techniques that incorporated the advance of Generative adversarial networks (GANs) could construct class representatives of the global data distribution among all clients, it is still challenging to distinguishably attack a specific client (i.e., user-level privacy leakage), which is a stronger privacy threat to precisely recover the private data from a specific client. This paper gives the first attempt to explore user-level privacy leakage against the federated learning by the attack from a malicious server. We propose a framework incorporating GAN with a multi-task discriminator, which simultaneously discriminates category, reality, and client identity of input samples. The novel discrimination on client identity enables the generator to recover user specified private data. Unlike existing works that tend to interfere the training process of the federated learning, the proposed method works \u201cinvisibly\u201d on the server side. The experimental results demonstrate the effectiveness of the proposed attacking approach and the superior to the state-of-the-art.",
      "year": 2018,
      "venue": "IEEE Conference on Computer Communications",
      "authors": [
        "Zhibo Wang",
        "Mengkai Song",
        "Zhifei Zhang",
        "Yang Song",
        "Qian Wang",
        "H. Qi"
      ],
      "citation_count": 869,
      "url": "https://www.semanticscholar.org/paper/33c3f816bde8ee63ee9f2e60d4387b9390696371",
      "pdf_url": "https://arxiv.org/pdf/1812.00535",
      "publication_date": "2018-12-03",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "484d1dbf15d231c0fe5bdb1fd70a59138ad931cc",
      "title": "Split Without a Leak: Reducing Privacy Leakage in Split Learning",
      "abstract": "The popularity of Deep Learning (DL) makes the privacy of sensitive data more imperative than ever. As a result, various privacy-preserving techniques have been implemented to preserve user data privacy in DL. Among various privacy-preserving techniques, collaborative learning techniques, such as Split Learning (SL) have been utilized to accelerate the learning and prediction process. Initially, SL was considered a promising approach to data privacy. However, subsequent research has demonstrated that SL is susceptible to many types of attacks and, therefore, it cannot serve as a privacy-preserving technique. Meanwhile, countermeasures using a combination of SL and encryption have also been introduced to achieve privacy-preserving deep learning. In this work, we propose a hybrid approach using SL and Homomorphic Encryption (HE). The idea behind it is that the client encrypts the activation map (the output of the split layer between the client and the server) before sending it to the server. Hence, during both forward and backward propagation, the server cannot reconstruct the client's input data from the intermediate activation map. This improvement is important as it reduces privacy leakage compared to other SL-based works, where the server can gain valuable information about the client's input. In addition, on the MIT-BIH dataset, our proposed hybrid approach using SL and HE yields faster training time (about 6 times) and significantly reduced communication overhead (almost 160 times) compared to other HE-based approaches, thereby offering improved privacy protection for sensitive data in DL.",
      "year": 2023,
      "venue": "Security and Privacy in Communication Networks",
      "authors": [
        "Khoa Nguyen",
        "Tanveer Khan",
        "A. Michalas"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/484d1dbf15d231c0fe5bdb1fd70a59138ad931cc",
      "pdf_url": "https://arxiv.org/pdf/2308.15783",
      "publication_date": "2023-08-30",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f883b770c55dc941887108231078254926526935",
      "title": "Comprehensive Analysis of Privacy Leakage in Vertical Federated Learning During Prediction",
      "abstract": "Abstract Vertical federated learning (VFL), a variant of federated learning, has recently attracted increasing attention. An active party having the true labels jointly trains a model with other parties (referred to as passive parties) in order to use more features to achieve higher model accuracy. During the prediction phase, all the parties collaboratively compute the predicted confidence scores of each target record and the results will be finally returned to the active party. However, a recent study by Luo et al. [28] pointed out that the active party can use these confidence scores to reconstruct passive-party features and cause severe privacy leakage. In this paper, we conduct a comprehensive analysis of privacy leakage in VFL frameworks during the prediction phase. Our study improves on previous work [28] regarding two aspects. We first design a general gradient-based reconstruction attack framework that can be flexibly applied to simple logistic regression models as well as multi-layer neural networks. Moreover, besides performing the attack under the white-box setting, we give the first attempt to conduct the attack under the black-box setting. Extensive experiments on a number of real-world datasets show that our proposed attack is effective under different settings and can achieve at best twice or thrice of a reduction of attack error compared to previous work [28]. We further analyze a list of potential mitigation approaches and compare their privacy-utility performances. Experimental results demonstrate that privacy leakage from the confidence scores is a substantial privacy risk in VFL frameworks during the prediction phase, which cannot be simply solved by crypto-based confidentiality approaches. On the other hand, processing the confidence scores with information compression and randomization approaches can provide strengthened privacy protection.",
      "year": 2022,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Xue Jiang",
        "Xuebing Zhou",
        "Jens Grossklags"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/f883b770c55dc941887108231078254926526935",
      "pdf_url": "https://petsymposium.org/popets/2022/popets-2022-0045.pdf",
      "publication_date": "2022-03-03",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0457852644ac60568aba800147af224dd2d547a1",
      "title": "Soteria: Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective",
      "abstract": "Federated learning (FL) is a popular distributed learning framework that can reduce privacy risks by not explicitly sharing private data. However, recent works have demonstrated that sharing model updates makes FL vulnerable to inference attack. In this work, we show our key observation that the data representation leakage from gradients is the essential cause of privacy leakage in FL. We also provide an analysis of this observation to explain how the data presentation is leaked. Based on this observation, we propose a defense called Soteria against model inversion attack in FL. The key idea of our defense is learning to perturb data representation such that the quality of the reconstructed data is severely degraded, while FL performance is maintained. In addition, we derive a certified robustness guarantee to FL and a convergence guarantee to FedAvg, after applying our defense. To evaluate our defense, we conduct experiments on MNIST and CIFAR10 for defending against the DLG attack and GS attack. Without sacrificing accuracy, the results demonstrate that our proposed defense can increase the mean squared error between the reconstructed data and the raw data by as much as 160\u00d7 for both DLG attack and GS attack, compared with baseline defense methods. Therefore, the privacy of the FL system is significantly improved. Our code can be found at https://github.com/jeremy313/Soteria.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Jingwei Sun",
        "Ang Li",
        "Binghui Wang",
        "Huanrui Yang",
        "Hai Li",
        "Yiran Chen"
      ],
      "citation_count": 191,
      "url": "https://www.semanticscholar.org/paper/0457852644ac60568aba800147af224dd2d547a1",
      "pdf_url": "",
      "publication_date": "2020-12-08",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "92d0df018da15c1179568f1459da25c4ab0dfe1a",
      "title": "Privacy Leakage in Text Classification A Data Extraction Approach",
      "abstract": "Recent work has demonstrated the successful extraction of training data from generative language models. However, it is not evident whether such extraction is feasible in text classification models since the training objective is to predict the class label as opposed to next-word prediction. This poses an interesting challenge and raises an important question regarding the privacy of training data in text classification settings. Therefore, we study the potential privacy leakage in the text classification domain by investigating the problem of unintended memorization of training data that is not pertinent to the learning task. We propose an algorithm to extract missing tokens of a partial text by exploiting the likelihood of the class label provided by the model. We test the effectiveness of our algorithm by inserting canaries into the training set and attempting to extract tokens in these canaries post-training. In our experiments, we demonstrate that successful extraction is possible to some extent. This can also be used as an auditing strategy to assess any potential unauthorized use of personal data without consent.",
      "year": 2022,
      "venue": "PRIVATENLP",
      "authors": [
        "Adel M. Elmahdy",
        "Huseyin A. Inan",
        "Robert Sim"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/92d0df018da15c1179568f1459da25c4ab0dfe1a",
      "pdf_url": "https://arxiv.org/pdf/2206.04591",
      "publication_date": "2022-06-09",
      "keywords_matched": [
        "privacy leakage",
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "476d9c70d357fa65294a5b82d763a2ea75652366",
      "title": "Privacy Leakage of Adversarial Training Models in Federated Learning Systems",
      "abstract": "Adversarial Training (AT) is crucial for obtaining deep neural networks that are robust to adversarial attacks, yet recent works found that it could also make models more vulnerable to privacy attacks. In this work, we further reveal this unsettling property of AT by designing a novel privacy attack that is practically applicable to the privacy-sensitive Federated Learning (FL) systems. Using our method, the attacker can exploit AT models in the FL system to accurately reconstruct users\u2019 private training images even when the training batch size is large. Code is available at https://github.com/zjysteven/PrivayAttack_AT_FL.",
      "year": 2022,
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Jingyang Zhang",
        "Yiran Chen",
        "Hai Helen Li"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/476d9c70d357fa65294a5b82d763a2ea75652366",
      "pdf_url": "https://arxiv.org/pdf/2202.10546",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5322e5936e4a46195b1a92001467a2350fe72782",
      "title": "KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Yuta Nakamura",
        "S. Hanaoka",
        "Y. Nomura",
        "N. Hayashi",
        "O. Abe",
        "Shuntaro Yada",
        "Shoko Wakamiya",
        "Eiji Aramaki The University of Tokyo",
        "Nara Institute of Science",
        "Technology",
        "The Department of Radiology",
        "The University of Tokyo Hospital",
        "The Department of Radiology",
        "Preventive Medicine"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/5322e5936e4a46195b1a92001467a2350fe72782",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3841ddd23292d060eb0a4b4d9674e1f904e49b4d",
      "title": "Systematically Quantifying IoT Privacy Leakage in Mobile Networks",
      "abstract": "Privacy leakage of Internet of Things (IoT) has become a great challenge with the popularity of IoT services through mobile networks, such as smart homes, wearables, and healthcare. While previous work summarized general structures to analyze IoT privacy and provide case studies of specific devices or scenarios, it is still challenging to conduct a comprehensive and systematic quantification study of large-scale IoT privacy leakage in real world. To combine systematic analyses with real-world measurements, we provide a method to quantify IoT privacy leakage on a large-scale mobile network traffic data set containing 47651 IoT devices. We generate privacy fingerprints and attribute them to a privacy quantification framework. The framework is constructed based on the semantics of multiple privacy sensitive markers selected from the traffic along with the involved network entity types in IoT (i.e., user, device, and platform), and the fingerprints are generated from sensitive information extracted in the traffic via their markers. Our quantification shows that IoT users, devices, and platforms have considerable risks, respectively. Moreover, IoT devices have a larger scale of privacy leakage than users and platforms, and they perform different daily patterns on privacy leakage following their working conditions. In addition, we present three case studies on the leakage of location information, application calling, and voice service, which illustrate that a third party can profile a network entity in both cyberspace and physical space.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Hui Shuodi",
        "W. Zhenhua",
        "Xueshi Hou",
        "Xiao Wang",
        "Huandong Wang",
        "Yong Li",
        "Depeng Jin"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/3841ddd23292d060eb0a4b4d9674e1f904e49b4d",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2212da1f8c968a60f70227623678a41fb63c3cf",
      "title": "Toward Invisible Adversarial Examples Against DNN-Based Privacy Leakage for Internet of Things",
      "abstract": "Deep neural networks (DNNs) can be utilized maliciously for compromising the privacy stored in electronic devices, e.g., identifying the images stored in a mobile phone connected to the Internet of Things (IoT). However, recent studies demonstrated that DNNs are vulnerable to adversarial examples, which are artificially designed perturbations in the original samples for misleading DNNs. Adversarial examples can be used to protect the DNN-based privacy leakage in mobile phones by replacing the photos with adversarial examples. To avoid affecting the normal use of photos, the adversarial examples need to be highly similar to original images. To handle a large number of photos stored in the devices at a proper time, the time efficiency of a method needs to be high enough. Previous methods cannot do well on both sides. In this article, we propose a broad class of selective gradient sign iterative algorithms to make adversarial examples useful in protecting the privacy of photos in IoT devices. By neglecting the unimportant image pixels in the iterative process of attacks according to the sort of first-order partial derivative, we control the optimization direction meticulously to reduce image distortions of adversarial examples without leveraging high time-consuming tricks. Extensive experimental results show that the proposed methods successfully fool the neural network classifiers for the image classification task with a small change in the visual effects and consume little calculating time simultaneously.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xuyang Ding",
        "Shuai Zhang",
        "Mengkai Song",
        "Xiaocong Ding",
        "Fagen Li"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b2212da1f8c968a60f70227623678a41fb63c3cf",
      "pdf_url": "",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3ca9a34a98c7ee9449b2a0563da1b49cc5972c60",
      "title": "Privacy Leakage of SIFT Features via Deep Generative Model Based Image Reconstruction",
      "abstract": "Many practical applications, e.g., content based image retrieval and object recognition, heavily rely on the local features extracted from the query image. As these local features are usually exposed to untrustworthy parties, the privacy leakage problem of image local features has received increasing attention in recent years. In this work, we thoroughly evaluate the privacy leakage of Scale Invariant Feature Transform (SIFT), which is one of the most widely-used image local features. We first consider the case that the adversary can fully access the SIFT features, i.e., both the SIFT descriptors and the coordinates are available. We propose a novel end-to-end, coarse-to-fine deep generative model for reconstructing the latent image from its SIFT features. The designed deep generative model consists of two networks, where the first one attempts to learn the structural information of the latent image by transforming from SIFT features to Local Binary Pattern (LBP) features, while the second one aims to reconstruct the pixel values guided by the learned LBP. Compared with the state-of-the-art algorithms, the proposed deep generative model produces much improved reconstructed results over three public datasets. Furthermore, we address more challenging cases that only partial SIFT features (either SIFT descriptors or coordinates) are accessible to the adversary. It is shown that, if the adversary can only have access to the SIFT descriptors while not their coordinates, then the modest success of reconstructing the latent image might be achieved for highly-structured images (e.g., faces) and probably would fail in general settings. In addition, the latent image usually can be reconstructed with acceptable quality solely from the SIFT coordinates. Our results would suggest that the privacy leakage problem can be avoided to a certain extent if the SIFT coordinates can be well protected.",
      "year": 2020,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haiwei Wu",
        "Jiantao Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/3ca9a34a98c7ee9449b2a0563da1b49cc5972c60",
      "pdf_url": "https://arxiv.org/pdf/2009.01030",
      "publication_date": "2020-09-02",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "211b9586557801a3afa9abf0490b3b6f978ca2b7",
      "title": "Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage",
      "abstract": "Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user's privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, i.e., Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zhuohang Li",
        "Jiaxin Zhang",
        "Lu Liu",
        "Jian Liu"
      ],
      "citation_count": 148,
      "url": "https://www.semanticscholar.org/paper/211b9586557801a3afa9abf0490b3b6f978ca2b7",
      "pdf_url": "http://arxiv.org/pdf/2203.15696",
      "publication_date": "2022-03-29",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c472d1b4bbbe87eea5f7d3f9712e3e084d5c8695",
      "title": "Protect Privacy from Gradient Leakage Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is susceptible to gradient leakage attacks, as recent studies show the feasibility of obtaining private training data on clients from publicly shared gradients. Existing work solves this problem by incorporating a series of privacy protection mechanisms, such as homomorphic encryption and local differential privacy to prevent data leakage. However, these solutions either incur significant communication and computation costs, or significant training accuracy loss. In this paper, we show that the sensitivity of gradient changes w.r.t. training data is an essential measure of information leakage risk. Based on this observation, we present a novel defense, whose intuition is perturbing gradients to match information leakage risk such that the defense overhead is lightweight while privacy protection is adequate. Our another key observation is that global correlations of gradients could compensate for this perturbation. Based on such compensation, training can achieve guaranteed accuracy. We conduct experiments on MNIST, Fashion-MNIST and CIFAR-10 for defending against two gradient leakage attacks. Without sacrificing accuracy, the results demonstrate that our lightweight defense can decrease the PSNR and SSIM between the reconstructed images and raw images by up to more than 60% for both two attacks, compared with baseline defensive methods.",
      "year": 2022,
      "venue": "IEEE Conference on Computer Communications",
      "authors": [
        "Junxiao Wang",
        "Song Guo",
        "Xin Xie",
        "Heng Qi"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/c472d1b4bbbe87eea5f7d3f9712e3e084d5c8695",
      "pdf_url": "",
      "publication_date": "2022-05-02",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4e49e307cde2e93f898ca1591f7fee72a2df17e4",
      "title": "Leakage Inversion: Towards Quantifying Privacy in Searchable Encryption",
      "abstract": "Searchable encryption (SE) provides cryptographic guarantees that a user can efficiently search over encrypted data while only disclosing patterns about the data, also known as leakage. Recently, the community has developed leakage-abuse attacks that shed light on what an attacker can infer about the underlying sensitive information using the aforementioned leakage. A glaring missing piece in this effort is the absence of a systematic and rigorous method that quantifies the privacy guarantees of SE. In this work, we put forth the notion of leakage inversion that quantifies privacy in SE. Our insight is that the leakage is a function and, thus, one can define its inverse which corresponds to the collection of databases that reveal structurally equivalent patterns to the original plaintext database. We call this collection of databases the reconstruction space and we rigorously study its properties that impact the privacy of an SE scheme such as the entropy of the reconstruction space and the distance of its members from the original plaintext database. Leakage inversion allows for a foundational algorithmic analysis of the privacy offered by SE and we demonstrate this by defining closed-form expressions and lower/upper bounds on the properties of the reconstruction space for both keyword-based and range-based databases. We use leakage inversion in three scenarios: (i) we quantify the impact that auxiliary information, a typical cryptanalytic assumption, has to the overall privacy, (ii) we quantify how privacy is affected in case of restricting range schemes to respond to a limited number of queries, and (iii) we study the efficiency vs. privacy trade-off offered by proposed padding defenses. We use real-world databases in all three scenarios and we draw theoretically-grounded new insights about the interplay between leakage, attacks, defenses, and efficiency.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Evgenios M. Kornaropoulos",
        "Nathan Moyer",
        "Charalampos Papamanthou",
        "Alexandros Psomas"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/4e49e307cde2e93f898ca1591f7fee72a2df17e4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3548606.3560593",
      "publication_date": "2022-11-07",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d04c8175a13df545f7ef85d1605b4d28549f7380",
      "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
      "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach for reconstructing training data from leaked gradients. GIT employs a generative attack model, whose architecture is tailored to align with the structure of the leaked model based on theoretical analysis. Once trained offline, GIT can be deployed efficiently and only relies on the leaked gradients to reconstruct the input data, rendering it applicable under various distributed learning environments. When used as a prior for other iterative optimization-based methods, GIT not only accelerates convergence but also enhances the overall reconstruction quality. GIT consistently outperforms existing methods across multiple datasets and demonstrates strong robustness under challenging conditions, including inaccurate gradients, data distribution shifts and discrepancies in model parameters.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xinping Chen",
        "Chen Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d04c8175a13df545f7ef85d1605b4d28549f7380",
      "pdf_url": "",
      "publication_date": "2025-05-26",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4046a8c652d4fb90bfff4c436aae738ddac0c499",
      "title": "Simulating Training Dynamics to Reconstruct Training Data from Deep Neural Networks",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Hanling Tian",
        "Yuhang Liu",
        "M. He",
        "Zhengbao He",
        "Zhehao Huang",
        "Ruikai Yang",
        "Xiaolin Huang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4046a8c652d4fb90bfff4c436aae738ddac0c499",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1d8bcaea2678ca2ce88bdf9c5add07746a9b3f8",
      "title": "Training Data Reconstruction: Privacy Due to Uncertainty?",
      "abstract": "Being able to reconstruct training data from the parameters of a neural network is a major privacy concern. Previous works have shown that reconstructing training data, under certain circumstances, is possible. In this work, we analyse such reconstructions empirically and propose a new formulation of the reconstruction as a solution to a bilevel optimisation problem. We demonstrate that our formulation as well as previous approaches highly depend on the initialisation of the training images $x$ to reconstruct. In particular, we show that a random initialisation of $x$ can lead to reconstructions that resemble valid training samples while not being part of the actual training dataset. Thus, our experiments on affine and one-hidden layer networks suggest that when reconstructing natural images, yet an adversary cannot identify whether reconstructed images have indeed been part of the set of training samples.",
      "year": 2024,
      "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Christina Runkel",
        "Kanchana Vaishnavi Gandikota",
        "Jonas Geiping",
        "C. Schonlieb",
        "Michael Moeller"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f1d8bcaea2678ca2ce88bdf9c5add07746a9b3f8",
      "pdf_url": "",
      "publication_date": "2024-12-11",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a53d7c1db42554f6d423b8755e38dcf519ae12f",
      "title": "Reconstructing training data from document understanding models",
      "abstract": "Document understanding models are increasingly employed by companies to supplant humans in processing sensitive documents, such as invoices, tax notices, or even ID cards. However, the robustness of such models to privacy attacks remains vastly unexplored. This paper presents CDMI, the first reconstruction attack designed to extract sensitive fields from the training data of these models. We attack LayoutLM and BROS architectures, demonstrating that an adversary can perfectly reconstruct up to 4.1% of the fields of the documents used for fine-tuning, including some names, dates, and invoice amounts up to six-digit numbers. When our reconstruction attack is combined with a membership inference attack, our attack accuracy escalates to 22.5%. In addition, we introduce two new end-to-end metrics and evaluate our approach under various conditions: unimodal or bimodal data, LayoutLM or BROS backbones, four fine-tuning tasks, and two public datasets (FUNSD and SROIE). We also investigate the interplay between overfitting, predictive performance, and susceptibility to our attack. We conclude with a discussion on possible defenses against our attack and potential future research directions to construct robust document understanding models.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "J\u00e9r\u00e9mie Dentan",
        "Arnaud Paran",
        "A. Shabou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0a53d7c1db42554f6d423b8755e38dcf519ae12f",
      "pdf_url": "",
      "publication_date": "2024-06-05",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1ee2d51749bb6cd12bbfb61c38762c857d9ec112",
      "title": "Reconstructing Training Data with Informed Adversaries",
      "abstract": "Given access to a machine learning model, can an adversary reconstruct the model\u2019s training data? This work studies this question from the lens of a powerful informed adversary who knows all the training data points except one. By instantiating concrete attacks, we show it is feasible to reconstruct the remaining data point in this stringent threat model. For convex models (e.g. logistic regression), reconstruction attacks are simple and can be derived in closed-form. For more general models (e.g. neural networks), we propose an attack strategy based on training a reconstructor network that receives as input the weights of the model under attack and produces as output the target data point. We demonstrate the effectiveness of our attack on image classifiers trained on MNIST and CIFAR-10, and systematically investigate which factors of standard machine learning pipelines affect reconstruction success. Finally, we theoretically investigate what amount of differential privacy suffices to mitigate reconstruction attacks by informed adversaries. Our work provides an effective reconstruction attack that model developers can use to assess memorization of individual points in general settings beyond those considered in previous works (e.g. generative language models or access to training gradients); it shows that standard models have the capacity to store enough information to enable high-fidelity reconstruction of training data points; and it demonstrates that differential privacy can successfully mitigate such attacks in a parameter regime where utility degradation is minimal.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Borja Balle",
        "Giovanni Cherubin",
        "Jamie Hayes"
      ],
      "citation_count": 198,
      "url": "https://www.semanticscholar.org/paper/1ee2d51749bb6cd12bbfb61c38762c857d9ec112",
      "pdf_url": "https://arxiv.org/pdf/2201.04845",
      "publication_date": "2022-01-13",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fdcbec22c07d2842ced9a43ba84d73462cac3454",
      "title": "Reconstructing Training Data from Multiclass Neural Networks",
      "abstract": "Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "G. Buzaglo",
        "Niv Haim",
        "Gilad Yehudai",
        "Gal Vardi",
        "M. Irani"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/fdcbec22c07d2842ced9a43ba84d73462cac3454",
      "pdf_url": "http://arxiv.org/pdf/2305.03350",
      "publication_date": "2023-05-05",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "83b8b0b7a3ad6e100f39c3cc4c5cf757c38918f5",
      "title": "Reconstructing Training Data from Model Gradient, Provably",
      "abstract": "Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Zihan Wang",
        "Jason D. Lee",
        "Qi Lei"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/83b8b0b7a3ad6e100f39c3cc4c5cf757c38918f5",
      "pdf_url": "http://arxiv.org/pdf/2212.03714",
      "publication_date": "2022-12-07",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8adf0217e316e23bba95899ca7d80ac68629023",
      "title": "Fast Generation-Based Gradient Leakage Attacks: An Approach to Generate Training Data Directly From the Gradient",
      "abstract": "Federated learning (FL) is a distributed machine learning technique that guarantees the privacy of user data. However, FL has been shown to be vulnerable to gradient leakage attacks (GLA), which have the ability to reconstruct private training data from public gradients with high probability. These attacks are either analytic-based, requiring modification of the FL model, or optimization-based, requiring long convergence times and failing to effectively address the challenge of dealing with highly compressed gradients in practical FL systems. This paper presents a pioneering generation-based GLA method called FGLA that can reconstruct batches of user data without the need for the optimization process. We specifically design a feature separation technique that first extracts the features of each sample in a batch and then directly generates the user data. Our extensive experiments on multiple image datasets show that FGLA can reconstruct user images in seconds with a batch size of 256 from highly compressed gradients (0.8% compression ratio or higher), thereby significantly outperforming state-of-the-art methods.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haomiao Yang",
        "Dongyun Xue",
        "Mengyu Ge",
        "Jingwei Li",
        "Guowen Xu",
        "Hongwei Li",
        "Rongxing Lu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/a8adf0217e316e23bba95899ca7d80ac68629023",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "74d058a1b5acb55c9d74bf2a42b50b20e019e519",
      "title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models",
      "abstract": "High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we recover what training data is known to LLMs? In this work, we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights or token probabilities, by using information-guided probes. Our work builds on a key observation: text passages with high surprisal are good search material for memorization probes. By evaluating a model's ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.",
      "year": 2025,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Abhilasha Ravichander",
        "Jillian R. Fisher",
        "Taylor Sorensen",
        "Ximing Lu",
        "Yuchen Lin",
        "Maria Antoniak",
        "Niloofar Mireshghallah",
        "Chandra Bhagavatula",
        "Yejin Choi"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/74d058a1b5acb55c9d74bf2a42b50b20e019e519",
      "pdf_url": "",
      "publication_date": "2025-03-15",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9122c57c4a6be1aceeeace20a543fadee1103e93",
      "title": "On Reconstructing Training Data From Bayesian Posteriors and Trained Models",
      "abstract": "Publicly releasing the specification of a model with its trained parameters means an adversary can attempt to reconstruct information about the training data via training data reconstruction attacks, a major vulnerability of modern machine learning methods. This paper makes three primary contributions: establishing a mathematical framework to express the problem, characterising the features of the training data that are vulnerable via a maximum mean discrepancy equivalance and outlining a score matching framework for reconstructing data in both Bayesian and non-Bayesian models, the former is a first in the literature.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "George Wynne"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9122c57c4a6be1aceeeace20a543fadee1103e93",
      "pdf_url": "",
      "publication_date": "2025-07-24",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "80abaf4187f560ddeb1966669370dff507a4de0d",
      "title": "Understanding Training-Data Leakage from Gradients in Neural Networks for Image Classification",
      "abstract": "Federated learning of deep learning models for supervised tasks, e.g. image classification and segmentation, has found many applications: for example in human-in-the-loop tasks such as film post-production where it enables sharing of domain expertise of human artists in an efficient and effective fashion. In many such applications, we need to protect the training data from being leaked when gradients are shared in the training process due to IP or privacy concerns. Recent works have demonstrated that it is possible to reconstruct the training data from gradients for an image-classification model when its architecture is known. However, there is still an incomplete theoretical understanding of the efficacy and failure of such attacks. In this paper, we analyse the source of training-data leakage from gradients. We formulate the problem of training data reconstruction as solving an optimisation problem iteratively for each layer. The layer-wise objective function is primarily defined by weights and gradients from the current layer as well as the output from the reconstruction of the subsequent layer, but it might also involve a 'pull-back' constraint from the preceding layer. Training data can be reconstructed when we solve the problem backward from the output of the network through each layer. Based on this formulation, we are able to attribute the potential leakage of the training data in a deep network to its architecture. We also propose a metric to measure the level of security of a deep learning model against gradient-based attacks on the training data.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Cangxiong Chen",
        "Neill D. F. Campbell"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/80abaf4187f560ddeb1966669370dff507a4de0d",
      "pdf_url": "",
      "publication_date": "2021-11-19",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "062dab1552ce046ced426c573df9a0f3ac84c993",
      "title": "Reconstructing Training Data from Diverse ML Models by Ensemble Inversion",
      "abstract": "Model Inversion (MI), in which an adversary abuses access to a trained Machine Learning (ML) model attempting to infer sensitive information about its original training data, has attracted increasing research attention. During MI, the trained model under attack (MUA) is usually frozen and used to guide the training of a generator, such as a Generative Adversarial Network (GAN), to reconstruct the distribution of the original training data of that model. This might cause leakage of original training samples, and if successful, the privacy of dataset subjects will be at risk if the training data contains Personally Identifiable Information (PII). Therefore, an in-depth investigation of the potentials of MI techniques is crucial for the development of corresponding defense techniques. High-quality reconstruction of training data based on a single model is challenging. However, existing MI literature does not explore targeting multiple models jointly, which may provide additional information and diverse perspectives to the adversary.We propose the ensemble inversion technique that estimates the distribution of original training data by training a generator constrained by an ensemble (or set) of trained models with shared subjects or entities. This technique leads to noticeable improvements of the quality of the generated samples with distinguishable features of the dataset entities compared to MI of a single ML model. We achieve high quality results without any dataset and show how utilizing an auxiliary dataset that\u2019s similar to the presumed training data improves the results. The impact of model diversity in the ensemble is thoroughly investigated and additional constraints are utilized to encourage sharp predictions and high activations for the reconstructed samples, leading to more accurate reconstruction of training images.",
      "year": 2021,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Qian Wang",
        "Daniel Kurz"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/062dab1552ce046ced426c573df9a0f3ac84c993",
      "pdf_url": "https://arxiv.org/pdf/2111.03702",
      "publication_date": "2021-11-05",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39cfe9b0f4ff8a325e83049e3e87b7f16384b910",
      "title": "Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training",
      "abstract": "Masked Autoencoders (MAE) have shown promising performance in self-supervised learning for both 2D and 3D computer vision. However, existing MAE-style methods can only learn from the data of a single modality, i.e., either images or point clouds, which neglect the implicit semantic and geometric correlation between 2D and 3D. In this paper, we explore how the 2D modality can benefit 3D masked autoencoding, and propose Joint-MAE, a 2D-3D joint MAE framework for self-supervised 3D point cloud pre-training. Joint-MAE randomly masks an input 3D point cloud and its projected 2D images, and then reconstructs the masked information of the two modalities. For better cross-modal interaction, we construct our JointMAE by two hierarchical 2D-3D embedding modules, a joint encoder, and a joint decoder with modal-shared and model-specific decoders. On top of this, we further introduce two cross-modal strategies to boost the 3D representation learning, which are local-aligned attention mechanisms for 2D-3D semantic cues, and a cross-reconstruction loss for 2D-3D geometric constraints. By our pre-training paradigm, Joint-MAE achieves superior performance on multiple downstream tasks, e.g., 92.4% accuracy for linear SVM on ModelNet40 and 86.07% accuracy on the hardest split of ScanObjectNN.",
      "year": 2023,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Ziyu Guo",
        "Xianzhi Li",
        "P. Heng"
      ],
      "citation_count": 75,
      "url": "https://www.semanticscholar.org/paper/39cfe9b0f4ff8a325e83049e3e87b7f16384b910",
      "pdf_url": "https://arxiv.org/pdf/2302.14007",
      "publication_date": "2023-02-27",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "stealing model",
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "68661cad6ee64aa0b0eea029a50b65286525a58a",
      "title": "Learning Not to Reconstruct Anomalies",
      "abstract": "Video anomaly detection is often seen as one-class classification (OCC) problem due to the limited availability of anomaly examples. Typically, to tackle this problem, an autoencoder (AE) is trained to reconstruct the input with training set consisting only of normal data. At test time, the AE is then expected to well reconstruct the normal data while poorly reconstructing the anomalous data. However, several studies have shown that, even with only normal data training, AEs can often start reconstructing anomalies as well which depletes the anomaly detection performance. To mitigate this problem, we propose a novel methodology to train AEs with the objective of reconstructing only normal data, regardless of the input (i.e., normal or abnormal). Since no real anomalies are available in the OCC settings, the training is assisted by pseudo anomalies that are generated by manipulating normal data to simulate the out-of-normal-data distribution. We additionally propose two ways to generate pseudo anomalies: patch and skip frame based. Extensive experiments on three challenging video anomaly datasets demonstrate the effectiveness of our method in improving conventional AEs, achieving state-of-the-art performance.",
      "year": 2021,
      "venue": "British Machine Vision Conference",
      "authors": [
        "M. Astrid",
        "M. Zaheer",
        "Jae-Yeong Lee",
        "Seung-Ik Lee"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/68661cad6ee64aa0b0eea029a50b65286525a58a",
      "pdf_url": "",
      "publication_date": "2021-10-19",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bbb4a525d3e1dbf92ae70f7fe342c7eb8b095a05",
      "title": "Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds",
      "abstract": "Masked autoencoding has become a successful pretraining paradigm for Transformer models for text, images, and, recently, point clouds. Raw automotive datasets are suitable candidates for self-supervised pre-training as they gener-ally are cheap to collect compared to annotations for tasks like 3D object detection (OD). However, the development of masked autoencoders for point clouds has focused solely on synthetic and indoor data. Consequently, existing meth-ods have tailored their representations and models toward small and dense point clouds with homogeneous point den-sities. In this work, we study masked autoencoding for point clouds in an automotive setting, which are sparse and for which the point density can vary drastically among ob-jects in the same scene. To this end, we propose Voxel-MAE, a simple masked autoencoding pre-training scheme designed for voxel representations. We pre-train the back-bone of a Transformer-based 3D object detector to reconstruct masked voxels and to distinguish between empty and non-empty voxels. Our method improves the 3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes dataset. Further, we show that by pre-training with Voxel-MAE, we require only 40% of the annotated data to outperform a randomly initialized equivalent. Code is available at https://github.com/georghess/voxel-mae.",
      "year": 2022,
      "venue": "2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "authors": [
        "Georg Hess",
        "Johan Jaxing",
        "Elias Svensson",
        "David Hagerman",
        "Christoffer Petersson",
        "Lennart Svensson"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/bbb4a525d3e1dbf92ae70f7fe342c7eb8b095a05",
      "pdf_url": "https://research.chalmers.se/publication/536005/file/536005_Fulltext.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e800534e25a439f30365b49768850657f0a1447a",
      "title": "Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses",
      "abstract": "Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training samples on the reconstructability.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "G. Buzaglo",
        "Niv Haim",
        "Gilad Yehudai",
        "Gal Vardi",
        "Yakir Oz",
        "Yaniv Nikankin",
        "M. Irani"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/e800534e25a439f30365b49768850657f0a1447a",
      "pdf_url": "https://arxiv.org/pdf/2307.01827",
      "publication_date": "2023-07-04",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec0395f95ae917b2a831d216f2728fdf2b82d2fc",
      "title": "Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation",
      "abstract": "Federated learning (FL) is a framework for users to jointly train a machine learning model. FL is promoted as a privacy-enhancing technology (PET) that provides data minimization: data never \"leaves\" personal devices and users share only model updates with a server (e.g., a company) coordinating the distributed training. While prior work showed that in vanilla FL a malicious server can extract users\u2019 private data from the model updates, in this work we take it further and demonstrate that a malicious server can reconstruct user data even in hardened versions of the protocol. More precisely, we propose an attack against FL protected with distributed differential privacy (DDP) and secure aggregation (SA). Our attack method is based on the introduction of sybil devices that deviate from the protocol to expose individual users\u2019 data for reconstruction by the server. The underlying root cause for the vulnerability to our attack is a power imbalance: the server orchestrates the whole protocol and users are given little guarantees about the selection of other users participating in the protocol. Moving forward, we discuss requirements for privacy guarantees in FL. We conclude that users should only participate in the protocol when they trust the server or they apply local primitives such as local DP, shifting power away from the server. Yet, the latter approaches come at significant overhead in terms of performance degradation of the trained model, making them less likely to be deployed in practice.",
      "year": 2023,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Franziska Boenisch",
        "Adam Dziedzic",
        "R. Schuster",
        "A. Shamsabadi",
        "Ilia Shumailov",
        "Nicolas Papernot"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/ec0395f95ae917b2a831d216f2728fdf2b82d2fc",
      "pdf_url": "https://arxiv.org/pdf/2301.04017",
      "publication_date": "2023-01-09",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d47cceb88d64c5b930c3204207f09bf059ca46f7",
      "title": "Learning to Generate and Reconstruct 3D Meshes with only 2D Supervision",
      "abstract": "We present a unified framework tackling two problems: class-specific 3D reconstruction from a single image, and generation of new 3D shape samples. These tasks have received considerable attention recently; however, existing approaches rely on 3D supervision, annotation of 2D images with keypoints or poses, and/or training with multiple views of each object instance. Our framework is very general: it can be trained in similar settings to these existing approaches, while also supporting weaker supervision scenarios. Importantly, it can be trained purely from 2D images, without ground-truth pose annotations, and with a single view per instance. We employ meshes as an output representation, instead of voxels used in most prior work. This allows us to exploit shading information during training, which previous 2D-supervised methods cannot. Thus, our method can learn to generate and reconstruct concave object classes. We evaluate our approach on synthetic data in various settings, showing that (i) it learns to disentangle shape from pose; (ii) using shading in the loss improves performance; (iii) our model is comparable or superior to state-of-the-art voxel-based approaches on quantitative metrics, while producing results that are visually more pleasing; (iv) it still performs well when given supervision weaker than in prior works.",
      "year": 2018,
      "venue": "British Machine Vision Conference",
      "authors": [
        "Paul Henderson",
        "Vittorio Ferrari"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/d47cceb88d64c5b930c3204207f09bf059ca46f7",
      "pdf_url": "",
      "publication_date": "2018-07-24",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee8ef5218ce893497135564134793470ae198d5c",
      "title": "A Data-Driven Approach to System Invertibility and Input Reconstruction",
      "abstract": "We consider the problems of system invertibility and input reconstruction for linear time-invariant (LTI) systems using only measured data. The two problems are connected in the sense that input reconstruction is possible provided that the system is left invertible. To verify the latter property without model knowledge, we leverage behavioral systems theory and develop two data-driven algorithms: one based on input/state/output data and the other based only on input/output data. We then consider the problem of input reconstruction for both noise-free and noisy data settings. In the case of noisy data, a statistical approach is leveraged to formulate the problem as a maximum likelihood estimation (MLE) problem. The proposed approaches are finally illustrated with numerical examples that show: exact input reconstruction in the noise-free setting; and the better performance of the MLE-based approach compared to the standard least-norm solution.",
      "year": 2023,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Vikas Kumar Mishra",
        "Andrea Iannelli",
        "N. Baj\u00e7inca"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ee8ef5218ce893497135564134793470ae198d5c",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "196aba56504e109555cdf3f9b311a545aef9701e",
      "title": "Minimal Model Structure Analysis for Input Reconstruction in Federated Learning",
      "abstract": "\\ac{fl} proposed a distributed \\ac{ml} framework where every distributed worker owns a complete copy of global model and their own data. The training is occurred locally, which assures no direct transmission of training data. However, the recent work \\citep{zhu2019deep} demonstrated that input data from a neural network may be reconstructed only using knowledge of gradients of that network, which completely breached the promise of \\ac{fl} and sabotaged the user privacy. In this work, we aim to further explore the theoretical limits of reconstruction, speedup and stabilize the reconstruction procedure. We show that a single input may be reconstructed with the analytical form, regardless of network depth using a fully-connected neural network with one hidden node. Then we generalize this result to a gradient averaged over batches of size $B$. In this case, the full batch can be reconstructed if the number of hidden units exceeds $B$. For a \\ac{cnn}, the number of required kernels in convolutional layers is decided by multiple factors, e.g., padding, kernel and stride size, etc. We require the number of kernels $h\\geq (\\frac{d}{d^{\\prime}})^2C$, where we define $d$ as input width, $d^{\\prime}$ as output width after convolutional layer, and $C$ as channel number of input. We validate our observation and demonstrate the improvements using bio-medical (fMRI, \\ac{wbc}) and benchmark data (MNIST, Kuzushiji-MNIST, CIFAR100, ImageNet and face images).",
      "year": 2020,
      "venue": "",
      "authors": [
        "Jia Qian",
        "Hiba Nassar",
        "L. K. Hansen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/196aba56504e109555cdf3f9b311a545aef9701e",
      "pdf_url": "",
      "publication_date": "2020-10-29",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33c9a9ed7aa06fef54145f617295a417abdf5ba8",
      "title": "PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding",
      "abstract": "The latest and most impactful advances in large models stem from their increased size. Unfortunately, this translates into an improved memorization capacity, raising data privacy concerns. Specifically, it has been shown that models can output personal identifiable information (PII) contained in their training data. However, reported PII extraction performance varies widely, and there is no consensus on the optimal methodology to evaluate this risk, resulting in underestimating realistic adversaries. In this work, we empirically demonstrate that it is possible to improve the extractability of PII by over ten-fold by grounding the prefix of the manually constructed extraction prompt with in-domain data. This approach achieves phone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308 queries, respectively, i.e., the phone number of 1 person in 15 is extractable.",
      "year": 2024,
      "venue": "PRIVATENLP",
      "authors": [
        "K. K. Nakka",
        "Ahmed Frikha",
        "Ricardo Mendes",
        "Xue Jiang",
        "Xuebing Zhou"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/33c9a9ed7aa06fef54145f617295a417abdf5ba8",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4af71c4806a542b59eac8cbcaf02050eb3718f64",
      "title": "Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models",
      "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide range of tasks. However, recent studies have shown that LLMs can memorize training data and simple repeated tokens can trick the model to leak the data. In this paper, we take a step further and show that certain special characters or their combinations with English letters are stronger memory triggers, leading to more severe data leakage. The intuition is that, since LLMs are trained with massive data that contains a substantial amount of special characters (e.g. structural symbols {, } of JSON files, and @, # in emails and online posts), the model may memorize the co-occurrence between these special characters and the raw texts. This motivates us to propose a simple but effective Special Characters Attack (SCA) to induce training data leakage. Our experiments verify the high effectiveness of SCA against state-of-the-art LLMs: they can leak diverse training data, such as code corpus, web pages, and personally identifiable information, and sometimes generate non-stop outputs as a byproduct. We further show that the composition of the training data corpus can be revealed by inspecting the leaked data -- one crucial piece of information for pre-training high-performance LLMs. Our work can help understand the sensitivity of LLMs to special characters and identify potential areas for improvement.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yang Bai",
        "Ge Pei",
        "Jindong Gu",
        "Yong Yang",
        "Xingjun Ma"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/4af71c4806a542b59eac8cbcaf02050eb3718f64",
      "pdf_url": "",
      "publication_date": "2024-05-09",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0abd29e67b1e52e922660c315bcdeedd9b1eab7e",
      "title": "Bag of Tricks for Training Data Extraction from Language Models",
      "abstract": "With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the baseline by a large margin in most cases, providing a much stronger baseline for future research. The code is available at https://github.com/weichen-yu/LM-Extraction.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Weichen Yu",
        "Tianyu Pang",
        "Qian Liu",
        "Chao Du",
        "Bingyi Kang",
        "Yan Huang",
        "Min Lin",
        "Shuicheng Yan"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/0abd29e67b1e52e922660c315bcdeedd9b1eab7e",
      "pdf_url": "http://arxiv.org/pdf/2302.04460",
      "publication_date": "2023-02-09",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0fbf7ea1a3bd1754ed9aa12ed25906b731ece589",
      "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
      "abstract": "As the deployment of pre-trained language models (PLMs) expands, pressing security concerns have arisen regarding the potential for malicious extraction of training data, posing a threat to data privacy.This study is the first to provide a comprehensive survey of training data extraction from PLMs.Our review covers more than 100 key papers in fields such as natural language processing and security.First, preliminary knowledge is recapped and a taxonomy of various definitions of memorization is presented.The approaches for attack and defense are then systemized.Furthermore, the empirical findings of several quantitative studies are highlighted.Finally, future research directions based on this review are suggested.",
      "year": 2023,
      "venue": "TRUSTNLP",
      "authors": [
        "Shotaro Ishihara"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/0fbf7ea1a3bd1754ed9aa12ed25906b731ece589",
      "pdf_url": "http://arxiv.org/pdf/2305.16157",
      "publication_date": "2023-05-25",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97010556749971d3e54039edb26fd47c713a735c",
      "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
      "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhexin Zhang",
        "Jiaxin Wen",
        "Minlie Huang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/97010556749971d3e54039edb26fd47c713a735c",
      "pdf_url": "https://arxiv.org/pdf/2307.04401",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fc7ee1828030a818f52518022a39f6a3ada60222",
      "title": "Scalable Extraction of Training Data from (Production) Language Models",
      "abstract": "This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Milad Nasr",
        "Nicholas Carlini",
        "Jonathan Hayase",
        "Matthew Jagielski",
        "A. F. Cooper",
        "Daphne Ippolito",
        "Christopher A. Choquette-Choo",
        "Eric Wallace",
        "Florian Tram\u00e8r",
        "Katherine Lee"
      ],
      "citation_count": 460,
      "url": "https://www.semanticscholar.org/paper/fc7ee1828030a818f52518022a39f6a3ada60222",
      "pdf_url": "",
      "publication_date": "2023-11-28",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df7d26339adf4eb0c07160947b9d2973c24911ba",
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss",
        "Katherine Lee",
        "Adam Roberts",
        "Tom B. Brown",
        "D. Song",
        "\u00da. Erlingsson",
        "Alina Oprea",
        "Colin Raffel"
      ],
      "citation_count": 2443,
      "url": "https://www.semanticscholar.org/paper/df7d26339adf4eb0c07160947b9d2973c24911ba",
      "pdf_url": "",
      "publication_date": "2020-12-14",
      "keywords_matched": [
        "extracting model",
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f",
      "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
      "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Martin Josifoski",
        "Marija Sakota",
        "Maxime Peyrard",
        "Robert West"
      ],
      "citation_count": 107,
      "url": "https://www.semanticscholar.org/paper/f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f",
      "pdf_url": "http://arxiv.org/pdf/2303.04132",
      "publication_date": "2023-03-07",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "title": "InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion",
      "abstract": "Cloud service providers, including Google, Amazon, and Alibaba, have now launched machine-learning-as-a-service (MLaaS) platforms, allowing clients to access sophisticated cloud-based machine learning models via APIs. Unfortunately, however, the commercial value of these models makes them alluring targets for theft, and their strategic position as part of the IT infrastructure of many companies makes them an enticing springboard for conducting further adversarial attacks. In this paper, we put forth a novel and effective attack strategy, dubbed InverseNet, that steals the functionality of black-box cloud-based models with only a small number of queries. The crux of the innovation is that, unlike existing model extraction attacks that rely on public datasets or adversarial samples, InverseNet constructs inversed training samples to increase the similarity between the extracted substitute model and the victim model. Further, only a small number of data samples with high confidence scores (rather than an entire dataset) are used to reconstruct the inversed dataset, which substantially reduces the attack cost. Extensive experiments conducted on three simulated victim models and Alibaba Cloud's commercially-available API demonstrate that InverseNet yields a model with significantly greater functional similarity to the victim model than the current state-of-the-art attacks at a substantially lower query budget.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Guanghao Mei",
        "Qian Wang"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0336.pdf",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e4bbcf6c84bfcdbeafecf75f2b0b98eaa1020e63",
      "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
      "abstract": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Mustafa Safa Ozdayi",
        "Charith Peris",
        "Jack G. M. FitzGerald",
        "Christophe Dupuy",
        "Jimit Majmudar",
        "Haidar Khan",
        "Rahil Parikh",
        "Rahul Gupta"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/e4bbcf6c84bfcdbeafecf75f2b0b98eaa1020e63",
      "pdf_url": "http://arxiv.org/pdf/2305.11759",
      "publication_date": "2023-05-19",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "890915a280d0b7d0203dc9cad8c037e1a9ae12da",
      "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
      "abstract": "It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yuxin Wen",
        "Leo Marchyok",
        "Sanghyun Hong",
        "Jonas Geiping",
        "Tom Goldstein",
        "Nicholas Carlini"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/890915a280d0b7d0203dc9cad8c037e1a9ae12da",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "membership inference",
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9a9cc660876c380650fd99fae63ce2bed1df7c56",
      "title": "Discriminative Adversarial Privacy: Balancing Accuracy and Membership Privacy in Neural Networks",
      "abstract": "The remarkable proliferation of deep learning across various industries has underscored the importance of data privacy and security in AI pipelines. As the evolution of sophisticated Membership Inference Attacks (MIAs) threatens the secrecy of individual-specific information used for training deep learning models, Differential Privacy (DP) raises as one of the most utilized techniques to protect models against malicious attacks. However, despite its proven theoretical properties, DP can significantly hamper model performance and increase training time, turning its use impractical in real-world scenarios. Tackling this issue, we present Discriminative Adversarial Privacy (DAP), a novel learning technique designed to address the limitations of DP by achieving a balance between model performance, speed, and privacy. DAP relies on adversarial training based on a novel loss function able to minimise the prediction error while maximising the MIA's error. In addition, we introduce a novel metric named Accuracy Over Privacy (AOP) to capture the performance-privacy trade-off. Finally, to validate our claims, we compare DAP with diverse DP scenarios, providing an analysis of the results from performance, time, and privacy preservation perspectives.",
      "year": 2023,
      "venue": "British Machine Vision Conference",
      "authors": [
        "Eugenio Lomurno",
        "Alberto Archetti",
        "Francesca Ausonio",
        "Matteo Matteucci"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/9a9cc660876c380650fd99fae63ce2bed1df7c56",
      "pdf_url": "http://arxiv.org/pdf/2306.03054",
      "publication_date": "2023-06-05",
      "keywords_matched": [
        "membership privacy",
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b707df9745c21c4d544020d1908aec5755595fa9",
      "title": "Investigating the Effect of Misalignment on Membership Privacy in the White-box Setting",
      "abstract": "Machine learning models have been shown to leak sensitive information about their training datasets. Models are increasingly deployed on devices, raising concerns that white-box access to the model parameters increases the attack surface compared to black-box access which only provides query access. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A potential reason is misalignment, a known characteristic of deep neural networks. In the shadow modelling context, misalignment means that, while the shadow models learn similar features in each layer, the features are located in different positions. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause. We then extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align the layers of a shadow model to those of the target model.We show re-alignment techniques to significantly reduce the measured misalignment between the target and shadow models. Finally, we perform a comprehensive evaluation of white-box membership inference attacks (MIA). Our analysis reveals that internal layer activation-based MIAs suffer strongly from shadow model misalignment, while gradient-based MIAs are only sometimes significantly affected. We show that re-aligning the shadow models strongly improves the former's performance and can also improve the latter's performance, although less frequently. On the CIFAR10 dataset with a false positive rate of 1%, white-box MIA using re-aligned shadow models improves the true positive rate by 4.5%.Taken together, our results highlight that on-device deployment increases the attack surface and that the newly available information can be used to build more powerful attacks.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Ana-Maria Cre\u0163u",
        "Daniel Jones",
        "Yves-Alexandre de Montjoye",
        "Shruti Tople"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b707df9745c21c4d544020d1908aec5755595fa9",
      "pdf_url": "https://arxiv.org/pdf/2306.05093",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "122fb9401248e694b32ad55181c4c15fd9aea257",
      "title": "Trade-offs between membership privacy&adversarially robust learning",
      "abstract": "Historically, machine learning methods have not been designed with security in mind. In turn, this has given rise to adversarial examples, carefully perturbed input samples aimed to mislead detection at test time, which have been applied to attack spam and malware classification, and more recently to attack image classification. Consequently, an abundance of research has been devoted to designing machine learning methods that are robust to adversarial examples. Unfortunately, there are desiderata besides robustness that a secure and safe machine learning model must satisfy, such as fairness and privacy. Recent work by Song et al. (2019) has shown, empirically, that there exists a trade-off between robust and private machine learning models. Models designed to be robust to adversarial examples often overfit on training data to a larger extent than standard (non-robust) models. If a dataset contains private information, then any statistical test that separates training and test data by observing a model's outputs can represent a privacy breach, and if a model overfits on training data, these statistical tests become easier. In this work, we identify settings where standard models will overfit to a larger extent in comparison to robust models, and as empirically observed in previous works, settings where the opposite behavior occurs. Thus, it is not necessarily the case that privacy must be sacrificed to achieve robustness. The degree of overfitting naturally depends on the amount of data available for training. We go on to characterize how the training set size factors into the privacy risks exposed by training a robust model on a simple Gaussian data task, and show empirically that our findings hold on image classification benchmark datasets, such as CIFAR-10 and CIFAR-100.",
      "year": 2020,
      "venue": "",
      "authors": [
        "Jamie Hayes"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/122fb9401248e694b32ad55181c4c15fd9aea257",
      "pdf_url": "",
      "publication_date": "2020-06-08",
      "keywords_matched": [
        "membership privacy"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d1101476c85ae324142440e9f568ecbf41625be5",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Nils Lukas",
        "A. Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B'eguelin"
      ],
      "citation_count": 313,
      "url": "https://www.semanticscholar.org/paper/d1101476c85ae324142440e9f568ecbf41625be5",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "membership information",
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c3843b5a1511d9fc131433c7993f74e3012c7d25",
      "title": "CLIPping Privacy: Identity Inference Attacks on Multi-Modal Machine Learning Models",
      "abstract": null,
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Dominik Hintersdorf",
        "Lukas Struppek",
        "K. Kersting"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/c3843b5a1511d9fc131433c7993f74e3012c7d25",
      "pdf_url": "http://arxiv.org/pdf/2209.07341",
      "publication_date": null,
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ead2942c14c880a6cd2a3759a9aaf6875517cd34",
      "title": "IDT: Dual-Task Adversarial Attacks for Privacy Protection",
      "abstract": "Natural language processing (NLP) models may leak private information in different ways, including membership inference, reconstruction or attribute inference attacks. Sensitive information may not be explicit in the text, but hidden in underlying writing characteristics. Methods to protect privacy can involve using representations inside models that are demonstrated not to detect sensitive attributes or -- for instance, in cases where users might not trust a model, the sort of scenario of interest here -- changing the raw text before models can have access to it. The goal is to rewrite text to prevent someone from inferring a sensitive attribute (e.g. the gender of the author, or their location by the writing style) whilst keeping the text useful for its original intention (e.g. the sentiment of a product review). The few works tackling this have focused on generative techniques. However, these often create extensively different texts from the original ones or face problems such as mode collapse. This paper explores a novel adaptation of adversarial attack techniques to manipulate a text to deceive a classifier w.r.t one task (privacy) whilst keeping the predictions of another classifier trained for another task (utility) unchanged. We propose IDT, a method that analyses predictions made by auxiliary and interpretable models to identify which tokens are important to change for the privacy task, and which ones should be kept for the utility task. We evaluate different datasets for NLP suitable for different tasks. Automatic and human evaluations show that IDT retains the utility of text, while also outperforming existing methods when deceiving a classifier w.r.t privacy task.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Pedro Faustini",
        "S. Tonni",
        "Annabelle Mciver",
        "Qiongkai Xu",
        "M. Dras"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ead2942c14c880a6cd2a3759a9aaf6875517cd34",
      "pdf_url": "",
      "publication_date": "2024-06-28",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "212da291c23fcb65470bc16d7dfe07ef976374f1",
      "title": "An Efficient and Secure Federated Learning Communication Framework",
      "abstract": "Federated Learning (FL) is widely recognized as one of the most effective collaborative learning methodologies for training various local models using private datasets. However, model inversion or membership inference may be used by an eavesdropper or by the central server, which is frequently a reliable but curious entity, to know details about local client datasets. To prevent these attacks during communication or on the server side, several security solutions are presented. However, these existing solutions suffer from high overhead in terms of computation in addition to storage and communication overhead, especially for real-time applications or for limited client devices. Therefore, in this work, a secure scheme that consists of two layers of the Symmetric Encryption Algorithm (SCA) is proposed. The first layer uses a novel lightweight additive homomorphic cipher scheme that is adapted and modified, while the second layer uses a traditional SCA. This work focuses on the effect of employing symmetric HE in an FL model instead of using an asymmetric approach, which is the base of existing HE cipher schemes. Experimental results indicate a significant gain in terms of computation, resources, storage, communication, delay overhead, and immunity against a wide variety of attacks, especially with the usage of a dynamic key approach with variable cryptographic primitives per block or iteration.",
      "year": 2024,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Hassan N. Noura",
        "Khalil Hariss"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/212da291c23fcb65470bc16d7dfe07ef976374f1",
      "pdf_url": "",
      "publication_date": "2024-05-27",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3db0f52e3e748e4304e0da026578e874509d93ee",
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "abstract": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Savvy Sharma",
        "George Petrovic",
        "Sarthak Kaushik"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3db0f52e3e748e4304e0da026578e874509d93ee",
      "pdf_url": "",
      "publication_date": "2025-12-24",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8c89779e6106f08608053ddbf526c789840e363c",
      "title": "IDT: Dual-Task Adversarial Rewriting for Attribute Anonymisation",
      "abstract": "\n Natural language processing (NLP) models may leak private information in different ways, including membership inference, reconstruction or attribute inference attacks. Sensitive information may not be explicit in the text, but hidden in underlying writing characteristics. Methods to protect privacy can involve using representations inside models that are demonstrated not to detect sensitive attributes or \u2014 for instance, in cases where users might be at risk from an untrustworthy model, the sort of scenario of interest here \u2014 changing the raw text before models can have access to it. The goal is to rewrite text to prevent someone from inferring a sensitive attribute (e.g. the gender of the author, or their location by the writing style) whilst keeping the text useful for its original intention (e.g. the sentiment of a product review). The few works tackling this have focused on generative techniques. However, these often create extensively different texts from the original ones or face problems such as mode collapse. This paper explores a novel adaptation of adversarial attack techniques to manipulate a text to deceive a classifier w.r.t one task (privacy) whilst keeping the predictions of another classifier trained for another task (utility) unchanged. We propose IDT, a method that analyses predictions made by auxiliary and interpretable models to identify which tokens are important to change for the privacy task, and which ones should be kept for the utility task. We evaluate different datasets for NLP suitable for different tasks. Automatic and human evaluations show that IDT retains the utility of text, while also outperforming existing methods when deceiving a classifier w.r.t a privacy task.",
      "year": 2025,
      "venue": "Computational Linguistics",
      "authors": [
        "Pedro Faustini",
        "S. Tonni",
        "Annabelle Mciver",
        "Qiongkai Xu",
        "M. Dras"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8c89779e6106f08608053ddbf526c789840e363c",
      "pdf_url": "",
      "publication_date": "2025-07-08",
      "keywords_matched": [
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "834fda9481ff83dd7b8715748c8a73a6ef08826e",
      "title": "Disparate Vulnerability: on the Unfairness of Privacy Attacks Against Machine Learning",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Mohammad Yaghini",
        "Bogdan Kulynych",
        "C. Troncoso"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/834fda9481ff83dd7b8715748c8a73a6ef08826e",
      "pdf_url": "",
      "publication_date": "2019-06-02",
      "keywords_matched": [
        "unfairness attack",
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c48706d0f9820cdbd726370974ccc478bdb204ad",
      "title": "OPUPO: Defending Against Membership Inference Attacks With Order-Preserving and Utility-Preserving Obfuscation",
      "abstract": "In this work, we present OPUPO to protect machine learning classifiers against black-box membership inference attacks by alleviating the prediction difference between training and non-training samples. Specifically, we apply order-preserving and utility-preserving obfuscation to prediction vectors. The order-preserving constraint strictly maintains the order of confidence scores in the prediction vectors, guaranteeing that the model's classification accuracy is not affected. The utility-preserving constraint, on the other hand, enables adaptive distortions to the prediction vectors in order to protect their utility. Moreover, OPUPO is proved to be adversary resistant that even well-informed defense-aware adversaries cannot restore the original prediction vectors to bypass the defense. We evaluate OPUPO on machine learning and deep learning classifiers trained with four popular datasets. Experiments verify that OPUPO can effectively defend against state-of-the-art attack techniques with negligible computation overhead. In specific, the inference accuracy could be reduced from as high as 87.66% to around 50%, i.e., random guess, and the prediction time will increase by only 0.44% on average. The experiments also show that OPUPO could achieve better privacy-utility trade-off than existing defenses.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Yaru Liu",
        "Hongcheng Li",
        "Gang Huang",
        "Wei Hua"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c48706d0f9820cdbd726370974ccc478bdb204ad",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41a7bfe6e9dd9dcefec186a9705ad4b26707b72f",
      "title": "Protect and Extend - Using GANs for Synthetic Data Generation of Time-Series Medical Records",
      "abstract": "Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. The privacy preservation of the respective models is assessed by applying membership inference attacks to determine potential data leakage risks. Our experiments indicate the superiority of the privacy-preserving GAN (PPGAN) model over other models regarding privacy preservation while maintaining an acceptable level of QoG. The presented results can support better data protection for medical use cases in the future.",
      "year": 2023,
      "venue": "International Workshop on Quality of Multimedia Experience",
      "authors": [
        "Navid Ashrafi",
        "Vera Schmitt",
        "R. Spang",
        "Sebastian M\u00f6ller",
        "Jan-Niklas Voigt-Antons"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/41a7bfe6e9dd9dcefec186a9705ad4b26707b72f",
      "pdf_url": "",
      "publication_date": "2023-06-20",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2e9cff85a2bfc17f62a0bd9671068f8cf2dcd5a6",
      "title": "Protect privacy of deep classification networks by exploiting their generative power",
      "abstract": "Research showed that deep learning models are vulnerable to membership inference attacks, which aim to determine if an example is in the training set of the model. We propose a new framework to defend against this sort of attack. Our key insight is that if we retrain the original classifier with a new dataset that is independent of the original training set while their elements are sampled from the same distribution, the retrained classifier will leak no information that cannot be inferred from the distribution about the original training set. Our framework consists of three phases. First, we transferred the original classifier to a Joint Energy-based Model (JEM) to exploit the model\u2019s implicit generative power. Then, we sampled from the JEM to create a new dataset. Finally, we used the new dataset to retrain or fine-tune the original classifier. We empirically studied different transfer learning schemes for the JEM and fine-tuning/retraining strategies for the classifier against shadow-model attacks. Our evaluation shows that our framework can suppress the attacker\u2019s membership advantage to a negligible level while keeping the classifier\u2019s accuracy acceptable. We compared it with other state-of-the-art defenses considering adaptive attackers and showed our defense is effective even under the worst-case scenario. Besides, we also found that combining other defenses with our framework often achieves better robustness. Our code will be made available at https://github.com/ChenJiyu/meminf-defense.git.",
      "year": 2021,
      "venue": "Machine-mediated learning",
      "authors": [
        "Jiyu Chen",
        "Yiwen Guo",
        "Qianjun Zheng",
        "Hao Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2e9cff85a2bfc17f62a0bd9671068f8cf2dcd5a6",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s10994-021-05951-6.pdf",
      "publication_date": "2021-04-01",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "671ce139cdc68544ac8142d0f2046f1ce6073a0e",
      "title": "Bounding Training Data Reconstruction in DP-SGD",
      "abstract": "Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Finally, we use our methods to demonstrate that different settings of the DP-SGD parameters leading to the same DP guarantees can result in significantly different success rates for reconstruction, indicating that the DP guarantee alone might not be a good proxy for controlling the protection against reconstruction attacks.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jamie Hayes",
        "Saeed Mahloujifar",
        "Borja Balle"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/671ce139cdc68544ac8142d0f2046f1ce6073a0e",
      "pdf_url": "http://arxiv.org/pdf/2302.07225",
      "publication_date": "2023-02-14",
      "keywords_matched": [
        "protect membership"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "title": "Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting",
      "abstract": "Model stealing attacks involve creating copies of machine learning models that have similar functionalities to the original model without proper authorization. Such attacks raise significant concerns about the intellectual property of the machine learning models. Nonetheless, current defense mechanisms against such attacks tend to exhibit certain drawbacks, notably in terms of utility, and robustness. For example, watermarking-based defenses require victim models to be retrained for embedding watermarks, which can potentially impact the main task performance. Moreover, other defenses, especially fingerprinting-based methods, often rely on specific samples like adversarial examples to verify ownership of the target model. These approaches might prove less robust against adaptive attacks, such as model stealing with adversarial training. It remains unclear whether normal examples, as opposed to adversarial ones, can effectively reflect the characteristics of stolen models. To tackle these challenges, we propose a novel method that leverages a neural network as a decoder to inverse the suspicious model\u2019s outputs. Inspired by model inversion attacks, we argue that this decoding process will unveil hidden patterns inherent in the original outputs of the suspicious model. Drawing from these decoding outcomes, we calculate specific metrics to determine the legitimacy of the suspicious models. We validate the efficacy of our defense technique against diverse model stealing attacks, specifically within the domain of classification tasks based on deep neural networks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Shuai Zhou",
        "Tianqing Zhu",
        "Dayong Ye",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "title": "Streamlining DNN Obfuscation to Defend Against Model Stealing Attacks",
      "abstract": "Side-channel-based Deep Neural Network (DNN) model stealing has become a major concern with the advent of learning-based attacks. In respond to this threat, defence mechanisms have been presented to obfuscate the DNN execution, making it difficult to infer the correlation between side-channel information and DNN architecture. However, state-of-the-art (SOTA) DNN obfuscation is time-consuming, requires expert-level changes in existing DNN compilers (e.g., Tensor Virtual Machine (TVM)), and often relies on prior knowledge of the attack models. In this work, we study the impact of various obfuscation levels on the defence effectiveness, and present a streamlined DNN obfuscation process that is extremely fast and is agnostic to any attack models. Our study reveals that by just modifying the scheduling of DNN operations on the GPU, we can achieve comparable defense performance as the SOTA in an attack agnostic manner. We also propose a simple algorithm that determines an effective scheduling configuration for mitigating DNN model stealing at a fraction of a time required by SOTA obfuscation methods. Our method can be easily integrated into existing DNN compilers as a security feature, even by non-experts, to protect their DNN against side-channel attacks.",
      "year": 2024,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Siew-Kei Lam",
        "Guiyuan Jiang",
        "Peilan He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "980451bad2ecb99c5d932e8c05d992a12324dad8",
      "title": "Hermes Attack: Steal DNN Models with Lossless Inference Accuracy",
      "abstract": "Deep Neural Networks (DNNs) models become one of the most valuable enterprise assets due to their critical roles in all aspects of applications. With the trend of privatization deployment of DNN models, the data leakage of the DNN models is becoming increasingly serious and widespread. All existing model-extraction attacks can only leak parts of targeted DNN models with low accuracy or high overhead. In this paper, we first identify a new attack surface -- unencrypted PCIe traffic, to leak DNN models. Based on this new attack surface, we propose a novel model-extraction attack, namely Hermes Attack, which is the first attack to fully steal the whole victim DNN model. The stolen DNN models have the same hyper-parameters, parameters, and semantically identical architecture as the original ones. It is challenging due to the closed-source CUDA runtime, driver, and GPU internals, as well as the undocumented data structures and the loss of some critical semantics in the PCIe traffic. Additionally, there are millions of PCIe packets with numerous noises and chaos orders. Our Hermes Attack addresses these issues by huge reverse engineering efforts and reliable semantic reconstruction, as well as skillful packet selection and order correction. We implement a prototype of the Hermes Attack, and evaluate two sequential DNN models (i.e., MINIST and VGG) and one consequential DNN model (i.e., ResNet) on three NVIDIA GPU platforms, i.e., NVIDIA Geforce GT 730, NVIDIA Geforce GTX 1080 Ti, and NVIDIA Geforce RTX 2080 Ti. The evaluation results indicate that our scheme is able to efficiently and completely reconstruct ALL of them with making inferences on any one image. Evaluated with Cifar10 test dataset that contains 10,000 images, the experiment results show that the stolen models have the same inference accuracy as the original ones (i.e., lossless inference accuracy).",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yuankun Zhu",
        "Yueqiang Cheng",
        "Husheng Zhou",
        "Yantao Lu"
      ],
      "citation_count": 113,
      "url": "https://www.semanticscholar.org/paper/980451bad2ecb99c5d932e8c05d992a12324dad8",
      "pdf_url": "",
      "publication_date": "2020-06-23",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "title": "Model inversion attacks against collaborative inference",
      "abstract": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and datasets, to show their effectiveness and generalization. We also study the characteristics of deep learning models that make them susceptible to such inference privacy threats. This provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.",
      "year": 2019,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "R. Lee"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "pdf_url": "",
      "publication_date": "2019-12-09",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "title": "Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhenyi Wang",
        "Li Shen",
        "Tongliang Liu",
        "Tiehang Duan",
        "Yanjun Zhu",
        "Dongling Zhan",
        "D. Doermann",
        "Mingchen Gao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/114c2f2cc2ab8a80a6229339a0c532ff4d59caed",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal model",
        "model extraction defense",
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb73ae451c6a0fa5d32b94c0fdbcae9c5f74b308",
      "title": "GDPArrrrr: Using Privacy Laws to Steal Identities",
      "abstract": "The General Data Protection Regulation (GDPR) has become a touchstone model for modern privacy law, in part because it empowers consumers with unprecedented control over the use of their personal information. However, this same power may be susceptible to abuse by malicious attackers. In this paper, we consider how legal ambiguity surrounding the \"Right of Access\" process may be abused by social engineers. This hypothesis is tested through an adversarial case study of more than 150 businesses. We find that many organizations fail to employ adequate safeguards against Right of Access abuse and thus risk exposing sensitive information to unauthorized third parties. This information varied in sensitivity from simple public records to Social Security Numbers and account passwords. These findings suggest a critical need to improve the implementation of the subject access request process. To this end, we propose possible remediations which may be appropriate for further consideration by government, industry and individuals.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "James Pavur",
        "Casey Knerr"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/eb73ae451c6a0fa5d32b94c0fdbcae9c5f74b308",
      "pdf_url": "",
      "publication_date": "2019-12-02",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "title": "Model Extraction Attacks on Split Federated Learning",
      "abstract": "Federated Learning (FL) is a popular collaborative learning scheme involving multiple clients and a server. FL focuses on protecting clients' data but turns out to be highly vulnerable to Intellectual Property (IP) threats. Since FL periodically collects and distributes the model parameters, a free-rider can download the latest model and thus steal model IP. Split Federated Learning (SFL), a recent variant of FL that supports training with resource-constrained clients, splits the model into two, giving one part of the model to clients (client-side model), and the remaining part to the server (server-side model). Thus SFL prevents model leakage by design. Moreover, by blocking prediction queries, it can be made resistant to advanced IP threats such as traditional Model Extraction (ME) attacks. While SFL is better than FL in terms of providing IP protection, it is still vulnerable. In this paper, we expose the vulnerability of SFL and show how malicious clients can launch ME attacks by querying the gradient information from the server side. We propose five variants of ME attack which differs in the gradient usage as well as in the data assumptions. We show that under practical cases, the proposed ME attacks work exceptionally well for SFL. For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90% accuracy with less than 2% accuracy degradation with VGG-11 on CIFAR-10.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jingtao Li",
        "A. S. Rakin",
        "Xing Chen",
        "Li Yang",
        "Zhezhi He",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "pdf_url": "http://arxiv.org/pdf/2303.08581",
      "publication_date": "2023-03-13",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction",
      "abstract": "We investigate whether model extraction can be used to \u2018steal\u2019 the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.",
      "year": 2021,
      "venue": "ACM Conference on Recommender Systems",
      "authors": [
        "Zhenrui Yue",
        "Zhankui He",
        "Huimin Zeng",
        "Julian McAuley"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460231.3474275",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 177,
      "url": "https://www.semanticscholar.org/paper/35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "title": "MEGEX: Data-Free Model Extraction Attack Against Gradient-Based Explainable AI",
      "abstract": "Explainable AI encourages machine learning applications in the real world, whereas data-free model extraction attacks (DFME), in which an adversary steals a trained machine learning model by creating input queries with generative models instead of collecting training data, have attracted attention as a serious threat. In this paper, we propose MEGEX, a data-free model extraction attack against explainable AI that provides gradient-based explanations for inference results, and investigate whether the gradient-based explanations increase the vulnerability to the data-free model extraction attacks. In MEGEX, an adversary leverages explanations by Vanilla Gradient as derivative values for training a generative model. We prove that MEGEX is identical to white-box data-free knowledge distillation, whereby the adversary can train the generative model with the exact gradients. Our experiments show that the adversary in MEGEX can steal highly accurate models - 0.98\u00d7, 0.91\u00d7, and 0.96\u00d7 the victim model accuracy on SVHN, Fashion-MNIST, and CIFAR-10 datasets given 1.5M, 5M, 20M queries, respectively. In addition, we also apply sophisticated gradient-based explanations, i.e., SmoothGrad and Integrated Gradients, to MEGEX. The experimental results indicate that these explanations are potential countermeasures to MEGEX. We also found that the accuracy of the model stolen by the adversary depends on the diversity of query inputs by the generative model.",
      "year": 2021,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "T. Miura",
        "Toshiki Shibahara",
        "Naoto Yanai"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "pdf_url": "https://arxiv.org/pdf/2107.08909",
      "publication_date": "2021-07-19",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33158daa5df8197872a06e415f2f277b026d9988",
      "title": "High-Fidelity Model Extraction Attacks via Remote Power Monitors",
      "abstract": "This paper shows the first side-channel attack on neural network (NN) IPs through a remote power monitor. We demonstrate that a remote monitor implemented with time-to-digital converters can be exploited to steal the weights from a hardware implementation of NN inference. Such an attack alleviates the need to have physical access to the target device and thus expands the attack vector to multi-tenant cloud FPGA platforms. Our results quantify the effectiveness of the attack on an FPGA implementation of NN inference and compare it to an attack with physical access. We demonstrate that it is indeed possible to extract the weights using DPA with 25000 traces if the SNR is sufficient. The paper, therefore, motivates secure virtualization-to protect the confidentiality of high-valued NN model IPs in multi-tenant execution environments, platform developers need to employ strong countermeasures against physical side-channel attacks.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Anuj Dubey",
        "Emre Karabulut",
        "Amro Awad",
        "Aydin Aysu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33158daa5df8197872a06e415f2f277b026d9988",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "steal model",
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0d8c912b20d2d081672f07b6926b620950a39c8",
      "title": "Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel",
      "abstract": "Machine learning has been attracting strong interests in recent years. Numerous companies have invested great efforts and resources to develop customized deep-learning models, which are their key intellectual properties. In this work, we investigate to what extent the secret of deep-learning models can be inferred by attackers. In particular, we focus on the scenario that a model developer and an adversary share the same GPU when training a Deep Neural Network (DNN) model. We exploit the GPU side-channel based on context-switching penalties. This side-channel allows us to extract the fine-grained structural secret of a DNN model, including its layer composition and hyper-parameters. Leveraging this side-channel, we developed an attack prototype named MosConS, which applies LSTM-based inference models to identify the structural secret. Our evaluation of MosConS shows the structural information can be accurately recovered. Therefore, we believe new defense mechanisms should be developed to protect training against the GPU side-channel.",
      "year": 2020,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Junyin Wei",
        "Yicheng Zhang",
        "Zhe Zhou",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/d0d8c912b20d2d081672f07b6926b620950a39c8",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a60ad0bb136c19bedb0488fc6887dd582834885",
      "title": "Extracting spatial effects from machine learning model using local interpretation method: An example of SHAP and XGBoost",
      "abstract": null,
      "year": 2022,
      "venue": "Computers, Environment and Urban Systems",
      "authors": [
        "Ziqi Li"
      ],
      "citation_count": 714,
      "url": "https://www.semanticscholar.org/paper/3a60ad0bb136c19bedb0488fc6887dd582834885",
      "pdf_url": "https://doi.org/10.1016/j.compenvurbsys.2022.101845",
      "publication_date": "2022-09-01",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
      "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",
      "year": 2015,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Y. Gal",
        "Zoubin Ghahramani"
      ],
      "citation_count": 10621,
      "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
      "pdf_url": "",
      "publication_date": "2015-06-06",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "title": "Model Extraction Defense using Modified Variational Autoencoder",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Yash Gupta"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b3f8a8d2accdc5c80a3e901f3f3abd0391c644de",
      "pdf_url": "",
      "publication_date": "2020-06-04",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "title": "Defense against Model Extraction Attack by Bayesian Active Watermarking",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhenyi Wang",
        "Yihan Wu",
        "Heng Huang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9caf69e7bab1932d0b77dd20dc8f47e1b340135a",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "title": "Defense Against Model Extraction Attacks on Recommender Systems",
      "abstract": "The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.",
      "year": 2023,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Sixiao Zhang",
        "Hongzhi Yin",
        "Hongxu Chen",
        "Cheng Long"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635751",
      "publication_date": "2023-10-25",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot"
      ],
      "citation_count": 261,
      "url": "https://www.semanticscholar.org/paper/abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "pdf_url": "",
      "publication_date": "2020-02-27",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "title": "Model Extraction Attack and Defense on Deep Generative Models",
      "abstract": "The security issues of machine learning have aroused much attention and model extraction attack is one of them. The definition of model extraction attack is that an adversary can collect data through query access to a victim model and train a substitute model with it in order to steal the functionality of the target model. At present, most of the related work has focused on the research of model extraction attack against discriminative models while this paper pays attention to deep generative models. First, considering the difference of an adversary` goals, the attacks are taxonomized into two different types: accuracy extraction attack and fidelity extraction attack and the effect is evaluated by 1-NN accuracy. Attacks among three main types of deep generative models and the influence of the number of queries are also researched. Finally, this paper studies different defensive techniques to safeguard the models according to their architectures.",
      "year": 2022,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Sheng Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/2189/1/012024",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "title": "Beowulf: Mitigating Model Extraction Attacks Via Reshaping Decision Regions",
      "abstract": "Machine Learning as a Service (MLaaS) enables resource-constrained users to access well-trained models through a publicly accessible Application Programming Interface (API) on a pay-per-query basis. Nevertheless, model owners may face the potential threats of model extraction attacks where malicious users replicate valuable commercial models based on query results. Existing defenses against model extraction attacks, however, either sacrifice prediction accuracy or fail to thwart more advanced attacks. In this paper, we propose a novel model extraction defense, dubbed Beowulf 1 , which draws inspiration from theoretical findings that models with complex and narrow decision regions are difficult to be reproduced. Rather than arbitrarily altering decision regions, which may jeopardize the predictive capacity of the victim model, we introduce a dummy class, carefully synthesized using both random and adversarial noises. The random noise broadens the coverage of the dummy class, and the adversarial noise impacts decision regions near decision boundaries with normal classes. To further improve the model utility, we propose to employ data augmentation methods to seamlessly integrate the dummy class and the normal classes. Extensive evaluations on CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets demonstrate that Beowulf can significantly reduce the extraction accuracy of 6 state-of-the-art model extraction attacks by as much as 80%. Moreover, we show that Beowulf is also robust to adaptive model extraction attacks.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xueluan Gong",
        "Rubin Wei",
        "Ziyao Wang",
        "Yuchen Sun",
        "Jiawen Peng",
        "Yanjiao Chen",
        "Qian Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670267",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "title": "BODAME: Bilevel Optimization for Defense Against Model Extraction",
      "abstract": "Model extraction attacks have become serious issues for service providers using machine learning. We consider an adversarial setting to prevent model extraction under the assumption that attackers will make their best guess on the service provider's model using query accesses, and propose to build a surrogate model that significantly keeps away the predictions of the attacker's model from those of the true model. We formulate the problem as a non-convex constrained bilevel optimization problem and show that for kernel models, it can be transformed into a non-convex 1-quadratically constrained quadratic program with a polynomial-time algorithm to find the global optimum. Moreover, we give a tractable transformation and an algorithm for more complicated models that are learned by using stochastic gradient descent-based algorithms. Numerical experiments show that the surrogate model performs well compared with existing defense models when the difference between the attacker's and service provider's distributions is large. We also empirically confirm the generalization ability of the surrogate model.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Y. Mori",
        "Atsushi Nitanda",
        "A. Takeda"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "pdf_url": "",
      "publication_date": "2021-03-11",
      "keywords_matched": [
        "model extraction defense",
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "title": "Obfuscation for Deep Neural Networks against Model Extraction: Attack Taxonomy and Defense Optimization",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yulian Sun",
        "Vedant Bonde",
        "Li Duan",
        "Yong Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d240894a8e0b69d756d2e3b9c2ac6436d5cf98e5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "761cb683cf1bf94b878f6d7527600c2c62aee796",
      "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
      "abstract": "While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yi Xie",
        "Jie Zhang",
        "Shiqian Zhao",
        "Tianwei Zhang",
        "Xiaofeng Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/761cb683cf1bf94b878f6d7527600c2c62aee796",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3c26202058ff573620e70e340c9c8cafb2094678",
      "title": "Model Extraction Attacks on DistilBERT",
      "abstract": null,
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Amro Salman",
        "Ayman Saeed",
        "Khalid Elmadani",
        "S. Babiker"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3c26202058ff573620e70e340c9c8cafb2094678",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "title": "Image Translation-Based Deniable Encryption against Model Extraction Attack",
      "abstract": "In cloud storage applications, data owners\u2019 original images are usually encrypted before being outsourced to the cloud for preserving data owners\u2019 privacy. However, in deep learning model-based image encryption methods, an adversary can conduct the model extraction attack to reveal the model parameters and thus restore the privacy information by obtaining numerous encrypted images. In this paper, we propose an image translation-based deniable encryption (ITDE) scheme to achieve encryption deniability and defend against model extraction attacks. Differing from traditional encryption methods in which encrypted images are visually meaningless, ITDE applies image translation to generate encrypted images in the form of human faces. Moreover, ITDE provides deniability for data owners to keep the encryption parameters private. To defend against model extraction attacks, the defense mechanism is introduced in our proposed ITDE to preserve deep learning models. Experimental results demonstrate the superiority of our proposed methods in terms of encryption deniability and privacy preservation.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Yiling Chen",
        "Yuanzhi Yao",
        "Nenghai Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "pdf_url": "",
      "publication_date": "2023-10-08",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
      "abstract": "Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86c0661fdb12bcde0709e91416510fda72f10f13",
      "title": "A Comprehensive Survey of Model Extraction Attacks: Current Trends, Defenses, and Future Directions",
      "abstract": "Model extraction attacks pose a significant threat to Machine Learning (ML) systems, especially in cloud-based services like Machine Learning as a Service (MLaaS). Attacks aim to steal proprietary models by replicating their functionality or extracting their internal parameters. This paper reviews model extraction attack types, examining existing defensive techniques and weaknesses in current defenses. Promising defense mechanisms are discussed, including adaptive privacy budgets, hybrid defense strategies, combining multiple methods, and hardwarebased security solutions. Emerging attack models like collaborative attacks in federated learning environments are explored. Future research focuses on adaptive defenses and Artificial Intelligence (AI)-driven detection methods to improve model robustness and contribute to more resilient machine learning systems.",
      "year": 2025,
      "venue": "2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)",
      "authors": [
        "Quazi Rian Hasnaine",
        "Yaodan Hu",
        "Mohamed I. Ibrahem",
        "Mostafa M. Fouda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/86c0661fdb12bcde0709e91416510fda72f10f13",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "52a222d38a8640499010d470d5589a81882bc425",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input. \nTo extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights. \nWe perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alexey Kurakin",
        "Nicolas Papernot"
      ],
      "citation_count": 424,
      "url": "https://www.semanticscholar.org/paper/52a222d38a8640499010d470d5589a81882bc425",
      "pdf_url": "",
      "publication_date": "2019-09-03",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9fa2a77d1deff6419f797900049396e39ffefcd8",
      "title": "A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks",
      "abstract": "Model Inversion (MI) attacks aim to reconstruct privacy-sensitive training data from released models by utilizing output information, raising extensive concerns about the security of Deep Neural Networks (DNNs). Recent advances in generative adversarial networks (GANs) have contributed significantly to the improved performance of MI attacks due to their powerful ability to generate realistic images with high fidelity and appropriate semantics. However, previous MI attacks have solely disclosed private information in the latent space of GAN priors, limiting their semantic extraction and transferability across multiple target models and datasets. To address this challenge, we propose a novel method, Intermediate Features enhanced Generative Model Inversion (IF-GMI), which disassembles the GAN structure and exploits features between intermediate blocks. This allows us to extend the optimization space from latent code to intermediate features with enhanced expressive capabilities. To prevent GAN priors from generating unrealistic images, we apply a L1 ball constraint to the optimization process. Experiments on multiple benchmarks demonstrate that our method significantly outperforms previous approaches and achieves state-of-the-art results under various settings, especially in the out-of-distribution (OOD) scenario. Our code is available at: https://github.com/final-solution/IF-GMI",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixiang Qiu",
        "Hao Fang",
        "Hongyao Yu",
        "Bin Chen",
        "Meikang Qiu",
        "Shutao Xia"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/9fa2a77d1deff6419f797900049396e39ffefcd8",
      "pdf_url": "",
      "publication_date": "2024-07-18",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da",
      "title": "Deep Private-Feature Extraction",
      "abstract": "We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.",
      "year": 2018,
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": [
        "S. A. Ossia",
        "A. Taheri",
        "A. Shamsabadi",
        "Kleomenis Katevas",
        "H. Haddadi",
        "H. Rabiee"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da",
      "pdf_url": "https://arxiv.org/pdf/1802.03151",
      "publication_date": "2018-02-09",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "359f56a7fee457c2729f0a2d2ce27757c33da6c7",
      "title": "Feature Inference Attack on Model Predictions in Vertical Federated Learning",
      "abstract": "Federated learning (FL) is an emerging paradigm for facilitating multiple organizations\u2019 data collaboration without revealing their private data to each other. Recently, vertical FL, where the participating organizations hold the same set of samples but with disjoint features and only one organization owns the labels, has received increased attention. This paper presents several feature inference attack methods to investigate the potential privacy leakages in the model prediction stage of vertical FL. The attack methods consider the most stringent setting that the adversary controls only the trained vertical FL model and the model predictions, relying on no background information of the attack target\u2019s data distribution. We first propose two specific attacks on the logistic regression (LR) and decision tree (DT) models, according to individual prediction output. We further design a general attack method based on multiple prediction outputs accumulated by the adversary to handle complex models, such as neural networks (NN) and random forest (RF) models. Experimental evaluations demonstrate the effectiveness of the proposed attacks and highlight the need for designing private mechanisms to protect the prediction outputs in vertical FL.",
      "year": 2020,
      "venue": "IEEE International Conference on Data Engineering",
      "authors": [
        "Xinjian Luo",
        "Yuncheng Wu",
        "Xiaokui Xiao",
        "B. Ooi"
      ],
      "citation_count": 267,
      "url": "https://www.semanticscholar.org/paper/359f56a7fee457c2729f0a2d2ce27757c33da6c7",
      "pdf_url": "https://arxiv.org/pdf/2010.10152",
      "publication_date": "2020-10-20",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3f0e5b5c1b0eb19f7d275983ca8449477e0792b2",
      "title": "SHIFT SNARE: Uncovering Secret Keys in FALCON via Single-Trace Analysis",
      "abstract": "This paper presents a novel singletrace sidechannel attack on FALCON a latticebased postquantum digital signature protocol recently approved for standardization by NIST We target the discrete Gaussian sampling operation within FALCONs key generation scheme and demonstrate that a single power trace is sufficient to mount a successful attack Notably negating the results of a 63bit rightshift operation on 64bit secret values leaks critical information about the assignment of 1 versus 0 to intermediate coefficients during sampling These leaks enable full recovery of the secret key We demonstrate a groundup approach to the attack on an ARM CortexM4 microcontroller executing both the reference and optimized implementations from FALCONs NIST round 3 software package We successfully recovered all of the secret polynomials in FALCON We further quantify the attackers success rate using a univariate Gaussian template model providing generalizable guarantees Statistical analysis with over 500000 tests reveals a percoefficient success rate of 999999999478 and a fullkey recovery rate of 9999994654 for FALCON512 We verify that this vulnerability is present in all implementations included in FALCONs NIST submission package This highlights the vulnerability of current software implementations to singletrace attacks and underscores the urgent need for singletrace resilient software in embedded systems",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Jinyi Qiu",
        "Aydin Aysu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3f0e5b5c1b0eb19f7d275983ca8449477e0792b2",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ffca61ff581efb26d52be9189ba028ccb10eccd1",
      "title": "Auditing Data Provenance in Text-Generation Models",
      "abstract": "To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we develop a new model auditing technique that helps users check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of popular online services and are often trained on personal data such as users' messages, searches, chats, and comments. We design and evaluate a black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). We empirically show that our method can successfully audit well-generalized models that are not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.",
      "year": 2018,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Congzheng Song",
        "Vitaly Shmatikov"
      ],
      "citation_count": 280,
      "url": "https://www.semanticscholar.org/paper/ffca61ff581efb26d52be9189ba028ccb10eccd1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3292500.3330885",
      "publication_date": "2018-11-01",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "64c354f76a2dd1825a2cf431cb2a1623021d874e",
      "title": "Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack",
      "abstract": "Recent advancements in pre-trained large language models (LLMs) have significantly influenced various domains. Adapting these models for specific tasks often involves fine-tuning (FT) with private, domain-specific data. However, privacy concerns keep this data undisclosed, and the computational demands for deploying LLMs pose challenges for resource-limited data holders. This has sparked interest in split learning (SL), a Model-as-a-Service (MaaS) paradigm that divides LLMs into smaller segments for distributed training and deployment, transmitting only intermediate activations instead of raw data. SL has garnered substantial interest in both industry and academia as it aims to balance user data privacy, model ownership, and resource challenges in the private fine-tuning of LLMs. Despite its privacy claims, this paper reveals significant vulnerabilities arising from the combination of SL and LLM-FT: the Not-too-far property of fine-tuning and the auto-regressive nature of LLMs. Exploiting these vulnerabilities, we propose Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL. BiSR utilizes pre-trained weights as prior knowledge, combining a learning-based attack with a bidirectional optimization-based approach for highly effective data reconstruction. Additionally, it incorporates a Noise-adaptive Mixture of Experts (NaMoE) model to enhance reconstruction performance under perturbation. We conducted systematic experiments on various mainstream LLMs and different setups, empirically demonstrating BiSR's state-of-the-art performance. Furthermore, we thoroughly examined three representative defense mechanisms, showcasing our method's capability to reconstruct private data even in the presence of these defenses.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Guanzhong Chen",
        "Zhenghan Qin",
        "Mingxin Yang",
        "Yajie Zhou",
        "Tao Fan",
        "Tianyu Du",
        "Zenglin Xu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/64c354f76a2dd1825a2cf431cb2a1623021d874e",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690295",
      "publication_date": "2024-09-02",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90272fee166238841cec8f02a1f38815afb2e3ba",
      "title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning",
      "abstract": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising paradigm for privacy-preserving and efficient adaptation of Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It preserves data privacy by keeping the data decentralized and training the model on local devices, ensuring that raw data never leaves the user's device. Moreover, the integration of PEFT methods such as LoRA significantly reduces the number of trainable parameters compared to fine-tuning the entire model, thereby minimizing communication costs and computational overhead. Despite its potential, the security implications of FedPEFT remain underexplored. This paper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack (PaaA), which exposes how PEFT can be exploited as an attack vector to circumvent PLMs' safety alignment and generate harmful content in response to malicious prompts. Our evaluation of PaaA reveals that with less than 1% of the model's parameters set as trainable, and a small subset of clients acting maliciously, the attack achieves an approximate 80% attack success rate using representative PEFT methods such as LoRA. To mitigate this threat, we further investigate potential defense strategies, including Robust Aggregation Schemes (RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis highlights the limitations of these defenses, i.e., even the most advanced RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in scenarios with highly heterogeneous data distributions. Similarly, while PPSA can reduce attack success rates to below 10%, it severely degrades the model's accuracy on the target task. Our results underscore the urgent need for more effective defense mechanisms that simultaneously ensure security and maintain the performance of the FedPEFT paradigm.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shenghui Li",
        "Edith C. H. Ngai",
        "Fanghua Ye",
        "Thiemo Voigt"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/90272fee166238841cec8f02a1f38815afb2e3ba",
      "pdf_url": "",
      "publication_date": "2024-11-28",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "425737a02a33799dc8e5f86c2f8eab9e777812db",
      "title": "Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk",
      "abstract": "While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by 5.4% (absolute difference) AUC and can increase extracted private samples from almost 0 samples to 16.3 samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at https://github.com/VITA-Group/Shake-to-Leak.",
      "year": 2024,
      "venue": "2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Zhangheng Li",
        "Junyuan Hong",
        "Bo Li",
        "Zhangyang Wang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/425737a02a33799dc8e5f86c2f8eab9e777812db",
      "pdf_url": "",
      "publication_date": "2024-03-14",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e39e0d195bd7de7ed9c5f0f5e0911d7bac12889",
      "title": "Invernet: An Inversion Attack Framework to Infer Fine-Tuning Datasets through Word Embeddings",
      "abstract": "Word embedding aims to learn the dense representation of words and has become a regular input preparation in many NLP tasks. Due to the data and computation intensive nature of learning embeddings from scratch, a more af-fordable way is to borrow the pretrained embedding available in public and fine-tune the embedding through a domain specific downstream dataset. A privacy concern can arise if a malicious owner of the pretrained embedding gets access to the fine-tuned embedding and tries to infer the critical information from the downstream datasets. In this study, we pro-pose a novel embedding inversion framework called Invernet that materializes the privacy concern by inferring the context distribution in the downstream dataset, which can lead to key information breach. With extensive experimental studies on two real-world news datasets: Antonio Gulli\u2019s News and New York Times, we validate the feasibility of proposed privacy attack and demonstrate the effectiveness of Invernet on inferring downstream datasets based on multiple word embedding methods.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "I. Hayet",
        "Zijun Yao",
        "Bo Luo"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/9e39e0d195bd7de7ed9c5f0f5e0911d7bac12889",
      "pdf_url": "https://aclanthology.org/2022.findings-emnlp.368.pdf",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack",
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33f6b9aa455a8563252b242cb575705e782958f0",
      "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
      "abstract": "The rapid advancements of large language models (LLMs) have raised public concerns about the privacy leakage of personally identifiable information (PII) within their extensive training datasets. Recent studies have demonstrated that an adversary could extract highly sensitive privacy data from the training data of LLMs with carefully designed prompts. However, these attacks suffer from the model's tendency to hallucinate and catastrophic forgetting (CF) in the pre-training stage, rendering the veracity of divulged PIIs negligible. In our research, we propose a novel attack, Janus, which exploits the fine-tuning interface to recover forgotten PIIs from the pre-training data in LLMs. We formalize the privacy leakage problem in LLMs and explain why forgotten PIIs can be recovered through empirical analysis on open-source language models. Based upon these insights, we evaluate the performance of Janus on both open-source language models and two latest LLMs, i.e., GPT-3.5-Turbo and LLaMA-2-7b. Our experiment results show that Janus amplifies the privacy risks by over 10 times in comparison with the baseline and significantly outperforms the state-of-the-art privacy extraction attacks including prefix attacks and in-context learning (ICL). Furthermore, our analysis validates that existing fine-tuning APIs provided by OpenAI and Azure AI Studio are susceptible to our Janus attack, allowing an adversary to conduct such an attack at a low cost.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xiaoyi Chen",
        "Siyuan Tang",
        "Rui Zhu",
        "Shijun Yan",
        "Lei Jin",
        "Zihao Wang",
        "Liya Su",
        "Zhikun Zhang",
        "Xiaofeng Wang",
        "Haixu Tang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/33f6b9aa455a8563252b242cb575705e782958f0",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690325",
      "publication_date": "2023-10-24",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86f95df970183931add1216a32db8c3f21c5d488",
      "title": "Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?",
      "abstract": "Machine learning practitioners often fine-tune generative pre-trained models like GPT-3 to improve model performance at specific tasks. Previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. Companies such as OpenAI offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. In this work, we simulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our objective is to determine if personally identifiable information (PII) can be extracted from this model. We (1) explore the use of naive prompting methods on a GPT-3 fine-tuned classification model, and (2) we design a practical word generation task called Autocomplete to investigate the extent of PII memorization in fine-tuned GPT-3 within a real-world context. Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset. To encourage further research, we have made our codes and datasets publicly available on GitHub at: https://github.com/albertsun1/gpt3-pii-attacks",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "A. Sun",
        "Eliott Zemour",
        "Arushi Saxena",
        "Udith Vaidyanathan",
        "Eric Lin",
        "Christian Lau",
        "Vaikkunth Mugunthan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/86f95df970183931add1216a32db8c3f21c5d488",
      "pdf_url": "https://arxiv.org/pdf/2307.16382",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b6f0aae08c9dc80dff1a117fd180b758e44bdd5",
      "title": "Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data",
      "abstract": "This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\\%), GPT-2 (+9.8\\%), and BERT (+7.8\\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Reem Al-Saidi",
        "Erman Ayday",
        "Ziad Kobti"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2b6f0aae08c9dc80dff1a117fd180b758e44bdd5",
      "pdf_url": "",
      "publication_date": "2025-11-09",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d7378cb4f8a430734b3094a7b3807050ca429c3",
      "title": "Comprehensive Vulnerability Evaluation of Face Recognition Systems to Template Inversion Attacks via 3D Face Reconstruction",
      "abstract": "In this article, we comprehensively evaluate the vulnerability of state-of-the-art face recognition systems to template inversion attacks using 3D face reconstruction. We propose a new method (called GaFaR) to reconstruct 3D faces from facial templates using a pretrained geometry-aware face generation network, and train a mapping from facial templates to the intermediate latent space of the face generator network. We train our mapping with a semi-supervised approach using real and synthetic face images. For real face images, we use a generative adversarial network (GAN)-based framework to learn the distribution of generator intermediate latent space. For synthetic face images, we directly learn the mapping from facial templates to the generator intermediate latent code. Furthermore, to improve the success attack rate, we use two optimization methods on the camera parameters of the GNeRF model. We propose our method in the whitebox and blackbox attacks against face recognition systems and compare the transferability of our attack with state-of-the-art methods across other face recognition systems on the MOBIO and LFW datasets. We also perform practical presentation attacks on face recognition systems using the digital screen replay and printed photographs, and evaluate the vulnerability of face recognition systems to different template inversion attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Hatef Otroshi Shahreza",
        "S\u00e9bastien Marcel"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/2d7378cb4f8a430734b3094a7b3807050ca429c3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10239446.pdf",
      "publication_date": "2023-09-05",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e79e2e2bf73c3be35a7899b34490e29faafaf42e",
      "title": "Fairness and Robustness in Machine Unlearning",
      "abstract": "Machine unlearning poses the challenge of ''how to eliminate the influence of specific data from a pretrained model'' in regard to privacy concerns. While prior research on approximated unlearning has demonstrated accuracy and efficiency in time complexity, we claim that it falls short of achieving exact unlearning, and we are the first to focus on fairness and robustness in machine unlearning algorithms. Our study presents fairness Conjectures for a well-trained model, based on the variance-bias trade-off characteristic, and considers their relevance to robustness. Our Conjectures are supported by experiments conducted on the two most widely used model architectures-ResNet and ViT-demonstrating the correlation between fairness and robustness: the higher fairness-gap is, the more the model is sensitive and vulnerable. In addition, our experiments demonstrate the vulnerability of current state-of-the-art approximated unlearning algorithms to adversarial attacks, where their unlearned models suffer a significant drop in accuracy compared to the exact-unlearned models. We claim that our fairness-gap measurement and robustness metric should be used to evaluate the unlearning algorithm. Furthermore, we demonstrate that unlearning in the intermediate and last layers is sufficient and cost-effective for time and memory complexity.",
      "year": 2025,
      "venue": "The Web Conference",
      "authors": [
        "Khoa Tran",
        "Simon S. Woo"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e79e2e2bf73c3be35a7899b34490e29faafaf42e",
      "pdf_url": "",
      "publication_date": "2025-04-18",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "58eb2d747e102932989f2ad1fedda30854e24102",
      "title": "GradViT: Gradient Inversion of Vision Transformers",
      "abstract": "In this work we demonstrate the vulnerability of vision transformers (ViTs) to gradient-based inversion attacks. During this attack, the original data batch is reconstructed given model weights and the corresponding gradients. We introduce a method, named GradViT, that optimizes random noise into naturally looking images via an iterative process. The optimization objective consists of (i) a loss on matching the gradients, (ii) image prior in the form of distance to batch-normalization statistics of a pretrained CNN model, and (iii) a total variation regularization on patches to guide correct recovery locations. We propose a unique loss scheduling function to overcome local minima during optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and observe unprecedentedly high fidelity and closeness to the original (hidden) data. During the analysis we find that vision transformers are significantly more vulnerable than previously studied CNNs due to the presence of the attention mechanism. Our method demonstrates new state-of-the-art results for gradient inversion in both qualitative and quantitative metrics. Project page at https://gradvit.github.io/.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Ali Hatamizadeh",
        "Hongxu Yin",
        "H. Roth",
        "Wenqi Li",
        "Jan Kautz",
        "Daguang Xu",
        "Pavlo Molchanov"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/58eb2d747e102932989f2ad1fedda30854e24102",
      "pdf_url": "https://arxiv.org/pdf/2203.11894",
      "publication_date": "2022-03-22",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "947cf0610c58fb87e9b4d09d3e4e99311d3a7a05",
      "title": "On the Efficacy of Differentially Private Few-shot Image Classification",
      "abstract": "There has been significant recent progress in training differentially private (DP) models which achieve accuracy that approaches the best non-private models. These DP models are typically pretrained on large public datasets and then fine-tuned on private downstream datasets that are relatively large and similar in distribution to the pretraining data. However, in many applications including personalization and federated learning, it is crucial to perform well (i) in the few-shot setting, as obtaining large amounts of labeled data may be problematic; and (ii) on datasets from a wide variety of domains for use in various specialist settings. To understand under which conditions few-shot DP can be effective, we perform an exhaustive set of experiments that reveals how the accuracy and vulnerability to attack of few-shot DP image classification models are affected as the number of shots per class, privacy level, model architecture, downstream dataset, and subset of learnable parameters in the model vary. We show that to achieve DP accuracy on par with non-private models, the shots per class must be increased as the privacy level increases. We also show that learning parameter-efficient FiLM adapters under DP is competitive with learning just the final classifier layer or learning all of the network parameters. Finally, we evaluate DP federated learning systems and establish state-of-the-art performance on the challenging FLAIR benchmark.",
      "year": 2023,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Marlon Tobaben",
        "Aliaksandra Shysheya",
        "J. Bronskill",
        "Andrew J. Paverd",
        "Shruti Tople",
        "Santiago Zanella-B\u00e9guelin",
        "Richard E. Turner",
        "Antti Honkela"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/947cf0610c58fb87e9b4d09d3e4e99311d3a7a05",
      "pdf_url": "http://arxiv.org/pdf/2302.01190",
      "publication_date": "2023-02-02",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "585af96f984117df5fd0cdca75a4701673965fac",
      "title": "ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning",
      "abstract": "This work aims to tackle Model Inversion (MI) attack on Split Federated Learning (SFL). SFL is a recent distributed training scheme where multiple clients send intermediate activations (i. e., feature map), instead of raw data, to a central server. While such a scheme helps reduce the computational load at the client end, it opens itself to reconstruction of raw data from intermediate activation by the server. Existing works on protecting SFL only consider inference and do not handle attacks during training. So we propose ResSFL, a Split Federated Learning Framework that is designed to be MI-resistant during training. It is based on deriving a resistant feature extractor via attacker-aware training, and using this extractor to initialize the client-side model prior to standard SFL training. Such a method helps in reducing the computational complexity due to use of strong inversion model in client-side adversarial training as well as vulnerability of attacks launched in early training epochs. On CIFAR-100 dataset, our proposed framework successfully mitigates MI attack on a VGG-11 model with a high reconstruction Mean-Square-Error of 0.050 compared to 0.005 obtained by the baseline system. The frame-work achieves 67.5% accuracy (only 1 % accuracy drop) with very low computation overhead. Code is released at: https://github.com/zlijingtao/ResSFL.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Jingtao Li",
        "A. S. Rakin",
        "Xing Chen",
        "Zhezhi He",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 81,
      "url": "https://www.semanticscholar.org/paper/585af96f984117df5fd0cdca75a4701673965fac",
      "pdf_url": "https://arxiv.org/pdf/2205.04007",
      "publication_date": "2022-05-09",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f803600afb4a371b6f9c1018ef0f603de9979535",
      "title": "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries",
      "abstract": "This study investigates the privacy risks associated with text embeddings, focusing on the scenario where attackers cannot access the original embedding model. Contrary to previous research requiring direct model access, we explore a more realistic threat model by developing a transfer attack method. This approach uses a surrogate model to mimic the victim model's behavior, allowing the attacker to infer sensitive information from text embeddings without direct access. Our experiments across various embedding models and a clinical dataset demonstrate that our transfer attack significantly outperforms traditional methods, revealing the potential privacy vulnerabilities in embedding technologies and emphasizing the need for enhanced security measures.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yu-Hsiang Huang",
        "Yu-Che Tsai",
        "Hsiang Hsiao",
        "Hong-Yi Lin",
        "Shou-De Lin"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/f803600afb4a371b6f9c1018ef0f603de9979535",
      "pdf_url": "https://arxiv.org/pdf/2406.10280",
      "publication_date": "2024-06-12",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c6e7bacdabf6ea914d5da699d3fbea24624deadc",
      "title": "Label-Only Model Inversion Attacks via Knowledge Transfer",
      "abstract": "In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information. In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Ngoc-Bao Nguyen",
        "Keshigeyan Chandrasegaran",
        "Milad Abdollahzadeh",
        "Ngai-Man Cheung"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/c6e7bacdabf6ea914d5da699d3fbea24624deadc",
      "pdf_url": "",
      "publication_date": "2023-10-30",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35efa06e8c55a209677bcb48a6790b654d8b322f",
      "title": "Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches",
      "abstract": "Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Zhiyuan Cheng",
        "J. Liang",
        "Hongjun Choi",
        "Guanhong Tao",
        "Zhiwen Cao",
        "Dongfang Liu",
        "Xiangyu Zhang"
      ],
      "citation_count": 123,
      "url": "https://www.semanticscholar.org/paper/35efa06e8c55a209677bcb48a6790b654d8b322f",
      "pdf_url": "http://arxiv.org/pdf/2207.04718",
      "publication_date": "2022-07-11",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f9079a7408d08ae6cd0dd74ebfaa728216b3626",
      "title": "Towards Sentence Level Inference Attack Against Pre-trained Language Models",
      "abstract": "In recent years, pre-trained language models (e.g., BERT and GPT) have shown the superior capability of textual representation learning, benefiting from their large architectures and massive training corpora. The industry has also quickly embraced language models to develop various downstream NLP applications. For example, Google has already used BERT to improve its search system. The utility of the language embeddings also brings about potential privacy risks. Prior works have revealed that an adversary can either identify whether a keyword exists or gather a set of possible candidates for each word in a sentence embedding. However, these attacks cannot recover coherent sentences which leak high-level semantic information from the original text. To demonstrate that the adversary can go beyond the word-level attack, we present a novel decoder-based attack, which can reconstruct meaningful text from private embeddings after being pre-trained on a public dataset of the same domain. This attack is more challenging than a word-level attack due to the complexity of sentence structures. We comprehensively evaluate our attack in two domains and with different settings to show its superiority over the baseline attacks. Quantitative experimental results show that our attack can identify up to 3.5X of the number of keywords identified by the baseline attacks. Although our method reconstructs high-quality sentences in many cases, it often produces lower-quality sentences as well. We discuss these cases and the limitations of our method in detail",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Kang Gu",
        "Ehsanul Kabir",
        "Neha Ramsurrun",
        "Soroush Vosoughi",
        "Shagufta Mehnaz"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/1f9079a7408d08ae6cd0dd74ebfaa728216b3626",
      "pdf_url": "https://petsymposium.org/popets/2023/popets-2023-0070.pdf",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bc0f56a2bc17461f9f761f95dacf6e89890ec21f",
      "title": "Clinical Text Anonymization, its Influence on Downstream NLP Tasks and the Risk of Re-Identification",
      "abstract": "While text-based medical applications have become increasingly prominent, access to clinicaldata remains a major concern. To resolve this issue, further de-identification and anonymization of the data are required. This might, however, alter the contextual information within the clinical texts and therefore influence the learning and performance of possible language models. This paper systematically analyses the potential effects of various anonymization techniques on the performance of state-of-the-art machine learning models based on several datasets corresponding to five different NLP tasks. On this basis, we derive insightful findings and recommendations concerning text anonymization with regard to the performance of machine learning models. In addition, we present a simple re-identification attack applied to the anonymized text data, which can break the anonymization.",
      "year": 2023,
      "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
      "authors": [
        "Iyadh Ben Cheikh Larbi",
        "A. Burchardt",
        "R. Roller"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/bc0f56a2bc17461f9f761f95dacf6e89890ec21f",
      "pdf_url": "https://aclanthology.org/2023.eacl-srw.11.pdf",
      "publication_date": null,
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d6255d2173b6709dd8b13e096097dfd02cf83d9",
      "title": "Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack",
      "abstract": "Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Liwen Wang",
        "Yuanmeng Yan",
        "Keqing He",
        "Yanan Wu",
        "Weiran Xu"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/7d6255d2173b6709dd8b13e096097dfd02cf83d9",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.293.pdf",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2156f3f68804ec15f59543ce7fa0c93201765b81",
      "title": "Diffusion-aided Task-oriented Semantic Communications with Model Inversion Attack",
      "abstract": "Semantic communication enhances transmission efficiency by conveying semantic information rather than raw input symbol sequences. Task-oriented semantic communication is a variant that tries to retains only task-specific information, thus achieving greater bandwidth savings. However, these neural-based communication systems are vulnerable to model inversion attacks, where adversaries try to infer sensitive input information from eavesdropped transmitted data. The key challenge, therefore, lies in preserving privacy while ensuring transmission correctness and robustness. While prior studies typically assume that adversaries aim to fully reconstruct the raw input in task-oriented settings, there exist scenarios where pixel-level metrics such as PSNR or SSIM are low, yet the adversary's outputs still suffice to accomplish the downstream task, indicating leakage of sensitive information. We therefore adopt the attacker's task accuracy as a more appropriate metric for evaluating attack effectiveness. To optimize the gap between the legitimate receiver's accuracy and the adversary's accuracy, we propose DiffSem, a diffusion-aided framework for task-oriented semantic communication. DiffSem integrates a transmitter-side self-noising mechanism that adaptively regulates semantic content while compensating for channel noise, and a receiver-side diffusion U-Net that enhances task performance and can be optionally strengthened by self-referential label embeddings. Our experiments demonstrate that DiffSem enables the legitimate receiver to achieve higher accuracy, thereby validating the superior performance of the proposed framework.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Xuesong Wang",
        "Mo Li",
        "Xingyan Shi",
        "Zhaoqian Liu",
        "Shenghao Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2156f3f68804ec15f59543ce7fa0c93201765b81",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6a3ee4308471d0bf3e6f06b99037fc7f85933695",
      "title": "FMDL: Federated Mutual Distillation Learning for Defending Backdoor Attacks",
      "abstract": "Federated learning is a distributed machine learning algorithm that enables collaborative training among multiple clients without sharing sensitive information. Unlike centralized learning, it emphasizes the distinctive benefits of safeguarding data privacy. However, two challenging issues, namely heterogeneity and backdoor attacks, pose severe challenges to standardizing federated learning algorithms. Data heterogeneity affects model accuracy, target heterogeneity fragments model applicability, and model heterogeneity compromises model individuality. Backdoor attacks inject trigger patterns into data to deceive the model during training, thereby undermining the performance of federated learning. In this work, we propose an advanced federated learning paradigm called Federated Mutual Distillation Learning (FMDL). FMDL allows clients to collaboratively train a global model while independently training their private models, subject to server requirements. Continuous bidirectional knowledge transfer is performed between local models and private models to achieve model personalization. FMDL utilizes the technique of attention distillation, conducting mutual distillation during the local update phase and fine-tuning on clean data subsets to effectively erase the backdoor triggers. Our experiments demonstrate that FMDL benefits clients from different data, tasks, and models, effectively defends against six types of backdoor attacks, and validates the effectiveness and efficiency of our proposed approach.",
      "year": 2023,
      "venue": "Electronics",
      "authors": [
        "Hanqi Sun",
        "Wanquan Zhu",
        "Ziyu Sun",
        "Mingsheng Cao",
        "Wenbin Liu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6a3ee4308471d0bf3e6f06b99037fc7f85933695",
      "pdf_url": "https://www.mdpi.com/2079-9292/12/23/4838/pdf?version=1701332113",
      "publication_date": "2023-11-30",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3c9dd60925589ec0f61c80e1b76af88716af8308",
      "title": "A Data Reconstruction Attack Against Vertical Federated Learning Based on Knowledge Transfer",
      "abstract": "Vertical federated learning allows multiple parties, each possessing data on different features of the same samples, to collaboratively train a machine learning model while keeping their data private. Naive vertical federated learning systems, however, face severe limitations degrading their usability; for example, they require all parties to be involved even for basic inference tasks. A state-of-the-art approach overcomes the limitations by incorporating knowledge distillation techniques based on federated singular value decomposition. This allows parties to collaboratively derive latent representations of their data without exposing the actual data to each other. Each party can then independently train a model and perform inference on that model, enhancing the practicality of vertical federated learning. Although the original data of each party is not exposed to other parties during these procedures, we reveal that a semi-honest adversary can potentially reconstruct the original data from its latent representation. The proposed attack exploits the linear relationship between the original data and the corresponding latent representation and deduces the relationship using a metaheuristic approach based on a GAN-like neural network. The effectiveness of the attack is validated using several well-known real-world datasets.",
      "year": 2024,
      "venue": "Conference on Computer Communications Workshops",
      "authors": [
        "Takumi Suimon",
        "Y. Koizumi",
        "Junji Takemasa",
        "Toru Hasegawa"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3c9dd60925589ec0f61c80e1b76af88716af8308",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e65583cd5172dbef3b114e690a5c01e7438082be",
      "title": "Federated Zero-Shot Learning with Mid-Level Semantic Knowledge Transfer",
      "abstract": "Conventional centralised deep learning paradigms are not feasible when data from different sources cannot be shared due to data privacy or transmission limitation. To resolve this problem, federated learning has been introduced to transfer knowledge across multiple sources (clients) with non-shared data while optimising a globally generalised central model (server). Existing federated learning paradigms mostly focus on transferring holistic high-level knowledge (such as class) across models, which are closely related to speci\ufb01c objects of interest so may suffer from inverse attack. In contrast, in this work, we consider transferring mid-level semantic knowledge (such as attribute) which is not sensitive to speci\ufb01c objects of interest and therefore is more privacy-preserving and scalable. To this end, we formulate a new Federated Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at multiple local clients with non-shared local data and cumulatively aggregate a globally generalised central model for deployment. To improve model discriminative ability, we propose to explore semantic knowledge augmentation from external knowledge for enriching the mid-level semantic space in FZSL. Extensive experiments on \ufb01ve zero-shot learning benchmark datasets validate the effectiveness of our approach for optimising a generalisable federated learning model with mid-level semantic knowledge transfer.",
      "year": 2024,
      "venue": "Pattern Recognition",
      "authors": [
        "Shitong Sun",
        "Chenyang Si",
        "Shaogang Gong",
        "Guile Wu"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/e65583cd5172dbef3b114e690a5c01e7438082be",
      "pdf_url": "http://arxiv.org/pdf/2208.13465",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ddce3aeabf807f74355b98374a1aed6f008538bd",
      "title": "A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack Against Split Learning",
      "abstract": "Split Learning (SL) is a distributed learning framework renowned for its privacy-preserving features and minimal computational requirements. Previous research consistently highlights the potential privacy breaches in SL systems by server adversaries reconstructing training data. However, these studies often rely on strong assumptions or compromise system utility to enhance attack performance. This paper introduces a new semi-honest Data Reconstruction Attack on SL, named Feature-Oriented Reconstruction Attack (FORA). In contrast to prior works, FORA relies on limited prior knowledge, specifically that the server utilizes auxiliary samples from the public without knowing any client's private information. This allows FORA to conduct the attack stealthily and achieve robust performance. The key vulnerability exploited by FORA is the revelation of the model representation preference in the smashed data output by victim client. FORA constructs a substitute client through feature-level transfer learning, aiming to closely mimic the victim client's representation preference. Leveraging this substitute client, the server trains the attack model to effectively reconstruct private data. Extensive experiments showcase FORA's superior performance compared to state-of-the-art methods. Furthermore, the paper systematically evaluates the proposed method's applicability across diverse settings and advanced defense strategies.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiaoyang Xu",
        "Mengda Yang",
        "Wenzhe Yi",
        "Ziang Li",
        "Juan Wang",
        "Hongxin Hu",
        "Yong Zhuang",
        "Yaxin Liu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/ddce3aeabf807f74355b98374a1aed6f008538bd",
      "pdf_url": "http://arxiv.org/pdf/2405.04115",
      "publication_date": "2024-05-07",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "79b65cce8d475c2102bd522153d7cffb95e3ae96",
      "title": "Privacy Leakage from Logits Attack and its Defense in Federated Distillation",
      "abstract": "Federated Distillation (FD), a popular variant of Federated Learning (FL), has attracted researchers' attention due to its ability to support heterogeneous model training. Generally, FD allows clients to upload logits associated with public datasets for knowledge transfer, yet logits may pose privacy risks. In this study, we provide the first demonstration of the impact of privacy risks caused by logits. Specifically, we design a data reconstruction attack against logits named L-Attack which can reveal sensitive information about the target client without access to the target model. Via the zeroth-order optimization technique, L-Attack involves training a server-side generator that unveils certain features of private data owned by the target client. To defend against L-Attack, we propose a label aggregation-based FD algorithm called LabelAvg which allows clients to upload predicted hard labels for knowledge transfer instead of logits. Due to the insufficient information in labels for distillation, LabelAvg provides a voting-based label smoothing mechanism that enables the server to construct smooth labels from received labels. The generated smooth labels which stand for the consensus among all clients, indicate the approximate probability distribution. Thus, these smoothed labels bear a striking similarity to logits and can be used for distillation. Analysis and experimental results prove LabelAvg is superior to baselines in terms of accuracy, privacy, and communication data volume.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Danyang Xiao",
        "Diying Yang",
        "Jialun Li",
        "Xu Chen",
        "Weigang Wu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/79b65cce8d475c2102bd522153d7cffb95e3ae96",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dd1d279ffe4b5d71c4cb5f6c1a6215f23fc46ff2",
      "title": "Consecutive Load Redistribution Attack Without Line Admittance Information",
      "abstract": "This paper develops a novel method for launching a stealthy Load Redistribution Attack (LRA) without requiring knowledge of the power network\u2019s admittance matrix. Initially, equations involving the admittance matrix in the conventional LRA model are substituted with equivalent conditions utilizing the Power Transfer Distribution Factor (PTDF) matrix. Subsequently, a ridge regression approach is applied to estimate the PTDF matrix based on the hijacked Supervisory Control and Data Acquisition (SCADA) data. To maximize the damage inflicted by the proposed LRA, a consecutively small-scale attack strategy is designed to gather more informative data, and the PTDF matrix estimation is then updated accordingly to enhance its accuracy. Ultimately, a stealthy LRA is completed using the final PTDF matrix estimation. The rationale behind employing the PTDF matrix estimation in the proposed method, as opposed to the estimation of the admittance matrix, is that the latter requires information on bus voltage phase angles not supplied by the SCADA system. Simulations on the IEEE 30-bus and IEEE 118-bus system, without admittance information for grid transmission lines, demonstrate the accuracy and efficacy of the proposed attack.",
      "year": 2025,
      "venue": "IEEE Transactions on Smart Grid",
      "authors": [
        "Zelin Liu",
        "Tao Liu",
        "Yue Song",
        "David J. Hill"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dd1d279ffe4b5d71c4cb5f6c1a6215f23fc46ff2",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5d0bc08bff998a92ccad11bb43bbf9d199a47e5e",
      "title": "DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation",
      "abstract": "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation&filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yuang Ai",
        "Xiaoqiang Zhou",
        "Huaibo Huang",
        "Xiaotian Han",
        "Zhengyu Chen",
        "Quanzeng You",
        "Hongxia Yang"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/5d0bc08bff998a92ccad11bb43bbf9d199a47e5e",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "14acc0f14fe441d063a1dd73b8e432c9acd0f7f1",
      "title": "Model Inversion Robustness: Can Transfer Learning Help?",
      "abstract": "Model Inversion (MI) attacks aim to reconstruct private training data by abusing access to machine learning models. Contemporary MI attacks have achieved impressive attack performance, posing serious threats to privacy. Meanwhile, all existing MI defense methods rely on regularization that is in direct conflict with the training objective, resulting in noticeable degradation in model utility. In this work, we take a different perspective, and propose a novel and simple Transfer Learning-based Defense against Model Inversion (TL-DMI) to render MI-robust models. Particularly, by leveraging TL, we limit the number of layers encoding sensitive information from private training dataset, thereby degrading the performance of MI attack. We conduct an analysis using Fisher Information to justify our method. Our defense is remarkably simple to implement. Without bells and whistles, we show in extensive experiments that TL-DMI achieves state-of-the-art (SOTA) MI robustness. Our code, pre-trained models, demo and inverted data are available at: https://hosytuyen.github.io/projectsITL-DMI",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sy-Tuyen Ho",
        "Koh Jun Hao",
        "Keshigeyan Chandrasegaran",
        "Ngoc-Bao Nguyen",
        "Ngai-Man Cheung"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/14acc0f14fe441d063a1dd73b8e432c9acd0f7f1",
      "pdf_url": "https://arxiv.org/pdf/2405.05588",
      "publication_date": "2024-05-09",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f37af88de368a86046b1c72becaaddc8de89bb3b",
      "title": "Can Private Machine Learning Be Fair?",
      "abstract": "We show that current SOTA methods for privately and fairly training models are unreliable in many practical scenarios. Specifically, we (1) introduce a new type of adversarial attack that seeks to introduce unfairness into private model training, and (2) demonstrate that the use of methods for training on private data that are robust to adversarial attacks often leads to unfair models, regardless of the use of fairness-enhancing training methods. This leads to a dilemma when attempting to train fair models on private data: either (A) we use a robust training method which may introduce unfairness to the model itself, or (B) we train models which are vulnerable to adversarial attacks that introduce unfairness. This paper highlights flaws in robust learning methods when training fair models, yielding a new perspective for the design of robust and private learning systems.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Joseph Rance",
        "Filip Svoboda"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f37af88de368a86046b1c72becaaddc8de89bb3b",
      "pdf_url": "https://doi.org/10.1609/aaai.v39i19.34216",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "93d526ea37f3b89cba2c2884b82261d5130f0423",
      "title": "Don't Hash Me Like That: Exposing and Mitigating Hash-Induced Unfairness in Local Differential Privacy",
      "abstract": "Local differential privacy (LDP) has become a widely accepted framework for privacy-preserving data collection. In LDP, many protocols rely on hash functions to implement user-side encoding and perturbation. However, the security and privacy implications of hash function selection have not been previously investigated. In this paper, we expose that the hash functions may act as a source of unfairness in LDP protocols. We show that although users operate under the same protocol and privacy budget, differences in hash functions can lead to significant disparities in vulnerability to inference and poisoning attacks. To mitigate hash-induced unfairness, we propose Fair-OLH (F-OLH), a variant of OLH that enforces an entropy-based fairness constraint on hash function selection. Experiments show that F-OLH is effective in mitigating hash-induced unfairness under acceptable time overheads.",
      "year": 2025,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Berkay Kemal Balioglu",
        "Alireza Khodaie",
        "M. E. Gursoy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/93d526ea37f3b89cba2c2884b82261d5130f0423",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eda84e4a546a2316a32fa5654c1cfda5a6d4aeec",
      "title": "Adaptive and robust watermark against model extraction attack",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/eda84e4a546a2316a32fa5654c1cfda5a6d4aeec",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cb17186112b1a1f233d760c6ac11ccd485b41190",
      "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction",
      "abstract": null,
      "year": 2020,
      "venue": "Journal of Biomedical Informatics",
      "authors": [
        "S. Pfohl",
        "Agata Foryciarz",
        "N. Shah"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/cb17186112b1a1f233d760c6ac11ccd485b41190",
      "pdf_url": "https://doi.org/10.1016/j.jbi.2020.103621",
      "publication_date": "2020-07-20",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8f4ee05a638812932c322702ed0438e90750b27c",
      "title": "Strategic Best-Response Fairness in Fair Machine Learning Algorithms",
      "abstract": "Machine learning algorithms have become increasingly common and have affect many aspects of our life. However, because the objective of most of the standard, off-the-shelf machine learning algorithms is to maximize the prediction performance, the results produced by these algorithms could be discriminatory. The discrimination issue has gain the interest from both academic researchers and practitioners to develop machine learning algorithms that are fair. Even then, most such algorithms focus on decreasing the disparity in predictions of successful outcomes. However, these algorithms tend to ignore the strategic behavior of prediction subpopulations, resulting in disparity in the behavior of prediction subjects at equilibrium. One exception is those algorithms that use equalized odds as a fairness criterion which can decrease disparity in behavior. However, they cannot be used in many practical settings. We propose a new class of fair machine learning algorithms that alleviate disparity in prediction results, disparity in behavior of prediction subjects, and does not need to account for the sensitive variable explicitly. Our algorithm also complies with the notion of equal treatment and explainable AI, and can be applied to a wide variety of prediction tasks. We demonstrate the theoretical performance of our algorithm in the asymptotic scenario. In addition, we show the practical performance of the proposed algorithm by comparing its performance with that of other ordinary off-the-shelf algorithms and that of existing fair machine learning algorithms available in the IBM Fairness 360 suite.",
      "year": 2019,
      "venue": "Social Science Research Network",
      "authors": [
        "Hajime Shimao",
        "Junpei Komiyama",
        "Warut Khern-am-nuai",
        "Karthik N. Kannan"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8f4ee05a638812932c322702ed0438e90750b27c",
      "pdf_url": "",
      "publication_date": "2019-10-09",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c3aa2ccd6b260a90e8d8b95d1dd794bb2b48e474",
      "title": "Fair Machine Unlearning: Data Removal while Mitigating Disparities",
      "abstract": "The Right to be Forgotten is a core principle outlined by regulatory frameworks such as the EU's General Data Protection Regulation (GDPR). This principle allows individuals to request that their personal data be deleted from deployed machine learning models. While\"forgetting\"can be naively achieved by retraining on the remaining dataset, it is computationally expensive to do to so with each new request. As such, several machine unlearning methods have been proposed as efficient alternatives to retraining. These methods aim to approximate the predictive performance of retraining, but fail to consider how unlearning impacts other properties critical to real-world applications such as fairness. In this work, we demonstrate that most efficient unlearning methods cannot accommodate popular fairness interventions, and we propose the first fair machine unlearning method that can efficiently unlearn data instances from a fair objective. We derive theoretical results which demonstrate that our method can provably unlearn data and provably maintain fairness performance. Extensive experimentation with real-world datasets highlight the efficacy of our method at unlearning data instances while preserving fairness.",
      "year": 2023,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Alexander X. Oesterling",
        "Jiaqi W. Ma",
        "F. Calmon",
        "Himabindu Lakkaraju"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/c3aa2ccd6b260a90e8d8b95d1dd794bb2b48e474",
      "pdf_url": "https://arxiv.org/pdf/2307.14754",
      "publication_date": "2023-07-27",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "810b9fa1e50768a10367fa8787ecdac579ba72a4",
      "title": "Fair Data Representation for Machine Learning at the Pareto Frontier",
      "abstract": "As machine learning powered decision-making becomes increasingly important in our daily lives, it is imperative to strive for fairness in the underlying data processing. We propose a pre-processing algorithm for fair data representation via which L2(\u2119)-objective supervised learning results in estimations of the Pareto frontier between prediction error and statistical disparity. Particularly, the present work applies the optimal affine transport to approach the post-processing Wasserstein barycenter characterization of the optimal fair L2-objective supervised learning via a pre-processing data deformation. Furthermore, we show that the Wasserstein geodesics from learning outcome marginals to their barycenter characterizes the Pareto frontier between L2-loss and total Wasserstein distance among the marginals. Numerical simulations underscore the advantages: (1) the pre-processing step is compositive with arbitrary conditional expectation estimation supervised learning methods and unseen data; (2) the fair representation protects the sensitive information by limiting the inference capability of the remaining data with respect to the sensitive data; (3) the optimal affine maps are computationally efficient even for high-dimensional data.",
      "year": 2022,
      "venue": "Journal of machine learning research",
      "authors": [
        "Shizhou Xu",
        "T. Strohmer"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/810b9fa1e50768a10367fa8787ecdac579ba72a4",
      "pdf_url": "",
      "publication_date": "2022-01-02",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8c489373339c3f1b098e6f8d6403621ba0e439bc",
      "title": "Attacks and Defenses Against LLM Fingerprinting",
      "abstract": "As large language models are increasingly deployed in sensitive environments, fingerprinting attacks pose significant privacy and security risks. We present a study of LLM fingerprinting from both offensive and defensive perspectives. Our attack methodology uses reinforcement learning to automatically optimize query selection, achieving better fingerprinting accuracy with only 3 queries compared to randomly selecting 3 queries from the same pool. Our defensive approach employs semantic-preserving output filtering through a secondary LLM to obfuscate model identity while maintaining semantic integrity. The defensive method reduces fingerprinting accuracy across tested models while preserving output quality. These contributions show the potential to improve fingerprinting tools capabilities while providing practical mitigation strategies against fingerprinting attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kevin Kurian",
        "Ethan Holland",
        "Sean Oesch"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8c489373339c3f1b098e6f8d6403621ba0e439bc",
      "pdf_url": "",
      "publication_date": "2025-08-12",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
      "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical machine learning, primarily to solve classification, regression and generative tasks. However, its rapid development raises critical security challenges in the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines adversarial threats unique to QML systems, focusing on vulnerabilities in cloud-based deployments, hybrid architectures, and quantum generative models. Key attack vectors include model stealing via transpilation or output extraction, data poisoning through quantum-specific perturbations, reverse engineering of proprietary variational quantum circuits, and backdoor attacks. Adversaries exploit noise-prone quantum hardware and insufficiently secured QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership, and functionality. Defense mechanisms leverage quantum properties to counter these threats. Noise signatures from training hardware act as non-invasive watermarks, while hardware-aware obfuscation techniques and ensemble strategies disrupt cloning attempts. Emerging solutions also adapt classical adversarial training and differential privacy to quantum settings, addressing vulnerabilities in quantum neural networks and generative architectures. However, securing QML requires addressing open challenges such as balancing noise levels for reliability and security, mitigating cross-platform attacks, and developing quantum-classical trust frameworks. This chapter summarizes recent advances in attacks and defenses, offering a roadmap for researchers and practitioners to build robust, trustworthy QML systems resilient to evolving adversarial landscapes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Archisman Ghosh",
        "Satwik Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3b934ef84c918a6adfb9bf1291b6e364ba9149ea",
      "title": "Memory Tampering Attack on Binary GCD Based Inversion Algorithms",
      "abstract": null,
      "year": 2018,
      "venue": "International journal of parallel programming",
      "authors": [
        "Alejandro Cabrera Aldaya",
        "B. Brumley",
        "Alejandro J. Cabrera Sarmiento",
        "S. S\u00e1nchez-Solano"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/3b934ef84c918a6adfb9bf1291b6e364ba9149ea",
      "pdf_url": "",
      "publication_date": "2018-11-29",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "afd74c544056d7d40a12c5f677cde193f769de36",
      "title": "AI-Driven Image-to-Audio Encryption with Data Hiding",
      "abstract": "The proposed framework addresses the critical need for secure multimedia data sharing by integrating image-to-audio encryption with advanced AI-based data-hiding techniques. The core concept involves transforming static visual content into dynamic auditory experiences, injecting semantic depth and artistic creativity into the resulting audio output. Unlike conventional methods, this approach goes beyond translating pixels into sound waves. It holds promise in multimedia communication, artistic expression, and data protection. To enhance security, this framework introduces AIenhanced data-hiding techniques, leveraging adversarial training and reinforcement learning. This extra layer ensures resistance to unauthorized access and tampering. In summary, this approach fuses image-to-audio encryption and AI-enhanced data hiding, aiming to revolutionize multimedia, thus offering a robust solution, transcending boundaries by safeguarding visual data through the immersive medium of audio.",
      "year": 2024,
      "venue": "International Conference on Computing Communication and Networking Technologies",
      "authors": [
        "Anjali Chennupati",
        "Bhamidipati Prahas",
        "Bharadwaj Aaditya Ghali",
        "T. V. Nidhin Prabhakar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/afd74c544056d7d40a12c5f677cde193f769de36",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7a84a692327534fd227fa1e07fcb3816b633c591",
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "abstract": "At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.",
      "year": 2018,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Arthur Jacot",
        "Franck Gabriel",
        "Cl\u00e9ment Hongler"
      ],
      "citation_count": 3644,
      "url": "https://www.semanticscholar.org/paper/7a84a692327534fd227fa1e07fcb3816b633c591",
      "pdf_url": "",
      "publication_date": "2018-06-20",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2e9b85a45dd37e374c14a53fc3b4d6046bfc6ff8",
      "title": "Oblivious Defense in ML Models: Backdoor Removal without Detection",
      "abstract": "As society grows more reliant on machine learning, ensuring the security of machine learning systems against sophisticated attacks becomes a pressing concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (FOCS \u201922) shows that an adversary can plant undetectable backdoors in machine learning models, allowing the adversary to covertly control the model\u2019s behavior. Backdoors can be planted in such a way that the backdoored machine learning model is computationally indistinguishable from an honest model without backdoors. In this paper, we present strategies for defending against backdoors in ML models, even if they are undetectable. The key observation is that it is sometimes possible to provably mitigate or even remove backdoors without needing to detect them, using techniques inspired by the notion of random self-reducibility. This depends on properties of the ground-truth labels (chosen by nature), and not of the proposed ML model (which may be chosen by an attacker). We give formal definitions for secure backdoor mitigation, and proceed to show two types of results. First, we show a \u201cglobal mitigation\u201d technique, which removes all backdoors from a machine learning model under the assumption that the ground-truth labels are close to a Fourier-heavy function. Second, we consider distributions where the ground-truth labels are close to a linear or polynomial function in \u211dn. Here, we show \u201clocal mitigation\u201d techniques, which remove backdoors with high probability for every input of interest, and are computationally cheaper than global mitigation. All of our constructions are black-box, so our techniques work without needing access to the model\u2019s representation (i.e., its code or parameters). Along the way we prove a simple result for robust mean estimation.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "S. Goldwasser",
        "Jonathan Shafer",
        "Neekon Vafa",
        "Vinod Vaikuntanathan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2e9b85a45dd37e374c14a53fc3b4d6046bfc6ff8",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f65c2209859b8de0bd7edbfeacbe62e263af6bd7",
      "title": "Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion",
      "abstract": "Many backdoor removal techniques in machine learning models require clean in-distribution data, which may not always be available due to proprietary datasets. Model inversion techniques, often considered privacy threats, can reconstruct realistic training samples, potentially eliminating the need for in-distribution data. Prior attempts to combine backdoor removal and model inversion yielded limited results. Our work is the first to provide a thorough understanding of leveraging model inversion for effective backdoor removal by addressing key questions about reconstructed samples' properties, perceptual similarity, and the potential presence of backdoor triggers. We establish that relying solely on perceptual similarity is insufficient for robust defenses, and the stability of model predictions in response to input and parameter perturbations is also crucial. To tackle this, we introduce a novel bi-level optimization-based framework for model inversion, promoting stability and visual quality. Interestingly, we discover that reconstructed samples from a pre-trained generator's latent space are backdoor-free, even when utilizing signals from a backdoored model. We provide a theoretical analysis to support this finding. Our evaluation demonstrates that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without clean in-distribution data, matching or surpassing performance using the same amount of clean samples.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Si Chen",
        "Yi Zeng",
        "J. T.Wang",
        "Won Park",
        "Xun Chen",
        "L. Lyu",
        "Zhuoqing Mao",
        "R. Jia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f65c2209859b8de0bd7edbfeacbe62e263af6bd7",
      "pdf_url": "",
      "publication_date": "2022-06-14",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9af4ee4d5255739283cdd728864168a7e6336393",
      "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Yue Teng",
        "Ke He",
        "Qiongkai Xu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9af4ee4d5255739283cdd728864168a7e6336393",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d6b3c161816fd409d13ce99d6c2b243f2b8937e2",
      "title": "Vertical Federated Unlearning via Backdoor Certification",
      "abstract": "Vertical Federated Learning (VFL) offers a novel paradigm in machine learning, enabling distinct entities to train models cooperatively while maintaining data privacy. This method is particularly pertinent when entities possess datasets with identical sample identifiers but diverse attributes. Recent privacy regulations emphasize an individual's right to be forgotten, which necessitates the ability for models to unlearn specific training data. The primary challenge is to develop a mechanism to eliminate the influence of a specific client from a model without erasing all relevant data from other clients. Our research investigates the removal of a single client's contribution within the VFL framework. We introduce an innovative modification to traditional VFL by employing a mechanism that inverts the typical learning trajectory with the objective of extracting specific data contributions. This approach seeks to optimize model performance using gradient ascent, guided by a pre-defined constrained model. We also introduce a backdoor mechanism to verify the effectiveness of the unlearning procedure. Our method avoids fully accessing the initial training data and avoids storing parameter updates. Empirical evidence shows that the results align closely with those achieved by retraining from scratch. Utilizing gradient ascent, our unlearning approach addresses key challenges in VFL, laying the groundwork for future advancements in this domain.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Mengde Han",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Huan Huo",
        "Wanlei Zhou"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/d6b3c161816fd409d13ce99d6c2b243f2b8937e2",
      "pdf_url": "http://arxiv.org/pdf/2412.11476",
      "publication_date": "2024-12-16",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6eb9a3be3bffeec5e0cc7a05333aa247a204dae",
      "title": "Never Too Late: Tracing and Mitigating Backdoor Attacks in Federated Learning",
      "abstract": "The privacy-preserving nature of Federated Learning (FL) exposes such a distributed learning paradigm to the planting of backdoors with locally corrupted data. We discover that FL backdoors, under a new on-off multi-shot attack form, are essentially stealthy against existing defenses that are built on model statistics and spectral analysis. First-hand observations of such attacks show that the backdoored models are indistinguishable from normal ones w.r.t. both low-level and high-level representations. We thus emphasize that a critical redemption, if not the only, for the tricky stealthiness is reactive tracing and posterior mitigation. A three-step remedy framework is then proposed by exploring the temporal and inferential correlations of models on a trapped sample from an attack. In particular, we use shift ensemble detection and co-occurrence analysis for adversary identification, and repair the model via malicious ingredients removal under theoretical error guarantee. Extensive experiments on various backdoor settings demonstrate that our framework can achieve accuracy on attack round identification of \u223c80% and on attackers of \u223c50%, which are \u223c28.76% better than existing proactive defenses. Meanwhile, it can successfully eliminate the influence of backdoors with only a 5%\u223c6% performance drop.",
      "year": 2022,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "Hui Zeng",
        "Tongqing Zhou",
        "Xinyi Wu",
        "Zhiping Cai"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/e6eb9a3be3bffeec5e0cc7a05333aa247a204dae",
      "pdf_url": "",
      "publication_date": "2022-09-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}