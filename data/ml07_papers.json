{
  "updated": "2026-01-06",
  "total": 400,
  "owasp_id": "ML07",
  "owasp_name": "Transfer Learning Attack",
  "description": "Attacks that exploit transfer learning and pre-trained models. This includes\n        attacks on fine-tuning, exploiting pre-trained representations, transferability\n        of adversarial examples across models, and vulnerabilities introduced by\n        using pre-trained models as feature extractors or starting points.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "435640467611474172ccf702d631bc652f6cd7a8",
      "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
      "abstract": "Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications. The code of our method is available on GitHub: https://github.com/mist-project/mist.git.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chumeng Liang",
        "Xiaoyu Wu",
        "Yang Hua",
        "Jiaru Zhang",
        "Yiming Xue",
        "Tao Song",
        "Zhengui Xue",
        "Ruhui Ma",
        "Haibing Guan"
      ],
      "citation_count": 180,
      "url": "https://www.semanticscholar.org/paper/435640467611474172ccf702d631bc652f6cd7a8",
      "pdf_url": "http://arxiv.org/pdf/2302.04578",
      "publication_date": "2023-02-09",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "58c143069444c7dff4be53531a47efefc40be497",
      "title": "On Adaptive Attacks to Adversarial Example Defenses",
      "abstract": "Adaptive attacks have (rightfully) become the de facto standard for evaluating defenses to adversarial examples. We find, however, that typical adaptive evaluations are incomplete. We demonstrate that thirteen defenses recently published at ICLR, ICML and NeurIPS---and chosen for illustrative and pedagogical purposes---can be circumvented despite attempting to perform evaluations using adaptive attacks. While prior evaluation papers focused mainly on the end result---showing that a defense was ineffective---this paper focuses on laying out the methodology and the approach necessary to perform an adaptive attack. We hope that these analyses will serve as guidance on how to properly perform adaptive attacks against defenses to adversarial examples, and thus will allow the community to make further progress in building more robust models.",
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Florian Tram\u00e8r",
        "Nicholas Carlini",
        "Wieland Brendel",
        "A. Ma\u0327dry"
      ],
      "citation_count": 903,
      "url": "https://www.semanticscholar.org/paper/58c143069444c7dff4be53531a47efefc40be497",
      "pdf_url": "",
      "publication_date": "2020-02-19",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "750a5127db2375e6e5a1d022e66f25fb2982fcff",
      "title": "Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection",
      "abstract": "Recent studies in deepfake detection have yielded promising results when the training and testing face forgeries are from the same dataset. However, the problem remains challenging when one tries to generalize the detector to forgeries created by unseen methods in the training dataset. This work addresses the generalizable deepfake detection from a simple principle: a generalizable representation should be sensitive to diverse types of forgeries. Following this principle, we propose to enrich the \u201cdiversity\u201d of forgeries by synthesizing augmented forgeries with a pool of forgery configurations and strengthen the \u201csensitivity\u201d to the forgeries by enforcing the model to predict the forgery configurations. To effectively explore the large forgery augmentation space, we further propose to use the adversarial training strategy to dynamically synthesize the most challenging forgeries to the current model. Through extensive experiments, we show that the proposed strategies are surprisingly effective (see Figure 1), and they could achieve superior performance than the current state-of-the-art methods. Code is available at https://github.com/liangchen527/SLADD.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Liang Chen",
        "Yong Zhang",
        "Yibing Song",
        "Lingqiao Liu",
        "Jue Wang"
      ],
      "citation_count": 274,
      "url": "https://www.semanticscholar.org/paper/750a5127db2375e6e5a1d022e66f25fb2982fcff",
      "pdf_url": "https://arxiv.org/pdf/2203.12208",
      "publication_date": "2022-03-23",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ec75d91c6f79db1e66c96beff5475e966dc4844",
      "title": "LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity",
      "abstract": "We propose transferability from Large Geometric Vicinity (LGV), a new technique to increase the transferability of black-box adversarial attacks. LGV starts from a pretrained surrogate model and collects multiple weight sets from a few additional training epochs with a constant and high learning rate. LGV exploits two geometric properties that we relate to transferability. First, models that belong to a wider weight optimum are better surrogates. Second, we identify a subspace able to generate an effective surrogate ensemble among this wider optimum. Through extensive experiments, we show that LGV alone outperforms all (combinations of) four established test-time transformations by 1.8 to 59.9 percentage points. Our findings shed new light on the importance of the geometry of the weight space to explain the transferability of adversarial examples.",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Martin Gubri",
        "Maxime Cordy",
        "Mike Papadakis",
        "Y. L. Traon",
        "Koushik Sen"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/2ec75d91c6f79db1e66c96beff5475e966dc4844",
      "pdf_url": "http://arxiv.org/pdf/2207.13129",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5399f235088deab0bd9ff31dea8441a28283dcbe",
      "title": "Adversarial example detection for DNN models: a review and experimental comparison",
      "abstract": "Deep learning (DL) has shown great success in many human-related tasks, which has led to its adoption in many computer vision based applications, such as security surveillance systems, autonomous vehicles and healthcare. Such safety-critical applications have to draw their path to success deployment once they have the capability to overcome safety-critical challenges. Among these challenges are the defense against or/and the detection of the adversarial examples (AEs). Adversaries can carefully craft small, often imperceptible, noise called perturbations to be added to the clean image to generate the AE. The aim of AE is to fool the DL model which makes it a potential risk for DL applications. Many test-time evasion attacks and countermeasures, i.e., defense or detection methods, are proposed in the literature. Moreover, few reviews and surveys were published and theoretically showed the taxonomy of the threats and the countermeasure methods with little focus in AE detection methods. In this paper, we focus on image classification task and attempt to provide a survey for detection methods of test-time evasion attacks on neural network classifiers. A detailed discussion for such methods is provided with experimental results for eight state-of-the-art detectors under different scenarios on four datasets. We also provide potential challenges and future perspectives for this research direction.",
      "year": 2021,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Ahmed Aldahdooh",
        "W. Hamidouche",
        "Sid Ahmed Fezza",
        "O. D\u00e9forges"
      ],
      "citation_count": 158,
      "url": "https://www.semanticscholar.org/paper/5399f235088deab0bd9ff31dea8441a28283dcbe",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32b7fa9a396f723c502b7d3bd2c13fc1d9998a43",
      "title": "Robust Android Malware Detection against Adversarial Example Attacks",
      "abstract": "Adversarial examples pose severe threats to Android malware detection because they can render the machine learning based detection systems useless. How to effectively detect Android malware under various adversarial example attacks becomes an essential but very challenging issue. Existing adversarial example defense mechanisms usually rely heavily on the instances or the knowledge of adversarial examples, and thus their usability and effectiveness are significantly limited because they often cannot resist the unseen-type adversarial examples. In this paper, we propose a novel robust Android malware detection approach that can resist adversarial examples without requiring their instances or knowledge by jointly investigating malware detection and adversarial example defenses. More precisely, our approach employs a new VAE (variational autoencoder) and an MLP (multi-layer perceptron) to detect malware, and combines their detection outcomes to make the final decision. In particular, we share a feature extraction network between the VAE and the MLP to reduce model complexity and design a new loss function to disentangle the features of different classes, hence improving detection performance. Extensive experiments confirm our model\u2019s advantage in accuracy and robustness. Our method outperforms 11 state-of-the-art robust Android malware detection models when resisting 7 kinds of adversarial example attacks.",
      "year": 2021,
      "venue": "The Web Conference",
      "authors": [
        "Heng Li",
        "Shiyao Zhou",
        "Wei Yuan",
        "Xiapu Luo",
        "Cuiying Gao",
        "Shuiyan Chen"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/32b7fa9a396f723c502b7d3bd2c13fc1d9998a43",
      "pdf_url": "",
      "publication_date": "2021-04-19",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a87bc35899e998131b59b1b2d4ac78e43a2a1e7f",
      "title": "Adversarial-Example Attacks Toward Android Malware Detection System",
      "abstract": "Recently, it was shown that the generative adversarial network (GAN) based adversarial-example attacks could thoroughly defeat the existing Android malware detection systems. However, they can be easily defended through deploying a firewall (i.e., adversarial example detector) to filter adversarial examples. To evade both malware detection and adversarial example detection, we develop a new adversarial-example attack method based on our proposed bi-objective GAN. Experiments show that over <inline-formula><tex-math notation=\"LaTeX\">$\\text{95}\\%$</tex-math></inline-formula> of adversarial examples generated by our method break through the firewall-equipped Android malware detection system, outperforming the state-of-the-art method by <inline-formula><tex-math notation=\"LaTeX\">$\\text{247.68}\\%$</tex-math></inline-formula>.",
      "year": 2020,
      "venue": "IEEE Systems Journal",
      "authors": [
        "Heng Li",
        "Shiyao Zhou",
        "Wei Yuan",
        "Jiahuan Li",
        "H. Leung"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/a87bc35899e998131b59b1b2d4ac78e43a2a1e7f",
      "pdf_url": "",
      "publication_date": "2020-03-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90dcaf5b7ded95f61666711aa94a59eae77bc304",
      "title": "Enhancing Adversarial Example Transferability With an Intermediate Level Attack",
      "abstract": "Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.",
      "year": 2019,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Qian Huang",
        "Isay Katsman",
        "Horace He",
        "Zeqi Gu",
        "Serge J. Belongie",
        "Ser-Nam Lim"
      ],
      "citation_count": 276,
      "url": "https://www.semanticscholar.org/paper/90dcaf5b7ded95f61666711aa94a59eae77bc304",
      "pdf_url": "https://arxiv.org/pdf/1907.10823",
      "publication_date": "2019-07-23",
      "keywords_matched": [
        "adversarial example",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2a799582775ff5527de1c24167aef0a9a6bf614b",
      "title": "Adversarial Example Games",
      "abstract": "The existence of adversarial examples capable of fooling trained neural network classifiers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the challenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical observations (e.g., that momentum helps), lacking principled transferability guarantees. In this work, we provide a theoretical foundation for crafting transferable adversarial examples to entire hypothesis classes. We introduce Adversarial Example Games (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classifier. AEG provides a new way to design adversarial examples by adversarially training a generator and a classifier from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classifier from the corresponding hypothesis class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming prior state-of-the-art approaches with an average relative improvement of $29.9\\%$ and $47.2\\%$ against undefended and robust models (Table 2 & 3) respectively.",
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "A. Bose",
        "G. Gidel",
        "Hugo Berrard",
        "Andre Cianflone",
        "Pascal Vincent",
        "Simon Lacoste-Julien",
        "William L. Hamilton"
      ],
      "citation_count": 56,
      "url": "https://www.semanticscholar.org/paper/2a799582775ff5527de1c24167aef0a9a6bf614b",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "53317982549671e13930ce51cb55293ab1125041",
      "title": "Towards Accuracy-Fairness Paradox: Adversarial Example-based Data Augmentation for Visual Debiasing",
      "abstract": "Machine learning fairness concerns about the biases towards certain protected or sensitive group of people when addressing the target tasks. This paper studies the debiasing problem in the context of image classification tasks. Our data analysis on facial attribute recognition demonstrates (1) the attribution of model bias from imbalanced training data distribution and (2) the potential of adversarial examples in balancing data distribution. We are thus motivated to employ adversarial example to augment the training data for visual debiasing. Specifically, to ensure the adversarial generalization as well as cross-task transferability, we propose to couple the operations of target task classifier training, bias task classifier training, and adversarial example generation. The generated adversarial examples supplement the target task training dataset via balancing the distribution over bias variables in an online fashion. Results on simulated and real-world debiasing experiments demonstrate the effectiveness of the proposed solution in simultaneously improving model accuracy and fairness. Preliminary experiment on few-shot learning further shows the potential of adversarial attack-based pseudo sample generation as alternative solution to make up for the training data lackage.",
      "year": 2020,
      "venue": "ACM Multimedia",
      "authors": [
        "Yi Zhang",
        "Jitao Sang"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/53317982549671e13930ce51cb55293ab1125041",
      "pdf_url": "",
      "publication_date": "2020-07-27",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7cf5c840fa6006767289db6d351a803d8590ce94",
      "title": "Review on Image Processing Based Adversarial Example Defenses in Computer Vision",
      "abstract": "Recent research works showed that deep neural networks are vulnerable to adversarial examples, which are usually maliciously created by carefully adding deliberate and imperceptible perturbations to examples. Several states of the art defense methods are proposed based on the existing image processing methods like image compression and image denoising. However, such approaches are not the final optimal solution for defense adversarial perturbations in DNN models. In this paper, we reviewed two main approaches to deploying image processing methods as a defense. By analyzing and discus!sing the remaining issues, we present two open questions for future research direction including the definition of adversarial perturbations and noises, the novel defense-aware threat model. A further research direction is also given by re-thinking the impacts of adversarial perturbations on all frequency bands.",
      "year": 2020,
      "venue": "2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",
      "authors": [
        "Meikang Qiu",
        "Han Qiu"
      ],
      "citation_count": 83,
      "url": "https://www.semanticscholar.org/paper/7cf5c840fa6006767289db6d351a803d8590ce94",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d1d4a3f52edb60ef97a3257a398ca9e60c9fed46",
      "title": "Adversarial example generation with adabelief optimizer and crop invariance",
      "abstract": "Deep neural networks are vulnerable to adversarial examples, which are crafted by applying small, human-imperceptible perturbations on the original images, so as to mislead deep neural networks to output inaccurate predictions. Adversarial attacks can thus be an important method to evaluate and select robust models in safety-critical applications. However, under the challenging black-box setting, most existing adversarial attacks often achieve relatively low success rates on normally trained networks and adversarially trained networks. In this paper, we regard the generation process of adversarial examples as an optimization process similar to deep neural network training. From this point of view, we introduce AdaBelief optimizer and crop invariance into the generation of adversarial examples, and propose AdaBelief Iterative Fast Gradient Method (ABI-FGM) and Crop-Invariant attack Method (CIM) to improve the transferability of adversarial examples. By adopting adaptive learning rate into the iterative attacks, ABI-FGM can optimize the convergence process, resulting in more transferable adversarial examples. CIM is based on our discovery on the crop-invariant property of deep neural networks, which we thus leverage to optimize the adversarial perturbations over an ensemble of crop copies so as to avoid overfitting on the white-box model being attacked and improve the transferability of adversarial examples. ABI-FGM and CIM can be readily integrated to build a strong gradient-based attack to further boost the success rates of adversarial examples for black-box attacks. Moreover, our method can also be naturally combined with other gradient-based attack methods to build a more robust attack to generate more transferable adversarial examples against the defense models. Extensive experiments on the ImageNet dataset demonstrate the method\u2019s effectiveness. Whether on normally trained networks or adversarially trained networks, our method has higher success rates than state-of-the-art gradient-based attack methods.",
      "year": 2021,
      "venue": "Applied intelligence (Boston)",
      "authors": [
        "Bo Yang",
        "Hengwei Zhang",
        "Yuchen Zhang",
        "Kaiyong Xu",
        "Jin-dong Wang"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/d1d4a3f52edb60ef97a3257a398ca9e60c9fed46",
      "pdf_url": "http://arxiv.org/pdf/2102.03726",
      "publication_date": "2021-02-07",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20da2a4c9bdbf785330a4173e9e157253366e094",
      "title": "SmsNet: A New Deep Convolutional Neural Network Model for Adversarial Example Detection",
      "abstract": "The emergence of adversarial examples has had a significant impact on the development and application of deep learning. In this paper, a novel convolutional neural network model, the stochastic multifilter statistical network (SmsNet), is proposed for the detection of adversarial examples. A feature statistical layer is constructed to collect statistical data of feature map output from each convolutional layer in SmsNet by combining manual features with a neural network. The entire model is an end-to-end detection model, so the feature statistical layer is not independent of the network, and its output is directly transmitted to the fully connected layer by a short-cut connection called the SmsConnection. Additionally, a dynamic pruning strategy is introduced to simplify the model structure for better performance. The experiments demonstrate the effectiveness of the network structure and pruning strategy, and the proposed model achieves high detection rates against state-of-the-art adversarial attacks.",
      "year": 2021,
      "venue": "IEEE transactions on multimedia",
      "authors": [
        "Jinwei Wang",
        "Junjie Zhao",
        "Qilin Yin",
        "Xiangyang Luo",
        "Y. Zheng",
        "Yun-Qing Shi",
        "S. K. Jha"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/20da2a4c9bdbf785330a4173e9e157253366e094",
      "pdf_url": "",
      "publication_date": "2021-01-08",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7378f30cd38496acf315bb18fd64e468f0f8001e",
      "title": "Principal Component Adversarial Example",
      "abstract": "Despite having achieved excellent performance on various tasks, deep neural networks have been shown to be susceptible to adversarial examples, i.e., visual inputs crafted with structural imperceptible noise. To explain this phenomenon, previous works implicate the weak capability of the classification models and the difficulty of the classification tasks. These explanations appear to account for some of the empirical observations but lack deep insight into the intrinsic nature of adversarial examples, such as the generation method and transferability. Furthermore, previous works generate adversarial examples completely rely on a specific classifier (model). Consequently, the attack ability of adversarial examples is strongly dependent on the specific classifier. More importantly, adversarial examples cannot be generated without a trained classifier. In this paper, we raise a question: what is the real cause of the generation of adversarial examples? To answer this question, we propose a new concept, called the adversarial region, which explains the existence of adversarial examples as perturbations perpendicular to the tangent plane of the data manifold. This view yields a clear explanation of the transfer property across different models of adversarial examples. Moreover, with the notion of the adversarial region, we propose a novel target-free method to generate adversarial examples via principal component analysis. We verify our adversarial region hypothesis on a synthetic dataset and demonstrate through extensive experiments on real datasets that the adversarial examples generated by our method have competitive or even strong transferability compared with model-dependent adversarial example generating methods. Moreover, our experiment shows that the proposed method is more robust to defensive methods than previous methods.",
      "year": 2020,
      "venue": "IEEE Transactions on Image Processing",
      "authors": [
        "Yonggang Zhang",
        "Xinmei Tian",
        "Ya Li",
        "Xinchao Wang",
        "D. Tao"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/7378f30cd38496acf315bb18fd64e468f0f8001e",
      "pdf_url": "",
      "publication_date": "2020-02-28",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5298753606b23865d13d4e09d37d27f6a70c9ead",
      "title": "The Defense of Adversarial Example with Conditional Generative Adversarial Networks",
      "abstract": "Deep neural network approaches have made remarkable progress in many machine learning tasks. However, the latest research indicates that they are vulnerable to adversarial perturbations. An adversary can easily mislead the network models by adding well-designed perturbations to the input. The cause of the adversarial examples is unclear. Therefore, it is challenging to build a defense mechanism. In this paper, we propose an image-to-image translation model to defend against adversarial examples. The proposed model is based on a conditional generative adversarial network, which consists of a generator and a discriminator. The generator is used to eliminate adversarial perturbations in the input. The discriminator is used to distinguish generated data from original clean data to improve the training process. In other words, our approach can map the adversarial images to the clean images, which are then fed to the target deep learning model. The defense mechanism is independent of the target model, and the structure of the framework is universal. A series of experiments conducted on MNIST and CIFAR10 show that the proposed method can defend against multiple types of attacks while maintaining good performance.",
      "year": 2020,
      "venue": "Secur. Commun. Networks",
      "authors": [
        "Fangchao Yu",
        "L. xilinx Wang",
        "Xianjin Fang",
        "Youwen Zhang"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/5298753606b23865d13d4e09d37d27f6a70c9ead",
      "pdf_url": "https://doi.org/10.1155/2020/3932584",
      "publication_date": "2020-08-25",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7528b8c48f26a3a96651133aed8681c47a4aec7c",
      "title": "Adversarial Example in Remote Sensing Image Recognition",
      "abstract": "With the wide application of remote sensing technology in various fields, the accuracy and security requirements for remote sensing images (RSIs) recognition are also increasing. In recent years, due to the rapid development of deep learning in the field of image recognition, RSI recognition models based on deep convolution neural networks (CNNs) outperform traditional hand-craft feature techniques. However, CNNs also pose security issues when they show their capability of accurate classification. By adding a very small variation of the adversarial perturbation to the input image, the CNN model can be caused to produce erroneous results with extremely high confidence, and the modification of the image is not perceived by the human eye. This added adversarial perturbation image is called an adversarial example, which poses a serious security problem for systems based on CNN model recognition results. This paper, for the first time, analyzes adversarial example problem of RSI recognition under CNN models. In the experiments, we used different attack algorithms to fool multiple high-accuracy RSI recognition models trained on multiple RSI datasets. The results show that RSI recognition models are also vulnerable to adversarial examples, and the models with different structures trained on the same RSI dataset also have different vulnerabilities. For each RSI dataset, the number of features also affects the vulnerability of the model. Many features are good for defensive adversarial examples. Further, we find that the attacked class of RSI has an attack selectivity property. The misclassification of adversarial examples of the RSIs are related to the similarity of the original classes in the CNN feature space. In addition, adversarial examples in RSI recognition are of great significance for the security of remote sensing applications, showing a huge potential for future research.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Li Chen",
        "Guowei Zhu",
        "Qi Li",
        "Haifeng Li"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/7528b8c48f26a3a96651133aed8681c47a4aec7c",
      "pdf_url": "",
      "publication_date": "2019-10-29",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6f61d15a31d6d051aeee3bf6d1482d332e68ebfe",
      "title": "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong",
      "abstract": "Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Warren He",
        "James Wei",
        "Xinyun Chen",
        "Nicholas Carlini",
        "D. Song"
      ],
      "citation_count": 242,
      "url": "https://www.semanticscholar.org/paper/6f61d15a31d6d051aeee3bf6d1482d332e68ebfe",
      "pdf_url": "",
      "publication_date": "2017-06-15",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5df3a8538d04393a6bb76f31fdf97b8fd3a22ce5",
      "title": "DLA: Dense-Layer-Analysis for Adversarial Example Detection",
      "abstract": "In recent years Deep Neural Networks (DNNs) have achieved remarkable results and even showed superhuman capabilities in a broad range of domains. This led people to trust in DNN classifications even in security-sensitive environments like autonomous driving. Despite their impressive achievements, DNNs are known to be vulnerable to adversarial examples. Such inputs contain small perturbations to intentionally fool the attacked model. In this paper, we present a novel end-to-end framework to detect such attacks without influencing the target model's performance. Inspired by research in neuron-coverage guided testing we show that dense layers of DNNs carry security-sensitive information. With a secondary DNN we analyze the activation patterns of the dense layers during classification run-time, which enables effective and real-time detection of adversarial examples. Our prototype implementation successfully detects adversarial examples in image, natural language, and audio processing. Thereby, we cover a variety of target DNN architectures. In addition to effectively defending against state-of-the-art attacks, our approach generalizes between different sets of adversarial examples. Our experiments indicate that we are able to detect future, yet unknown, attacks. Finally, during white-box adaptive attacks, we show our method cannot be easily bypassed.",
      "year": 2019,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Philip Sperl",
        "Ching-yu Kao",
        "Peng Chen",
        "Konstantin B\u00f6ttinger"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/5df3a8538d04393a6bb76f31fdf97b8fd3a22ce5",
      "pdf_url": "https://arxiv.org/pdf/1911.01921",
      "publication_date": "2019-11-05",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "938196ec2a1a880c42e474d3034f6d589dd6890d",
      "title": "Adversarial Example Defense: Ensembles of Weak Defenses are not Strong",
      "abstract": null,
      "year": 2017,
      "venue": "Workshop on Offensive Technologies",
      "authors": [
        "Warren He",
        "James Wei",
        "Xinyun Chen",
        "Nicholas Carlini",
        "D. Song"
      ],
      "citation_count": 206,
      "url": "https://www.semanticscholar.org/paper/938196ec2a1a880c42e474d3034f6d589dd6890d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c97ed28ff4a5e56896bc8f03fd59d946a8bac46f",
      "title": "Using FGSM Targeted Attack to Improve the Transferability of Adversarial Example",
      "abstract": "At present, many people pay attention to the safety problems of artificial intelligence, and the emergence of adversarial examples is one of these problems. The adversarial examples can be used to attack a neural network classification model to make its classification wrong. It is an important method to improve the attack effect of adversarial examples by improving the transferability of adversarial examples and enabling them to attack multiple different neural network classification models at the same time. When we use FGSM algorithm to attack a model, first, we set \u220a a medium magnitude value, and then use targeted attack, which can improve the transferability of the adversarial examples generated by this algorithm. We can use FGSM algorithm to carry out white-box attack on the neural network model. The first step is to set a fixed value. Then, because FGSM algorithm is non-targeted attack, when adding a termination condition of iterative attack, we can use FGSM algorithm to carry out targeted attack, so that the generated adversarial examples can be identified as specific tags by the neural network model. We found that the transferability of the adversarial examples generated in this way is improved and these adversarial examples can attack many different neural network models.",
      "year": 2019,
      "venue": "International Conference on Electrical and Control Engineering",
      "authors": [
        "Jin Xu",
        "Zhendong Cai",
        "Wei Shen"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/c97ed28ff4a5e56896bc8f03fd59d946a8bac46f",
      "pdf_url": "",
      "publication_date": "2019-12-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f5bf1d83a4d3e55e75dcc28a7bda4e99cce85836",
      "title": "BERT-ATTACK: Adversarial Attack against BERT Using BERT",
      "abstract": "Adversarial attacks for discrete data (such as text) has been proved significantly more challenging than continuous data (such as image), since it is difficult to generate adversarial samples with gradient-based methods. Currently, the successful attack methods for text usually adopt heuristic replacement strategies on character or word level, which remains challenging to find the optimal solution in the massive space of possible combination of replacements, while preserving semantic consistency and language fluency. In this paper, we propose \\textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models for downstream tasks. Our method successfully misleads the target models to predict incorrectly, outperforming state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations.",
      "year": 2020,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Linyang Li",
        "Ruotian Ma",
        "Qipeng Guo",
        "X. Xue",
        "Xipeng Qiu"
      ],
      "citation_count": 806,
      "url": "https://www.semanticscholar.org/paper/f5bf1d83a4d3e55e75dcc28a7bda4e99cce85836",
      "pdf_url": "https://www.aclweb.org/anthology/2020.emnlp-main.500.pdf",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "adversarial attack",
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8680c075abfb7832ff1321b5f409f2a5e57570f2",
      "title": "Frequency Domain Model Augmentation for Adversarial Attack",
      "abstract": ". For black-box attacks, the gap between the substitute model and the victim model is usually large, which manifests as a weak attack performance. Motivated by the observation that the transferability of adversarial examples can be improved by attacking diverse models simultaneously, model augmentation methods which simulate different models by using transformed images are proposed. However, existing transformations for spatial domain do not translate to significantly diverse augmented models. To tackle this issue, we propose a novel spectrum simulation attack to craft more transferable adversarial examples against both normally trained and defense models. Specifically, we apply a spectrum transformation to the input and thus perform the model augmentation in the frequency domain. We theoretically prove that the transformation derived from frequency domain leads to a diverse spectrum saliency map, an indicator we proposed to reflect the diversity of substitute models. Notably, our method can be generally combined with existing attacks. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method, e.g. , attacking nine state-of-the-art defense models with an average success rate of 95.4% . Our code is available in https://github.com/yuyang-long/SSA .",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yuyang Long",
        "Qi-li Zhang",
        "Boheng Zeng",
        "Lianli Gao",
        "Xianglong Liu",
        "Jian Zhang",
        "Jingkuan Song"
      ],
      "citation_count": 217,
      "url": "https://www.semanticscholar.org/paper/8680c075abfb7832ff1321b5f409f2a5e57570f2",
      "pdf_url": "http://arxiv.org/pdf/2207.05382",
      "publication_date": "2022-07-12",
      "keywords_matched": [
        "adversarial attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c3eee48481b3b8f4be18026e389fadf9a53ad192",
      "title": "Content-based Unrestricted Adversarial Attack",
      "abstract": "Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic, demonstrating their ability to deceive human perception and deep neural networks with stealth and success. However, current works usually sacrifice unrestricted degrees and subjectively select some image content to guarantee the photorealism of unrestricted adversarial examples, which limits its attack performance. To ensure the photorealism of adversarial examples and boost attack performance, we propose a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack. By leveraging a low-dimensional manifold that represents natural images, we map the images onto the manifold and optimize them along its adversarial direction. Therefore, within this framework, we implement Adversarial Content Attack based on Stable Diffusion and can generate high transferable unrestricted adversarial examples with various adversarial contents. Extensive experimentation and visualization demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models and defense methods, respectively.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zhaoyu Chen",
        "Bo Li",
        "Shuang Wu",
        "Kaixun Jiang",
        "Shouhong Ding",
        "Wenqiang Zhang"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/c3eee48481b3b8f4be18026e389fadf9a53ad192",
      "pdf_url": "http://arxiv.org/pdf/2305.10665",
      "publication_date": "2023-05-18",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8aa63acc55ff58423aadb8ad4a2884d751673109",
      "title": "A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion",
      "abstract": "Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion and ask if an adversarial text prompt can be obtained even in the absence of end-to-end model queries. We call the resulting problem \u2018query-free attack generation\u2019. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, e.g., the CLIP text encoder used for attacking Stable Diffusion. Based on such insight, we propose both untargeted and targeted query-free attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions. By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the targeted image content without causing much change in untargeted image content.",
      "year": 2023,
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Haomin Zhuang",
        "Yihua Zhang",
        "Sijia Liu"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/8aa63acc55ff58423aadb8ad4a2884d751673109",
      "pdf_url": "https://arxiv.org/pdf/2303.16378",
      "publication_date": "2023-03-29",
      "keywords_matched": [
        "adversarial attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a34c81636b793c50991199cfbcc2f0a7d55296f",
      "title": "Generalizable Black-Box Adversarial Attack With Meta Learning",
      "abstract": "In the scenario of black-box adversarial attack, the target model's parameters are unknown, and the attacker aims to find a successful adversarial perturbation based on query feedback under a query budget. Due to the limited feedback information, existing query-based black-box attack methods often require many queries for attacking each benign example. To reduce query cost, we propose to utilize the feedback information across historical attacks, dubbed example-level adversarial transferability. Specifically, by treating the attack on each benign example as one task, we develop a meta-learning framework by training a meta generator to produce perturbations conditioned on benign examples. When attacking a new benign example, the meta generator can be quickly fine-tuned based on the feedback information of the new task as well as a few historical attacks to produce effective perturbations. Moreover, since the meta-train procedure consumes many queries to learn a generalizable generator, we utilize model-level adversarial transferability to train the meta generator on a white-box surrogate model, then transfer it to help the attack against the target model. The proposed framework with the two types of adversarial transferability can be naturally combined with any off-the-shelf query-based attack methods to boost their performance, which is verified by extensive experiments. The source code is available at https://github.com/SCLBD/MCG-Blackbox.",
      "year": 2023,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Fei Yin",
        "Yong Zhang",
        "Baoyuan Wu",
        "Yan Feng",
        "Jingyi Zhang",
        "Yanbo Fan",
        "Yujiu Yang"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/0a34c81636b793c50991199cfbcc2f0a7d55296f",
      "pdf_url": "https://arxiv.org/pdf/2301.00364",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "perturbation attack",
        "adversarial attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7f1cac8703829d08984b532948f6e7e15a5cac5c",
      "title": "An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability",
      "abstract": "While the transferability property of adversarial examples allows the adversary to perform black-box attacks (i.e., the attacker has no knowledge about the target model), the transfer-based adversarial attacks have gained great attention. Previous works mostly study gradient variation or image transformations to amplify the distortion on critical parts of inputs. These methods can work on transferring across models with limited differences, i.e., from CNNs to CNNs, but always fail in transferring across models with wide differences, such as from CNNs to ViTs. Alternatively, model ensemble adversarial attacks are proposed to fuse outputs from surrogate models with diverse architectures to get an ensemble loss, making the generated adversarial example more likely to transfer to other models as it can fool multiple models concurrently. However, existing ensemble attacks simply fuse the outputs of the surrogate models evenly, thus are not efficacious to capture and amplify the intrinsic transfer information of adversarial examples. In this paper, we propose an adaptive ensemble attack, dubbed AdaEA, to adaptively control the fusion of the outputs from each model, via monitoring the discrepancy ratio of their contributions towards the adversarial objective. Furthermore, an extra disparity-reduced filter is introduced to further synchronize the update direction. As a result, we achieve considerable improvement over the existing ensemble attacks on various datasets, and the proposed AdaEA can also boost existing transfer-based attacks, which further demonstrates its efficacy and versatility. The source code: https://github.com/CHENBIN99/AdaEA",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "B. Chen",
        "Jia-Li Yin",
        "Shukai Chen",
        "Bo-Hao Chen",
        "Ximeng Liu"
      ],
      "citation_count": 73,
      "url": "https://www.semanticscholar.org/paper/7f1cac8703829d08984b532948f6e7e15a5cac5c",
      "pdf_url": "",
      "publication_date": "2023-08-05",
      "keywords_matched": [
        "transfer attack",
        "adversarial attack",
        "model zoo attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e30525cfd243b29c7c3e3831e4a92d0a35ec7258",
      "title": "Targeted Adversarial Attack Against Deep Cross-Modal Hashing Retrieval",
      "abstract": "Deep cross-modal hashing has achieved excellent retrieval performance with the powerful representation capability of deep neural networks. Regrettably, current methods are inevitably vulnerable to adversarial attacks, especially well-designed subtle perturbations that can easily fool deep cross-modal hashing models into returning irrelevant or the attacker\u2019s specified results. Although adversarial attacks have attracted increasing attention, there are few studies on specialized attacks against deep cross-modal hashing. To solve these issues, we propose a targeted adversarial attack method against deep cross-modal hashing retrieval in this paper. To the best of our knowledge, this is the first work in this research field. Concretely, we first build a progressive fusion module to extract fine-grained target semantics through a progressive attention mechanism. Meanwhile, we design a semantic adaptation network to generate the target prototype code and reconstruct the category label, thus realizing the semantic interaction between the target semantics and the implicit semantics of the attacked model. To bridge modality gaps and preserve local example details, a semantic translator seamlessly translates the target semantics and then embeds them into benign examples in collaboration with a U-Net framework. Moreover, we construct a discriminator for adversarial training, which enhances the visual realism and category discrimination of adversarial examples, thus improving their targeted attack performance. Extensive experiments on widely tested cross-modal retrieval datasets demonstrate the superiority of our proposed method. Also, transferable attacks show that our generated adversarial examples have well generalization capability on targeted attacks. The source codes and datasets are available at https://github.com/tswang0116/TA-DCH.",
      "year": 2023,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Tianshi Wang",
        "Lei Zhu",
        "Zheng Zhang",
        "Huaxiang Zhang",
        "Junwei Han"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e30525cfd243b29c7c3e3831e4a92d0a35ec7258",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7b9b39cc2fe17aa5bb3d8a07ea4292d7e91938de",
      "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
      "abstract": "Deep learning approaches have demonstrated great achievements in the field of computer-aided medical image analysis, improving the precision of diagnosis across a range of medical disorders. These developments have not, however, been immune to the appearance of adversarial attacks, creating the possibility of incorrect diagnosis with substantial clinical implications. Concurrently, the field has seen notable advancements in defending against such targeted adversary intrusions in deep medical diagnostic systems. In the context of medical image analysis, this article provides a comprehensive survey of current advancements in adversarial attacks and their accompanying defensive strategies. In addition, a comprehensive conceptual analysis is presented, including several adversarial attacks and defensive strategies designed for the interpretation of medical images. This survey, which draws on qualitative and quantitative findings, concludes with a thorough discussion of the problems with adversarial attack and defensive mechanisms that are unique to medical image analysis systems, opening up new directions for future research. We identified that the main problems with adversarial attack and defense in medical imaging include dataset and labeling, computational resources, robustness against target attacks, evaluation of transferability and adaptability, interpretability and explainability, real-time detection and response, and adversarial attacks in multi-modal fusion. The area of medical imaging adversarial attack and defensive mechanisms might move toward more secure, dependable, and therapeutically useful deep learning systems by filling in these research gaps and following these future objectives.",
      "year": 2023,
      "venue": "Mathematics",
      "authors": [
        "G. W. Muoka",
        "Ding Yi",
        "C. Ukwuoma",
        "Albert Mutale",
        "C. Ejiyi",
        "Asha Khamis Mzee",
        "Emmanuel S. A. Gyarteng",
        "Ali Alqahtani",
        "M. A. Al-antari"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/7b9b39cc2fe17aa5bb3d8a07ea4292d7e91938de",
      "pdf_url": "https://www.mdpi.com/2227-7390/11/20/4272/pdf?version=1697184975",
      "publication_date": "2023-10-13",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8aaa734440d37614943705009885ed316a349269",
      "title": "Enhancing Adversarial Attacks via Parameter Adaptive Adversarial Attack",
      "abstract": "In recent times, the swift evolution of adversarial attacks has captured widespread attention, particularly concerning their transferability and other performance attributes. These techniques are primarily executed at the sample level, frequently overlooking the intrinsic parameters of models. Such neglect suggests that the perturbations introduced in adversarial samples might have the potential for further reduction. Given the essence of adversarial attacks is to impair model integrity with minimal noise on original samples, exploring avenues to maximize the utility of such perturbations is imperative. Against this backdrop, we have delved into the complexities of adversarial attack algorithms, dissecting the adversarial process into two critical phases: the Directional Supervision Process (DSP) and the Directional Optimization Process (DOP). While DSP determines the direction of updates based on the current samples and model parameters, it has been observed that existing model parameters may not always be conducive to adversarial attacks. The impact of models on adversarial efficacy is often overlooked in current research, leading to the neglect of DSP. We propose that under certain conditions, fine-tuning model parameters can significantly enhance the quality of DSP. For the first time, we propose that under certain conditions, fine-tuning model parameters can significantly improve the quality of the DSP. We provide, for the first time, rigorous mathematical definitions and proofs for these conditions, and introduce multiple methods for fine-tuning model parameters within DSP. Our extensive experiments substantiate the effectiveness of the proposed P3A method. Our code is accessible at: https://anonymous.4open.science/r/P3A-A12C/",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhibo Jin",
        "Jiayu Zhang",
        "Zhiyu Zhu",
        "Chenyu Zhang",
        "Jiahao Huang",
        "Jianlong Zhou",
        "Fang Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8aaa734440d37614943705009885ed316a349269",
      "pdf_url": "",
      "publication_date": "2024-08-14",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8a778cd7d136a57b20c0864e643d4afe2bc83ffc",
      "title": "Towards Adversarial Attack on Vision-Language Pre-training Models",
      "abstract": "While vision-language pre-training model (VLP) has shown revolutionary improvements on various vision-language (V+L) tasks, the studies regarding its adversarial robustness remain largely unexplored. This paper studied the adversarial attack on popular VLP models and V+L tasks. First, we analyzed the performance of adversarial attacks under different settings. By examining the influence of different perturbed objects and attack targets, we concluded some key observations as guidance on both designing strong multimodal adversarial attack and constructing robust VLP models. Second, we proposed a novel multimodal attack method on the VLP models called Collaborative Multimodal Adversarial Attack (Co-Attack), which collectively carries out the attacks on the image modality and the text modality. Experimental results demonstrated that the proposed method achieves improved attack performances on different V+L downstream tasks and VLP models. The analysis observations and novel attack method hopefully provide new understanding into the adversarial robustness of VLP models, so as to contribute their safe and reliable deployment in more real-world scenarios.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Jiaming Zhang",
        "Qiaomin Yi",
        "Jitao Sang"
      ],
      "citation_count": 143,
      "url": "https://www.semanticscholar.org/paper/8a778cd7d136a57b20c0864e643d4afe2bc83ffc",
      "pdf_url": "https://arxiv.org/pdf/2206.09391",
      "publication_date": "2022-06-19",
      "keywords_matched": [
        "adversarial attack",
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d99f35a8df7e093fd8cdbba41dd703871171c33",
      "title": "Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity",
      "abstract": "Current adversarial attack research reveals the vulnerability of learning-based classifiers against carefully crafted perturbations. However, most existing attack methods have inherent limitations in cross-dataset generalization as they rely on a classification layer with a closed set of categories. Furthermore, the perturbations generated by these methods may appear in regions easily perceptible to the human visual system (HVS). To circumvent the former problem, we propose a novel algorithm that attacks semantic similarity on feature representations. In this way, we are able to fool classifiers without limiting attacks to a specific dataset. For imperceptibility, we introduce the low-frequency constraint to limit perturbations within high-frequency components, ensuring perceptual similarity between adversarial examples and originals. Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online platforms indicate that our attack can yield misleading and transferable adversarial examples across architectures and datasets. Additionally, visualization results and quantitative performance (in terms of four different metrics) show that the proposed algorithm generates more imperceptible perturbations than the state-of-the-art methods. Code is made available at https://github.com/LinQinLiang/SSAH-adversarial-attack.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Cheng Luo",
        "Qinliang Lin",
        "Weicheng Xie",
        "Bizhu Wu",
        "Jinheng Xie",
        "Linlin Shen"
      ],
      "citation_count": 145,
      "url": "https://www.semanticscholar.org/paper/6d99f35a8df7e093fd8cdbba41dd703871171c33",
      "pdf_url": "https://arxiv.org/pdf/2203.05151",
      "publication_date": "2022-03-10",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3495d45d3c3f61a1b45273cc3489ade3020fbcd2",
      "title": "Restricted Black-Box Adversarial Attack Against DeepFake Face Swapping",
      "abstract": "DeepFake face swapping presents a significant threat to online security and social media, which can replace the source face in an arbitrary photo/video with the target face of an entirely different person. In order to prevent this fraud, some researchers have begun to study the adversarial methods against DeepFake or face manipulation. However, existing works mainly focus on the white-box setting or the black-box setting driven by abundant queries, which severely limits the practical application of these methods. To tackle this problem, we introduce a practical adversarial attack that does not require any queries to the facial image forgery model. Our method is built on a substitute model based on face reconstruction and then transfers adversarial examples from the substitute model directly to inaccessible black-box DeepFake models. Specially, we propose the Transferable Cycle Adversary Generative Adversarial Network (TCA-GAN) to construct the adversarial perturbation for disrupting unknown DeepFake systems. We also present a novel post-regularization module for enhancing the transferability of generated adversarial examples. To comprehensively measure the effectiveness of our approaches, we construct a challenging baseline of DeepFake adversarial attacks for future development. Extensive experiments impressively show that the proposed adversarial attack method makes the visual quality of DeepFake face images plummet so that they are easier to be detected by humans and algorithms. Moreover, we demonstrate that the proposed algorithm can be generalized to offer face image protection against various face translation methods.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Junhao Dong",
        "Yuan Wang",
        "Jianhuang Lai",
        "Xiaohua Xie"
      ],
      "citation_count": 71,
      "url": "https://www.semanticscholar.org/paper/3495d45d3c3f61a1b45273cc3489ade3020fbcd2",
      "pdf_url": "http://arxiv.org/pdf/2204.12347",
      "publication_date": "2022-04-26",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85014c0fd19701850183e491075b1b7c56a53039",
      "title": "Query-Efficient Black-Box Adversarial Attack With Customized Iteration and Sampling",
      "abstract": "It is a challenging task to fool an image classifier based on deep neural networks under the black-box setting where the target model can only be queried. Among existing black-box attacks, transfer-based methods tend to overfit the substitute model on parameter settings. Decision-based methods have low query efficiency due to fixed sampling and greedy search strategy. To alleviate the above problems, we present a new framework for query-efficient black-box adversarial attack by bridging transfer-based and decision-based attacks. We reveal the relationship between current noise and variance of sampling, the monotonicity of noise compression, and the influence of transition function on the decision-based attack. Guided by the new framework, we propose a black-box adversarial attack named Customized Iteration and Sampling Attack (CISA). CISA estimates the distance from nearby decision boundary to set the stepsize, and uses a dual-direction iterative trajectory to find the intermediate adversarial example. Based on the intermediate adversarial example, CISA conducts customized sampling according to the noise sensitivity of each pixel to further compress noise, and relaxes the state transition function to achieve higher query efficiency. Extensive experiments demonstrate CISA\u2019s advantage in query efficiency of black-box adversarial attacks.",
      "year": 2022,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Yucheng Shi",
        "Yahong Han",
        "Q. Hu",
        "Yi Yang",
        "Qi Tian"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/85014c0fd19701850183e491075b1b7c56a53039",
      "pdf_url": "",
      "publication_date": "2022-04-25",
      "keywords_matched": [
        "adversarial attack",
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "454507e5139cfde1efa03fb03e6ade1d3d6dcbeb",
      "title": "Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition",
      "abstract": "Deep learning models have shown their vulnerability when dealing with adversarial attacks. Existing attacks almost perform on low-level instances, such as pixels and super-pixels, and rarely exploit semantic clues. For face recognition attacks, existing methods typically generate the l_p-norm perturbations on pixels, however, resulting in low attack transferability and high vulnerability to denoising defense models. In this work, instead of performing perturbations on the low-level pixels, we propose to generate attacks through perturbing on the high-level semantics to improve attack transferability. Specifically, a unified flexible framework, Adversarial Attributes (Adv-Attribute), is designed to generate inconspicuous and transferable attacks on face recognition, which crafts the adversarial noise and adds it into different attributes based on the guidance of the difference in face recognition features from the target. Moreover, the importance-aware attribute selection and the multi-objective optimization strategy are introduced to further ensure the balance of stealthiness and attacking strength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that the proposed Adv-Attribute method achieves the state-of-the-art attacking success rates while maintaining better visual effects against recent attack methods.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Shuai Jia",
        "Bangjie Yin",
        "Taiping Yao",
        "Shouhong Ding",
        "Chunhua Shen",
        "Xiaokang Yang",
        "Chao Ma"
      ],
      "citation_count": 70,
      "url": "https://www.semanticscholar.org/paper/454507e5139cfde1efa03fb03e6ade1d3d6dcbeb",
      "pdf_url": "https://arxiv.org/pdf/2210.06871",
      "publication_date": "2022-10-13",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "848db0c9842cc66379ec7975ca535adc8d90c92d",
      "title": "Transferable Adversarial Attack based on Integrated Gradients",
      "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can find highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seamlessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods. The code will available at https://github.com/yihuang2016/TAIG",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Y. Huang",
        "A. Kong"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/848db0c9842cc66379ec7975ca535adc8d90c92d",
      "pdf_url": "https://arxiv.org/pdf/2205.13152",
      "publication_date": "2022-05-26",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "29fbf409aaad5e3a0f3468c44637a140dc2c545b",
      "title": "Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability",
      "abstract": "The black-box adversarial attack has attracted impressive attention for its practical use in the field of deep learning security. Meanwhile, it is very challenging as there is no access to the network architecture or internal weights of the target model. Based on the hypothesis that if an example remains adversarial for multiple models, then it is more likely to transfer the attack capability to other models, the ensemble-based adversarial attack methods are efficient and widely used for black-box attacks. However, ways of ensemble attack are rather less investigated, and existing ensemble attacks simply fuse the outputs of all the models evenly. In this work, we treat the iterative ensemble attack as a stochastic gradient descent optimization process, in which the variance of the gradients on different models may lead to poor local optima. To this end, we propose a novel attack method called the stochastic variance reduced ensemble (SVRE) attack, which could reduce the gradient variance of the ensemble models and take full advantage of the ensemble attack. Empirical results on the standard ImageNet dataset demonstrate that the proposed method could boost the adversarial transferability and outperforms existing ensemble attacks significantly. Code is available at https://github.com/JHL-HUST/SVRE.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yifeng Xiong",
        "Jiadong Lin",
        "Min Zhang",
        "J. Hopcroft",
        "Kun He"
      ],
      "citation_count": 147,
      "url": "https://www.semanticscholar.org/paper/29fbf409aaad5e3a0f3468c44637a140dc2c545b",
      "pdf_url": "https://arxiv.org/pdf/2111.10752",
      "publication_date": "2021-11-21",
      "keywords_matched": [
        "adversarial attack",
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df415323353b9344154b966949b70e3bd7c4cad9",
      "title": "Towards Efficient Data Free Blackbox Adversarial Attack",
      "abstract": "Classic black-box adversarial attacks can take advantage of transferable adversarial examples generated by a similar substitute model to successfully fool the target model. However, these substitute models need to be trained by target models' training data, which is hard to acquire due to privacy or transmission reasons. Recognizing the limited availability of real data for adversarial queries, recent works proposed to train substitute models in a data-free black-box scenario. However, their generative adversarial networks (GANs) based framework suffers from the convergence failure and the model collapse, resulting in low efficiency. In this paper, by rethinking the collaborative relationship between the generator and the substitute model, we design a novel black-box attack framework. The proposed method can efficiently imitate the target model through a small number of queries and achieve high attack success rate. The comprehensive experiments over six datasets demonstrate the effectiveness of our method against the state-of-the-art attacks. Especially, we conduct both label-only and probability-only attacks on the Microsoft Azure online model, and achieve a 100% attack success rate with only 0.46% query budget of the SOTA method [49].",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "J Zhang",
        "Bo Li",
        "Jianghe Xu",
        "Shuang Wu",
        "Shouhong Ding",
        "Lei Zhang",
        "Chao Wu"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/df415323353b9344154b966949b70e3bd7c4cad9",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70822930be45d019caccda92f8a80e372a6011af",
      "title": "Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios",
      "abstract": "Visual detection is a key task in autonomous driving, and it serves as a crucial foundation for self-driving planning and control. Deep neural networks have achieved promising results in various visual tasks, but they are known to be vulnerable to adversarial attacks. A comprehensive understanding of deep visual detectors\u2019 vulnerability is required before people can improve their robustness. However, only a few adversarial attack/defense works have focused on object detection, and most of them employed only classification and/or localization losses, ignoring the objectness aspect. In this paper, we identify a serious objectness-related adversarial vulnerability in YOLO detectors and present an effective attack strategy targeting the objectness aspect of visual detection in autonomous vehicles. Furthermore, to address such vulnerability, we propose a new objectness-aware adversarial training approach for visual detection. Experiments show that the proposed attack targeting the objectness aspect is 45.17% and 43.50% more effective than those generated from classification and/or localization losses on the KITTI and COCO_traffic datasets, respectively. Also, the proposed adversarial defense approach can improve the detectors\u2019 robustness against objectness-oriented attacks by up to 21% and 12% mAP on KITTI and COCO_traffic, respectively.",
      "year": 2022,
      "venue": "2022 IEEE Intelligent Vehicles Symposium (IV)",
      "authors": [
        "Jung Im Choi",
        "Qing Tian"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/70822930be45d019caccda92f8a80e372a6011af",
      "pdf_url": "https://arxiv.org/pdf/2202.04781",
      "publication_date": "2022-02-10",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4736c30a5ab86c79e86b64477da75c05f845c156",
      "title": "Detection Tolerant Black-Box Adversarial Attack Against Automatic Modulation Classification With Deep Learning",
      "abstract": "Advances in adversarial attack and defense technologies will enhance the reliability of deep learning (DL) systems spirally. Most existing adversarial attack methods make overly ideal assumptions, which creates the illusion that the DL system can be attacked simply and has restricted the further improvement on DL systems. To perform practical adversarial attacks, a detection tolerant black-box adversarial-attack (DTBA) method against DL-based automatic modulation classification (AMC) is presented in this article. In the DTBA method, the local DL model as a substitution of the remote target DL model is trained first. The training dataset is generated by an attacker, labeled by the target model, and augmented by Jacobian transformation. Then, the conventional gradient attack method is utilized to generate adversarial attack examples toward the local DL model. Moreover, before launching attack to the target model, the local model estimates the misclassification probability of the perturbed examples in advance and deletes those invalid adversarial examples. Compared with related attack methods of different criteria on public datasets, the DTBA method can reduce the attack cost while increasing the rate of successful attack. Adversarial attack transferability of the proposed method on the target model has increased by more than 20%. The DTBA method will be suitable for launching flexible and effective black-box adversarial attacks against DL-based AMC systems.",
      "year": 2022,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "Peihan Qi",
        "Tao Jiang",
        "Lizhang Wang",
        "Xu Yuan",
        "Zan Li"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/4736c30a5ab86c79e86b64477da75c05f845c156",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "adversarial attack",
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46ac470bae546357ca83787422ff3a0bc3de42c2",
      "title": "AdvDrop: Adversarial Attack to DNNs by Dropping Information",
      "abstract": "Human can easily recognize visual objects with lost information: even losing most details with only contour reserved, e.g. cartoon. However, in terms of visual perception of Deep Neural Networks (DNNs), the ability for recognizing abstract objects (visual objects with lost information) is still a challenge. In this work, we investigate this issue from an adversarial viewpoint: will the performance of DNNs decrease even for the images only losing a little information? Towards this end, we propose a novel adversarial attack, named AdvDrop, which crafts adversarial examples by dropping existing information of images. Previously, most adversarial attacks add extra disturbing information on clean images explicitly. Opposite to previous works, our proposed work explores the adversarial robustness of DNN models in a novel perspective by dropping imperceptible de-tails to craft adversarial examples. We demonstrate the effectiveness of AdvDrop by extensive experiments, and show that this new type of adversarial examples is more difficult to be defended by current defense systems.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Ranjie Duan",
        "Yuefeng Chen",
        "Dantong Niu",
        "Yun Yang",
        "A. K. Qin",
        "Yuan He"
      ],
      "citation_count": 114,
      "url": "https://www.semanticscholar.org/paper/46ac470bae546357ca83787422ff3a0bc3de42c2",
      "pdf_url": "https://arxiv.org/pdf/2108.09034",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "132679d2fff8f8aed41bf8388a39b7f0aa30d4fd",
      "title": "Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models",
      "abstract": "Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks. Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models. However, existing work has mainly focused on investigating white-box attacks. In this paper, we present the first study to investigate the adversarial transferability of recent VLP models. We observe that existing methods exhibit much lower transferability, compared to the strong attack performance in white-box settings. The transferability degradation is partly caused by the under-utilization of cross-modal interactions. Particularly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, e.g., an image can be described in various natural languages. To this end, we propose a highly transferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance. Experimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across different VLP models on multiple downstream vision-language tasks. On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF to TCL by a large margin (at least 9.78% and up to 30.21%), compared to the state-of-the-art. Our code is available at https://github.com/Zoky-2020/SGA.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Dong Lu",
        "Zhiqiang Wang",
        "Teng Wang",
        "Weili Guan",
        "Hongchang Gao",
        "Feng Zheng"
      ],
      "citation_count": 106,
      "url": "https://www.semanticscholar.org/paper/132679d2fff8f8aed41bf8388a39b7f0aa30d4fd",
      "pdf_url": "https://arxiv.org/pdf/2307.14061",
      "publication_date": "2023-07-26",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e41b2e3c7c3a699cd1809e134cbdc1f20ebd2d71",
      "title": "Multi-Objective GAN-Based Adversarial Attack Technique for Modulation Classifiers",
      "abstract": "Deep learning is increasingly being used for many tasks in wireless communications, such as modulation classification. However, it has been shown to be vulnerable to adversarial attacks, which introduce specially crafted imperceptible perturbations, inducing models to make mistakes. This letter proposes an input-agnostic adversarial attack technique that is based on generative adversarial networks (GANs) and multi-task loss. Our results show that our technique reduces the accuracy of a modulation classifier more than a jamming attack and other adversarial attack techniques. Furthermore, it generates adversarial samples at least 335 times faster than the other techniques evaluated, which raises serious concerns about using deep learning-based modulation classifiers.",
      "year": 2022,
      "venue": "IEEE Communications Letters",
      "authors": [
        "Paulo Freitas de Araujo-Filho",
        "Georges Kaddoum",
        "Mohamed Naili",
        "E. T. Fapi",
        "Zhongwen Zhu"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/e41b2e3c7c3a699cd1809e134cbdc1f20ebd2d71",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "695685be105a0b9bf8b5cd50d80a6dff78204081",
      "title": "FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack",
      "abstract": "Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle\u2019s surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Donghua Wang",
        "Tingsong Jiang",
        "Jialiang Sun",
        "Weien Zhou",
        "Xiaoya Zhang",
        "Zhiqiang Gong",
        "W. Yao",
        "Xiaoqian Chen"
      ],
      "citation_count": 133,
      "url": "https://www.semanticscholar.org/paper/695685be105a0b9bf8b5cd50d80a6dff78204081",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20141/19900",
      "publication_date": "2021-09-15",
      "keywords_matched": [
        "adversarial attack",
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d1811ebd9e5802e292a0169e0faffad0a0de630",
      "title": "Boosting Adversarial Transferability via Gradient Relevance Attack",
      "abstract": "Plentiful adversarial attack researches have revealed the fragility of deep neural networks (DNNs), where the imperceptible perturbations can cause drastic changes in the output. Among the diverse types of attack methods, gradient-based attacks are powerful and easy to implement, arousing wide concern for the security problem of DNNs. However, under the black-box setting, the existing gradient-based attacks have much trouble in breaking through DNN models with defense technologies, especially those adversarially trained models. To make adversarial examples more transferable, in this paper, we explore the fluctuation phenomenon on the plus-minus sign of the adversarial perturbations\u2019 pixels during the generation of adversarial examples, and propose an ingenious Gradient Relevance Attack (GRA). Specifically, two gradient relevance frameworks are presented to better utilize the information in the neighbor-hood of the input, which can correct the update direction adaptively. Then we adjust the update step at each iteration with a decay indicator to counter the fluctuation. Experiment results on a subset of the ILSVRC 2012 validation set forcefully verify the effectiveness of GRA. Furthermore, the attack success rates of 68.7% and 64.8% on Tencent Cloud and Baidu AI Cloud further indicate that GRA can craft adversarial examples with the ability to transfer across both datasets and model architectures. Code is released at https://github.com/RYC-98/GRA.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Hegui Zhu",
        "Yuchen Ren",
        "Xiaoyan Sui",
        "Lianping Yang",
        "Wuming Jiang"
      ],
      "citation_count": 65,
      "url": "https://www.semanticscholar.org/paper/9d1811ebd9e5802e292a0169e0faffad0a0de630",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b4075b25bf107270fc589784aaa6d933c1ce918d",
      "title": "Meta Gradient Adversarial Attack",
      "abstract": "In recent years, research on adversarial attacks has be-come a hot spot. Although current literature on the transfer-based adversarial attack has achieved promising results for improving the transferability to unseen black-box models, it still leaves a long way to go. Inspired by the idea of meta-learning, this paper proposes a novel architecture called Meta Gradient Adversarial Attack (MGAA), which is plug-and-play and can be integrated with any existing gradient-based attack method for improving the cross-model transferability. Specifically, we randomly sample multiple models from a model zoo to compose different tasks and iteratively simulate a white-box attack and a black-box attack in each task. By narrowing the gap between the gradient directions in white-box and black-box attacks, the transfer-ability of adversarial examples on the black-box setting can be improved. Extensive experiments on the CIFAR10 and ImageNet datasets show that our architecture outperforms the state-of-the-art methods for both black-box and white-box attack settings.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Zheng Yuan",
        "J. Zhang",
        "Yunpei Jia",
        "Chuanqi Tan",
        "Tao Xue",
        "S. Shan"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/b4075b25bf107270fc589784aaa6d933c1ce918d",
      "pdf_url": "https://arxiv.org/pdf/2108.04204",
      "publication_date": "2021-08-09",
      "keywords_matched": [
        "adversarial attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "15e318ceb6b972098590fc3b7214fe49fb3e3de4",
      "title": "Pre-Trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness",
      "abstract": "Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks, and exhibit remarkable zero-shot generalization capability, while they are also vulnerable to impercep-tible adversarial examples. Existing works typically em-ploy adversarial training (fine-tuning) as a defense method against adversarial examples. However, direct application to the CLIP model may result in overfitting, compromising the model's capacity for generalization. In this paper, we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch, to enhance the model's zero-shot ad-versarial robustness. Specifically, PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model, aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outper-forms the state-of-the-art method, improving the top-1 ro-bust accuracy by an average of 4.99%. Furthermore, our approach consistently improves clean accuracy by an aver-age of 8.72%. Our code is available at here.1",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sibo Wang",
        "Jie Zhang",
        "Zheng Yuan",
        "Shiguang Shan"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/15e318ceb6b972098590fc3b7214fe49fb3e3de4",
      "pdf_url": "https://arxiv.org/pdf/2401.04350",
      "publication_date": "2024-01-09",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "51ed81d2a394ae395eb22285a7c57c03ae34f558",
      "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
      "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Kuofeng Gao",
        "Yang Bai",
        "Jiawang Bai",
        "Yong Yang",
        "Shu-Tao Xia"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/51ed81d2a394ae395eb22285a7c57c03ae34f558",
      "pdf_url": "",
      "publication_date": "2024-05-16",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8cbef71ed9a7f0f26611c8e989436f2b3da8633",
      "title": "Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective",
      "abstract": "Pretrained vision-language models (VLMs) like CLIP exhibit exceptional generalization across diverse downstream tasks. While recent studies reveal their vulnerability to adversarial attacks, research to date has primarily focused on enhancing the robustness of image encoders against image-based attacks, with defenses against text-based and multimodal attacks remaining largely unexplored. To this end, this work presents the first comprehensive study on improving the adversarial robustness of VLMs against attacks targeting image, text, and multimodal inputs. This is achieved by proposing multimodal contrastive adversarial training (MMCoA). Such an approach strengthens the robustness of both image and text encoders by aligning the clean text embeddings with adversarial image embeddings, and adversarial text embeddings with clean image embeddings. The robustness of the proposed MMCoA is examined against existing defense methods over image, text, and multimodal attacks on the CLIP model. Extensive experiments on 15 datasets across two tasks reveal the characteristics of different adversarial defense methods under distinct distribution shifts and dataset complexities across the three attack types. This paves the way for a unified framework of adversarial robustness against different modality attacks, opening up new possibilities for securing VLMs against multimodal attacks. The code is available at https://github.com/ElleZWQ/MMCoA.git.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Wanqi Zhou",
        "Shuanghao Bai",
        "Qibin Zhao",
        "Badong Chen"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/a8cbef71ed9a7f0f26611c8e989436f2b3da8633",
      "pdf_url": "",
      "publication_date": "2024-04-30",
      "keywords_matched": [
        "pretrained model vulnerability",
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ad84f102acd65a62306d3187d8bb8f39874c1e48",
      "title": "Adversarial Robustness Enhancement for Deep Learning-Based Soft Sensors: An Adversarial Training Strategy Using Historical Gradients and Domain Adaptation",
      "abstract": "Despite their high prediction accuracy, deep learning-based soft sensor (DLSS) models face challenges related to adversarial robustness against malicious adversarial attacks, which hinder their widespread deployment and safe application. Although adversarial training is the primary method for enhancing adversarial robustness, existing adversarial-training-based defense methods often struggle with accurately estimating transfer gradients and avoiding adversarial robust overfitting. To address these issues, we propose a novel adversarial training approach, namely domain-adaptive adversarial training (DAAT). DAAT comprises two stages: historical gradient-based adversarial attack (HGAA) and domain-adaptive training. In the first stage, HGAA incorporates historical gradient information into the iterative process of generating adversarial samples. It considers gradient similarity between iterative steps to stabilize the updating direction, resulting in improved transfer gradient estimation and stronger adversarial samples. In the second stage, a soft sensor domain-adaptive training model is developed to learn common features from adversarial and original samples through domain-adaptive training, thereby avoiding excessive leaning toward either side and enhancing the adversarial robustness of DLSS without robust overfitting. To demonstrate the effectiveness of DAAT, a DLSS model for crystal quality variables in silicon single-crystal growth manufacturing processes is used as a case study. Through DAAT, the DLSS achieves a balance between defense against adversarial samples and prediction accuracy on normal samples to some extent, offering an effective approach for enhancing the adversarial robustness of DLSS.",
      "year": 2024,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Runyuan Guo",
        "Qingyuan Chen",
        "Hanqing Liu",
        "Wenqing Wang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/ad84f102acd65a62306d3187d8bb8f39874c1e48",
      "pdf_url": "https://www.mdpi.com/1424-8220/24/12/3909/pdf?version=1718700396",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05c0b2d72a943dc0be26ef9e8fedd1380f2ff9ba",
      "title": "(Certified!!) Adversarial Robustness for Free!",
      "abstract": "In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "K. Dvijotham",
        "J. Z. Kolter"
      ],
      "citation_count": 168,
      "url": "https://www.semanticscholar.org/paper/05c0b2d72a943dc0be26ef9e8fedd1380f2ff9ba",
      "pdf_url": "http://arxiv.org/pdf/2206.10550",
      "publication_date": "2022-06-21",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16596dd03fa40ba278f9533ea9986982dcc81fb6",
      "title": "Understanding Zero-Shot Adversarial Robustness for Large-Scale Models",
      "abstract": "Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of \\emph{adapting large-scale models for zero-shot adversarial robustness}. We first identify two key factors during model adaption -- training losses and adaptation methods -- that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of over 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Chengzhi Mao",
        "Scott Geng",
        "Junfeng Yang",
        "Xin Eric Wang",
        "Carl Vondrick"
      ],
      "citation_count": 106,
      "url": "https://www.semanticscholar.org/paper/16596dd03fa40ba278f9533ea9986982dcc81fb6",
      "pdf_url": "http://arxiv.org/pdf/2212.07016",
      "publication_date": "2022-12-14",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6133a72012f69caf6876e6b135b93d870e441be",
      "title": "Language-Driven Anchors for Zero-Shot Adversarial Robustness",
      "abstract": "Deep Neural Networks (DNNs) are known to be susceptible to adversarial attacks. Previous researches mainly fo-cus on improving adversarial robustness in the fully super-vised setting, leaving the challenging domain of zero-shot adversarial robustness an open question. In this work, we investigate this domain by leveraging the recent advances in large vision-language models, such as CLIP, to introduce zero-shot adversarial robustness to DNNs. We pro-pose LAAT, a Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes the features of a text en-coder for each category as fixed anchors (normalized feature embeddings) for each category, which are then employed for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT aims to enhance the adversarial robustness of the image model on novel cate-gories. However, naively using text encoders leads to poor results. Through analysis, we identified the issue to be the high cosine similarity between text encoders. We then design an expansion algorithm and an alignment cross-entropy loss to alleviate the problem. Our experimental results demonstrated that LAAT significantly improves zero-shot adversarial robustness over state-of-the-art methods. LAAT has the potential to enhance adversarial robustness by large-scale multimodal models, especially when labeled data is unavailable during training. Code is available at https://github.com/LixiaoTHU/LAAT.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiao Li",
        "Wei Zhang",
        "Yining Liu",
        "Zhan Hu",
        "Bo Zhang",
        "Xiaolin Hu"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/e6133a72012f69caf6876e6b135b93d870e441be",
      "pdf_url": "https://arxiv.org/pdf/2301.13096",
      "publication_date": "2023-01-30",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0def290ae38abb4a04e35e0bcdc86b71d237f494",
      "title": "On the Adversarial Robustness of Vision Transformers",
      "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides a comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including ConvNeXt, and this observation also holds for certified robustness. Through frequency analysis and feature visualization, we summarize the following main observations contributing to the improved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that have spurious correlation, which helps explain why ViTs are less sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model learns high-frequency features and its robustness against different frequency-based perturbations. 2) Introducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs can improve classification accuracy but at the cost of adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs including activation function, layer norm, larger kernel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge the performance gap between ViTs and CNNs not only in terms of performance, but also certified and empirical adversarial robustness. Moreover, we show adversarial training is also applicable to ViT for training robust models, and sharpness-aware minimization can also help improve robustness, while pre-training with clean images on larger datasets does not significantly improve adversarial robustness.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Rulin Shao",
        "Zhouxing Shi",
        "Jinfeng Yi",
        "Pin-Yu Chen",
        "Cho-Jui Hsieh"
      ],
      "citation_count": 175,
      "url": "https://www.semanticscholar.org/paper/0def290ae38abb4a04e35e0bcdc86b71d237f494",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46b28c37f372b9152c1c8becf4122b7c15bf2d9f",
      "title": "Evaluating the Adversarial Robustness of Adaptive Test-time Defenses",
      "abstract": "Adaptive defenses, which optimize at test time, promise to improve adversarial robustness. We categorize such adaptive test-time defenses, explain their potential benefits and drawbacks, and evaluate a representative variety of the latest adaptive defenses for image classification. Unfortunately, none significantly improve upon static defenses when subjected to our careful case study evaluation. Some even weaken the underlying static model while simultaneously increasing inference computation. While these results are disappointing, we still believe that adaptive test-time defenses are a promising avenue of research and, as such, we provide recommendations for their thorough evaluation. We extend the checklist of Carlini et al. (2019) by providing concrete steps specific to adaptive defenses.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Francesco Croce",
        "Sven Gowal",
        "T. Brunner",
        "Evan Shelhamer",
        "Matthias Hein",
        "Ali Taylan Cemgil"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/46b28c37f372b9152c1c8becf4122b7c15bf2d9f",
      "pdf_url": "",
      "publication_date": "2022-02-28",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d21ff9eb99fb2482461ef6269f89d4350cb9450",
      "title": "Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack",
      "abstract": "Defense models against adversarial attacks have grown significantly, but the lack of practical evaluation methods has hindered progress. Evaluation can be defined as looking for defense models' lower bound of robustness given a budget number of iterations and a test dataset. A practical evaluation method should be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and reliable (i.e., approaching the lower bound of robustness). Towards this target, we propose a parameter-free Adaptive Auto Attack (A3) evaluation method which addresses the efficiency and reliability in a test-time-training fashion. Specifically, by observing that adversarial examples to a specific defense model follow some regularities in their starting points, we design an Adaptive Direction Initialization strategy to speed up the evaluation. Furthermore, to approach the lower bound of robustness under the budget number of iterations, we propose an online statistics-based discarding strategy that automatically identifies and abandons hard-to-attack images. Extensive experiments on nearly 50 widely-used defense models demonstrate the effectiveness of our A3. By consuming much fewer iterations than existing methods, i.e., 1/10 on average (10\u00d7 speed up), we achieve lower robust accuracy in all cases. Notably, we won first place out of 1681 teams in CVPR 2021 White-box Adversarial Attacks on Defense Models competitions with this method. Code is available at: https://github.com/liuye6666/adaptive_auto_attack",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Ye Liu",
        "Yaya Cheng",
        "Lianli Gao",
        "Xianglong Liu",
        "Qilong Zhang",
        "Jingkuan Song"
      ],
      "citation_count": 73,
      "url": "https://www.semanticscholar.org/paper/8d21ff9eb99fb2482461ef6269f89d4350cb9450",
      "pdf_url": "https://arxiv.org/pdf/2203.05154",
      "publication_date": "2022-03-10",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d3e6cc6e36afc9820cd242799e6563c0370c2fb",
      "title": "Out-of-Distribution Detection in Dermatology Using Input Perturbation and Subset Scanning",
      "abstract": "Recent advances in deep learning have led to breakthroughs in the development of automated skin disease classification. As we observe an increasing interest in these models in the dermatology space, it is crucial to address aspects such as the robustness towards input data distribution shifts. Current models tend to make incorrect inferences for test samples from different hardware devices and clinical settings or unknown disease samples, which are out-of-distribution (OOD) from the training samples. To this end, we propose a simple yet effective approach that detects these OOD samples prior to making any decision. The detection is performed via scanning in the latent space representation (e.g., activations of the inner layers of any pre-trained skin disease classifier). The input samples are also perturbed to maximise divergence of OOD samples. We validate our OOD detection approach in two use cases: 1) identify samples collected from different protocols, and 2) detect samples from unknown disease classes. Our experiments yield competitive performance across multiple datasets for both use cases. As most skin datasets are reported to suffer from bias in skin tone distribution, we further evaluate the fairness of these OOD detectors across different skin tones.",
      "year": 2021,
      "venue": "IEEE International Symposium on Biomedical Imaging",
      "authors": [
        "Hannah Kim",
        "G. Tadesse",
        "C. Cintas",
        "Skyler Speakman",
        "Kush R. Varshney"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/8d3e6cc6e36afc9820cd242799e6563c0370c2fb",
      "pdf_url": "https://arxiv.org/pdf/2105.11160",
      "publication_date": "2021-05-24",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d8ce779bafc4a930230aa36721f93af7e8ca2ae1",
      "title": "That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency",
      "abstract": null,
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yanmao Man",
        "Raymond Muller",
        "Ming Li",
        "Z. B. Celik",
        "Ryan M. Gerdes"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/d8ce779bafc4a930230aa36721f93af7e8ca2ae1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd41106b0ccfcba489277f8ac707c1ba85c5b527",
      "title": "AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference",
      "abstract": "In recent years, Deep Neural Networks (DNNs) have become increasingly integral to IoT-based environments, enabling realtime visual computing. However, the limited computational capacity of these devices has motivated the adoption of collaborative DNN inference, where the IoT device offloads part of the inference-related computation to a remote server. Such offloading often requires dynamic DNN partitioning information to be exchanged among the participants over an unsecured network or via relays/hops, leading to novel privacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial variational autoencoder (VAE)-based misclassification attack, leveraging classifiers to detect model information and a VAE to generate untraceable manipulated samples, specifically designed to compromise the collaborative inference process. AdVAR-DNN attack uses the sensitive information exchange vulnerability of collaborative DNN inference and is black-box in nature in terms of having no prior knowledge about the DNN model and how it is partitioned. Our evaluation using the most popular object classification DNNs on the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms of high attack success rate with little to no probability of detection.",
      "year": 2025,
      "venue": "IEEE Conference on Local Computer Networks",
      "authors": [
        "S. Yousefi",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/cd41106b0ccfcba489277f8ac707c1ba85c5b527",
      "pdf_url": "",
      "publication_date": "2025-08-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4c1467fd46e443835a5fd3c202ad2befdef140cc",
      "title": "Rapid Misclassification Sample Generation Attack on Deep Neural Network",
      "abstract": "Deep neural networks (DNNs) provide good performance for machine learning tasks such as image recognition and object recognition. However, DNNs are vulnerable to an adversarial example. An adversarial example is an attack sample that causes the neural network to recognize it incorrectly by adding minimal noise to the original sample. However, the disadvantage is that it takes a long time to generate such an adversarial example. Therefore, in some cases, an attack may be necessary that quickly causes the neural network to recognize it incorrectly. In this paper, we propose a fast misclassification sample that can rapidly attack neural networks. The proposed method does not consider the distortion of the original sample when adding noise. We used MNIST and CIFAR10 as experimental data and Tensorflow as a machine learning library. Experimental results show that the fast misclassification sample generated by the proposed method can be generated with 50% and 80% reduced number of iterations for MNIST and CIFAR10, respectively, compared to the conventional Carlini method, and has 100% attack rate.",
      "year": 2020,
      "venue": "Jouranl of Information and Security",
      "authors": [
        "Hyung-Min Kwon",
        "Sangjun Park",
        "Yongchul Kim"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4c1467fd46e443835a5fd3c202ad2befdef140cc",
      "pdf_url": "",
      "publication_date": "2020-06-30",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "580d6d603b0bbb31f8a931d37e20898a65f2f25f",
      "title": "FAWA: Fast Adversarial Watermark Attack",
      "abstract": "Recently, adversarial attacks have shown to lead the state-of-the-art deep neural networks (DNNs) to misclassification. However, most adversarial attacks are generated according to whether they are perceptual to human visual system, measured by geometric metrics such as the <inline-formula><tex-math notation=\"LaTeX\">$\\ell _2$</tex-math><alternatives><mml:math><mml:msub><mml:mi>\u2113</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href=\"jiang-ieq1-3065172.gif\"/></alternatives></inline-formula>-norm, which ignores the common watermarks in cyber-physical systems. In this article, we propose a fast adversarial watermark attack (FAWA) method based on fast differential evolution technique, which optimally superimposes a watermark on an image to fool DNNs. We also attempt to explain the reason why the attack is successful and propose two hypotheses on the vulnerability of DNN classifiers and the influence of the watermark attack on higher-layer features extraction respectively. In addition, we propose two countermeasure methods against FAWA based on random rotation and median filtering respectively. Experimental results show that our method achieves 41.3 percent success rate in fooling VGG-16 and have good transferability. Our approach is also shown to be effective in deceiving deep learning as a service (DLaaS) systems as well as the physical world. The proposed FAWA, hypotheses, and the countermeasure methods, provide a timely help for DNN designers to gain some knowledge of model vulnerability while designing DNN classifiers and related DLaaS applications.",
      "year": 2024,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Hao Jiang",
        "Jintao Yang",
        "Guang Hua",
        "Lixia Li",
        "Ying Wang",
        "Shenghui Tu",
        "Song Xia"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/580d6d603b0bbb31f8a931d37e20898a65f2f25f",
      "pdf_url": "",
      "publication_date": "2024-02-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2f185b884a065801cd8a1c55be2a1cb956e447d",
      "title": "Fooling Aerial Detectors by Background Attack via Dual-Adversarial-Induced Error Identification",
      "abstract": "Recent developments in adversarial attack have witnessed the success of background attack against object detectors. However, most existing methods attack detectors by luring targets into background. Therefore, an innovative background attack framework via dual-adversarial-induced error identification (BADEI) is proposed to attack detectors by deceiving background as targets, as well as deceiving targets as background, where the attack performance can be greatly enhanced by these two kinds of induced error identification. Specifically, a mechanism that generates the adversarial background is proposed to result in dual error detection, where the background can conceal the specified targets and cause the misclassification of the adversarial pattern in the background as a specific category. Moreover, an unoccluded training strategy (UTS) that leverages the target mask of an image is introduced to strategically place adaptive adversarial background beneath the targets while optimizing and updating the pixel values of the background outside the target region, which can enhance attack effectiveness for adversarial background, significantly degrade the targets\u2019 average accuracy, and enhance the robustness of background. Finally, a dual deceptive loss function (D2LF) is carefully formulated to generate false negatives (FNs) and false positives (FPs) to achieve untargeted attacks for hiding objects as well as targeted attacks for erroneously recognizing objects. Extensive experiments and comparative analysis of various victim network models on two datasets (including the dataset for object detection in aerial images (DOTA) and RSOD dataset) confirm that the proposed framework exhibits superior performance over the state-of-the-art methods in both digital and physical scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Xiaofei Wang",
        "Shaohui Mei",
        "Jiawei Lian",
        "Yingjie Lu"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/c2f185b884a065801cd8a1c55be2a1cb956e447d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c7ca55fb0d65bb19501db2ac307b94540ee6e506",
      "title": "URL based phishing attack detection using BiLSTM-gated highway attention block convolutional neural network",
      "abstract": null,
      "year": 2024,
      "venue": "Multimedia tools and applications",
      "authors": [
        "Manika Nanda",
        "Shivani Goel"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c7ca55fb0d65bb19501db2ac307b94540ee6e506",
      "pdf_url": "",
      "publication_date": "2024-01-31",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82664094523e5b3efa5f073b6a10c7131d73d1fe",
      "title": "Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent Diffusion Model",
      "abstract": "Adversarial attacks involve adding perturbations to the source image to cause misclassification by the target model, which demonstrates the potential of attacking face recognition models. Existing adversarial face image generation methods still can\u2019t achieve satisfactory performance because of low transferability and high detectability. In this paper, we propose a unified framework Adv-Diffusion that can generate imperceptible adversarial identity perturbations in the latent space but not the raw pixel space, which utilizes strong inpainting capabilities of the latent diffusion model to generate realistic adversarial images. Specifically, we propose the identity-sensitive conditioned diffusion generative model to generate semantic perturbations in the surroundings. The designed adaptive strength-based adversarial perturbation algorithm can ensure both attack transferability and stealthiness. Extensive qualitative and quantitative experiments on the public FFHQ and CelebA-HQ datasets prove the proposed method achieves superior performance compared with the state-of-the-art methods without an extra generative model training process. The source code is available at https://github.com/kopper-xdu/Adv-Diffusion.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Decheng Liu",
        "Xijun Wang",
        "Chunlei Peng",
        "Nannan Wang",
        "Ruimin Hu",
        "Xinbo Gao"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/82664094523e5b3efa5f073b6a10c7131d73d1fe",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "misclassification attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "38661257a3f3e1150c41caeb21a5e025bad199b9",
      "title": "A Fine-Grained Detection Network Model for Soldier Targets Adopting Attack Action",
      "abstract": "Owing to its ability to provide more accurate and detailed battlefield situational information, fine-grained detection research on soldier targets is of significant importance for military decision-making and firepower threat assessment. To address the issues of low detection accuracy and inaccurate classification in the fine-grained detection of soldier targets, we propose a fine-gain soldier target detection model based on the improved YOLOv8 (You Only Look Once v8). First, we developed a multi-branch feature fusion module to effectively fuse multi-scale feature information and used a dynamic deformable attention mechanism to help the detection model focus on key areas in deep-level features. Second, we proposed a decoupled lightweight dynamic head to extract the position and category information of soldier targets separately, effectively solving the problem of misclassification of soldier targets\u2019 attack actions under different poses. Finally, we used the Inner Minimum Points Distance Intersection over Union (Inner-MPDIoU) to further improve the convergence speed and accuracy of the network model. The proposed improvements are evaluated through comparative experiments conducted in published twenty-six test groups, and the effectiveness of the proposed method is demonstrated. Compared with the original model, our method achieved a detection precision of 78.9%, a 6.91% improvement; the mAP@50 (mean Average Precision at 50) was 79.6%, a 3.51% increase; and an mAP@50-95 of 63.8%, a gain of 5.28%. The proposed method achieves high precision and recall while reducing the computational complexity of the model, thereby enhancing its efficiency and robustness for fine-grained soldier target detection.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Yu You",
        "Jianzhong Wang",
        "Zibo Yu",
        "Yong Sun",
        "Yiguo Peng",
        "Sheng Zhang",
        "Shaobo Bian",
        "Endi Wang",
        "Weichao Wu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/38661257a3f3e1150c41caeb21a5e025bad199b9",
      "pdf_url": "https://doi.org/10.1109/access.2024.3436709",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "67a98305a5e022b7ad41c25dc6c1c1e27048a0cd",
      "title": "Attacking-Distance-Aware Attack: Semi-targeted Model Poisoning on Federated Learning",
      "abstract": "Existing model poisoning attacks on federated learning (FL) assume that an adversary has access to the full data distribution. In reality, an adversary usually has limited prior knowledge about clients' data. A poorly chosen target class renders an attack less effective. This article considers a semitargeted situation where the source class is predetermined but the target class is not. The goal is to cause the misclassification of the global classifier on data from the source class. Approaches such as label flipping have been used to inject malicious parameters into FL. Nevertheless, it has been shown that their performances are usually class sensitive, varying with different target classes. Typically, an attack becomes less effective when shifting to a different target class. To overcome this challenge, we propose the attacking-distance-aware attack (ADA) that enhances model poisoning in FL by finding the optimized target class in the feature space. ADA deduces pairwise class attacking distances using a fast layer gradient method. Extensive evaluations were performed on five benchmark image classification tasks and three model architectures using varying attacking frequencies. Furthermore, ADA's robustness to conventional defenses of Byzantine-robust aggregation and differential privacy was validated. The results showed that ADA succeeded in increasing attack performance to 2.8 times in the most challenging case with an attacking frequency of 0.01 and bypassed existing defenses, where differential privacy that was the most effective defense still could not reduce the attack performance to below 50%.",
      "year": 2024,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Yuwei Sun",
        "H. Ochiai",
        "Jun Sakuma"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/67a98305a5e022b7ad41c25dc6c1c1e27048a0cd",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9078688/9184921/10136777.pdf",
      "publication_date": "2024-02-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "904e9df114222dd2d7ce80f4a4ae899680a40f39",
      "title": "SAR-PATT: A Physical Adversarial Attack for SAR Image Automatic Target Recognition",
      "abstract": "Deep neural network-based synthetic aperture radar (SAR) automatic target recognition (ATR) systems are susceptible to attack by adversarial examples, which leads to misclassification by the SAR ATR system, resulting in theoretical model robustness problems and security problems in practice. Inspired by optical images, current SAR ATR adversarial example generation is performed in the image domain. However, the imaging principle of SAR images is based on the imaging of the echo signals interacting between the SAR and objects. Generating adversarial examples only in the image domain cannot change the physical world to achieve adversarial attacks. To solve these problems, this article proposes a framework for generating SAR adversarial examples in a 3D physical scene. First, adversarial attacks are implemented in the 2D image space, and the perturbation in the image space is converted into simulated rays that constitute SAR images through backpropagation optimization methods. The mapping between the simulated rays constituting SAR images and the 3D model is established through coordinate transformation, and point correspondence to triangular faces and intensity values to texture parameters are established. Thus, the simulated rays constituting SAR images are mapped to the 3D model, and the perturbation in the 2D image space is converted back to the 3D physical space to obtain the position and intensity of the perturbation in the 3D physical space, thereby achieving physical adversarial attacks. The experimental results show that our attack method can effectively perform SAR adversarial attacks in the physical world. In the digital world, we achieved an average fooling rate of up to 99.02% for three objects in six classification networks. In the physical world, we achieved an average fooling rate of up to 97.87% for these objects, with a certain degree of transferability across the six different network architectures. To the best of our knowledge, this is the first work to implement physical attacks in a full physical simulation condition. Our research establishes a theoretical foundation for the future concealment of SAR targets in practical settings and offers valuable insights for enhancing the attack and defense capabilities of subsequent DNNs in SAR ATR systems.",
      "year": 2024,
      "venue": "Remote Sensing",
      "authors": [
        "Binyan Luo",
        "Hang Cao",
        "Jiahao Cui",
        "Xun Lv",
        "Jinqiang He",
        "Haifeng Li",
        "Chengli Peng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/904e9df114222dd2d7ce80f4a4ae899680a40f39",
      "pdf_url": "",
      "publication_date": "2024-12-25",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1075ca7787994fbdf588bc34f4496d503ba5fa83",
      "title": "Enhancing Security in Real-Time Video Surveillance: A Deep Learning-Based Remedial Approach for Adversarial Attack Mitigation",
      "abstract": "This paper introduces an innovative methodology to disrupt deep-learning (DL) surveillance systems by implementing an adversarial framework strategy, inducing misclassification in live video objects and extending attacks to real-time models. Focusing on the vulnerability of image-categorization models, the study evaluates the effectiveness of face mask surveillance against adversarial threats. A real-time system, employing the ShuffleNet V1 transfer-learning algorithm, was trained on a Kaggle dataset for face mask detection accuracy. Using a white-box Fast Gradient Sign Method (FGSM) attack with epsilon at 0.13, the study successfully generated adversarial frames, deceiving the face mask detection system and prompting unintended video predictions. The findings highlight the risks posed by adversarial attacks on critical video surveillance systems, specifically those designed for face mask detection. The paper emphasizes the need for proactive measures to safeguard these systems before real-world deployment, crucial for ensuring their robustness and reliability in the face of potential adversarial threats.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Gyana Ranjana Panigrahi",
        "Dr. Prabira Kumar Sethy",
        "S. Behera",
        "Manoj Gupta",
        "Farhan A. Alenizi",
        "Aziz Nanthaamornphong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1075ca7787994fbdf588bc34f4496d503ba5fa83",
      "pdf_url": "https://doi.org/10.1109/access.2024.3418614",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6e01162b8af27f9c66340f1b189ec3cd2b4a5940",
      "title": "FLDATN: Black\u2010Box Attack for Face Liveness Detection Based on Adversarial Transformation Network",
      "abstract": "Aiming at the shortcomings of the current face liveness detection attack methods in the low generation speed of adversarial examples and the implementation of white\u2010box attacks, a novel black\u2010box attack method for face liveness detection named as FLDATN is proposed based on adversarial transformation network (ATN). In FLDATN, a convolutional block attention module (CBAM) is used to improve the generalization ability of adversarial examples, and the misclassification loss function based on feature similarity is defined. Experiments and analysis on the Oulu\u2010NPU dataset show that the adversarial examples generated by the FLDATN have a good black\u2010box attack effect on the task of face liveness detection and can achieve better generalization performance than the traditional methods. In addition, since FLDATN does not need to perform multiple gradient calculations for each image, it can significantly improve the generation speed of the adversarial examples.",
      "year": 2024,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Yali Peng",
        "Jianbo Liu",
        "Min Long",
        "Fei Peng"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6e01162b8af27f9c66340f1b189ec3cd2b4a5940",
      "pdf_url": "https://doi.org/10.1155/2024/8436216",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82b1dd0fa0eebb155fc348a55b115f6fc101b492",
      "title": "Deep Learning Security Breach by Evolutionary Universal Perturbation Attack (EUPA)",
      "abstract": "The potential for sabotaging deep convolutions neural networks classifiers by universal perturbation attack (UPA) has proved itself as an effective threat to fool deep learning models in sensitive applications such as autonomous vehicles, clinical diagnosis, face recognition, and so on. The prospective application of UPA is for adversarial training of deep convolutional networks against the attacks. Although evolutionary algorithms have already shown their tremendous ability in solving nonconvex complex problems, the literature has limited exploration of evolutionary techniques and strategies for UPA, thus, it needs to be explored on evolutionary algorithms to minimize the magnitude and number of perturbation pixels while maximizing the misclassification of maximum data samples. In this research. This work focuses on utilizing an integer coded genetic algorithm within an evolutionary framework to evolve the UPA. The evolutionary UPA has been structured, analyzed, and compared for two evolutionary optimization structures: 1) constrained single-objective evolutionary UPA; and 2) Pareto double-objective evolutionary UPA. The efficiency of the methodology is analyzed on GoogleNet convolution neural network for its effectiveness on the Imagenet dataset. The results show that under the same experimental conditions, the constrained single objective technique outperforms the Pareto double objective one, and manages a successful breach on a deep network wherein the average detection score falls to $0.446429$. It is observed that besides the minimization of the detection rate score, the constraint of invisibility of noise is much more effective rather than having a conflicting objective of noise power minimization.",
      "year": 2024,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Neeraj Gupta",
        "Mahdi Khosravy",
        "Antoine Pasquali",
        "Olaf Witkowski"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/82b1dd0fa0eebb155fc348a55b115f6fc101b492",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "142d08ca8a38d828e6291f74fc7dea0acffbb089",
      "title": "Adversarial Defense on Harmony: Reverse Attack for Robust AI Models Against Adversarial Attacks",
      "abstract": "Deep neural networks (DNNs) are crucial in safety-critical applications but vulnerable to adversarial attacks, where subtle perturbations cause misclassification. Existing defense mechanisms struggle with small perturbations and face accuracy-robustness trade-offs. This study introduces the \u201cReverse Attack\u201d method to address these challenges. Our approach uniquely reconstructs and classifies images by applying perturbations opposite to the attack direction, using a complementary \u201cRevenant\u201d classifier to maintain original image accuracy. The proposed method significantly outperforms existing strategies, maintaining clean image accuracy with only a 2.92% decrease while achieving over 70% robust accuracy against all benchmarked adversarial attacks. This contrasts with current mechanisms, which typically suffer an 18% reduction in clean image accuracy and only 36% robustness against adversarial examples. We evaluate our method on the CIFAR-10 dataset using ResNet50, testing against various attacks including PGD and components of Auto Attack. Although our approach incurs additional computational costs during reconstruction, our method represents a significant advancement in robust defenses against adversarial attacks while preserving clean input performance. This balanced approach paves the way for more reliable DNNs in critical applications. Future work will focus on optimization and exploring applicability to larger datasets and complex architectures.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Yebon Kim",
        "Jinhyo Jung",
        "Hyunjun Kim",
        "Hwisoo So",
        "Yohan Ko",
        "Aviral Shrivastava",
        "Kyoungwoo Lee",
        "Uiwon Hwang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/142d08ca8a38d828e6291f74fc7dea0acffbb089",
      "pdf_url": "https://doi.org/10.1109/access.2024.3505215",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d5284604be4bee5ace7322f49c25c540ec87e77a",
      "title": "Adversarial Attack Defense Techniques: A Study of Defensive Distillation and Adversarial Re-Training on CIFAR-10 and MNIST",
      "abstract": "Adversarial attacks pose a significant challenge to the reliability of machine learning models by introducing imperceptible perturbations that lead to misclassification. This study evaluates the effectiveness of adversarial Re-training on an MNIST model, which achieved a clean accuracy of 99.5% and an adversarial accuracy of 90%, as well as the effectiveness of Defensive Distillation on a CIFAR-10 model, achieving a clean accuracy of 98.83% and an adversarial accuracy of 81.3% against PGD attacks. These results demonstrate that adversarial retraining with FGSM effectively improves robustness for simpler datasets like MNIST, achieving 90% adversarial accuracy post-training. In contrast, Defensive Distillation shows promise for complex datasets like CIFAR-10 due to its ability to generate robust decision boundaries. Compared to prior works that reported varying levels of success for Defensive Distillation across different domains, our results align with findings that dataset complexity significantly influences the efficacy of defense strategies. This analysis underscores the necessity of tailoring defense techniques to dataset characteristics and attack types. The potential impacts of these techniques on end users include enhanced security and reliability in applications that rely on machine learning models, such as biometric authentication, autonomous systems, and financial fraud detection. Defensive strategies like FGSM retraining and Defensive Distillation can ensure more consistent performance under adversarial conditions, reducing vulnerabil-ities and instilling user confidence in AI-driven systems across both simple and complex domains.",
      "year": 2024,
      "venue": "International Conferences on Computing Advancements",
      "authors": [
        "Tahir Elgamrani",
        "Reda Elgaf",
        "Yousra Chtouki"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d5284604be4bee5ace7322f49c25c540ec87e77a",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8ae920111435a7db8da360c654c771c53f57c69a",
      "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
      "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
      "year": 2022,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Artem Vazhentsev",
        "Gleb Kuzmin",
        "Artem Shelmanov",
        "Akim Tsvigun",
        "Evgenii Tsymbalov",
        "Kirill Fedyanin",
        "Maxim Panov",
        "A. Panchenko",
        "Gleb Gusev",
        "M. Burtsev",
        "Manvel Avetisian",
        "L. Zhukov"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/8ae920111435a7db8da360c654c771c53f57c69a",
      "pdf_url": "https://aclanthology.org/2022.acl-long.566.pdf",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2de179082e2e7d8d5ecfe6c49ed449f6c315932",
      "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
      "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN\u2019s latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Shengwei An",
        "Y. Yao",
        "Qiuling Xu",
        "Shiqing Ma",
        "Guanhong Tao",
        "Siyuan Cheng",
        "Kaiyuan Zhang",
        "Yingqi Liu",
        "Guangyu Shen",
        "Ian Kelk",
        "Xiangyu Zhang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/b2de179082e2e7d8d5ecfe6c49ed449f6c315932",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b3c9d0d57d3ae053a7bc3ee76361da95bc28262c",
      "title": "One-Pixel Attack for Continuous-Variable Quantum Key Distribution Systems",
      "abstract": "Deep neural networks (DNNs) have been employed in continuous-variable quantum key distribution (CV-QKD) systems as attacking detection portions of defense countermeasures. However, the vulnerability of DNNs leaves security loopholes for hacking attacks, for example, adversarial attacks. In this paper, we propose to implement the one-pixel attack in CV-QKD attack detection networks and accomplish the misclassification on a minimum perturbation. This approach is based on the differential evolution, which makes our attack algorithm fool multiple DNNs with the minimal inner information of target networks. The simulation and experimental results show that, in four different CV-QKD detection networks, 52.8%, 26.4%, 21.2%, and 23.8% of the input data can be perturbed to another class by modifying just one feature, the same as one pixel for an image. We carry out this success rate in the context of the original accuracy reaching up to nearly 99% on average. Further, by enlarging the number of perturbed features, the success rate can be raised to a satisfactory higher level of about 80%. According to our experimental results, most of the CV-QKD detection networks can be deceived by launching one-pixel attacks.",
      "year": 2023,
      "venue": "Photonics",
      "authors": [
        "Yushen Guo",
        "Pengzhi Yin",
        "Duan Huang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b3c9d0d57d3ae053a7bc3ee76361da95bc28262c",
      "pdf_url": "https://www.mdpi.com/2304-6732/10/2/129/pdf?version=1676430172",
      "publication_date": "2023-01-27",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2778dd6ba81747934db4c08d3b24408c2b078e7",
      "title": "Adversarial Attack on Yolov5 for Traffic and Road Sign Detection",
      "abstract": "This paper implements and investigates popular adversarial attacks on the YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the YOLOv5 to adversarial attacks in the context of traffic and road sign detection. The paper investigates the impact of different types of attacks, including the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS), the Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&W) attack, the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD) attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on the accuracy of YOLOv5 in detecting traffic and road signs. The results show that YOLOv5 is susceptible to these attacks, with misclassification rates increasing as the magnitude of the perturbations increases. We also explain the results using saliency maps. The findings of this paper have important implications for the safety and reliability of object detection algorithms used in traffic and transportation systems, highlighting the need for more robust and secure models to ensure their effectiveness in real-world applications.",
      "year": 2023,
      "venue": "2024 4th International Conference on Applied Artificial Intelligence (ICAPAI)",
      "authors": [
        "Sanyam Jain"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a2778dd6ba81747934db4c08d3b24408c2b078e7",
      "pdf_url": "http://arxiv.org/pdf/2306.06071",
      "publication_date": "2023-05-27",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "06993f2d43cde4c816160ac6f3ea566cdb6427e7",
      "title": "Precision Strike: Targeted Misclassification of Accelerated CNNs with a Single Clock Glitch",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Arsalan Ali Malik",
        "Furkan Aydin",
        "Aydin Aysu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/06993f2d43cde4c816160ac6f3ea566cdb6427e7",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack",
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ccbf2eec849c5c461eb8d96197f6b36d55e65612",
      "title": "Adversarial Threats to AI-Driven Systems: Exploring the Attack Surface of Machine Learning Models and Countermeasures",
      "abstract": "Adversarial attacks pose a critical threat to the reliability of AI-driven systems, exploiting vulnerabilities at the data, model, and deployment levels. This study employs a quantitative analysis using the CIFAR-10 Adversarial Examples Dataset from IBM\u2019s Adversarial Robustness Toolbox and the MITRE ATLAS AI Model Vulnerabilities Dataset to assess attack success rates and attack surface exposure. A convolutional neural network (CNN) classifier was evaluated against Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Carlini & Wagner (C&W) attacks, yielding misclassification rates of 42.2%, 65.5%, and 86.8%, respectively. Statistical analysis using the Chi-Square Goodness-of-Fit Test (p < 0.001) confirmed a disproportionate targeting of model-level vulnerabilities (53.6%). These vulnerabilities pose severe risks across real-world AI applications. In cybersecurity, adversarial perturbations compromise intrusion detection systems, malware classification models, and spam filters, allowing cybercriminals to bypass AI-driven defenses. In autonomous vehicles, subtle adversarial modifications to traffic signs and road patterns can mislead AI-based navigation, increasing the likelihood of accidents. Similarly, in financial systems, adversarial attacks deceive fraud detection models, enabling unauthorized transactions and financial fraud. Countermeasure evaluation demonstrated that adversarial training provided the highest robustness gain (23.29%), while detection algorithms were least effective (15.34%). To enhance AI security, hybrid defense mechanisms integrating adversarial training with real-time anomaly detection should be prioritized, and standardized evaluation benchmarks should be established for AI security testing. These findings emphasize the necessity of hybrid AI security frameworks that combine adversarial training with real-time anomaly detection. Moreover, standardized security benchmarks should be established to ensure resilience across industries, particularly in high-stakes AI applications.",
      "year": 2025,
      "venue": "Journal of Engineering Research and Reports",
      "authors": [
        "Abayomi Titilola Olutimehin",
        "Adekunbi Justina Ajayi",
        "Olufunke Cynthia Metibemu",
        "A. Y. Balogun",
        "Tunbosun Oyewale Oladoyinbo",
        "O. O. Olaniyi"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/ccbf2eec849c5c461eb8d96197f6b36d55e65612",
      "pdf_url": "https://doi.org/10.9734/jerr/2025/v27i21413",
      "publication_date": "2025-02-13",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "faf781129098539d157dda2a20eae1ade26975b5",
      "title": "Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics",
      "abstract": "Modern neural-network-based no-reference image- and video-quality metrics exhibit performance as high as full-reference metrics. These metrics are widely used to improve visual quality in computer vision methods and compare video processing methods. However, these metrics are not stable to traditional adversarial attacks, which can cause incorrect results. Our goal is to investigate the boundaries of no-reference metrics applicability, and in this paper, we propose a fast adversarial perturbation attack on no-reference quality metrics. The proposed attack (FACPA) can be exploited as a preprocessing step in real-time video processing and compression algorithms. This research can yield insights to further aid in designing of stable neural-network-based no-reference quality metrics.",
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Ekaterina Shumitskaya",
        "Anastasia Antsiferova",
        "Dmitriy S. Vatolin"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/faf781129098539d157dda2a20eae1ade26975b5",
      "pdf_url": "http://arxiv.org/pdf/2305.15544",
      "publication_date": "2023-05-24",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "51cf507a9e6cbf6497946668335681f1ad5f6ba3",
      "title": "Evolutionary Perturbation Attack on Temporal Link Prediction",
      "abstract": null,
      "year": 2024,
      "venue": "Journal of the Physical Society of Japan",
      "authors": [
        "Ting Zhang",
        "Laishui Lv",
        "Dalal Bardou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/51cf507a9e6cbf6497946668335681f1ad5f6ba3",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e858f4159485b7ae0923ffeb3753e77802c010c5",
      "title": "An Improved Robust ClusterGAN with the Perturbation Attack",
      "abstract": "Generative adversarial networks (GAN) based deep clustering has achieved remarkable success in both underlying feature representation and cluster assignment. However, in these models, the local structure is easily destroyed by the clustering loss. Moreover, the clustering network is vulnerable to attack, and a small perturbation in the latent space will generate different clustering results. In this paper, an improved robust ClusterGAN model with the perturbation attack is presented. The GAN is trained along with an explicit inverse-mapping network regarding a clustering-specific loss. In addition, a reconstruction network is designed to ensure effective feature representation with the local structure preservation. Finally, a perturbation attack network for noise perturbation is combined to heighten the robustness of the encoder and further promote the clustering performance. Experimental results on real-world datasets verify the efficiency and superiority of the presented model in contrast to the existing GAN-based deep clustering methods.",
      "year": 2023,
      "venue": "2023 3rd International Conference on Robotics, Automation and Intelligent Control (ICRAIC)",
      "authors": [
        "Chunsun Duan",
        "Jiaqi Li",
        "Jie Liu",
        "Jin Zhou",
        "Yingxu Wang",
        "Tao Du",
        "Cheng Yang",
        "Bowen Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e858f4159485b7ae0923ffeb3753e77802c010c5",
      "pdf_url": "",
      "publication_date": "2023-11-24",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "69d7f223679b9416f71192fcb7f9d43d65b4f870",
      "title": "Cross-Modality Perturbation Synergy Attack for Person Re-identification",
      "abstract": "In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on three widely used cross-modality datasets, namely RegDB, SYSU, and LLCM. The results not only demonstrate the effectiveness of our method but also provide insights for future improvements in the robustness of cross-modality ReID systems. The code will be available at https://github.com/finger-monkey/cmps__attack.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yunpeng Gong",
        "Zhun Zhong",
        "Zhiming Luo",
        "Yansong Qu",
        "Rongrong Ji",
        "Min Jiang"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/69d7f223679b9416f71192fcb7f9d43d65b4f870",
      "pdf_url": "",
      "publication_date": "2024-01-18",
      "keywords_matched": [
        "perturbation attack",
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "466491945a4cfc2c6282250b348d6f251f3c8049",
      "title": "Universal Perturbation Attack on Differentiable No-Reference Image- and Video-Quality Metrics",
      "abstract": "Universal adversarial perturbation attacks are widely used to analyze image classifiers that employ convolutional neural networks. Nowadays, some attacks can deceive image- and video-quality metrics. So sustainability analysis of these metrics is important. Indeed, if an attack can confuse the metric, an attacker can easily increase quality scores. When developers of image- and video-algorithms can boost their scores through detached processing, algorithm comparisons are no longer fair. Inspired by the idea of universal adversarial perturbation for classifiers, we suggest a new method to attack differentiable no-reference quality metrics through universal perturbation. We applied this method to seven no-reference image- and video-quality metrics (PaQ-2-PiQ, Linearity, VSFA, MDTVSFA, KonCept512, Nima and SPAQ). For each one, we trained a universal perturbation that increases the respective scores. We also propose a method for assessing metric stability and identify the metrics that are the most vulnerable and the most resistant to our attack. The existence of successful universal perturbations appears to diminish the metric's ability to provide reliable scores. We therefore recommend our proposed method as an additional verification of metric reliability to complement traditional subjective tests and benchmarks.",
      "year": 2022,
      "venue": "British Machine Vision Conference",
      "authors": [
        "Ekaterina Shumitskaya",
        "Anastasia Antsiferova",
        "Dmitriy S. Vatolin"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/466491945a4cfc2c6282250b348d6f251f3c8049",
      "pdf_url": "https://arxiv.org/pdf/2211.00366",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "perturbation attack",
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d95ca2147206c28e1ef3e87932980306ec912a98",
      "title": "Practical Adversarial Attack on WiFi Sensing Through Unnoticeable Communication Packet Perturbation",
      "abstract": "The pervasive use of WiFi has driven the recent research in WiFi sensing, converting communication tech into sensing for applications such as activity recognition, user authentication, and vital sign monitoring. Despite the integration of deep learning into WiFi sensing systems, potential security vulnerabilities to adversarial attacks remain unexplored. This paper introduces the first physical attack focusing on deep learning-based WiFi sensing systems, demonstrating how adversaries can subtly manipulate WiFi packet preambles to affect channel state information (CSI), a critical feature in such systems, and thereby influence underlying deep learning models without disrupting regular communication. To realize the proposed attack in practical scenarios, we rigorously analyze and derive the intricate relationship between the pilot symbol and CSI. A novel mechanism is proposed to facilitate quantitive control of receiver-side CSI through minimal modifications to the pilot symbols of WiFi packets at the transmitter. We further develop a perturbation optimization method based on the Carlini & Wagner (CW) attack and a penalty-based training process to ensure the attack's universal efficacy across various CSI responses and noise. The physical attack is implemented and evaluated in two representative WiFi sensing systems (i.e., activity recognition and user authentication) with 35 participants over 3 months. Extensive experiments demonstrate the remarkable attack success rates of 90.47% and 83.83% for activity recognition and user authentication, respectively.",
      "year": 2024,
      "venue": "ACM/IEEE International Conference on Mobile Computing and Networking",
      "authors": [
        "Changming Li",
        "Mingjing Xu",
        "Yicong Du",
        "Limin Liu",
        "Cong Shi",
        "Yan Wang",
        "Hongbo Liu",
        "Yingying Chen"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d95ca2147206c28e1ef3e87932980306ec912a98",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3636534.3649367",
      "publication_date": "2024-05-29",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f2c782ca16a1c833e9c620a6a0f73bc7f60dcceb",
      "title": "Universal Perturbation Attack Against Image Retrieval",
      "abstract": "Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.",
      "year": 2018,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Jie Li",
        "R. Ji",
        "Hong Liu",
        "Xiaopeng Hong",
        "Yue Gao",
        "Q. Tian"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/f2c782ca16a1c833e9c620a6a0f73bc7f60dcceb",
      "pdf_url": "https://arxiv.org/pdf/1812.00552",
      "publication_date": "2018-12-03",
      "keywords_matched": [
        "perturbation attack",
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ae82b1ea43d1921fff5883d6c0ca7d41eaf4656f",
      "title": "Boosting Adversarial Transferability via Residual Perturbation Attack",
      "abstract": "Deep neural networks are susceptible to adversarial examples while suffering from incorrect predictions via imperceptible perturbations. Transfer-based attacks create adversarial examples for surrogate models and transfer these examples to target models under black-box scenarios. Recent studies reveal that adversarial examples in flat loss landscapes exhibit superior transferability to alleviate overfitting on surrogate models. However, the prior arts overlook the influence of perturbation directions, resulting in limited transferability. In this paper, we propose a novel attack method, named Residual Perturbation Attack (ResPA), relying on the residual gradient as the perturbation direction to guide the adversarial examples toward the flat regions of the loss function. Specifically, ResPA conducts an exponential moving average on the input gradients to obtain the first moment as the reference gradient, which encompasses the direction of historical gradients. Instead of heavily relying on the local flatness that stems from the current gradients as the perturbation direction, ResPA further considers the residual between the current gradient and the reference gradient to capture the changes in the global perturbation direction. The experimental results demonstrate the better transferability of ResPA than the existing typical transfer-based attack methods, while the transferability can be further improved by combining ResPA with the current input transformation methods. The code is available at https://github.com/ZezeTao/ResPA.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jinjia Peng",
        "Zeze Tao",
        "Huibing Wang",
        "Meng Wang",
        "Yang Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ae82b1ea43d1921fff5883d6c0ca7d41eaf4656f",
      "pdf_url": "",
      "publication_date": "2025-08-06",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "335fa2168895254047b282170d2cdc68b3f3085f",
      "title": "GRIP-GAN: An Attack-Free Defense Through General Robust Inverse Perturbation",
      "abstract": "Despite of its tremendous popularity and success in computer vision (CV) and natural language processing, deep learning is inherently vulnerable to adversarial attacks in which adversarial examples (AEs) are carefully crafted by imposing imperceptible perturbations on the clean examples to deceive the target deep neural networks (DNNs). Many defense solutions in CV have been proposed. However, most of them, e.g., adversarial training, suffer from a low generality due to the reliance on limited AEs. Moreover, some solutions even have a non-negligible negative impact on the classification accuracy of clean examples. Last but not least, they are impotent against the unconstrained attacks in which the attackers optimize the perturbation direction and size by additionally taking the defense methods into accounts. In this article, we propose GRIP-GAN to learn a general robust inverse perturbation (GRIP), which is not only able to offset any potential adversarial perturbations but also strengthen the target class-related features, purely from the clean images via a generative adversarial network (GAN). By feeding a random noise, GRIP-GAN is able to generate a dynamic GRIP for each input image to defend against unconstrained attacks. To further improve the defense performance, we also enable GRIP-GAN to generate a GRIP tailored to each input image via feeding input image specific noise to GRIP-GAN. Extensive experiments are carried out on MNIST, CIFAR10, and ImageNet datasets against 17 adversarial attacks. The results show that GRIP-GAN outperforms all the baselines. We further share insights on the success of GRIP-GAN and provide visualized proofs.",
      "year": 2022,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haibin Zheng",
        "Jinyin Chen",
        "Hang Du",
        "Weipeng Zhu",
        "S. Ji",
        "Xuhong Zhang"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/335fa2168895254047b282170d2cdc68b3f3085f",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16ab6586adebc68d8165003d117f6a7a052798fd",
      "title": "Data-free Universal Adversarial Perturbation and Black-box Attack",
      "abstract": "Universal adversarial perturbation (UAP), i.e. a single perturbation to fool the network for most images, is widely recognized as a more practical attack because the UAP can be generated beforehand and applied directly during the at-tack stage. One intriguing phenomenon regarding untargeted UAP is that most images are misclassified to a dominant label. This phenomenon has been reported in previous works while lacking a justified explanation, for which our work attempts to provide an alternative explanation. For a more practical universal attack, our investigation of untargeted UAP focuses on alleviating the dependence on the original training samples, from removing the need for sample labels to limiting the sample size. Towards strictly data-free untargeted UAP, our work proposes to exploit artificial Jigsaw images as the training samples, demonstrating competitive performance. We further investigate the possibility of exploiting the UAP for a data-free black-box attack which is arguably the most practical yet challenging threat model. We demonstrate that there exists optimization-free repetitive patterns which can successfully attack deep models. Code is available at https://bit.ly/3y0ZTIC.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Chaoning Zhang",
        "Philipp Benz",
        "Adil Karjauv",
        "In-So Kweon"
      ],
      "citation_count": 73,
      "url": "https://www.semanticscholar.org/paper/16ab6586adebc68d8165003d117f6a7a052798fd",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4cb55f935a2bbb9828b018ba495895f904d63b25",
      "title": "BESA: Boosting Encoder Stealing Attack With Perturbation Recovery",
      "abstract": "To boost the encoder stealing attack under the perturbation-based defense that hinders the attack performance, we propose a boosting encoder stealing attack with perturbation recovery named BESA. It aims to overcome perturbation-based defenses. The core of BESA consists of two modules: perturbation detection and perturbation recovery, which can be combined with canonical encoder stealing attacks. The perturbation detection module utilizes the feature vectors obtained from the target encoder to infer the defense mechanism employed by the service provider. Once the defense mechanism is detected, the perturbation recovery module leverages the well-designed generative model to restore a clean feature vector from the perturbed one. Through extensive evaluations based on various datasets, we demonstrate that BESA significantly enhances the surrogate encoder accuracy of existing encoder stealing attacks by up to 24.63% when facing state-of-the-art defenses and combinations of multiple defenses.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xuhao Ren",
        "Haotian Liang",
        "Yajie Wang",
        "Chuan Zhang",
        "Zehui Xiong",
        "Liehuang Zhu"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/4cb55f935a2bbb9828b018ba495895f904d63b25",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ff8dae7ce14fc8e740fbfc0455632051e59bfa0",
      "title": "DAMAD: Database, Attack, and Model Agnostic Adversarial Perturbation Detector",
      "abstract": "Adversarial perturbations have demonstrated the vulnerabilities of deep learning algorithms to adversarial attacks. Existing adversary detection algorithms attempt to detect the singularities; however, they are in general, loss-function, database, or model dependent. To mitigate this limitation, we propose DAMAD\u2014a generalized perturbation detection algorithm which is agnostic to model architecture, training data set, and loss function used during training. The proposed adversarial perturbation detection algorithm is based on the fusion of autoencoder embedding and statistical texture features extracted from convolutional neural networks. The performance of DAMAD is evaluated on the challenging scenarios of cross-database, cross-attack, and cross-architecture training and testing along with traditional evaluation of testing on the same database with known attack and model. Comparison with state-of-the-art perturbation detection algorithms showcase the effectiveness of the proposed algorithm on six databases: ImageNet, CIFAR-10, Multi-PIE, MEDS, point and shoot challenge (PaSC), and MNIST. Performance evaluation with nearly a quarter of a million adversarial and original images and comparison with recent algorithms show the effectiveness of the proposed algorithm.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Akshay Agarwal",
        "Gaurav Goswami",
        "Mayank Vatsa",
        "Richa Singh",
        "N. Ratha"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/6ff8dae7ce14fc8e740fbfc0455632051e59bfa0",
      "pdf_url": "",
      "publication_date": "2021-03-12",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e002b6ce5283d88ce0c21afbca27b3aea091e78f",
      "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
      "abstract": "Harmful fine-tuning attack poses serious safety concerns for large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that harmful perturbation over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at https://github.com/git-disl/Booster.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "S. Tekin",
        "Ling Liu"
      ],
      "citation_count": 56,
      "url": "https://www.semanticscholar.org/paper/e002b6ce5283d88ce0c21afbca27b3aea091e78f",
      "pdf_url": "",
      "publication_date": "2024-09-03",
      "keywords_matched": [
        "perturbation attack",
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a42df2cc7f85e133f2ea38b2bbd2cb2b21a0f904",
      "title": "Transferable Multimodal Attack on Vision-Language Pre-training Models",
      "abstract": "Vision-Language Pre-training (VLP) models have achieved remarkable success in practice, while easily being misled by adversarial attack. Though harmful, adversarial attacks are valuable in revealing the blind-spots of VLP models and promoting their robustness. However, existing adversarial attacking studies pay insufficient attention to the key roles of different modality-correlated features, leading to unsatisfactory transferable attacking performance. To tackle this issue, we propose the Transferable MultiModal (TMM) attack framework, which tailors both the modality consistency and modality discrepancy features. To promote transferability, we propose the attention-directed feature perturbation to disturb the modality-consistency features in critical attention regions. In light of the commonly employed cross-attention can represent the consistent features among diverse models, it is more possible to mislead the similar model perception for activating stronger transferability. For improving attacking ability, we proposed the orthogonal-guided feature heterogenization to guide the adversarial perturbation to contain more modality-discrepancy features in the encoded embeddings. Since VLP models rely more on aligned features among different modalities during decision-making, increasing the modality-discrepant could confuse the learned representation for better attacking ability. Extensive experiments under diverse settings demonstrate that the proposed TMM outperforms the comparisons by large margins, i.e., 20.47% improvements in transferable attacking ability on average. Moreover, we highlight that our TMM also shows outstanding attacking performance on large models, such as MiniGPT-4, Otter, etc.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Haodi Wang",
        "Kai Dong",
        "Zhilei Zhu",
        "Haotong Qin",
        "Aishan Liu",
        "Xiaolin Fang",
        "Jiakai Wang",
        "Xianglong Liu"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/a42df2cc7f85e133f2ea38b2bbd2cb2b21a0f904",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
      "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment",
      "abstract": "Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective\u2014it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving\u2014it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient\u2014it generates adversarial text with computational complexity linear to the text length.1",
      "year": 2019,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Di Jin",
        "Zhijing Jin",
        "Joey Tianyi Zhou",
        "Peter Szolovits"
      ],
      "citation_count": 1246,
      "url": "https://www.semanticscholar.org/paper/ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6311/6167",
      "publication_date": "2019-07-27",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2f9a8ed9ee2a0e7e3a351dc20704d4d14f1ca118",
      "title": "Security Analysis of WiFi-based Sensing Systems: Threats from Perturbation Attacks",
      "abstract": "Deep learning technologies are pivotal in enhancing the performance of WiFi-based wireless sensing systems. However, they are inherently vulnerable to adversarial perturbation attacks, and regrettably, there is lacking serious attention to this security issue within the WiFi sensing community. In this paper, we elaborate such an attack, called WiIntruder, distinguishing itself with universality, robustness, and stealthiness, which serves as a catalyst to assess the security of existing WiFi-based sensing systems. This attack encompasses the following salient features: (1) Maximizing transferability by differentiating user-state-specific feature spaces across sensing models, leading to a universally effective perturbation attack applicable to common applications; (2) Addressing perturbation signal distortion caused by device synchronization and wireless propagation when critical parameters are optimized through a heuristic particle swarm-driven perturbation generation algorithm; and (3) Enhancing attack pattern diversity and stealthiness through random switching of perturbation surrogates generated by a generative adversarial network. Extensive experimental results confirm the practical threats of perturbation attacks to common WiFi-based services, including user authentication and respiratory monitoring.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hangcheng Cao",
        "Wenbin Huang",
        "Guowen Xu",
        "Xianhao Chen",
        "Ziyang He",
        "Jingyang Hu",
        "Hongbo Jiang",
        "Yuguang Fang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/2f9a8ed9ee2a0e7e3a351dc20704d4d14f1ca118",
      "pdf_url": "",
      "publication_date": "2024-04-24",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eca815987e1c6a51e00fdc202342d50a288599f0",
      "title": "Generating Transferable 3D Adversarial Point Cloud via Random Perturbation Factorization",
      "abstract": "Recent studies have demonstrated that existing deep neural networks (DNNs) on 3D point clouds are vulnerable to adversarial examples, especially under the white-box settings where the adversaries have access to model parameters. However, adversarial 3D point clouds generated by existing white-box methods have limited transferability across different DNN architectures. They have only minor threats in real-world scenarios under the black-box settings where the adversaries can only query the deployed victim model. In this paper, we revisit the transferability of adversarial 3D point clouds. We observe that an adversarial perturbation can be randomly factorized into two sub-perturbations, which are also likely to be adversarial perturbations. It motivates us to consider the effects of the perturbation and its sub-perturbations simultaneously to increase the transferability for sub-perturbations also contain helpful information. In this paper, we propose a simple yet effective attack method to generate more transferable adversarial 3D point clouds. Specifically, rather than simply optimizing the loss of perturbation alone, we combine it with its random factorization. We conduct experiments on benchmark dataset, verifying our method's effectiveness in increasing transferability while preserving high efficiency.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bangyan He",
        "J. Liu",
        "Yiming Li",
        "Siyuan Liang",
        "Jingzhi Li",
        "Xiaojun Jia",
        "Xiaochun Cao"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/eca815987e1c6a51e00fdc202342d50a288599f0",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/25154/24926",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "333a8c09fee4dbf8fbd6d9291c9bc6aacd3cf7d9",
      "title": "Improving Adversarial Transferability by Intermediate-level Perturbation Decay",
      "abstract": "Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneously. In-depth discussion verifies the effectiveness of our method. Experimental results show that it outperforms state-of-the-arts by large margins in attacking various victim models on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average). Our code is at https://github.com/qizhangli/ILPD-attack.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Qizhang Li",
        "Yiwen Guo",
        "W. Zuo",
        "Hao Chen"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/333a8c09fee4dbf8fbd6d9291c9bc6aacd3cf7d9",
      "pdf_url": "http://arxiv.org/pdf/2304.13410",
      "publication_date": "2023-04-26",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "26fcdfb3df555914ac2efe8e0cfb11e37f6d514c",
      "title": "Query-Efficient Decision-Based Black-Box Patch Attack",
      "abstract": "Deep neural networks (DNNs) have been showed to be highly vulnerable to imperceptible adversarial perturbations. As a complementary type of adversary, patch attacks that introduce perceptible perturbations to the images have attracted the interest of researchers. Existing patch attacks rely on the architecture of the model or the probabilities of predictions and perform poorly in the decision-based setting, which can still construct a perturbation with the minimal information exposed \u2013 the top-1 predicted label. In this work, we first explore the decision-based patch attack. To enhance the attack efficiency, we model the patches using paired key-points and use targeted images as the initialization of patches, and parameter optimizations are all performed on the integer domain. Then, we propose a differential evolutionary algorithm named DevoPatch for query-efficient decision-based patch attacks. Experiments demonstrate that DevoPatch outperforms the state-of-the-art black-box patch attacks in terms of patch area and attack success rate within a given query budget on image classification and face verification. Additionally, we conduct the vulnerability evaluation of ViT and MLP on image classification in the decision-based patch attack setting for the first time. Using DevoPatch, we can evaluate the robustness of models to black-box patch attacks. We believe this method could inspire the design and deployment of robust vision models based on various DNN architectures in the future.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Zhaoyu Chen",
        "Bo Li",
        "Shuang Wu",
        "Shouhong Ding",
        "Wenqiang Zhang"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/26fcdfb3df555914ac2efe8e0cfb11e37f6d514c",
      "pdf_url": "http://arxiv.org/pdf/2307.00477",
      "publication_date": "2023-07-02",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6e4af9a1d0b46ccd023a22dddd6f4bc464b4a264",
      "title": "Towards a Robust Adversarial Patch Attack Against Unmanned Aerial Vehicles Object Detection",
      "abstract": "Object detection techniques for autonomous Un-manned Aerial Vehicles (UAV) are built upon Deep Neural Networks (DNN), which are known to be vulnerable to adversarial patch perturbation attacks that lead to object detection evasion. Yet, current adversarial patch generation schemes are not designed for UAV imagery settings. This paper proposes a new robust adversarial patch generation attack against object detection with UAVs. We build adversarial patches considering UAV-specific settings such as the UAV camera perspective, viewing angle, distance, and brightness changes. As a result, built patches can also degrade the accuracy of object detector models implemented with different initializations and architectures. Experiments conducted on the VisDrone dataset have shown the proposal's feasibility, achieving an attack success rate of up to 80% in a white-box setting. In addition, we also transfer the patch against DNN models with different initializations and different architectures, reaching attack success rates of up to 75% and 78%, respectively, in a gray-box setting. GitHub: https://github.com/SamSamhuns/yolov5_adversarial",
      "year": 2023,
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "authors": [
        "Samridha Shrestha",
        "Saurabh Pathak",
        "Eduardo K. Viegas"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/6e4af9a1d0b46ccd023a22dddd6f4bc464b4a264",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "538f0c8379f9e091d4769ec3bdff6eb45130554f",
      "title": "Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation",
      "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples, which can produce erroneous predictions by injecting imperceptible perturbations. In this work, we study the transferability of adversarial examples, which is significant due to its threat to real-world applications where model architecture or parameters are usually unknown. Many existing works reveal that the adversarial examples are likely to overfit the surrogate model that they are generated from, limiting its transfer attack performance against different target models. To mitigate the overfitting of the surrogate model, we propose a novel attack method, dubbed reverse adversarial perturbation (RAP). Specifically, instead of minimizing the loss of a single adversarial point, we advocate seeking adversarial example located at a region with unified low loss value, by injecting the worst-case perturbation (the reverse adversarial perturbation) for each step of the optimization procedure. The adversarial attack with RAP is formulated as a min-max bi-level optimization problem. By integrating RAP into the iterative process for attacks, our method can find more stable adversarial examples which are less sensitive to the changes of decision boundary, mitigating the overfitting of the surrogate model. Comprehensive experimental comparisons demonstrate that RAP can significantly boost adversarial transferability. Furthermore, RAP can be naturally combined with many existing black-box attack techniques, to further boost the transferability. When attacking a real-world image recognition system, Google Cloud Vision API, we obtain 22% performance improvement of targeted attacks over the compared method. Our codes are available at https://github.com/SCLBD/Transfer_attack_RAP.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Zeyu Qin",
        "Yanbo Fan",
        "Yi Liu",
        "Li Shen",
        "Yong Zhang",
        "Jue Wang",
        "Baoyuan Wu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/538f0c8379f9e091d4769ec3bdff6eb45130554f",
      "pdf_url": "http://arxiv.org/pdf/2210.05968",
      "publication_date": "2022-10-12",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7e59fdd13e3e9c8387d2a124adf47c05a6aeda8c",
      "title": "Towards Transferable Adversarial Attacks with Centralized Perturbation",
      "abstract": "Adversarial transferability enables black-box attacks on unknown victim deep neural networks (DNNs), rendering attacks viable in real-world scenarios. Current transferable attacks create adversarial perturbation over the entire image, resulting in excessive noise that overfit the source model. Concentrating perturbation to dominant image regions that are model-agnostic is crucial to improving adversarial efficacy. However, limiting perturbation to local regions in the spatial domain proves inadequate in augmenting transferability. To this end, we propose a transferable adversarial attack with fine-grained perturbation optimization in the frequency domain, creating centralized perturbation. We devise a systematic pipeline to dynamically constrain perturbation optimization to dominant frequency coefficients. The constraint is optimized in parallel at each iteration, ensuring the directional alignment of perturbation optimization with model prediction. Our approach allows us to centralize perturbation towards sample-specific important frequency features, which are shared by DNNs, effectively mitigating source model overfitting. Experiments demonstrate that by dynamically centralizing perturbation on dominating frequency coefficients, crafted adversarial examples exhibit stronger transferability, and allowing them to bypass various defenses.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Shangbo Wu",
        "Yu-an Tan",
        "Yajie Wang",
        "Ruinan Ma",
        "Wencong Ma",
        "Yuanzhang Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/7e59fdd13e3e9c8387d2a124adf47c05a6aeda8c",
      "pdf_url": "",
      "publication_date": "2023-12-11",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f530423c98a2a2a893f7214daeede1e90168c8d",
      "title": "Revisiting Visual Understanding in Multimodal Reasoning through a Lens of Image Perturbation",
      "abstract": "Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Yuting Li",
        "Lai Wei",
        "Kaipeng Zheng",
        "Jingyuan Huang",
        "Linghe Kong",
        "Lichao Sun",
        "Weiran Huang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1f530423c98a2a2a893f7214daeede1e90168c8d",
      "pdf_url": "",
      "publication_date": "2025-06-11",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "48408a6ddbbbbda6fe5a7830e8fa0dfe6803c64b",
      "title": "Perturbation-Seeking Generative Adversarial Networks: A Defense Framework for Remote Sensing Image Scene Classification",
      "abstract": "The methods for remote sensing image (RSI) scene classification based on deep convolutional neural networks (DCNNs) have achieved prominent success. However, confronted with adversarial examples obtained by adding imperceptible perturbations to clean images, the great vulnerability of DCNNs makes it worth exploring effective defense methods. To date, numerous countermeasures for adversarial examples have been proposed, but how to improve the defensive ability for unknown attacks still to be answered. To address this issue, in this article, we propose an effective defense framework specified for RSI scene classification, named perturbation-seeking generative adversarial networks (PSGANs). In brief, a new training framework is designed to train the classifier by introducing the examples generated during the image reconstruction process, in addition to clean examples and adversarial ones. These generated examples can be random kinds of unknown attacks during training and thus are utilized to eliminate the blind spots of a classifier. To assist the proposed training framework, a reconstruction method is developed. First, instead of modeling the distribution of clean examples, we model the distributions of the perturbations added in adversarial examples. Second, to make a tradeoff between the diversity of the reconstructed examples and the optimization of PSGAN, a scale factor named seeking radius is introduced to scale the generated perturbations before they are subtracted by the given adversarial examples. Comprehensive and extensive experimental results on three widely used benchmarks for RSI scene classification demonstrate the great effectiveness of PSGAN when faced with both known and unknown attacks. Our source code is available at https://github.com/xuxiangsun/PSGAN.",
      "year": 2022,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Gong Cheng",
        "Xuxiang Sun",
        "Ke Li",
        "Lei Guo",
        "Junwei Han"
      ],
      "citation_count": 97,
      "url": "https://www.semanticscholar.org/paper/48408a6ddbbbbda6fe5a7830e8fa0dfe6803c64b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c345edd42ebab7afcaf068229a62093f3a0766e",
      "title": "Image Transformation-Based Defense Against Adversarial Perturbation on Deep Learning Models",
      "abstract": "Deep learning algorithms provide state-of-the-art results on a multitude of applications. However, it is also well established that they are highly vulnerable to adversarial perturbations. It is often believed that the solution to this vulnerability of deep learning systems must come from deep networks only. Contrary to this common understanding, in this article, we propose a non-deep learning approach that searches over a set of well-known image transforms such as Discrete Wavelet Transform and Discrete Sine Transform, and classifying the features with a support vector machine-based classifier. Existing deep networks-based defense have been proven ineffective against sophisticated adversaries, whereas image transformation-based solution makes a strong defense because of the non-differential nature, multiscale, and orientation filtering. The proposed approach, which combines the outputs of two transforms, efficiently generalizes across databases as well as different unseen attacks and combinations of both (i.e., cross-database and unseen noise generation CNN model). The proposed algorithm is evaluated on large scale databases, including object database (validation set of ImageNet) and face recognition (MBGC) database. The proposed detection algorithm yields at-least 84.2% and 80.1% detection accuracy under seen and unseen database test settings, respectively. Besides, we also show how the impact of the adversarial perturbation can be neutralized using a wavelet decomposition-based filtering method of denoising. The mitigation results with different perturbation methods on several image databases demonstrate the effectiveness of the proposed method.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Akshay Agarwal",
        "Richa Singh",
        "Mayank Vatsa",
        "N. Ratha"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/6c345edd42ebab7afcaf068229a62093f3a0766e",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "99ef1c7f42cdad2ee5474adc615edbcd495fc2f7",
      "title": "On the (Im)Practicality of Adversarial Perturbation for Image Privacy",
      "abstract": "Abstract Image hosting platforms are a popular way to store and share images with family members and friends. However, such platforms typically have full access to images raising privacy concerns. These concerns are further exacerbated with the advent of Convolutional Neural Networks (CNNs) that can be trained on available images to automatically detect and recognize faces with high accuracy. Recently, adversarial perturbations have been proposed as a potential defense against automated recognition and classification of images by CNNs. In this paper, we explore the practicality of adversarial perturbation-based approaches as a privacy defense against automated face recognition. Specifically, we first identify practical requirements for such approaches and then propose two practical adversarial perturbation approaches \u2013 (i) learned universal ensemble perturbations (UEP), and (ii) k-randomized transparent image overlays (k-RTIO) that are semantic adversarial perturbations. We demonstrate how users can generate effective transferable perturbations under realistic assumptions with less effort. We evaluate the proposed methods against state-of-theart online and offline face recognition models, Clarifai.com and DeepFace, respectively. Our findings show that UEP and k-RTIO respectively achieve more than 85% and 90% success against face recognition models. Additionally, we explore potential countermeasures that classifiers can use to thwart the proposed defenses. Particularly, we demonstrate one effective countermeasure against UEP.",
      "year": 2020,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Arezoo Rajabi",
        "R. Bobba",
        "Mike Rosulek",
        "C. V. Wright",
        "W. Feng"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/99ef1c7f42cdad2ee5474adc615edbcd495fc2f7",
      "pdf_url": "https://www.sciendo.com/pdf/10.2478/popets-2021-0006",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "897c0aedead3105b7b6cd1afbb6aeb4f62a06e11",
      "title": "GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models",
      "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after\"fine-tuning\"on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply\"style cloaks\"to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Shawn Shan",
        "Jenna Cryan",
        "Emily Wenger",
        "Haitao Zheng",
        "Rana Hanocka",
        "Ben Y. Zhao"
      ],
      "citation_count": 238,
      "url": "https://www.semanticscholar.org/paper/897c0aedead3105b7b6cd1afbb6aeb4f62a06e11",
      "pdf_url": "https://arxiv.org/pdf/2302.04222",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "833e4ebf67867475e4b099f7a84524b88f343a77",
      "title": "Hybrid Perturbation Strategy for Semi-Supervised Crowd Counting",
      "abstract": "A simple yet effective semi-supervised method is proposed in this paper based on consistency regularization for crowd counting, and a hybrid perturbation strategy is used to generate strong, diverse perturbations, and enhance unlabeled images information mining. The conventional CNN-based counting methods are sensitive to texture perturbation and imperceptible noises raised by adversarial attack, therefore, the hybrid strategy is proposed to combine a spatial texture transformation and an adversarial perturbation module to perturb the unlabeled data in the semantic and non-semantic spaces, respectively. Moreover, a cross-distribution normalization technique is introduced to address the model optimization failure caused by BN layer in the strong perturbation, and to stabilize the optimization of the learning model. Extensive experiments have been conducted on the datasets of ShanghaiTech, UCF-QNRF, NWPU-Crowd, and JHU-Crowd++. The results demonstrate that the proposed semi-supervised counting method performs better over the state-of-the-art methods, and it shows better robustness to various perturbations.",
      "year": 2024,
      "venue": "IEEE Transactions on Image Processing",
      "authors": [
        "Xin Wang",
        "Yue Zhan",
        "Yang Zhao",
        "Tangwen Yang",
        "Qiuqi Ruan"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/833e4ebf67867475e4b099f7a84524b88f343a77",
      "pdf_url": "",
      "publication_date": "2024-02-08",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9867456c81f262162c2fdf9c8498218a8936c84d",
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "abstract": "Text-to-image generative models such as Stable Diffusion and DALL\u2022E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL\u2022E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: https://github.com/Yuchen413/text2image_safety.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuchen Yang",
        "Bo Hui",
        "Haolin Yuan",
        "N. Gong",
        "Yinzhi Cao"
      ],
      "citation_count": 144,
      "url": "https://www.semanticscholar.org/paper/9867456c81f262162c2fdf9c8498218a8936c84d",
      "pdf_url": "https://arxiv.org/pdf/2305.12082",
      "publication_date": "2023-05-20",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5fa2819ababc5d842e038a5f962915c6a36507bf",
      "title": "Improving Transferability of Universal Adversarial Perturbation With Feature Disruption",
      "abstract": "Deep neural networks (DNNs) are shown to be vulnerable to universal adversarial perturbations (UAP), a single quasi-imperceptible perturbation that deceives the DNNs on most input images. The current UAP methods can be divided into data-dependent and data-independent methods. The former exhibits weak transferability in black-box models due to overly relying on model-specific features. The latter shows inferior attack performance in white-box models as it fails to exploit the model\u2019s response information to benign images. To address the above issues, this paper proposes a novel universal adversarial attack to generate UAP with strong transferability by disrupting the model-agnostic features (e.g., edges or simple texture), which are invariant to the models. Specifically, we first devise an objective function to weaken the significant channel-wise features and strengthen the less significant channel-wise features, which are partitioned by the designed strategy. Furthermore, the proposed objective function eliminates the dependency on labeled samples, allowing us to utilize out-of-distribution (OOD) data to train UAP. To enhance the attack performance with limited training samples, we exploit the average gradient of the mini-batch input to update the UAP iteratively, which encourages the UAP to capture the local information inside the mini-batch input. In addition, we introduce the momentum term to accumulate the gradient information at each iterative step for the purpose of perceiving the global information over the training set. Finally, extensive experimental results demonstrate that the proposed methods outperform the existing UAP approaches. Additionally, we exhaustively investigate the transferability of the UAP across models, datasets, and tasks.",
      "year": 2023,
      "venue": "IEEE Transactions on Image Processing",
      "authors": [
        "Donghua Wang",
        "Wenbiao Yao",
        "Tingsong Jiang",
        "Xiaoqian Chen"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/5fa2819ababc5d842e038a5f962915c6a36507bf",
      "pdf_url": "",
      "publication_date": "2023-12-27",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "088569b34d7f58bf4525ade7f391de98d62d601a",
      "title": "Data Poisoning Attack against Unsupervised Node Embedding Methods",
      "abstract": "Unsupervised node embedding methods (e.g., DeepWalk, LINE, and node2vec) have attracted growing interests given their simplicity and effectiveness. However, although these methods have been proved effective in a variety of applications, none of the existing work has analyzed the robustness of them. This could be very risky if these methods are attacked by an adversarial party. In this paper, we take the task of link prediction as an example, which is one of the most fundamental problems for graph analysis, and introduce a data positioning attack to node embedding methods. We give a complete characterization of attacker's utilities and present efficient solutions to adversarial attacks for two popular node embedding methods: DeepWalk and LINE. We evaluate our proposed attack model on multiple real-world graphs. Experimental results show that our proposed model can significantly affect the results of link prediction by slightly changing the graph structures (e.g., adding or removing a few edges). We also show that our proposed model is very general and can be transferable across different embedding methods. Finally, we conduct a case study on a coauthor network to better understand our attack method.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Mingjie Sun",
        "Jian Tang",
        "Huichen Li",
        "Bo Li",
        "Chaowei Xiao",
        "Yao Chen",
        "D. Song"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/088569b34d7f58bf4525ade7f391de98d62d601a",
      "pdf_url": "",
      "publication_date": "2018-09-27",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5985a4dfb8fc6ea157beb6deca1e915e0331326d",
      "title": "Stealthy 3D Poisoning Attack on Video Recognition Models",
      "abstract": "Deep Neural Networks (DNNs) have been proven to be vulnerable to poisoning attacks that poison the training data with a trigger pattern and thus manipulate the trained model to misclassify data instances. In this article, we study the poisoning attacks on video recognition models. We reveal the major limitations of the state-of-the-art poisoning attacks on <italic>stealthiness</italic> and <italic>attack effectiveness</italic>: (i) the frame-by-frame poisoning trigger may cause temporal inconsistency among the video frames which can be leveraged to easily detect the attack; (ii) the feature collision-based method for crafting poisoned videos could lack both generalization and transferability. To address these limitations, we propose a novel stealthy and efficient poisoning attack framework which has the following advantages: (i) we design a 3D poisoning trigger as natural-like textures, which can maintain temporal consistency and human-imperceptibility; (ii) we formulate an ensemble attack oracle as the optimization objective to craft poisoned videos, which could construct convex polytope-like adversarial subspaces in the feature space and thus gain more generalization; (iii) our poisoning attack can be readily extended to the black-box setting with good transferability. We have experimentally validated the effectiveness of our attack (e.g., up to <inline-formula><tex-math notation=\"LaTeX\">$95\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>95</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"hong-ieq1-3163397.gif\"/></alternatives></inline-formula> success rates with only less than <inline-formula><tex-math notation=\"LaTeX\">$\\sim 0.5\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u223c</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"hong-ieq2-3163397.gif\"/></alternatives></inline-formula> poisoned dataset).",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Shangyu Xie",
        "Yan Yan",
        "Yuan Hong"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/5985a4dfb8fc6ea157beb6deca1e915e0331326d",
      "pdf_url": "",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a321e26734e8ee8d319e7848c19463860d8329f5",
      "title": "Unsupervised Graph Poisoning Attack via Contrastive Loss Back-propagation",
      "abstract": "Graph contrastive learning is the state-of-the-art unsupervised graph representation learning framework and has shown comparable performance with supervised approaches. However, evaluating whether the graph contrastive learning is robust to adversarial attacks is still an open problem because most existing graph adversarial attacks are supervised models, which means they heavily rely on labels and can only be used to evaluate the graph contrastive learning in a specific scenario. For unsupervised graph representation methods such as graph contrastive learning, it is difficult to acquire labels in real-world scenarios, making traditional supervised graph attack methods difficult to be applied to test their robustness. In this paper, we propose a novel unsupervised gradient-based adversarial attack that does not rely on labels for graph contrastive learning. We compute the gradients of the adjacency matrices of the two views and flip the edges with gradient ascent to maximize the contrastive loss. In this way, we can fully use multiple views generated by the graph contrastive learning models and pick the most informative edges without knowing their labels, and therefore can promisingly support our model adapted to more kinds of downstream tasks. Extensive experiments show that our attack outperforms unsupervised baseline attacks and has comparable performance with supervised attacks in multiple downstream tasks including node classification and link prediction. We further show that our attack can be transferred to other graph representation models as well.",
      "year": 2022,
      "venue": "The Web Conference",
      "authors": [
        "Sixiao Zhang",
        "Hongxu Chen",
        "Xiangguo Sun",
        "Yicong Li",
        "Guandong Xu"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/a321e26734e8ee8d319e7848c19463860d8329f5",
      "pdf_url": "https://arxiv.org/pdf/2201.07986",
      "publication_date": "2022-01-20",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "218bd4129c8399ae48eaeba9a64a71e6f1e8cfea",
      "title": "Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability",
      "abstract": "A recent source of concern for the security of neural networks is the emergence of clean-label dataset poisoning attacks, wherein correctly labeled poison samples are injected into the training dataset. While these poison samples look legitimate to the human observer, they contain malicious characteristics that trigger a targeted misclassification during inference. We propose a scalable and transferable clean-label poisoning attack against transfer learning, which creates poison images with their center close to the target image in the feature space. Our attack, Bullseye Polytope, improves the attack success rate of the current state-of-the-art by 26.75% in end-to-end transfer learning, while increasing attack speed by a factor of 12. We further extend Bullseye Polytope to a more practical attack model by including multiple images of the same object (e.g., from different angles) when crafting the poison samples. We demonstrate that this extension improves attack transferability by over 16% to unseen images (of the same object) without using extra poison samples.",
      "year": 2020,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "H. Aghakhani",
        "Dongyu Meng",
        "Yu-Xiang Wang",
        "C. Kruegel",
        "Giovanni Vigna"
      ],
      "citation_count": 122,
      "url": "https://www.semanticscholar.org/paper/218bd4129c8399ae48eaeba9a64a71e6f1e8cfea",
      "pdf_url": "https://arxiv.org/pdf/2005.00191",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bf58936a947e292b7d36050f6ad72eceadec5fbb",
      "title": "CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning",
      "abstract": "Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model\u2019s behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor attacks on multimodal contrastive learning. Code and pretrained checkpoints are available at https://github.com/nishadsinghi/CleanCLIP.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Hritik Bansal",
        "Nishad Singhi",
        "Yu Yang",
        "Fan Yin",
        "Aditya Grover",
        "Kai-Wei Chang"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/bf58936a947e292b7d36050f6ad72eceadec5fbb",
      "pdf_url": "",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86923a85225d18da6d7625e5768c0fc3f2a7299e",
      "title": "Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks",
      "abstract": "Contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. However, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of targeted data poisoning and backdoor attacks. Despite this vulnerability, robust contrastive vision-language pre-training against such attacks has remained unaddressed. In this work, we propose ROCLIP, the first effective method for robust pre-training multimodal vision-language models against targeted data poisoning and backdoor attacks. ROCLIP effectively breaks the association between poisoned image-caption pairs by considering a relatively large and varying pool of random captions, and matching every image with the text that is most similar to it in the pool instead of its own caption, every few epochs.It also leverages image and text augmentations to further strengthen the defense and improve the performance of the model. Our extensive experiments show that ROCLIP renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training CLIP models. In particular, ROCLIP decreases the success rate for targeted data poisoning attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while improving the model's linear probe performance by 10% and maintains a similar zero shot performance compared to CLIP. By increasing the frequency of matching, ROCLIP is able to defend strong attacks, which add up to 1% poisoned examples to the data, and successfully maintain a low attack success rate of 12.5%, while trading off the performance on some tasks.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Wenhan Yang",
        "Jingdong Gao",
        "Baharan Mirzasoleiman"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/86923a85225d18da6d7625e5768c0fc3f2a7299e",
      "pdf_url": "",
      "publication_date": "2023-03-13",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5a123d014b77652aefff24e6e26c7f95d43f409a",
      "title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning",
      "abstract": "Contrastive learning pre-trains an image encoder using a large amount of unlabeled data such that the image encoder can be used as a general-purpose feature extractor for various downstream tasks. In this work, we propose PoisonedEncoder, a data poisoning attack to contrastive learning. In particular, an attacker injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes. We formulate our data poisoning attack as a bilevel optimization problem, whose solution is the set of poisoning inputs; and we propose a contrastive-learning-tailored method to approximately solve it. Our evaluation on multiple datasets shows that PoisonedEncoder achieves high attack success rates while maintaining the testing accuracy of the downstream classifiers built upon the poisoned encoder for non-attacker-chosen inputs. We also evaluate five defenses against PoisonedEncoder, including one pre-processing, three in-processing, and one post-processing defenses. Our results show that these defenses can decrease the attack success rate of PoisonedEncoder, but they also sacrifice the utility of the encoder or require a large clean pre-training dataset.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hongbin Liu",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/5a123d014b77652aefff24e6e26c7f95d43f409a",
      "pdf_url": "http://arxiv.org/pdf/2205.06401",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e3a2e08095fd7402a97aaf6e73b6dbfdd58de79",
      "title": "Is Feature Selection Secure against Training Data Poisoning?",
      "abstract": "Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures.",
      "year": 2015,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Huang Xiao",
        "B. Biggio",
        "Gavin Brown",
        "G. Fumera",
        "C. Eckert",
        "F. Roli"
      ],
      "citation_count": 433,
      "url": "https://www.semanticscholar.org/paper/3e3a2e08095fd7402a97aaf6e73b6dbfdd58de79",
      "pdf_url": "",
      "publication_date": "2015-07-06",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ea7ab9302a8cba9b3b3edd9d8f3e4d67afc68ed7",
      "title": "Persistent Pre-Training Poisoning of LLMs",
      "abstract": "Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web. Prior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets. Our work evaluates for the first time whether language models can also be compromised during pre-training, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B). Our main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Yiming Zhang",
        "Javier Rando",
        "Ivan Evtimov",
        "Jianfeng Chi",
        "E. Smith",
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Daphne Ippolito"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/ea7ab9302a8cba9b3b3edd9d8f3e4d67afc68ed7",
      "pdf_url": "",
      "publication_date": "2024-10-17",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1764924b9c892ad85c677b95677c344d7ce99143",
      "title": "Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning",
      "abstract": "We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \\emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \\emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \\emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.",
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Amin Rakhsha",
        "Goran Radanovic",
        "Rati Devidze",
        "Xiaojin Zhu",
        "A. Singla"
      ],
      "citation_count": 136,
      "url": "https://www.semanticscholar.org/paper/1764924b9c892ad85c677b95677c344d7ce99143",
      "pdf_url": "",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37149662dd8eb59fdc0fae23bce8edc16a51cd79",
      "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
      "abstract": "There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xijia Tao",
        "Shuai Zhong",
        "Lei Li",
        "Qi Liu",
        "Lingpeng Kong"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/37149662dd8eb59fdc0fae23bce8edc16a51cd79",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "862d78925dbe0f6974b68a97783fb407b53db410",
      "title": "Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary",
      "abstract": "The practical needs of the \u201cright to be forgotten\u201d and poisoned data removal call for efficient machine unlearning techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lineage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the influence of the forgetting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet effective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel boundary shift methods, namely Boundary Shrink and Boundary Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 and Vggface2 datasets, and the results show that Boundary Unlearning can effectively forget the forgetting class on image classification and face recognition tasks, with an expected speed-up of 17x and 19x, respectively, compared with retraining from the scratch.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Min Chen",
        "Weizhuo Gao",
        "Gaoyang Liu",
        "Kai Peng",
        "Chen Wang"
      ],
      "citation_count": 134,
      "url": "https://www.semanticscholar.org/paper/862d78925dbe0f6974b68a97783fb407b53db410",
      "pdf_url": "",
      "publication_date": "2023-03-21",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dc9370008c84a07961c20b66f3f4079de55a3e9d",
      "title": "Securing Contrastive mmWave-based Human Activity Recognition against Adversarial Label Flipping",
      "abstract": "Wireless Human Activity Recognition (HAR), leveraging their non-intrusive nature, has the potential to revolutionize various sectors, including healthcare, virtual reality, and surveillance. The advent of millimeter wave (mmWave) technology has significantly enhanced the capabilities of wireless HAR systems. This paper presents the first systematic study on the vulnerabilities of mmWave-based HAR to label flipping poisoning attacks in the context of supervised contrastive learning. We identify three label poisoning attacks on the contrastive mmWave-based HAR and propose corresponding countermeasures. The efficacy of the attacks and also our countermeasures are experimentally validated on a prototype system. The attacks and countermeasures can be easily extended to other wireless HAR systems, thereby promoting security considerations in system design and deployment.",
      "year": 2024,
      "venue": "Wireless Network Security",
      "authors": [
        "Amit Singha",
        "Ziqian Bi",
        "Tao Li",
        "Yimin Chen",
        "Yanchao Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/dc9370008c84a07961c20b66f3f4079de55a3e9d",
      "pdf_url": "",
      "publication_date": "2024-05-27",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3d1aebed8b61e39c99014424e809305445a0e36",
      "title": "FL-ProtectX: Combating Label flipping in complex multi-agent environments",
      "abstract": "Federated Learning (FL) has emerged as a key enabler for secure and decentralized model training while preserving user data privacy. Nonetheless, FL is vulnerable to poisoning attacks; in which a malicious participant tries to corrupt the global model by sending manipulated updates. One common attack is label-flipping, where attackers intentionally mess up labels for training data on their end. Current defenses against these threats treat scenarios where a single adversarial group is cooperating in coordination. This misses the modern real-world scenario in which different adversarial groups have differing, conflicting objectives and act independently. This paper introduces FL-ProtectX, a defense mechanism designed to combat label-flipping attacks targeting heterogeneous multi-adversary environments. Our model also detects the adversary (one that pursues a set of attack goals) behind and can differentiate from other adversaries using their unique target attacks. FL-ProtectX: by utilizing last-layer gradient analysis and cosine similarity computation along with PCA for compression, provides accurate separation and ensures that malicious participants are detected. To facilitate global model aggregation, our method adaptively adjusts trust factors according to the calculated angular similarities between updates. We evaluate the effectiveness of FL-ProtectX on defending against adversary groups by experimentally studying label-flipping attacks on the CIFAR10 dataset and show that for most situations, it effectively neutralizes these attacks, reducing attack success rate from 17.12 to ${1 1. 0 3}$ while maintaining high accuracy in non-attack-related classes in multi-agent configurations.",
      "year": 2024,
      "venue": "International Symposium on Telecommunications",
      "authors": [
        "Mohammad Ali Zamani",
        "Fatemeh Amiri",
        "Pooya Jamshidi",
        "Seyed Mahdi Hosseini",
        "Nasser Yazdani"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f3d1aebed8b61e39c99014424e809305445a0e36",
      "pdf_url": "",
      "publication_date": "2024-10-09",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d1cefa929dd8522484f49d5c8caaad4f1791e74",
      "title": "Fast Adversarial Label-Flipping Attack on Tabular Data",
      "abstract": "Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on transforming the adversary's objective and employs linear programming to reduce computational complexity. Using ten real-world tabular datasets, we demonstrate FALFA's superior attack potential, highlighting the need for robust defenses against such threats.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xinglong Chang",
        "Gill Dobbie",
        "Jorg Wicker"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7d1cefa929dd8522484f49d5c8caaad4f1791e74",
      "pdf_url": "",
      "publication_date": "2023-10-16",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f947a70d75ef70bdc038953b2bd50b0b157a6c3",
      "title": "Lapa: Multi-Label-Flipping Adversarial Attacks on Graph Neural Networks",
      "abstract": "In recent years, the robustness of graph neural networks (GNNs) adversarial attacks has been favored by researchers, and related research has intensified. However, existing graph adversarial attacks are mainly performed by modifying the topology of the graph and node feature vectors, and attacks from the labels of nodes are rarely considered, especially for multi-label graph adversarial attacks, where an attacker can manipulate an obscure portion of the training labels and flip the sample labels in the training data to other classes, resulting in incorrect predictions of the retrained GNNs model. In this work, we present graph adversarial attack against multi-label flipping. We propose an effective attack model-Lapa, based on an adaptive genetic algorithm that searches the solution space for suitable adversarial samples to obtain near-optimal solutions. The adaptive genetic algorithm performs crossover and mutation operations utilizing adaptive probabilities. We demonstrate the effectiveness of our proposed Lapa attack model on GNNs after extensive experiments on four real datasets.",
      "year": 2023,
      "venue": "2023 International Seminar on Computer Science and Engineering Technology (SCSET)",
      "authors": [
        "Jianfeng Li",
        "Haoran Li",
        "Jing He",
        "Tianchen Dou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1f947a70d75ef70bdc038953b2bd50b0b157a6c3",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e410eb6a73db5601e7dfb1dd572b07222d224ad1",
      "title": "On defending against label flipping attacks on malware detection systems",
      "abstract": "Label manipulation attacks are a subclass of data poisoning attacks in adversarial machine learning used against different applications, such as malware detection. These types of attacks represent a serious threat to detection systems in environments having high noise rate or uncertainty, such as complex networks and Internet of Thing (IoT). Recent work in the literature has suggested using the K-nearest neighboring algorithm to defend against such attacks. However, such an approach can suffer from low to miss-classification rate accuracy. In this paper, we design an architecture to tackle the Android malware detection problem in IoT systems. We develop an attack mechanism based on silhouette clustering method, modified for mobile Android platforms. We proposed two convolutional neural network-type deep learning algorithms against this Silhouette Clustering-based Label Flipping Attack. We show the effectiveness of these two defense algorithms\u2014label-based semi-supervised defense and clustering-based semi-supervised defense\u2014in correcting labels being attacked. We evaluate the performance of the proposed algorithms by varying the various machine learning parameters on three Android datasets: Drebin, Contagio, and Genome and three types of features: API, intent, and permission. Our evaluation shows that using random forest feature selection and varying ratios of features can result in an improvement of up to 19% accuracy when compared with the state-of-the-art method in the literature.",
      "year": 2019,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "R. Taheri",
        "R. Javidan",
        "Mohammad Shojafar",
        "Zahra Pooranian",
        "A. Miri",
        "M. Conti"
      ],
      "citation_count": 97,
      "url": "https://www.semanticscholar.org/paper/e410eb6a73db5601e7dfb1dd572b07222d224ad1",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s00521-020-04831-9.pdf",
      "publication_date": "2019-08-13",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5aea84a731e981fcb71286129bc0772eedbecc53",
      "title": "A method for adequate selection of training data sets to reconstruct seismic data using a convolutional U-Net",
      "abstract": "Deep-learning (DL) methods have recently been introduced for seismic signal processing. Using DL methods, many researchers have adopted these novel techniques in an attempt to construct a DL model for seismic data reconstruction. The performance of DL-based methods depends heavily on what is learned from the training data. We focus on constructing the DL model that well reflect the features of target data sets. The main goal is to integrate DL with an intuitive data analysis approach that compares similar patterns prior to the DL training stage. We have developed a two-sequential method consisting of two stages: (1)\u00a0analyzing training and target data sets simultaneously for determining the target-informed training set and (2)\u00a0training the DL model with this training data set to effectively interpolate the seismic data. Here, we introduce the convolutional autoencoder t-distributed stochastic neighbor embedding (CAE t-SNE) analysis that can provide the insight into the results of interpolation through the analysis of training and target data sets prior to DL model training. Our method was tested with synthetic and field data. Dense seismic gathers (e.g.,\u00a0common-shot gathers) were used as a labeled training data set, and relatively sparse seismic gathers (e.g.,\u00a0common-receiver gathers [CRGs]) were reconstructed in both cases. The reconstructed results and signal-to-noise ratios demonstrated that the training data can be efficiently selected using CAE t-SNE analysis, and the spatial aliasing of CRGs was successfully alleviated by the trained DL model with this training data, which contain target features. These results imply that the data analysis for selecting target-informed training set is very important for successful DL interpolation. In addition, our analysis method can also be applied to investigate the similarities between training and target data sets for other DL-based seismic data reconstruction tasks.",
      "year": 2021,
      "venue": "Geophysics",
      "authors": [
        "Jiho Park",
        "Jihun Choi",
        "Soon Jee Seol",
        "J. Byun",
        "Y. Kim"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/5aea84a731e981fcb71286129bc0772eedbecc53",
      "pdf_url": "",
      "publication_date": "2021-05-28",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be55e8ec4213868db08f2c3168ae666001bea4b8",
      "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Stella Biderman",
        "Hailey Schoelkopf",
        "Quentin Anthony",
        "Herbie Bradley",
        "Kyle O'Brien",
        "Eric Hallahan",
        "Mohammad Aflah Khan",
        "Shivanshu Purohit",
        "USVSN Sai Prashanth",
        "Edward Raff",
        "Aviya Skowron",
        "Lintang Sutawika",
        "Oskar van der Wal"
      ],
      "citation_count": 1610,
      "url": "https://www.semanticscholar.org/paper/be55e8ec4213868db08f2c3168ae666001bea4b8",
      "pdf_url": "http://arxiv.org/pdf/2304.01373",
      "publication_date": "2023-04-03",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "49995802ad0d6ef327647868868458d7619d430d",
      "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
      "abstract": "This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C.",
      "year": 2022,
      "venue": "Interspeech",
      "authors": [
        "Junyi Ao",
        "Zi-Hua Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Haizhou Li",
        "Tom Ko",
        "Lirong Dai",
        "Jinyu Li",
        "Yao Qian",
        "Furu Wei"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/49995802ad0d6ef327647868868458d7619d430d",
      "pdf_url": "http://arxiv.org/pdf/2203.17113",
      "publication_date": "2022-03-31",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "47d22522c9268ae597bf426f5bd14412575091a1",
      "title": "Unsupervised Point Cloud Pre-training via Occlusion Completion",
      "abstract": "We describe a simple pre-training approach for point clouds. It works in three steps: 1. Mask all points occluded in a camera view; 2. Learn an encoder-decoder model to reconstruct the occluded points; 3. Use the encoder weights as initialisation for downstream point cloud tasks. We find that even when we pre-train on a single dataset (ModelNet40), this method improves accuracy across different datasets and encoders, on a wide range of downstream tasks. Specifically, we show that our method outperforms previous pre-training methods in object classification, and both part-based and semantic segmentation tasks. We study the pre-trained features and find that they lead to wide downstream minima, have high transformation invariance, and have activations that are highly correlated with part labels. Code and data are available at: https://github.com/hansen7/OcCo",
      "year": 2020,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Hanchen Wang",
        "Qi Liu",
        "Xiangyu Yue",
        "Joan Lasenby",
        "Matt J. Kusner"
      ],
      "citation_count": 301,
      "url": "https://www.semanticscholar.org/paper/47d22522c9268ae597bf426f5bd14412575091a1",
      "pdf_url": "https://arxiv.org/pdf/2010.01089",
      "publication_date": "2020-10-02",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4a751d520534771593d359069f99d7c4621a861a",
      "title": "Learning Graph Embedding With Adversarial Training Methods",
      "abstract": "Graph embedding aims to transfer a graph into vectors to facilitate subsequent graph-analytics tasks like link prediction and graph clustering. Most approaches on graph embedding focus on preserving the graph structure or minimizing the reconstruction errors for graph data. They have mostly overlooked the embedding distribution of the latent codes, which unfortunately may lead to inferior representation in many cases. In this article, we present a novel adversarially regularized framework for graph embedding. By employing the graph convolutional network as an encoder, our framework embeds the topological information and node content into a vector representation, from which a graph decoder is further built to reconstruct the input graph. The adversarial training principle is applied to enforce our latent codes to match a prior Gaussian or uniform distribution. Based on this framework, we derive two variants of the adversarial models, the adversarially regularized graph autoencoder (ARGA) and its variational version, and adversarially regularized variational graph autoencoder (ARVGA), to learn the graph embedding effectively. We also exploit other potential variations of ARGA and ARVGA to get a deeper understanding of our designs. Experimental results that compared 12 algorithms for link prediction and 20 algorithms for graph clustering validate our solutions.",
      "year": 2019,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Shirui Pan",
        "Ruiqi Hu",
        "S. Fung",
        "Guodong Long",
        "Jing Jiang",
        "Chengqi Zhang"
      ],
      "citation_count": 333,
      "url": "https://www.semanticscholar.org/paper/4a751d520534771593d359069f99d7c4621a861a",
      "pdf_url": "http://arxiv.org/pdf/1901.01250",
      "publication_date": "2019-01-04",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3ab64c8e9e05fd2426d6cc51f16d931279b862df",
      "title": "The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training",
      "abstract": "The self-supervised Masked Image Modeling (MIM) schema, following \"mask-and-reconstruct\" pipeline of recovering contents from masked image, has recently captured the increasing interest in the community, owing to the excellent ability of learning visual representation from unlabeled data. Aiming at learning representations with high semantics abstracted, a group of works attempts to reconstruct non-semantic pixels with large-ratio masking strategy, which may suffer from \"over-smoothing\" problem, while others directly infuse semantics into targets in off-line way requiring extra data. Different from them, we shift the perspective to the Fourier domain which naturally has global perspective and present a new Masked Image Modeling (MIM), termed Geminated Gestalt Autoencoder (Ge^2-AE) for visual pre-training. Specifically, we equip our model with geminated decoders in charge of reconstructing image contents from both pixel and frequency space, where each other serves as not only the complementation but also the reciprocal constraints. Through this way, more robust representations can be learned in the pre-trained encoders, of which the effectiveness is confirmed by the juxtaposing experimental results on downstream recognition tasks. We also conduct several quantitative and qualitative experiments to investigate the learning behavior of our method. To our best knowledge, this is the first MIM work to solve the visual pre-training through the lens of frequency domain.",
      "year": 2022,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Hao Liu",
        "Xinghua Jiang",
        "Xin Li",
        "Antai Guo",
        "Deqiang Jiang",
        "Bo Ren"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/3ab64c8e9e05fd2426d6cc51f16d931279b862df",
      "pdf_url": "http://arxiv.org/pdf/2204.08227",
      "publication_date": "2022-04-18",
      "keywords_matched": [
        "reconstruct training data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5ea773bf51ee7c6b5715ca123cfb0db10c924a5f",
      "title": "Transfer Learning with Input Reconstruction Loss",
      "abstract": "Neural networks have been widely utilized for wireless communication optimizations. In most of the literature, a dedicated neural network is trained for each specific optimization problem. However, under many scenarios, several distinct objectives are worth optimizing on the same wireless environment. Instead of exhaustively training a new model for every objective, it is more efficient to exploit the correlations between these objectives to train models with shared model parameters and feature representations. In the deep learning literature, transfer learning has been proposed to encourage knowledge transfer among models solving correlated problems. Unlike a majority of transfer learning applications where the high level features are relatively easy to locate in the neural networks, this paper considers wireless communication problems, in which it is much more difficult to identify high level features transferable to correlated tasks. To address this issue, this paper proposes to add an additional reconstruction loss when training the model. This new loss is for reconstructing the problem inputs starting from a selected neural network hidden layer. This approach encourages the features learnt to be general and descriptive about the inputs, instead of being solely responsible for minimizing the specific task-based loss. When a new objective is to be optimized, these features can be readily used for transfer learning. Simulation results in device-to-device wireless network power allocation optimization suggest that the proposed approach is highly efficient in data and model complexity, resilient to over-fitting, and supports competitive optimization performances.",
      "year": 2022,
      "venue": "Global Communications Conference",
      "authors": [
        "W. Cui",
        "Wei Yu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/5ea773bf51ee7c6b5715ca123cfb0db10c924a5f",
      "pdf_url": "",
      "publication_date": "2022-12-04",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f8cd509b1607d1431af58a88c0d903081c313166",
      "title": "Torch-feat: GNN sampling training data loader based on feature data extraction operators",
      "abstract": null,
      "year": 2025,
      "venue": "CCF Transactions on High Performance Computing",
      "authors": [
        "Jianzhi Yu",
        "Shiqi Yang",
        "Zhencheng Liu",
        "Guangjie Jin",
        "Yanhui Wang",
        "Jianguo Liang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8cd509b1607d1431af58a88c0d903081c313166",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "091c6502244f28cbc13bcbf209c52c89c2fbc2e1",
      "title": "The effect of feature extraction and data sampling on credit card fraud detection",
      "abstract": "Training a machine learning algorithm on a class-imbalanced dataset can be a difficult task, a process that could prove even more challenging under conditions of high dimensionality. Feature extraction and data sampling are among the most popular preprocessing techniques. Feature extraction is used to derive a richer set of reduced dataset features, while data sampling is used to mitigate class imbalance. In this paper, we investigate these two preprocessing techniques, using a credit card fraud dataset and four ensemble classifiers (Random Forest, CatBoost, LightGBM, and XGBoost). Within the context of feature extraction, the Principal Component Analysis (PCA) and Convolutional Autoencoder (CAE) methods are evaluated. With regard to data sampling, the Random Undersampling (RUS), Synthetic Minority Oversampling Technique (SMOTE), and SMOTE Tomek methods are evaluated. The F1 score and Area Under the Receiver Operating Characteristic Curve (AUC) metrics serve as measures of classification performance. Our results show that the implementation of the RUS method followed by the CAE method leads to the best performance for credit card fraud detection.",
      "year": 2023,
      "venue": "Journal of Big Data",
      "authors": [
        "Zahra Salekshahrezaee",
        "Joffrey L. Leevy",
        "T. Khoshgoftaar"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/091c6502244f28cbc13bcbf209c52c89c2fbc2e1",
      "pdf_url": "https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-023-00684-w",
      "publication_date": "2023-01-17",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "934e9a75879fcc2a3b2b386e357c849fca2d5235",
      "title": "Generative Adversarial Networks for Augmenting Training Data of Microscopic Cell Images",
      "abstract": "Generative adversarial networks (GANs) have recently been successfully used to create realistic synthetic microscopy cell images in 2D and predict intermediate cell stages. In the current paper we highlight that GANs can not only be used for creating synthetic cell images optimized for different fluorescent molecular labels, but that by using GANs for augmentation of training data involving scaling or other transformations the inherent length scale of biological structures is retained. In addition, GANs make it possible to create synthetic cells with specific shape features, which can be used, for example, to validate different methods for feature extraction. Here, we apply GANs to create 2D distributions of fluorescent markers for F-actin in the cell cortex of Dictyostelium cells (ABD), a membrane receptor (cAR1), and a cortex-membrane linker protein (TalA). The recent more widespread use of 3D lightsheet microscopy, where obtaining sufficient training data is considerably more difficult than in 2D, creates significant demand for novel approaches to data augmentation. We show that it is possible to directly generate synthetic 3D cell images using GANs, but limitations are excessive training times, dependence on high-quality segmentations of 3D images, and that the number of z-slices cannot be freely adjusted without retraining the network. We demonstrate that in the case of molecular labels that are highly correlated with cell shape, like F-actin in our example, 2D GANs can be used efficiently to create pseudo-3D synthetic cell data from individually generated 2D slices. Because high quality segmented 2D cell data are more readily available, this is an attractive alternative to using less efficient 3D networks.",
      "year": 2019,
      "venue": "Front. Comput. Sci.",
      "authors": [
        "S\u00e9bastien Tosi",
        "J. J. Merelo",
        "Carlos Castilla Ruiz",
        "T. Bretschneider",
        "P. Baniukiewicz",
        "\u2020. E.JosiahLutton",
        "Sharon Collier"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/934e9a75879fcc2a3b2b386e357c849fca2d5235",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/fcomp.2019.00010/pdf",
      "publication_date": "2019-11-26",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "57099cf852807c652fd55274a3ce4a8c9777ef56",
      "title": "Neural Model Stealing Attack to Smart Mobile Device on Intelligent Medical Platform",
      "abstract": "To date, the Medical Internet of Things (MIoT) technology has been recognized and widely applied due to its convenience and practicality. The MIoT enables the application of machine learning to predict diseases of various kinds automatically and accurately, assisting and facilitating effective and efficient medical treatment. However, the MIoT are vulnerable to cyberattacks which have been constantly advancing. In this paper, we establish a MIoT platform and demonstrate a scenario where a trained Convolutional Neural Network (CNN) model for predicting lung cancer complicated with pulmonary embolism can be attacked. First, we use CNN to build a model to predict lung cancer complicated with pulmonary embolism and obtain high detection accuracy. Then, we build a copycat model using only a small amount of data labeled by the target network, aiming to steal the established prediction model. Experimental results prove that the stolen model can also achieve a relatively high prediction outcome, revealing that the copycat network could successfully copy the prediction performance from the target network to a large extent. This also shows that such a prediction model deployed on MIoT devices can be stolen by attackers, and effective prevention strategies are open questions for researchers.",
      "year": 2020,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Liqiang Zhang",
        "Guanjun Lin",
        "Bixuan Gao",
        "Zhibao Qin",
        "Yonghang Tai",
        "Jun Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/57099cf852807c652fd55274a3ce4a8c9777ef56",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2020/8859489.pdf",
      "publication_date": "2020-11-26",
      "keywords_matched": [
        "copycat model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
      "abstract": "We introduce Adversarial Sparse Teacher (AST), a robust defense method against distillation-based model stealing attacks. Our approach trains a teacher model using adversarial examples to produce sparse logit responses and increase the entropy of the output distribution. Typically, a model generates a peak in its output corresponding to its prediction. By leveraging adversarial examples, AST modifies the teacher model\u2019s original response, embedding a few altered logits into the output, while keeping the primary response slightly higher. Concurrently, all remaining logits are elevated to further increase the output distribution\u2019s entropy. All these complex manipulations are performed using an optimization function with our proposed Exponential Predictive Divergence (EPD) loss function. EPD allows us to maintain higher entropy levels compared to traditional KL divergence, effectively confusing attackers. Experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that AST outperforms state-of-the-art methods, providing effective defense against model stealing, while preserving high accuracy. The source codes are publicly available at https://github.com/codeofanon/AdversarialSparseTeacher",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "E. Y\u0131lmaz",
        "H. Keles"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "pdf_url": "",
      "publication_date": "2024-03-08",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c09eaf8e58fabd2371294b26a37376be960546a2",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "N. Asokan"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c09eaf8e58fabd2371294b26a37376be960546a2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models",
      "abstract": "Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "Nirmal Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "pdf_url": "",
      "publication_date": "2021-04-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
      "abstract": "Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.11See our code in https://github.com/zeyangsha/Cont-Steal.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zeyang Sha",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "pdf_url": "https://arxiv.org/pdf/2201.07513",
      "publication_date": "2022-01-19",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a6e2407fd50b505a8763198650c2282ae9a7848e",
      "title": "Driver Identification Using Optimized Deep Learning Model in Smart Transportation",
      "abstract": "The Intelligent Transportation System (ITS) is said to revolutionize the travel experience by making it safe, secure, and comfortable for the people. Although vehicles have been automated up to a certain extent, it still has critical security issues that require thorough study and advanced solutions. The security vulnerabilities of ITS allows the attacker to steal the vehicle. Therefore, the identification of drivers is required in order to develop a safe and secure system so that the vehicles can be protected from theft. There are two ways in which a driver can be identified: 1) face recognition of the driver, and 2) based on driving behavior. Face recognition includes image processing of 2-D images and learning of the features, which require high computational power. Drivers are known to have unique driving styles, whose data can be captured by the sensors. Therefore, the second method identifies drivers based on the analysis of the sensor data and it requires comparatively lesser computational power. In this paper, an optimized deep learning model is trained on the sensor data to correctly identify the drivers. The Long Short-Term Memory (LSTM) deep learning model is optimized for better performance. The novelty of the approach in this work is the inclusion of hyperparameter tuning using a nature-inspired optimization algorithm, which is an important and essential step in discovering the optimal hyperparameters for training the model which in turn increases the accuracy. The CAN-BUS dataset is used for experimentation and evaluation of the training model. Evaluation parameters such as accuracy, precision score, F1 score, and ROC AUC curve are considered to evaluate the performance of the model.",
      "year": 2022,
      "venue": "ACM Trans. Internet Techn.",
      "authors": [
        "Chandrasekar Ravi",
        "Anmol Tigga",
        "G. T. Reddy",
        "S. Hakak",
        "M. Alazab"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/a6e2407fd50b505a8763198650c2282ae9a7848e",
      "pdf_url": "",
      "publication_date": "2022-02-12",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a70c8ab375402abf154d73b6668f41cd06a7d394",
      "title": "Cyber-Threat Detection System Using a Hybrid Approach of Transfer Learning and Multi-Model Image Representation",
      "abstract": "Currently, Android apps are easily targeted by malicious network traffic because of their constant network access. These threats have the potential to steal vital information and disrupt the commerce, social system, and banking markets. In this paper, we present a malware detection system based on word2vec-based transfer learning and multi-model image representation. The proposed method combines the textual and texture features of network traffic to leverage the advantages of both types. Initially, the transfer learning method is used to extract trained vocab from network traffic. Then, the malware-to-image algorithm visualizes network bytes for visual analysis of data traffic. Next, the texture features are extracted from malware images using a combination of scale-invariant feature transforms (SIFTs) and oriented fast and rotated brief transforms (ORBs). Moreover, a convolutional neural network (CNN) is designed to extract deep features from a set of trained vocab and texture features. Finally, an ensemble model is designed to classify and detect malware based on the combination of textual and texture features. The proposed method is tested using two standard datasets, CIC-AAGM2017 and CICMalDroid 2020, which comprise a total of 10.2K malware and 3.2K benign samples. Furthermore, an explainable AI experiment is performed to interpret the proposed approach.",
      "year": 2022,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Farhan Ullah",
        "Shamsher Ullah",
        "M. Naeem",
        "L. Mostarda",
        "Seungmin Rho",
        "Xiaochun Cheng"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/a70c8ab375402abf154d73b6668f41cd06a7d394",
      "pdf_url": "https://www.mdpi.com/1424-8220/22/15/5883/pdf?version=1660201938",
      "publication_date": "2022-08-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3940e444f0e668f2469473b17f3c78d5aad0c660",
      "title": "CCBLA: a Lightweight Phishing Detection Model Based on CNN, BiLSTM, and Attention Mechanism",
      "abstract": null,
      "year": 2022,
      "venue": "Cognitive Computation",
      "authors": [
        "Erzhou Zhu",
        "Qixiang Yuan",
        "Zhile Chen",
        "Xia Li",
        "Xianyong Fang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3940e444f0e668f2469473b17f3c78d5aad0c660",
      "pdf_url": "",
      "publication_date": "2022-05-18",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c5476533130ced082b53e6c7286712d52611de8d",
      "title": "An Effective Spam Message Detection Model using Feature Engineering and Bi-LSTM",
      "abstract": "In today's digital era, the unwanted messages are communicated with many people and attack their device through the messages. Spammers steal the secret data of the users that are stored in their mobile device and also creates huge loss to the end user. The various spam detection techniques are proposed by many researchers in the past. Even though, no technique achieved the required detection accuracy. For fulfilling the current user requirements, this paper proposes an effective classifier that combines the Vectorization based Feature Engineering process and Bidirectional Long Short-Term Memory (Bi-LSTM) for detecting the spam in short text message (SMS). The proposed method applies feature engineering process for identifying useful features and also perform effective data pre-processing using vectorization and also perform classification using Bi-LSTM. The proposed method is evaluated by conducting experiments and proved as better than other methods in terms of precision, recall, f1-measure and detection accuracy.",
      "year": 2022,
      "venue": "2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",
      "authors": [
        "Antony Rosewelt",
        "Naveen D Raju",
        "S. Ganapathy"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/c5476533130ced082b53e6c7286712d52611de8d",
      "pdf_url": "",
      "publication_date": "2022-01-28",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0704c06ea42be1210b5f00b7859915dc8b1ac75",
      "title": "A Multifaceted Deep Generative Adversarial Networks Model for Mobile Malware Detection",
      "abstract": "Malware\u2019s structural transformation to withstand the detection frameworks encourages hackers to steal the public\u2019s confidential content. Researchers are developing a protective shield against the intrusion of malicious malware in mobile devices. The deep learning-based android malware detection frameworks have ensured public safety; however, their dependency on diverse training samples has constrained their utilization. The handcrafted malware detection mechanisms have achieved remarkable performance, but their computational overheads are a major hurdle in their utilization. In this work, Multifaceted Deep Generative Adversarial Networks Model (MDGAN) has been developed to detect malware in mobile devices. The hybrid GoogleNet and LSTM features of the grayscale and API sequence have been processed in a pixel-by-pixel pattern through conditional GAN for the robust representation of APK files. The generator produces syntactic malicious features for differentiation in the discriminator network. Experimental validation on the combined AndroZoo and Drebin database has shown 96.2% classification accuracy and a 94.7% F-score, which remain superior to the recently reported frameworks.",
      "year": 2022,
      "venue": "Applied Sciences",
      "authors": [
        "Fahad Mazaed Alotaibi",
        "Fawad Fawad"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/f0704c06ea42be1210b5f00b7859915dc8b1ac75",
      "pdf_url": "https://www.mdpi.com/2076-3417/12/19/9403/pdf?version=1663907231",
      "publication_date": "2022-09-20",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ab4036bf29853d4b6e454184730eb6d26cbc4bc7",
      "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
      "abstract": "Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturing the concepts and avoiding overfitting. We employ a masked diffusion loss to enable handles to generate their assigned concepts, complemented by a novel loss on cross-attention maps to prevent entanglement. We also introduce union-sampling, a training strategy aimed to improve the ability of combining multiple concepts in generated images. We use several automatic metrics to quantitatively compare our method against several baselines, and further affirm the results using a user study. Finally, we showcase several applications of our method.",
      "year": 2023,
      "venue": "ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",
      "authors": [
        "Omri Avrahami",
        "Kfir Aberman",
        "Ohad Fried",
        "D. Cohen-Or",
        "D. Lischinski"
      ],
      "citation_count": 237,
      "url": "https://www.semanticscholar.org/paper/ab4036bf29853d4b6e454184730eb6d26cbc4bc7",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3610548.3618154",
      "publication_date": "2023-05-25",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e67393254aa1e30e792a3e02ef3cbdd2fb20b382",
      "title": "A New Deep Learning Model for Fault Diagnosis with Good Anti-Noise and Domain Adaptation Ability on Raw Vibration Signals",
      "abstract": "Intelligent fault diagnosis techniques have replaced time-consuming and unreliable human analysis, increasing the efficiency of fault diagnosis. Deep learning models can improve the accuracy of intelligent fault diagnosis with the help of their multilayer nonlinear mapping ability. This paper proposes a novel method named Deep Convolutional Neural Networks with Wide First-layer Kernels (WDCNN). The proposed method uses raw vibration signals as input (data augmentation is used to generate more inputs), and uses the wide kernels in the first convolutional layer for extracting features and suppressing high frequency noise. Small convolutional kernels in the preceding layers are used for multilayer nonlinear mapping. AdaBN is implemented to improve the domain adaptation ability of the model. The proposed model addresses the problem that currently, the accuracy of CNN applied to fault diagnosis is not very high. WDCNN can not only achieve 100% classification accuracy on normal signals, but also outperform the state-of-the-art DNN model which is based on frequency features under different working load and noisy environment conditions.",
      "year": 2017,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Wei Zhang",
        "Gaoliang Peng",
        "Chuanhao Li",
        "Yuanhang Chen",
        "Zhujun Zhang"
      ],
      "citation_count": 1408,
      "url": "https://www.semanticscholar.org/paper/e67393254aa1e30e792a3e02ef3cbdd2fb20b382",
      "pdf_url": "https://www.mdpi.com/1424-8220/17/2/425/pdf?version=1487762482",
      "publication_date": "2017-02-01",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "15a81a4579949690c706f1bd9bbdda93681c35c3",
      "title": "Poisoning-Free Defense Against Black-Box Model Extraction",
      "abstract": "Recent research has shown that an adversary can use a surrogate model to steal the functionality of a target deep learning model even under the black-box condition and without data curation, while the existing defense mainly relies on API poisoning to disturb the surrogate training. Unfortunately, due to poisoning, the defense is achieved at the price of fidelity loss, sacrificing the interests of honest users. To solve this problem, we propose an Adversarial Fine-Tuning (AdvFT) framework, incorporating the generative adversarial network (GAN) structure that disturbs the feature representations of out-of-distribution (OOD) queries while preserving those of in-distribution (ID) ones, circumventing the need for OOD sample collection and API poisoning. Extensive experiments verify the effectiveness of the proposed framework. Code is available at github.com/Hatins/AdvFT.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Wen Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/15a81a4579949690c706f1bd9bbdda93681c35c3",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "title": "FDINet: Protecting Against DNN Model Extraction Using Feature Distortion Index",
      "abstract": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINet, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary\u2019s queries, we reveal that the feature distribution of these queries deviates from that of the model\u2019s problem domain. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINet utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINet against six state-of-the-art extraction attacks on four benchmark datasets and four popular model architectures. Empirical results demonstrate the following findings: 1) FDINet proves to be highly effective in detecting model extraction, achieving a 100% detection accuracy on DFME and DaST. 2) FDINet is highly efficient, using just 50 queries to raise an extraction alarm with an average confidence of 96.08% for GTSRB. 3) FDINet exhibits the capability to identify colluding adversaries with an accuracy exceeding 91%. Additionally, it demonstrates the ability to detect two types of adaptive attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zheng Li",
        "Haiqin Weng",
        "Feng Xue",
        "Kui Ren",
        "Zhan Qin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "pdf_url": "",
      "publication_date": "2023-06-20",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2edf0e5fea51f989caa2ebf664845ef027b8b45d",
      "title": "FDINet: Protecting against DNN Model Extraction via Feature Distortion Index",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Hongwei Yao",
        "Zhengguang Li",
        "Haiqin Weng",
        "Feng Xue",
        "Kui Ren",
        "Zhan Qin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/2edf0e5fea51f989caa2ebf664845ef027b8b45d",
      "pdf_url": "http://arxiv.org/pdf/2306.11338",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6a4dab913871b24c71bb3e66db20babe2a68880f",
      "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
      "abstract": "Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Kacem Khaled",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6a4dab913871b24c71bb3e66db20babe2a68880f",
      "pdf_url": "",
      "publication_date": "2025-12-30",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "title": "MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction",
      "abstract": "As deep learning gains popularity, edge IoT devices have seen proliferating deployment of pre-trained Deep Neural Network (DNN) models. These DNNs represent valuable intellectual property and face significant confidentiality threats from side-channel analysis (SCA), particularly non-invasive Differential Electromagnetic (EM) Analysis (DEMA), which retrieves individual model parameters from EM traces collected during model inference. Traditional SCA mitigation methods, such as masking and shuffling, can still be applied to DNN inference, but will incur significant performance degradation due to the large volume of operations and parameters. Based on the insight that DNN models have high redundancy and are robust to input variation, we introduce MACPruning, a novel lightweight defense against DEMA-based parameter extraction attacks, exploiting specific characteristics of DNN execution. The design principle of MACPruning is to randomly deactivate input pixels and prune the operations (typically multiply-accumulate-MAC) on those pixels. The technique removes certain leakages and overall redistributes weight-dependent EM leakages temporally, and thus effectively mitigates DEMA. To maintain DNN performance, we propose an importance-aware pixel map that preserves critical input pixels, keeping randomness in the defense while minimizing its impact on DNN performance due to operation pruning. We conduct a comprehensive security analysis of MACPruning on various datasets for DNNs on edge devices. Our evaluations demonstrate that MACPruning effectively reduces EM leakages with minimal impact on the model accuracy and negligible computational overhead.",
      "year": 2025,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Ruyi Ding",
        "Gongye Cheng",
        "Davis Ranney",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b408a5e1a60c3afd5954613516126b6c512a7804",
      "title": "Model Extraction and Defenses on Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define accuracy and fidelity on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of accuracy extraction and fidelity extraction, according to the adversary's goals and background knowledge. We further conduct a case study where an adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b408a5e1a60c3afd5954613516126b6c512a7804",
      "pdf_url": "",
      "publication_date": "2021-01-06",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "title": "High-Frequency Matters: Attack and Defense for Image-Processing Model Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks with an almost 100% success rate. To counter this attack, we propose an adversarial framework for the watermarking network. The framework incorporates a specially-designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "120750da2c19e0a4398b6168a9c8ff574fb175e6",
      "title": "Transfer Learning-Based Lightweight SSD Model for Detection of Pests in Citrus",
      "abstract": "In citrus cultivation, it is a difficult task for farmers to classify different pests correctly and make proper decisions to prevent citrus damage. This work proposes an efficient modified lightweight transfer learning model which combines the effectiveness and accuracy of citrus pest characterization with mobile terminal counting. Firstly, we utilized typical transfer learning feature extraction networks such as ResNet50, InceptionV3, VGG16, and MobileNetV3, and pre-trained the single-shot multibox detector (SSD) network to compare and analyze the classification accuracy and efficiency of each model. Then, to further reduce the amount of calculations needed, we miniaturized the prediction convolution kernel at the end of the model and added a residual block of a 1 \u00d7 1 convolution kernel to predict category scores and frame offsets. Finally, we transplanted the preferred lightweight SSD model into the mobile terminals developed by us to verify its usability. Compared to other transfer learning models, the modified MobileNetV3+RPBM can enable the SSD network to achieve accurate detection of Panonychus Citri Mcgregor and Aphids, with a mean average precision (mAP) up to 86.10% and the counting accuracy reaching 91.0% and 89.0%, respectively. In terms of speed, the mean latency of MobileNetV3+RPBM is as low as 185 ms. It was concluded that this novel and efficient modified MobileNetV3+RPBM+SSD model is effective at classifying citrus pests, and can be integrated into devices that are embedded for mobile rapid detection as well as for counting pests in citrus orchards. The work presented herein can help encourage farm managers to judge the degree of pest damage and make correct decisions regarding pesticide application in orchard management.",
      "year": 2023,
      "venue": "Agronomy",
      "authors": [
        "Linhui Wang",
        "W. Shi",
        "Yonghong Tang",
        "Zhizhuang Liu",
        "Xiongkui He",
        "Hongyan Xiao",
        "Yu Yang"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/120750da2c19e0a4398b6168a9c8ff574fb175e6",
      "pdf_url": "https://www.mdpi.com/2073-4395/13/7/1710/pdf?version=1687937835",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0877bd35fec38a6d0180880033d1baa6c97e44c",
      "title": "A hybrid DNN\u2013LSTM model for detecting phishing URLs",
      "abstract": "Phishing is an attack targeting to imitate the official websites of corporations such as banks, e-commerce, financial institutions, and governmental institutions. Phishing websites aim to access and retrieve users\u2019 important information such as personal identification, social security number, password, e-mail, credit card, and other account information. Several anti-phishing techniques have been developed to cope with the increasing number of phishing attacks so far. Machine learning and particularly, deep learning algorithms are nowadays the most crucial techniques used to detect and prevent phishing attacks because of their strong learning abilities on massive datasets and their state-of-the-art results in many classification problems. Previously, two types of feature extraction techniques [i.e., character embedding-based and manual natural language processing (NLP) feature extraction] were used in isolation. However, researchers did not consolidate these features and therefore, the performance was not remarkable. Unlike previous works, our study presented an approach that utilizes both feature extraction techniques. We discussed how to combine these feature extraction techniques to fully utilize from the available data. This paper proposes hybrid deep learning models based on long short-term memory and deep neural network algorithms for detecting phishing uniform resource locator and evaluates the performance of the models on phishing datasets. The proposed hybrid deep learning models utilize both character embedding and NLP features, thereby simultaneously exploiting deep connections between characters and revealing NLP-based high-level connections. Experimental results showed that the proposed models achieve superior performance than the other phishing detection models in terms of accuracy metric.",
      "year": 2021,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "Alper Ozcan",
        "C. Catal",
        "Emrah Donmez",
        "Behcet Senturk"
      ],
      "citation_count": 115,
      "url": "https://www.semanticscholar.org/paper/f0877bd35fec38a6d0180880033d1baa6c97e44c",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s00521-021-06401-z.pdf",
      "publication_date": "2021-08-08",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cad5690c6a4585176b4f76b4fddc13476c7a4c16",
      "title": "DA-ActNN-YOLOV5: Hybrid YOLO v5 Model with Data Augmentation and Activation of Compression Mechanism for Potato Disease Identification",
      "abstract": "To solve the problems of weak generalization of potato early and late blight recognition models in real complex scenarios, susceptibility to interference from crop varieties, colour characteristics, leaf spot shapes, disease cycles and environmental factors, and strong dependence on storage and computational resources, an improved YOLO v5 model (DA-ActNN-YOLOV5) is proposed to study potato diseases of different cycles in multiple regional scenarios. Thirteen data augmentation techniques were used to expand the data to improve model generalization and prevent overfitting; potato leaves were extracted by YOLO v5 image segmentation and labelled with LabelMe for building data samples; the component modules of the YOLO v5 network were replaced using model compression technology (ActNN) for potato disease detection when the device is low on memory. Based on this, the features extracted from all network layers are visualized, and the extraction of features from each network layer can be distinguished, from which an understanding of the feature learning behavior of the deep model can be obtained. The results show that in the scenario of multiple complex factors interacting, the identification accuracy of early and late potato blight in this study reached 99.81%. The introduced data augmentation technique improved the average accuracy by 9.22%. Compared with the uncompressed YOLO v5 model, the integrated ActNN runs more efficiently, the accuracy loss due to compressed parameters is less than 0.65%, and the time consumption does not exceed 30\u2009min, which saves a lot of computational cost and time. In summary, this research method can accurately identify potato early and late blight in various scenarios.",
      "year": 2022,
      "venue": "Computational Intelligence and Neuroscience",
      "authors": [
        "Guowei Dai",
        "Lin Hu",
        "Jing-xing Fan"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/cad5690c6a4585176b4f76b4fddc13476c7a4c16",
      "pdf_url": "https://downloads.hindawi.com/journals/cin/2022/6114061.pdf",
      "publication_date": "2022-09-23",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9ab7319dbe80549ba80e3320d0546d741a7a5791",
      "title": "ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models",
      "abstract": "Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner's attack) and significantly outperforms existing black-box attacks via substitute models.",
      "year": 2017,
      "venue": "AISec@CCS",
      "authors": [
        "Pin-Yu Chen",
        "Huan Zhang",
        "Yash Sharma",
        "Jinfeng Yi",
        "Cho-Jui Hsieh"
      ],
      "citation_count": 2067,
      "url": "https://www.semanticscholar.org/paper/9ab7319dbe80549ba80e3320d0546d741a7a5791",
      "pdf_url": "https://arxiv.org/pdf/1708.03999",
      "publication_date": "2017-08-14",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "544b82074bd539bbb55fc4123850eb991db9dc33",
      "title": "Model Selection with Model Zoo via Graph Learning",
      "abstract": "Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph's effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.",
      "year": 2024,
      "venue": "IEEE International Conference on Data Engineering",
      "authors": [
        "Ziyu Li",
        "Hilco van der Wilk",
        "Danning Zhan",
        "Megha Khosla",
        "A. Bozzon",
        "Rihan Hai"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/544b82074bd539bbb55fc4123850eb991db9dc33",
      "pdf_url": "https://arxiv.org/pdf/2404.03988",
      "publication_date": "2024-04-05",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "450c9b54d208885348624643ce2dd6a8d430c79a",
      "title": "Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization",
      "abstract": "The proliferation of pretrained models, as a result of advancements in pretraining techniques, has led to the emergence of a vast zoo of publicly available models. Effectively utilizing these resources to obtain models with robust out-of-distribution generalization capabilities for downstream tasks has become a crucial area of research. Previous research has primarily focused on identifying the most powerful models within the model zoo, neglecting to fully leverage the diverse inductive biases contained within. This paper argues that the knowledge contained in weaker models is valuable and presents a method for leveraging the diversity within the model zoo to improve out-of-distribution generalization capabilities. Specifically, we investigate the behaviors of various pretrained models across different domains of downstream tasks by characterizing the variations in their encoded representations in terms of two dimensions: diversity shift and correlation shift. This characterization enables us to propose a new algorithm for integrating diverse pretrained models, not limited to the strongest models, in order to achieve enhanced out-of-distribution generalization performance. Our proposed method demonstrates state-of-the-art empirical results on a variety of datasets, thus validating the benefits of utilizing diverse knowledge.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Yimeng Chen",
        "Tianyang Hu",
        "Fengwei Zhou",
        "Zhenguo Li",
        "Zhiming Ma"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/450c9b54d208885348624643ce2dd6a8d430c79a",
      "pdf_url": "http://arxiv.org/pdf/2306.02595",
      "publication_date": "2023-06-05",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8467d3032147d2d3508b2f98e09aea566d8a03e",
      "title": "DAMix: Exploiting Deep Autoregressive Model Zoo for Improving Lossless Compression Generalization",
      "abstract": "Deep generative models have demonstrated superior performance in lossless compression on identically distributed data. However, in real-world scenarios, data to be compressed are of various distributions and usually cannot be known in advance. Thus, commercially expected neural compression must have strong Out-of-Distribution (OoD) generalization capabilities. Compared with traditional compression methods, deep learning methods have intrinsic flaws for OoD generalization. In this work, we make the attempt to tackle this challenge via exploiting a zoo of Deep Autoregressive models (DAMix). We build a model zoo consisting of autoregressive models trained on data from diverse distributions. In the test phase, we select useful expert models by a simple model evaluation score and adaptively aggregate the predictions of selected models. By assuming the outputs from each expert model are biased in favor of their training distributions, a von Mises-Fisher based filter is proposed to recover the value of unbiased predictions that provides more accurate density estimations than a single model. We derive the posterior of unbiased predictions as well as concentration parameters in the filter, and a novel temporal Stein variational gradient descent for sequential data is proposed to adaptively update the posterior distributions. We evaluate DAMix on 22 image datasets, including in-distribution and OoD data, and demonstrate that making use of unbiased predictions has up to 45.6% improvement over the single model trained on ImageNet.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Qishi Dong",
        "Fengwei Zhou",
        "Ning Kang",
        "Chuanlong Xie",
        "Shifeng Zhang",
        "Jiawei Li",
        "Heng Peng",
        "Z. Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a8467d3032147d2d3508b2f98e09aea566d8a03e",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/25543/25315",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7471367f673778d98830fcd14202c287cfbdef7",
      "title": "BioImage Model Zoo: A Community-Driven Resource for Accessible Deep Learning in BioImage Analysis",
      "abstract": "Deep learning-based approaches are revolutionizing imaging-driven scientific research. However, the accessibility and reproducibility of deep learning-based workflows for imaging scientists remain far from sufficient. Several tools have recently risen to the challenge of democratizing deep learning by providing user-friendly interfaces to analyze new data with pre-trained or fine-tuned models. Still, few of the existing pre-trained models are interoperable between these tools, critically restricting a model\u2019s overall utility and the possibility of validating and reproducing scientific analyses. Here, we present the BioImage Model Zoo (https://bioimage.io): a community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep). To enable everyone to contribute and consume the Zoo resources, we provide a model standard to enable cross-compatibility, a rich list of example models and practical use-cases, developer tools, documentation, and the accompanying infrastructure for model upload, download and testing. Our contribution aims to lay the groundwork to make deep learning methods for microscopy imaging findable, accessible, interoperable, and reusable (FAIR) across software tools and platforms.",
      "year": 2022,
      "venue": "bioRxiv",
      "authors": [
        "Ouyang Wei",
        "Fynn Beuttenmueller",
        "Estibaliz G\u00f3mez-de-Mariscal",
        "Constantin Pape",
        "Tom Burke",
        "C. Garc\u00eda-L\u00f3pez-de-Haro",
        "Craig T Russell",
        "Luc\u00eda Moya-Sans",
        "Cristina de-la-Torre-Guti\u00e9rrez",
        "Deborah Schmidt",
        "D. Kutra",
        "Maksim Novikov",
        "Martin Weigert",
        "Uwe Schmidt",
        "P. Bankhead",
        "Guillaume Jacquemet",
        "D. Sage",
        "Ricardo Henriques",
        "A. Mu\u00f1oz-Barrutia",
        "E. Lundberg",
        "Florian Jug",
        "A. Kreshuk"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/b7471367f673778d98830fcd14202c287cfbdef7",
      "pdf_url": "https://www.biorxiv.org/content/biorxiv/early/2022/06/08/2022.06.07.495102.full.pdf",
      "publication_date": "2022-06-08",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "790b656eb0abfa355692c679866788718ce133a3",
      "title": "Discrepancies among pre-trained deep neural networks: a new threat to model zoo reliability",
      "abstract": "Training deep neural networks (DNNs) takes significant time and resources. A practice for expedited deployment is to use pre-trained deep neural networks (PTNNs), often from model zoos--collections of PTNNs; yet, the reliability of model zoos remains unexamined. In the absence of an industry standard for the implementation and performance of PTNNs, engineers cannot confidently incorporate them into production systems. As a first step, discovering potential discrepancies between PTNNs across model zoos would reveal a threat to model zoo reliability. Prior works indicated existing variances in deep learning systems in terms of accuracy. However, broader measures of reliability for PTNNs from model zoos are unexplored. This work measures notable discrepancies between accuracy, latency, and architecture of 36 PTNNs across four model zoos. Among the top 10 discrepancies, we find differences of 1.23%-2.62% in accuracy and 9%-131% in latency. We also find mismatches in architecture for well-known DNN architectures (e.g., ResNet and AlexNet). Our findings call for future works on empirical validation, automated tools for measurement, and best practices for implementation.",
      "year": 2022,
      "venue": "ESEC/SIGSOFT FSE",
      "authors": [
        "Diego Montes",
        "Pongpatapee Peerapatanapokin",
        "Jeff Schultz",
        "Chengjun Guo",
        "Wenxin Jiang",
        "James C. Davis"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/790b656eb0abfa355692c679866788718ce133a3",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3540250.3560881",
      "publication_date": "2022-11-07",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6129238f7de8acc0316aa2ac677e59bbeef3c6cc",
      "title": "ZooD: Exploiting Model Zoo for Out-of-Distribution Generalization",
      "abstract": "Recent advances on large-scale pre-training have shown great potentials of leveraging a large set of Pre-Trained Models (PTMs) for improving Out-of-Distribution (OoD) generalization, for which the goal is to perform well on possible unseen domains after fine-tuning on multiple training domains. However, maximally exploiting a zoo of PTMs is challenging since fine-tuning all possible combinations of PTMs is computationally prohibitive while accurate selection of PTMs requires tackling the possible data distribution shift for OoD tasks. In this work, we propose ZooD, a paradigm for PTMs ranking and ensemble with feature selection. Our proposed metric ranks PTMs by quantifying inter-class discriminability and inter-domain stability of the features extracted by the PTMs in a leave-one-domain-out cross-validation manner. The top-K ranked models are then aggregated for the target OoD task. To avoid accumulating noise induced by model ensemble, we propose an efficient variational EM algorithm to select informative features. We evaluate our paradigm on a diverse model zoo consisting of 35 models for various OoD tasks and demonstrate: (i) model ranking is better correlated with fine-tuning ranking than previous methods and up to 9859x faster than brute-force fine-tuning; (ii) OoD generalization after model ensemble with feature selection outperforms the state-of-the-art methods and the accuracy on most challenging task DomainNet is improved from 46.5\\% to 50.6\\%. Furthermore, we provide the fine-tuning results of 35 PTMs on 7 OoD datasets, hoping to help the research of model zoo and OoD generalization. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/zood.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Qishi Dong",
        "M. Awais",
        "Fengwei Zhou",
        "Chuanlong Xie",
        "Tianyang Hu",
        "Yongxin Yang",
        "S. Bae",
        "Zhenguo Li"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6129238f7de8acc0316aa2ac677e59bbeef3c6cc",
      "pdf_url": "http://arxiv.org/pdf/2210.09236",
      "publication_date": "2022-10-17",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a421549ffb06adfa0ddf8fa7047ffee7b6cf297e",
      "title": "A Model Zoo of Vision Transformers",
      "abstract": "The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing model zoos are limited in size and architecture and neglect the transformer, which is among the currently most successful neural network architectures. We address this gap by introducing the first model zoo of vision transformers (ViT). To better represent recent training approaches, we develop a new blueprint for model zoo generation that encompasses both pre-training and fine-tuning steps, and publish 250 unique models. They are carefully generated with a large span of generating factors, and their diversity is validated using a thorough choice of weight-space and behavioral metrics. To further motivate the utility of our proposed dataset, we suggest multiple possible applications grounded in both extensive exploratory experiments and a number of examples from the existing literature. By extending previous lines of similar work, our model zoo allows researchers to push their model population-based methods from the small model regime to state-of-the-art architectures. We make our model zoo available at github.com/ModelZoos/ViTModelZoo.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Damian Falk",
        "L\u00e9o Meynent",
        "Florence Pfammatter",
        "Konstantin Sch\u00fcrholt",
        "Damian Borth"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a421549ffb06adfa0ddf8fa7047ffee7b6cf297e",
      "pdf_url": "",
      "publication_date": "2025-04-14",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7667c4e40df9ad6e55b82638e61adff2c8f283a8",
      "title": "Jittor-GAN: A fast-training generative adversarial network model zoo based on Jittor",
      "abstract": "With the emergence of CNNs and massive datasets, the performance of many tasks in computer vision has been greatly improved, such as object detection, instance segmentation, and image generation. The last has many novel applications, including image-toimage translation, image inpainting, and image superresolution. It can generate authentic and creative images. GAN [1] is the current mainstream model for image generation. It usually consists of an encoder, a generator, and a discriminator, which are constructed from CNN layers. The encoder is responsible for mapping images to a latent space. The generator is responsible for generating images from latent vectors, using one or multiple images. The discriminator is responsible for distinguishing generated images from real images. Through joint adversarial training of the generator and the discriminator, the generative ability of the generator is continuously improved, thereby generating more and more realistic images. However, training GAN models is time consuming. Thus, we have implemented a GAN model zoo based on Jittor, a fully just-in-time (JIT) complied deep learning framework by Tsinghua University [2]. This model zoo is a collection of 27 mainstream GAN models published from 2014 to 2019, listed in Table 1. These models have an average of 3070 citations per model, and they have great influences and have been widely used in both academia and industry. Our model zoo covers 4 kinds of tasks, including image generation (G), image-to-image translation",
      "year": 2021,
      "venue": "Computational Visual Media",
      "authors": [
        "Wen-Yang Zhou",
        "Guo-Wei Yang",
        "Shimin Hu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/7667c4e40df9ad6e55b82638e61adff2c8f283a8",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s41095-021-0203-2.pdf",
      "publication_date": "2021-01-15",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2c3111cbd1327e9b8616082eef39dc0a3a516b2a",
      "title": "Natural Attack for Pre-trained Models of Code",
      "abstract": "Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement. In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pretrained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively.",
      "year": 2022,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zhou Yang",
        "Jieke Shi",
        "Junda He",
        "David Lo"
      ],
      "citation_count": 186,
      "url": "https://www.semanticscholar.org/paper/2c3111cbd1327e9b8616082eef39dc0a3a516b2a",
      "pdf_url": "https://ink.library.smu.edu.sg/sis_research/7654",
      "publication_date": "2022-01-21",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c077e8f9ab701270dec08ad8934901474a46ecf1",
      "title": "FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",
      "abstract": "The incredible development of federated learning (FL) has benefited various tasks in the domains of computer vision and natural language processing, and the existing frameworks such as TFF and FATE has made the deployment easy in real-world applications. However, federated graph learning (FGL), even though graph data are prevalent, has not been well supported due to its unique characteristics and requirements. The lack of FGL-related framework increases the efforts for accomplishing reproducible research and deploying in real-world applications. Motivated by such strong demand, in this paper, we first discuss the challenges in creating an easy-to-use FGL package and accordingly present our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo and ModelZoo for out-of-the-box FGL capability; (3) an efficient model auto-tuning component; and (4) off-the-shelf privacy attack and defense abilities. We validate the effectiveness of FS-G by conducting extensive experiments, which simultaneously gains many valuable insights about FGL for the community. Moreover, we employ FS-G to serve the FGL application in real-world E-commerce scenarios, where the attained improvements indicate great potential business benefits. We publicly release FS-G, as submodules of FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's research and enable broad applications that would otherwise be infeasible due to the lack of a dedicated package.",
      "year": 2022,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhen Wang",
        "Weirui Kuang",
        "Yuexiang Xie",
        "Liuyi Yao",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "citation_count": 99,
      "url": "https://www.semanticscholar.org/paper/c077e8f9ab701270dec08ad8934901474a46ecf1",
      "pdf_url": "https://arxiv.org/pdf/2204.05562",
      "publication_date": "2022-04-12",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3445171a33472c65d0a207e55fc5d7383d192549",
      "title": "Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization",
      "abstract": null,
      "year": 2023,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Zijing Yin",
        "Yiwen Xu",
        "Fuchen Ma",
        "Haohao Gao",
        "Lei Qiao",
        "Yu Jiang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/3445171a33472c65d0a207e55fc5d7383d192549",
      "pdf_url": "",
      "publication_date": "2023-02-13",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d510d65614dd80b7902ea94e5fb6708f27d89894",
      "title": "Real-Time Locational Detection of Stealthy False Data Injection Attack in Smart Grid: Using Multivariate-Based Multi-Label Classification Approach",
      "abstract": "Recently, false data injection attacks (FDIAs) have been identified as a significant category of cyber-attacks targeting smart grids\u2019 state estimation and monitoring systems. These cyber-attacks aim to mislead control system operations by compromising the readings of various smart grid meters. The real-time and precise locational identification of FDIAs is crucial for smart grid security and reliability. This paper proposes a multivariate-based multi-label locational detection (MMLD) mechanism to detect the presence and locations of FDIAs in real-time measurements with precise locational detection accuracy. The proposed architecture is a parallel structure that concatenates Long Short-Term Memory (LSTM) with Temporal Convolutional Neural Network (TCN). The proposed architecture is trained using Keras with Tensorflow libraries, and its performance is verified using an IEEE standard bus system in the MATPOWER package. Extensive testing has shown that the proposed approach effectively improves the presence-detection accuracy for locating stealthy FDIAs in small and large systems under various attack conditions. In addition, this work provides a customized loss function for handling the class imbalance problem. Simulation results reveal that our MMLD technique has a modest advantage in some aspects. First, our mechanism outperforms benchmark models because the problem is formulated as a multivariate-based multi-label classification problem. Second, it needs fewer iterations for training and reaching the optimal model. More specifically, our approach is less complex and more scalable than benchmark algorithms.",
      "year": 2022,
      "venue": "Energies",
      "authors": [
        "Hanem I. Hegazy",
        "Adly S. Tag Eldien",
        "M. M. Tantawy",
        "M. Fouda",
        "Heba A. Tageldien"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/d510d65614dd80b7902ea94e5fb6708f27d89894",
      "pdf_url": "https://www.mdpi.com/1996-1073/15/14/5312/pdf?version=1658470373",
      "publication_date": "2022-07-21",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "679cd39680fd662103ca2e24992eb6b9d2e94a7f",
      "title": "ATG: An Attack Traffic Generation Tool for Security Testing of In-vehicle CAN Bus",
      "abstract": "In-vehicle security research is challenging because it is hard for most researchers to get a real vehicle for security evaluation. On the other hand, the existing software solutions are either very expensive or having very limited functionality. There is a high demand for a convenient tool which can generate flexible datasets for in-vehicle attack and defense evaluation. In this work, we design and develop an Attack Traffic Generation (ATG) tool for security testing of in-vehicle CAN bus. It removes the barrier for research in this area by providing an open-source software package which works with a cheap, widely available hardware configuration. ATG provides a free and functional toolkit to automotive security researchers for easy and effective interaction with real or simulated CAN bus. One of the most important features of ATG is automatic generation of attack payloads. The payloads can be preconfigured and used within multiple attack modes. ATG can inject attack packets into CAN bus and record the CAN bus traffic in real time. The replay mode enables effective evaluation of CAN bus security implementations using the pre-classified datasets. In addition, a unified data format for raw re-playable CAN sequences enables different automotive research teams to exchange datasets and preform security testing simultaneously against different vehicles and simulation hardware.",
      "year": 2018,
      "venue": "ARES",
      "authors": [
        "Tianxiang Huang",
        "Jianying Zhou",
        "Andrei Bytes"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/679cd39680fd662103ca2e24992eb6b9d2e94a7f",
      "pdf_url": "",
      "publication_date": "2018-08-27",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16ed8c879aa4d161407d97bb9e7bf3254c905d8b",
      "title": "Attack modelling : towards a second generation benchmark",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "Svyatoslav Voloshynovskyy",
        "S. Pereira",
        "Iquise Mier",
        "Victor Max Pun",
        "S. Voloshynovskiy",
        "V. Iquise",
        "T. Pun"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/16ed8c879aa4d161407d97bb9e7bf3254c905d8b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a2cb949e9ce2891291fb4bc99567d8a81944122",
      "title": "Exploiting Robust Model Watermarking Against the Model Fine-Tuning Attack via Flat Minima Aware Optimizers",
      "abstract": "With the rapid advancement of deep neural networks (DNNs), model watermarking has emerged as a widely adopted technique for safeguarding model copyrights. A prevalent method involves utilizing a watermark decoder to retrieve watermark bits from generated outputs, but such methods are often vulnerable to model fine-tuning attacks. Traditionally, this challenge is mitigated through adversarial training or data augmentation, both of which significantly increase the computational burden. In this paper, we present a solution employing Flat Minima Aware (FMA) optimizers to bolster the robustness of model watermarking without requiring additional training data. By optimizing the watermark loss with flat minima awareness, our approaches significantly enhance the robustness of watermarks against the model fine-tuning attack. Comprehensive experiments have demonstrated our method\u2019s superior ability to preserve watermark integrity. These findings suggest that this innovative optimization strategy offers a robust and efficient pathway for protecting models, thereby contributing to more secure and reliable model copyright protection mechanisms.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Dongdong Lin",
        "Yue Li",
        "Bin Li",
        "Jiwu Huang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3a2cb949e9ce2891291fb4bc99567d8a81944122",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "76db10284badc7ccb317321dfa6a400210ec97d1",
      "title": "Fine-tuning a pre-trained ResNet50 model to detect distributed denial of service attack",
      "abstract": "Distributed denial-of-service (DDoS) attacks pose a significant risk to the dependability and consistency of network services. The utilization of deep learning (DL) models has displayed encouraging outcomes in the identification of DDoS attacks. Nevertheless, crafting a precise DL model necessitates an extensive volume of labeled data and substantial computational capabilities. Within this piece, we introduce a technique to enhance a pre-trained DL model for the identification of DDoS attacks. Our strategy\u2019s efficacy is showcased on an openly accessible dataset, revealing that the fine-tuned model we propose surpasses both the initial pre-trained model and other cutting-edge approaches in performance. The suggested fine-tuned model attained 95.1% accuracy, surpassing the initial pre-trained model as well as other leading-edge techniques. Please note that the specific evaluation metrics and their values may vary depending on the implementation, hyperparameter settings, number of datasets, or dataset characteristics. The proposed approach has several advantages, including reducing the amount of labeled data required and accelerating the training process. Initiating with a pre-existing ResNet50 model can also enhance the eventual model\u2019s accuracy, given that the pre-trained model has already acquired the ability to extract significant features from unprocessed data.",
      "year": 2024,
      "venue": "Bulletin of Electrical Engineering and Informatics",
      "authors": [
        "Ahmad Sanmorino",
        "H. Kesuma"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/76db10284badc7ccb317321dfa6a400210ec97d1",
      "pdf_url": "https://beei.org/index.php/EEI/article/download/7014/3659",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18a37eb07951c79c3290a4be400a938383a8d31c",
      "title": "Targeted Vaccine: Safety Alignment for Large Language Models Against Harmful Fine-Tuning via Layer-Wise Perturbation",
      "abstract": "Harmful fine-tuning attack poses a serious threat to the online fine-tuning service. Vaccine, a recent alignment-stage defense, applies uniform perturbation to all layers of embedding to make the model robust to the simulated embedding drift. However, applying layer-wise uniform perturbation may lead to excess perturbations for some particular non-safety-critical layers, resulting in defense performance degradation and unnecessary memory consumption. To address this limitation, we propose a Targeted Vaccine (T-Vaccine), a memory-efficient safety alignment method that applies perturbation to only selected layers of the model. T-Vaccine follows two core steps: First, it uses the harmful gradient norm as a statistical metric to identify the safety-critical layers. Second, instead of applying uniform perturbation across all layers, T-Vaccine only applies perturbation to the safety-critical layers while keeping other layers frozen during training. Results show that T-Vaccine outperforms Vaccine in terms of both defense effectiveness and resource efficiency. Comparison with other defense baselines, e.g., RepNoise and TAR also demonstrate the superiority of T-Vaccine. Notably, T-Vaccine is the first defense that enables a fine-tuning-based alignment method for 7B pre-trained models trained on consumer GPUs with limited memory (e.g., RTX 4090).",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Guozhi Liu",
        "Weiwei Lin",
        "Qi Mu",
        "Tiansheng Huang",
        "Ruichao Mo",
        "Yuren Tao",
        "Li Shen"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/18a37eb07951c79c3290a4be400a938383a8d31c",
      "pdf_url": "",
      "publication_date": "2024-10-13",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32837cb2baad460c3a86f8f152c2f5c219022413",
      "title": "Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal Attack for DNN Models",
      "abstract": "Watermarking has become the tendency in protecting the intellectual property of DNN models. Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. However, these attacks mainly adopted sophisticated fine-tuning techniques, which have certain fatal drawbacks or unrealistic assumptions. In this paper, we propose a novel watermark removal attack from a different perspective. Instead of just fine-tuning the watermarked models, we design a simple yet powerful transformation algorithm by combining imperceptible pattern embedding and spatial-level transformations, which can effectively and blindly destroy the memorization of watermarked models to the watermark samples. We also introduce a lightweight fine-tuning strategy to preserve the model performance. Our solution requires much less resource or knowledge about the watermarking scheme than prior works. Extensive experimental results indicate that our attack can bypass state-of-the-art watermarking solutions with very high success rates. Based on our attack, we propose watermark augmentation techniques to enhance the robustness of existing watermarks.",
      "year": 2020,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Shangwei Guo",
        "Tianwei Zhang",
        "Han Qiu",
        "Yi Zeng",
        "Tao Xiang",
        "Yang Liu"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/32837cb2baad460c3a86f8f152c2f5c219022413",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0500.pdf",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ff1ed695b203785b9b44a953bfc3f8b914fcdfd9",
      "title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models",
      "abstract": "Vision-language models (VLMs) have achieved significant strides in recent times specially in multimodal tasks, yet they remain susceptible to adversarial attacks on their vision components. To address this, we propose Sim-CLIP, an unsupervised adversarial fine-tuning method that enhances the robustness of the widely-used CLIP vision encoder against such attacks while maintaining semantic richness and specificity. By employing a Siamese architecture with cosine similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder exhibit significantly enhanced robustness against adversarial attacks, while preserving semantic meaning of the perturbed images. Notably, Sim-CLIP does not require additional training or fine-tuning of the VLM itself; replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to provide robustness. This work underscores the significance of reinforcing foundational models like CLIP to safeguard the reliability of downstream VLM applications, paving the way for more secure and effective multimodal systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Md. Zarif Hossain",
        "Ahmed Imteaj"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/ff1ed695b203785b9b44a953bfc3f8b914fcdfd9",
      "pdf_url": "",
      "publication_date": "2024-07-20",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "408d582ce712b8273cd6b514688abdaa5b8ff715",
      "title": "Enhancing Targeted Transferability VIA Feature Space Fine-Tuning",
      "abstract": "Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. Yet, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features conducive to the target class and discourage the features to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks' targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rest on training target-specific classifiers or generators with additional data. The code is available at: github.com/zengh5/TA_feature_FT.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hui Zeng",
        "Biwei Chen",
        "Anjie Peng"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/408d582ce712b8273cd6b514688abdaa5b8ff715",
      "pdf_url": "https://arxiv.org/pdf/2401.02727",
      "publication_date": "2024-01-05",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b69c7677e181ac113c281aa1f351f188f09bf4a9",
      "title": "VTT-LLM: Advancing Vulnerability-to-Tactic-and-Technique Mapping through Fine-Tuning of Large Language Model",
      "abstract": "Vulnerabilities are often accompanied by cyberattacks. CVE is the largest repository of open vulnerabilities, which keeps expanding. ATT&CK models known multi-step attacks both tactically and technically and remains up to date. It is valuable to correlate the vulnerability in CVE with the corresponding tactic and technique of ATT&CK which exploit the vulnerability, for active defense. Mappings manually is not only time-consuming but also difficult to keep up-to-date. Existing language-based automated mapping methods do not utilize the information associated with attack behaviors outside of CVE and ATT&CK and are therefore ineffective. In this paper, we propose a novel framework named VTT-LLM for mapping Vulnerabilities to Tactics and Techniques based on Large Language Models, which consists of a generation model and a mapping model. In order to generate fine-tuning instructions for LLM, we create a template to extract knowledge of CWE (a standardized list of common weaknesses) and CAPEC (a standardized list of common attack patterns). We train the generation model of VTT-LLM by fine-tuning the LLM according to the above instructions. The generation model correlates vulnerability and attack through their descriptions. The mapping model transforms the descriptions of ATT&CK tactics and techniques into vectors through text embedding and further associates them with attacks through semantic matching. By leveraging the knowledge of CWE and CAPEC, VTT-LLM can eventually automate the process of linking vulnerabilities in CVE to the attack techniques and tactics of ATT&CK. Experiments on the latest public dataset, ChatGPT-VDMEval, show the effectiveness of VTT-LLM with an accuracy of 85.18%, which is 13.69% and 54.42% higher than the existing CVET and ChatGPT-based methods, respectively. In addition, compared to fine-tuning without outside knowledge, the accuracy of VTT-LLM with chain fine-tuning is 9.24% higher on average across different LLMs.",
      "year": 2024,
      "venue": "Mathematics",
      "authors": [
        "Chenhui Zhang",
        "Le Wang",
        "Dunqiu Fan",
        "Junyi Zhu",
        "Tang Zhou",
        "Liyi Zeng",
        "Zhaohua Li"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b69c7677e181ac113c281aa1f351f188f09bf4a9",
      "pdf_url": "https://www.mdpi.com/2227-7390/12/9/1286/pdf?version=1713961778",
      "publication_date": "2024-04-24",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b6feff61b5d04e2732e44c60b206795217f1238",
      "title": "Internet of Things (IOT) Intrusion Detection Utilizing CNN and Fine Tuning It Using Various Optimization Parameters",
      "abstract": "The Internet of Things (IOT) has experienced significant expansion in recent years, resulting in the development of countless gadgets and systems aimed at improving everyday life. This study explores the fundamental aspects of security in the Internet of Things (IOT), including vulnerabilities, potential attack routes, and essential countermeasures. This study presents a novel intrusion detection methodology for the Internet of Things (IOT) by leveraging Convolutional Neural Networks (CNNs). The proposed method aims to improve the performance of the CNN model by fine-tuning it with different optimization parameters. The initial approach involves the building of a convolutional neural network (CNN) model for intrusion detection, capitalizing on its inherent capability to extract information in a hierarchical manner. The objective of the fine-tuning procedure is to optimize the model's accuracy, sensitivity and specificity, hence enhancing its capacity to reliably identify and categorize intrusions within the Internet of Things (IOT) network. We conduct tests on a benchmark IOT intrusion detection dataset, assessing the proposed approach's performance in terms of Accuracy and Loss Graphs and accuracy. The findings confirm the effectiveness of utilizing Convolutional Neural Networks (CNNs) in the context of Intrusion Detection Systems (IDS) for Internet of Things (IOT) environments. Furthermore, the study highlights the need of tweaking the parameters of the CNN model to enhance its detection capabilities and achieve higher performance. Moreover, the present study highlights the ever-evolving nature of security in the Internet of Things (IOT), placing significant emphasis on the need of continuous research, collaborative endeavours across several sectors, and the establishment of regulatory frameworks to foster a safe and reliable IOT environment.",
      "year": 2023,
      "venue": "2023 3rd International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",
      "authors": [
        "Kaushiv",
        "Kanwarpartap Singh Gill",
        "Rahul Chauhan",
        "Sarishma Dangi",
        "D. Banerjee"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/2b6feff61b5d04e2732e44c60b206795217f1238",
      "pdf_url": "",
      "publication_date": "2023-12-29",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "175b32c07e56f881479be4c5a74bfa3c731cc454",
      "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
      "abstract": "Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.In this work, we present a novel fine-tuning approach called RObust SEletive fine-tuning (ROSE) to address this issue.ROSE conducts selective updates when adapting pre-trained models to downstream tasks, filtering out invaluable and unrobust updates of parameters.Specifically, we propose two strategies: the first-order and second-order ROSE for selecting target robust parameters.The experimental results show that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above.Furthermore, ROSE can be easily incorporated into existing fine-tuning methods to improve their adversarial robustness further.The empirical analysis confirms that ROSE eliminates unrobust spurious updates during fine-tuning, leading to solutions corresponding to flatter and wider optima than the conventional method.Code is available at https://github.com/jiangllan/ROSE.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lan Jiang",
        "Hao Zhou",
        "Yankai Lin",
        "Peng Li",
        "Jie Zhou",
        "R. Jiang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/175b32c07e56f881479be4c5a74bfa3c731cc454",
      "pdf_url": "http://arxiv.org/pdf/2210.09658",
      "publication_date": "2022-10-18",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "31bb493c4d3286562a5cbadce439bb591ba65d53",
      "title": "Toward Unified Data and Algorithm Fairness via Adversarial Data Augmentation and Adaptive Model Fine-tuning",
      "abstract": "There is some recent research interest in algorithmic fairness for biased data. There are a variety of pre-, in-, and post-processing methods designed for this problem. However, these methods are exclusively targeting data unfairness and algorithmic unfairness. In this paper, we propose a novel intra-processing method to broaden the application scenario of fairness methods, which can simultaneously address the two bias sources. Since training modern deep models from scratch is expensive due to the enormous training data and the complicated structures, we propose an augmentation and fine-tuning framework. First, we design an adversarial attack to generate weighted samples disentangled with the protected attribute. Next, we identify the fair sub-structure in the biased model and fine-tune the model via weight reactivation. At last, we provide an optional joint training scheme for the augmentation and the fine-tuning. Our method can be combined with a variety of fairness measures. We benchmark our method and some related baselines to show the advantage and the scalability. Experimental results on several standard datasets demonstrate that our approach can effectively learn fair augmentation and achieve superior results to the state-of-the-art baselines. Our method also generalizes well to different types of data.",
      "year": 2022,
      "venue": "Industrial Conference on Data Mining",
      "authors": [
        "Yanfu Zhang",
        "Runxue Bao",
        "Jian Pei",
        "Heng Huang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/31bb493c4d3286562a5cbadce439bb591ba65d53",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "unfairness attack",
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "474b99f635b369793bf857c6408ab8db40bbf903",
      "title": "Fine-tuning more stable neural text classifiers for defending word level adversarial attacks",
      "abstract": null,
      "year": 2022,
      "venue": "Applied intelligence (Boston)",
      "authors": [
        "Zibo Yi",
        "Jie Yu",
        "Yusong Tan",
        "Qingbo Wu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/474b99f635b369793bf857c6408ab8db40bbf903",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9c9fafc3105325428fe6f6ef58709be433510b2f",
      "title": "Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning",
      "abstract": null,
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Chenglei Si",
        "Zhengyan Zhang",
        "Fanchao Qi",
        "Zhiyuan Liu",
        "Yasheng Wang",
        "Qun Liu",
        "Maosong Sun"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/9c9fafc3105325428fe6f6ef58709be433510b2f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8a25739903cab07c74556b8c2d9743749e1be1e5",
      "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
      "abstract": "With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g.,\"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yu Wang",
        "Xiaogeng Liu",
        "Yu Li",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "citation_count": 99,
      "url": "https://www.semanticscholar.org/paper/8a25739903cab07c74556b8c2d9743749e1be1e5",
      "pdf_url": "",
      "publication_date": "2024-03-14",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3b5e7f456043fa96dc4673dc11156834fd9a985e",
      "title": "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
      "abstract": "Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming typically uses reinforcement learning to fine-tune an attacker language model to generate prompts that elicit undesirable responses from a target LLM, as measured, for example, by an auxiliary toxicity classifier. We show that even with explicit regularization to favor novelty and diversity, existing approaches suffer from mode collapse or fail to generate effective attacks. As a flexible and probabilistically principled alternative, we propose to use GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts. We find that the attacks generated by our method are effective against a wide range of target LLMs, both with and without safety tuning, and transfer well between target LLMs. Finally, we demonstrate that models safety-tuned using a dataset of red-teaming prompts generated by our method are robust to attacks from other RL-based red-teaming approaches.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Seanie Lee",
        "Minsu Kim",
        "Lynn Cherif",
        "David Dobre",
        "Juho Lee",
        "Sung Ju Hwang",
        "Kenji Kawaguchi",
        "G. Gidel",
        "Y. Bengio",
        "Nikolay Malkin",
        "Moksh Jain"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/3b5e7f456043fa96dc4673dc11156834fd9a985e",
      "pdf_url": "",
      "publication_date": "2024-05-28",
      "keywords_matched": [
        "fine-tuning attack",
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e41eafa8eb3bd1230a1f02e0588571dbec41c403",
      "title": "StagedVulBERT: Multigranular Vulnerability Detection With a Novel Pretrained Code Model",
      "abstract": "The emergence of pre-trained model-based vulnerability detection methods has significantly advanced the field of automated vulnerability detection. However, these methods still face several challenges, such as difficulty in learning effective feature representations of statements for fine-grained predictions and struggling to process overly long code sequences. To address these issues, this study introduces StagedVulBERT, a novel vulnerability detection framework that leverages a pre-trained code language model and employs a coarse-to-fine strategy. The key innovation and contribution of our research lies in the development of the CodeBERT-HLS component within our framework, specialized in hierarchical, layered, and semantic encoding. This component is designed to capture semantics at both the token and statement levels simultaneously, which is crucial for achieving more accurate multi-granular vulnerability detection. Additionally, CodeBERT-HLS efficiently processes longer code token sequences, making it more suited to real-world vulnerability detection. Comprehensive experiments demonstrate that our method enhances the performance of vulnerability detection at both coarse- and fine-grained levels. Specifically, in coarse-grained vulnerability detection, StagedVulBERT achieves an F1 score of 92.26%, marking a 6.58% improvement over the best-performing methods. At the fine-grained level, our method achieves a Top-5% accuracy of 65.69%, which outperforms the state-of-the-art methods by up to 75.17%.",
      "year": 2024,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Yuan Jiang",
        "Yujian Zhang",
        "Xiaohong Su",
        "Christoph Treude",
        "Tiantian Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e41eafa8eb3bd1230a1f02e0588571dbec41c403",
      "pdf_url": "http://arxiv.org/pdf/2410.05766",
      "publication_date": "2024-10-08",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3da5083c580cb001fbbe95973b2486231ff00531",
      "title": "Large Language Model for Vulnerability Detection: Emerging Results and Future Directions",
      "abstract": "Previous learning-based vulnerability detection methods relied on either medium-sized pretrained models or smaller neural networks from scratch. Recent advancements in Large Pre-Trained Language Models (LLMs) have showcased remarkable few-shot learning capabilities in various tasks. However, the effectiveness of LLMs in detecting software vulnerabilities is largely unexplored. This paper aims to bridge this gap by exploring how LLMs perform with various prompts, particularly focusing on two state-of-the-art LLMs: GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves competitive performance with the prior state-of-the-art vulnerability detection approach and GPT-4 consistently outperformed the state-of-the-art.",
      "year": 2024,
      "venue": "2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)",
      "authors": [
        "Xin Zhou",
        "Ting Zhang",
        "David Lo"
      ],
      "citation_count": 111,
      "url": "https://www.semanticscholar.org/paper/3da5083c580cb001fbbe95973b2486231ff00531",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3639476.3639762",
      "publication_date": "2024-01-27",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "168f7dd6961317cba1ac5e97ba4aa33d6c1037b2",
      "title": "PVDetector: Pretrained Vulnerability Detection on Vulnerability-enriched Code Semantic Graph",
      "abstract": "Automated vulnerability detection is a critical issue in software security. The advent of deep learning (DL) has led to numerous studies employing DL to detect vulnerabilities in software source code. However, existing approaches still perform poorly, particularly with real-world vulnerabilities, due to the difficulty in accurately capturing their properties. To this end, we introduce PVDetector, a DL-based approach that utilizes rich code semantics, incorporates vulnerability knowledge, and leverages pretrained code representations for precise vulnerability detection. At its core, PVDetector employs a new model called Vulnerability-enriched Code Semantic Graph (VCSG), which accurately characterizes functions by distinguishing the semantics of identical variables and more finely capturing control dependencies, data dependencies, and vulnerability relationships. Additionally, we introduce four pretraining tasks specifically designed to learn the semantics of control, data, vulnerability, and variables from the VCSG model. These pretraining tasks significantly enhance PVDetector's capability to detect vulnerabilities in downstream tasks. Experimental results indicate that PVDetector outperforms SOTAs by 5.0%-12.5% in precision, 0.2%-9.7% in recall, and 3.0%-15.1% in F1-score. Additionally, it supports six programming languages and demonstrates high efficiency (e.g., 10.6 \\(\\times\\) faster than DeepDFA). When applied to seven software products, PVDetector discovered 55 vulnerabilities, including 10 silently patched flaws that had not been previously reported.",
      "year": 2025,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Jiayuan Li",
        "Lei Cui",
        "Jie Zhang",
        "Lun Li",
        "Rongrong Xi",
        "Hongsong Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/168f7dd6961317cba1ac5e97ba4aa33d6c1037b2",
      "pdf_url": "",
      "publication_date": "2025-09-19",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb931368ad1576a1b504be1405ccf8b178c49f85",
      "title": "Finetuning Large Language Models for Vulnerability Detection",
      "abstract": "This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in Java source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder\u2019s training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, handling class imbalance, and improving performance on difficult vulnerability detection datasets. This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Aleksei Shestov",
        "Rodion Levichev",
        "Ravil Mussabayev",
        "Evgeny Maslov",
        "Pavel Zadorozhny",
        "Anton Cheshkov",
        "R. Mussabayev",
        "Alymzhan Toleu",
        "Gulmira Tolegen",
        "Alexander Krassovitskiy"
      ],
      "citation_count": 71,
      "url": "https://www.semanticscholar.org/paper/bb931368ad1576a1b504be1405ccf8b178c49f85",
      "pdf_url": "https://doi.org/10.1109/access.2025.3546700",
      "publication_date": "2024-01-30",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b25032da39f5ff34b8203e84094da3b832163c1d",
      "title": "The smart contract vulnerability detection based on pre-trained model feature fusion",
      "abstract": "With the widespread application of blockchain technology across various fields, the security of smart contracts has become increasingly important. In the field of anomaly detection, particularly in smart contract vulnerability detection, pre-trained models have demonstrated tremendous potential, improving the accuracy and efficiency of vulnerability detection. However, these models typically focus on a single modality, which limits their applicability. While integrating multiple models can mitigate this issue, effectively fusing features from different pretrained models remains a challenge that needs to be addressed.To tackle this problem, this paper proposes a feature fusion method for smart contract vulnerability detection based on pre-trained models (PFSCV). The method uses contrastive learning (CL) to capture fine-grained relational information between smart contracts, generating sample pairs based on the relationships between contracts to guide the fine-tuning of the pre-trained model CodeBERT, enabling it to learn contextual information from the source code. At the same time, the UnixCoder model is fine-tuned to extract data flow information from the contracts. Subsequently, we introduce an att-BiLSTM (Attention-based Bidirectional Long Short-Term Memory) model to fuse and integrate features extracted from different pre-trained models. Finally, extensive experiments on real-world smart contract datasets validate the effectiveness and reliability of the proposed method.",
      "year": 2025,
      "venue": "2025 4th International Symposium on Computer Applications and Information Technology (ISCAIT)",
      "authors": [
        "Deguang Wang",
        "Shuo Duan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b25032da39f5ff34b8203e84094da3b832163c1d",
      "pdf_url": "",
      "publication_date": "2025-03-21",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "26d20a4336809ca8ab989b543ee54620daf89080",
      "title": "Machine Learning-Based Vulnerability Detection in Rust Code Using LLVM IR and Transformer Model",
      "abstract": "Rust\u2019s growing popularity in high-integrity systems requires automated vulnerability detection in order to maintain its strong safety guarantees. Although Rust\u2019s ownership model and compile-time checks prevent many errors, sometimes unexpected bugs may occasionally pass analysis, underlining the necessity for automated safe and unsafe code detection. This paper presents Rust-IR-BERT, a machine learning approach to detect security vulnerabilities in Rust code by analyzing its compiled LLVM intermediate representation (IR) instead of the raw source code. This approach offers novelty by employing LLVM IR\u2019s language-neutral, semantically rich representation of the program, facilitating robust detection by capturing core data and control-flow semantics and reducing language-specific syntactic noise. Our method leverages a graph-based transformer model, GraphCodeBERT, which is a transformer architecture pretrained model to encode structural code semantics via data-flow information, followed by a gradient boosting classifier, CatBoost, that is capable of handling complex feature interactions\u2014to classify code as vulnerable or safe. The model was evaluated using a carefully curated dataset of over 2300 real-world Rust code samples (vulnerable and non-vulnerable Rust code snippets) from RustSec and OSV advisory databases, compiled to LLVM IR and labeled with corresponding Common Vulnerabilities and Exposures (CVEs) identifiers to ensure comprehensive and realistic coverage. Rust-IR-BERT achieved an overall accuracy of 98.11%, with a recall of 99.31% for safe code and 93.67% for vulnerable code. Despite these promising results, this study acknowledges potential limitations such as focusing primarily on known CVEs. Built on a representative dataset spanning over 2300 real-world Rust samples from diverse crates, Rust-IR-BERT delivers consistently strong performance. Looking ahead, practical deployment could take the form of a Cargo plugin or pre-commit hook that automatically generates and scans LLVM IR artifacts during the development cycle, enabling developers to catch vulnerabilities at an early stage in the development cycle.",
      "year": 2025,
      "venue": "Machine Learning and Knowledge Extraction",
      "authors": [
        "Young Lee",
        "Syeda Jannatul Boshra",
        "Jeong Yang",
        "Zechun Cao",
        "Gongbo Liang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/26d20a4336809ca8ab989b543ee54620daf89080",
      "pdf_url": "",
      "publication_date": "2025-08-06",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "125bbc217e3e95ab720b5f81a64761b39979c825",
      "title": "Probing the Robustness of Vision-Language Pretrained Models: A Multimodal Adversarial Attack Approach",
      "abstract": "Vision-language pretraining (VLP) with transformers has demonstrated exceptional performance across numerous multimodal tasks. However, the adversarial robustness of these models has not been thoroughly investigated. Existing multimodal attack methods have largely overlooked cross-modal interactions between visual and textual modalities, particularly in the context of cross-attention mechanisms. In this paper, we study the adversarial vulnerability of recent VLP transformers and design a novel Joint Multimodal Transformer Feature Attack (JMTFA) that concurrently introduces adversarial perturbations in both visual and textual modalities under white-box settings. JMTFA strategically targets attention relevance scores to disrupt important features within each modality, generating adversarial samples by fusing perturbations and leading to erroneous model predictions. Experimental results indicate that the proposed approach achieves high attack success rates on vision-language understanding and reasoning downstream tasks compared to existing baselines. Notably, our findings reveal that the textual modality significantly influences the complex fusion processes within VLP transformers. Moreover, we observe no apparent relationship between model size and adversarial robustness under our proposed attacks. These insights emphasize a new dimension of adversarial robustness and underscore potential risks in the reliable deployment of multimodal AI systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiwei Guan",
        "Tianyu Ding",
        "Longbing Cao",
        "Lei Pan",
        "Chen Wang",
        "Xi Zheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/125bbc217e3e95ab720b5f81a64761b39979c825",
      "pdf_url": "",
      "publication_date": "2024-08-24",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d4be92b32c3bd1c73512bce4dcb235cf2042e83",
      "title": "Vulnerability Analysis on Third Party Application",
      "abstract": "Road traffic has been an open challenge in a highly-populated area in the world. India is one of them where traffic management is a crucial problem to solve. Road traffic congestion, traffic rule violation, road accidents are the outcome of limitless traffic. The paper attempts to highlight the Real-Time Traffic Control System (RTTCS) monitoring vehicles, passengers, detecting objects on the road, and license plate detection. This paper proposes an RTTCS based on Machine Learning (ML) model, which takes traffic streaming as an input, monitors the traffic by detecting objects, finds the count of the objects per class, detects license plates and recognizes license plate characters using Easy-OCR. For depth analysis, the RTTCS analysis is performed for a particular location at various timestamps for easy traffic monitoring. The proposed system uses the you Only Look once version-4 (YOLOv4) for object detection (e.g., bicycle, motorbike, cars). The transfer learning concept uses pretrained convolutional weights to optimize the model.",
      "year": 2024,
      "venue": "International Conference on Advanced Infocomm Technology",
      "authors": [
        "Supreetha H H",
        "Rajani",
        "Suma H C",
        "Divya K P",
        "Suhasini"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3d4be92b32c3bd1c73512bce4dcb235cf2042e83",
      "pdf_url": "",
      "publication_date": "2024-07-24",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "61370cffedb3d86d4dd1c444970715e34b272302",
      "title": "Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection",
      "abstract": "Recently, pretrained language models have shown state-of-the-art performance on the vulnerability detection task. These models are pretrained on a large corpus of source code, then fine-tuned on a smaller supervised vulnerability dataset. Due to the different training objectives and the performance of the models, it is interesting to consider whether the models have learned the semantics of code relevant to vulnerability detection, namely bug semantics, and if so, how the alignment to bug semantics relates to model performance. In this paper, we analyze the models using three distinct methods: interpretability tools, attention analysis, and interaction matrix analysis. We compare the models' influential feature sets with the bug semantic features which define the causes of bugs, including buggy paths and Potentially Vulnerable Statements (PVS). We find that (1) better-performing models also aligned better with PVS, (2) the models failed to align strongly to PVS, and (3) the models failed to align at all to buggy paths. Based on our analysis, we developed two annotation methods which highlight the bug semantics inside the model's inputs. We evaluated our approach on four distinct transformer models and four vulnerability datasets and found that our annotations improved the models' performance in the majority of settings - 11 out of 16, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. We further found that with our annotations, the models aligned up to 232% better to potentially vulnerable statements. Our findings indicate that it is helpful to provide the model with information of the bug semantics, that the model can attend to it, and motivate future work in learning more complex path-based bug semantics. Our code and data are available at https://figshare.com/s/4a16a528d6874aad51a0.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Benjamin Steenhoek",
        "Md Mahbubur Rahman",
        "Shaila Sharmin",
        "Wei Le"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/61370cffedb3d86d4dd1c444970715e34b272302",
      "pdf_url": "",
      "publication_date": "2023-11-07",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "61afbd8916d1aa49984adda9fc979fa9b86270a3",
      "title": "TFHSVul: A Fine-Grained Hybrid Semantic Vulnerability Detection Method Based on Self-Attention Mechanism in IoT",
      "abstract": "Current vulnerability detection methods encounter challenges, such as inadequate feature representation, constrained feature extraction capabilities, and coarse-grained detection. To address these issues, we propose a fine-grained hybrid semantic vulnerability detection framework based on Transformer, named TFHSVul. Initially, the source code is transformed into sequential and graph-based representations to capture multilevel features, thereby solving the problem of insufficient information caused by a single intermediate representation. To enhance feature extraction capabilities, TFHSVul integrates multiscale fusion convolutional neural network, residual graph convolutional network, and pretrained language model into the core architecture, significantly boosting performance. We design a fine-grained detection method based on a self-attention mechanism, achieving statement-level detection to address the issue of coarse detection granularity. In comparison to existing baseline methods on public data sets, TFHSVul achieves a 0.58 improvement in F1 score at the function level compared to the best performing model. Moreover, it demonstrates a 10% enhancement in Top-10 accuracy at the statement-level detection compared to the best performing method.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Lijuan Xu",
        "Baolong An",
        "Xin Li",
        "Dawei Zhao",
        "Haipeng Peng",
        "Weizhao Song",
        "Fenghua Tong",
        "Xiaohui Han"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/61afbd8916d1aa49984adda9fc979fa9b86270a3",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37050e626a2de6d4980eb4d52d8db40eb1318aff",
      "title": "Adversarially Reprogramming Pretrained Neural Networks for Data-limited and Cost-efficient Malware Detection",
      "abstract": null,
      "year": 2022,
      "venue": "SDM",
      "authors": [
        "Lingwei Chen",
        "Xiaoting Li",
        "Dinghao Wu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/37050e626a2de6d4980eb4d52d8db40eb1318aff",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59da058dae40c2b01025aa6d4f9ead8ebd283d18",
      "title": "Code vulnerability detection based on augmented program dependency graph and optimized CodeBERT",
      "abstract": "The increasing complexity of software systems has rendered code vulnerability detection a critical aspect of software security. While deep learning-based approaches have advanced this field, challenges such as coarse-grained function-level detection, scalability limitations, and constrained accuracy persist. Although slice-level detection effectively reduces noise, it frequently sacrifices essential syntactic and semantic information, which undermines vulnerability representation, elevates false positive rates, and ultimately limits practical applicability. To address these challenges, this paper proposes a code vulnerability detection method based on an augmented program dependency graph and optimized CodeBERT. The method augments the traditional program dependency graph by extending its structure to capture richer semantic and structural information in the code. Furthermore, it employs the Code Bidirectional Encoder Representations from Transformers (CodeBERT) pretrained model for extracting code embedding features. Additionally, a hybrid loss function optimization strategy tailored for CodeBERT is proposed to address the long-tail distribution characteristics of code vulnerability detection. The experimental results demonstrate that, compared to other state-of-the-art classical methods, the detection accuracy and F1 score on synthetic and real-world datasets have been improved by an average of up to 8.34% and 29.71%, respectively.",
      "year": 2025,
      "venue": "Scientific Reports",
      "authors": [
        "Zhengbin Zou",
        "Tao Jiang",
        "Yizheng Wang",
        "Tiancheng Xue",
        "Nan Zhang",
        "Jie Luan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/59da058dae40c2b01025aa6d4f9ead8ebd283d18",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b10d50ad3aedaeb1db864df6f44556fb47ee47a7",
      "title": "Structure-Enhanced Prompt Learning for Graph-Based Code Vulnerability Detection",
      "abstract": "Recent advances in prompt learning have opened new avenues for enhancing natural language understanding in domain-specific tasks, including code vulnerability detection. Motivated by the limitations of conventional binary classification methods in capturing complex code semantics, we propose a novel framework that integrates a two-stage prompt optimization mechanism with hierarchical representation learning. Our approach leverages graphon theory to generate task-adaptive, structurally enriched prompts by encoding both contextual and graphical information into trainable vector representations. To further enhance representational capacity, we incorporate the pretrained model CodeBERTScore, a syntax-aware encoder, and Graph Neural Networks, enabling comprehensive modeling of both local syntactic features and global structural dependencies. Experimental results on three public datasets\u2014FFmpeg+Qemu, SVulD and Reveal\u2014demonstrate that our method performs competitively across all benchmarks, achieving accuracy rates of 64.40%, 83.44% and 90.69%, respectively. These results underscore the effectiveness of combining prompt-based learning with graph-based structural modeling, offering a more accurate and robust solution for automated vulnerability detection.",
      "year": 2025,
      "venue": "Applied Sciences",
      "authors": [
        "Wei Chang",
        "Chunyang Ye",
        "Hui Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b10d50ad3aedaeb1db864df6f44556fb47ee47a7",
      "pdf_url": "",
      "publication_date": "2025-05-29",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1d19b4248ed89a39559c265ac112b65bc136e69b",
      "title": "VFProber: A Vulnerability-Fixing Identification Framework Based on Code Changes and Semantic Adjustment",
      "abstract": "With the accelerated development of software, developers face the continuous challenge of fixing vulnerabilities but vulnerability-fixing commits often disassociated from the vulnerabilities, and the structural and semantic differences between code changes and natural language present significant challenges in identifying these commits. Existing approaches utilize machine learning and deep learning techniques to address this problem, but they often do not fully leverage the information about code changes. In this paper, we propose VFProber, a method based on a code change pretrained model, aiming to provide a comprehensive and unified framework for identifying vulnerability-fixing commits. VFProber uses semantic adjustment to distinguish between context-sensitive and context-insensitive code units in code changes, thereby enhancing the model\u2019s understanding of code changes during the training process. Secondly, VFProber employs a novel code change pretrained model as a feature extractor. Compared with ordinary code pretrained models, it can better meet the requirements of the vulnerability-fixing identification task. Moreover, we constructed a vulnerability-fixing dataset containing two common programming languages, Java and JavaScript, from industrial projects. In the experimental section, we designed three tasks to evaluate the method. The results show that, compared with the best baseline, VFProber performs better in the vulnerability-fixing identification task and can effectively reduce false positives and false negatives.",
      "year": 2025,
      "venue": "Annual International Computer Software and Applications Conference",
      "authors": [
        "Jianan Dong",
        "Guisheng Fan",
        "Yueming Yu",
        "Yuguo Liang",
        "Yujie Ye",
        "Huiqun Yu",
        "Wentao Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d19b4248ed89a39559c265ac112b65bc136e69b",
      "pdf_url": "",
      "publication_date": "2025-07-08",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3ab6434281b6359e0836ac9104ed848ffe7bf807",
      "title": "VulPr: A Prompt Learning-based Method for Vulnerability Detection",
      "abstract": "Effectively detecting software vulnerabilities remains a challenging task due to the complex semantics of source code. While pretrained language models (PLM) like CodeBERT have shown strong potential, fully fine-tuning such models is computationally expensive and risks overfitting. To address this, we propose VulPr, a lightweight vulnerability detection method based on prompt learning with parameter-efficient tuning. Specifically, we apply Low-Rank Adaptation (LoRA) to the final three Transformer layers of CodeBERT, enabling effective adaptation to the task while preserving the model\u2019s generalization ability. VulPr leverages the [CLS] token\u2019s representation and a linear classifier for prediction. Experimental results on the Reveal dataset demonstrates that VulPr outperforms baseline models, confirming the effectiveness of our design.",
      "year": 2025,
      "venue": "IEEE International Conference on Electro/Information Technology",
      "authors": [
        "Zhuo Chen",
        "Xuejun Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3ab6434281b6359e0836ac9104ed848ffe7bf807",
      "pdf_url": "",
      "publication_date": "2025-08-22",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9966f4a14fe0769c2af249052b230e5a0297fb42",
      "title": "Attention-Enhanced Progressive Transfer Learning for Scalable Seismic Vulnerability Assessment of RC Frame Buildings",
      "abstract": "Urban infrastructure in seismic zones demands efficient and scalable tools for damage prediction. This study introduces an attention-integrated progressive transfer learning (PTL) framework for the seismic vulnerability assessment (SVA) of reinforced concrete (RC) frame buildings. Traditional simulation-based vulnerability models are computationally expensive and dataset-specific, limiting their adaptability. To address this, we leverage a pretrained artificial neural network (ANN) model based on nonlinear static pushover analysis (NSPA) and Monte Carlo simulations for a 4-story RC frame, and extended its applicability to 2-, 8-, and 12-story configurations via PTL. An attention mechanism is incorporated to prioritize critical features, enhancing interpretability and classification accuracy. The model achieves 95.64% accuracy across five damage categories and an R2 of 0.98 for regression-based damage index predictions. Comparative evaluation against classical and deep learning models demonstrates superior generalization and computational efficiency. The proposed framework reduced retraining requirements across varying building heights, shows potential adaptability to other structural typologies, and maintains high predictive fidelity, making it a practical AI solution for structural risk evaluation in seismically active regions.",
      "year": 2025,
      "venue": "Buildings",
      "authors": [
        "K. Gondaliya",
        "K. Tsavdaridis",
        "Aanal Raval",
        "Jignesh A. Amin",
        "Komal Borisagar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9966f4a14fe0769c2af249052b230e5a0297fb42",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c786edee967c30ed2a1c4427302a4465634a47b8",
      "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
      "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Chaomeng Lu",
        "Bert Lagaisse"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c786edee967c30ed2a1c4427302a4465634a47b8",
      "pdf_url": "",
      "publication_date": "2025-12-11",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "da49232498c16524c32ad4c6789cc84e0ad002db",
      "title": "With Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning",
      "abstract": null,
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Bolun Wang",
        "Yuanshun Yao",
        "Bimal Viswanath",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/da49232498c16524c32ad4c6789cc84e0ad002db",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "pretrained model vulnerability",
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4620f6e5e223fef12e3413448ce8799e383defec",
      "title": "Policy-shaped prediction: avoiding distractions in model-based reinforcement learning",
      "abstract": "Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods -- including DreamerV3 and DreamerPro -- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Miles Hutson",
        "Isaac Kauvar",
        "Nick Haber"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4620f6e5e223fef12e3413448ce8799e383defec",
      "pdf_url": "",
      "publication_date": "2024-12-08",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "261956fa322d65d42895a7dbfdfc8732bdd3a223",
      "title": "SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence",
      "abstract": "Effective analysis of cybersecurity and threat intelligence data demands language models that can interpret specialized terminology, complex document structures, and the interdependence of natural language and source code. Encoder-only transformer architectures provide efficient and robust representations that support critical tasks such as semantic search, technical entity extraction, and semantic analysis, which are key to automated threat detection, incident triage, and vulnerability assessment. However, general-purpose language models often lack the domain-specific adaptation required for high precision. We present SecureBERT 2.0, an enhanced encoder-only language model purpose-built for cybersecurity applications. Leveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved long-context modeling and hierarchical encoding, enabling effective processing of extended and heterogeneous documents, including threat reports and source code artifacts. Pretrained on a domain-specific corpus more than thirteen times larger than its predecessor, comprising over 13 billion text tokens and 53 million code tokens from diverse real-world sources, SecureBERT 2.0 achieves state-of-the-art performance on multiple cybersecurity benchmarks. Experimental results demonstrate substantial improvements in semantic search for threat intelligence, semantic analysis, cybersecurity-specific named entity recognition, and automated vulnerability detection in code within the cybersecurity domain.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ehsan Aghaei",
        "Sarthak Jain",
        "Prashanth Arun",
        "Arjun Sambamoorthy"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/261956fa322d65d42895a7dbfdfc8732bdd3a223",
      "pdf_url": "",
      "publication_date": "2025-09-30",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "75cd0023bd8b45a8b38828629462a43e9ef324c6",
      "title": "ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing",
      "abstract": null,
      "year": 2022,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Ziqi Zhang",
        "Yuanchun Li",
        "Jindong Wang",
        "Bingyan Liu",
        "Ding Li",
        "Yao Guo",
        "Xiangqun Chen",
        "Yunxin Liu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/75cd0023bd8b45a8b38828629462a43e9ef324c6",
      "pdf_url": "",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "12a808d114ae5438a4f31892a15d90f76e5c15c3",
      "title": "Large Language Model based Smart Contract Auditing with LLMBugScanner",
      "abstract": "This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs. To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments. We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Yining Yuan",
        "Yifei Wang",
        "Yichang Xu",
        "Zachary Yahn",
        "Sihao Hu",
        "Ling Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/12a808d114ae5438a4f31892a15d90f76e5c15c3",
      "pdf_url": "",
      "publication_date": "2025-11-29",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "11eab19dbbe3fa2dae5184a22cb51c53d10a0008",
      "title": "AdaptGuard: Defending Against Universal Attacks for Model Adaptation",
      "abstract": "Model adaptation aims at solving the domain transfer problem under the constraint of only accessing the pretrained source models. With the increasing considerations of data privacy and transmission efficiency, this paradigm has been gaining recent popularity. This paper studies the vulnerability to universal attacks transferred from the source domain during model adaptation algorithms due to the existence of malicious providers. We explore both universal adversarial perturbations and backdoor attacks as loopholes on the source side and discover that they still survive in the target models after adaptation. To address this issue, we propose a model preprocessing framework, named AdaptGuard, to improve the security of model adaptation algorithms. AdaptGuard avoids direct use of the risky source parameters through knowledge distillation and utilizes the pseudo adversarial samples under adjusted radius to enhance the robustness. AdaptGuard is a plug-and-play module that requires neither robust pretrained models nor any changes for the following model adaptation algorithms. Extensive results on three commonly used datasets and two popular adaptation methods validate that AdaptGuard can effectively defend against universal attacks and maintain clean accuracy in the target domain simultaneously. We hope this research will shed light on the safety and robustness of transfer learning. Code is available at https://github.com/TomSheng21/AdaptGuard.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Lijun Sheng",
        "Jian Liang",
        "R. He",
        "Zilei Wang",
        "Tien-Ping Tan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/11eab19dbbe3fa2dae5184a22cb51c53d10a0008",
      "pdf_url": "http://arxiv.org/pdf/2303.10594",
      "publication_date": "2023-03-19",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "71f721bbcd7d468713769d6d767fed42663b3a70",
      "title": "SF(DA)2: Source-free Domain Adaptation Through the Lens of Data Augmentation",
      "abstract": "In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose implicit feature augmentation and feature disentanglement as regularization loss functions that effectively utilize class semantic information within the feature space. These regularizers simulate the inclusion of an unlimited number of augmented target features into the augmentation graph while minimizing computational and memory demands. Our method shows superior adaptation performance in SFDA scenarios, including 2D image and 3D point cloud datasets and a highly imbalanced dataset.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Uiwon Hwang",
        "Jonghyun Lee",
        "Juhyeon Shin",
        "Sungroh Yoon"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/71f721bbcd7d468713769d6d767fed42663b3a70",
      "pdf_url": "",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e039f65994cb9e51d1b278bc7ce8208f617d14e4",
      "title": "TD-Zero: Automatic Golden-Free Hardware Trojan Detection Using Zero-Shot Learning",
      "abstract": "Supply chain vulnerability provides the opportunity for the attackers to implant hardware Trojans in System-on-Chip (SoC) designs. While machine learning (ML) based Trojan detection is promising, it suffers from three practical limitations: 1) golden model may not be available; 2) lack of human expertise for Trojan feature selection; and 3) limited learning transferability can lead to unacceptable performance in new benchmarks with unseen Trojans. While recent approach based on transfer learning addresses some of these concerns, it still requires retraining for fine-tuning the model using domain-specific (e.g., hardware Trojan features) knowledge. In this article, we propose a Trojan detection framework utilizing zero-shot learning to address the above challenges. The proposed framework adopts the idea of self-supervised learning, where a pretrained graph convolutional network (GCN) is utilized to extract underlined common sense about hardware Trojans, and a metric learning task is used to measure the similarity between test inputs and malicious samples to make classification. Extensive experimental evaluation demonstrates that our approach has four major advantages compared to state-of-the-art techniques: 1) does not require any golden model during Trojan detection; 2) can handle both unknown Trojans and unseen benchmarks without any changes to the network; 3) drastic reduction in training time; and 4) significant improvement in detection efficiency (10.5% on average).",
      "year": 2024,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Zhixin Pan",
        "Prabhat Mishra"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/e039f65994cb9e51d1b278bc7ce8208f617d14e4",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "962a8ffc7d72990a28d505f49a39108b4803c223",
      "title": "Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning",
      "abstract": "Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pretrained models for the first time. We find these robust pretrained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (eg, 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pretrained models have diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Tianlong Chen",
        "Sijia Liu",
        "Shiyu Chang",
        "Yu Cheng",
        "Lisa Amini",
        "Zhangyang Wang"
      ],
      "citation_count": 273,
      "url": "https://www.semanticscholar.org/paper/962a8ffc7d72990a28d505f49a39108b4803c223",
      "pdf_url": "https://arxiv.org/pdf/2003.12862",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cbea783e613cd748df4c346fad8afb9e35bda85c",
      "title": "VulDetect: A novel technique for detecting software vulnerabilities using Language Models",
      "abstract": "Recently, deep learning techniques have garnered substantial attention for their ability to identify vulnerable code patterns accurately. However, current state-of-the-art deep learning models, such as Convolutional Neural Networks (CNN), and Long Short-Term Memories (LSTMs) require substantial computational resources. This results in a level of overhead that makes their implementation unfeasible for deployment in realtime settings. This study presents a novel transformer-based vulnerability detection framework, referred to as VulDetect, which is achieved through the fine-tuning of a pretrained large language model, (GPT) on various benchmark datasets of vulnerable code. Our empirical findings indicate that our framework is capable of identifying vulnerable software code with an accuracy of up to 92.65%. Our proposed technique outperforms SyseVR and VuIDeBERT, two state-of-the-art vulnerability detection techniques.",
      "year": 2023,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Marwan Omar",
        "S. Shiaeles"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/cbea783e613cd748df4c346fad8afb9e35bda85c",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "22523649a450f3f406d2cbdc95a843bcf20cb6f4",
      "title": "Unsupervised dense retrieval with conterfactual contrastive learning",
      "abstract": "Efficiently retrieving a concise set of candidates from a large document corpus remains a pivotal challenge in Information Retrieval (IR). Neural retrieval models, particularly dense retrieval models built with transformers and pretrained language models, have been popular due to their superior performance. However, criticisms have also been raised on their lack of explainability and vulnerability to adversarial attacks. In response to these challenges, we propose to improve the robustness of dense retrieval models by enhancing their sensitivity of fine-graned relevance signals. A model achieving sensitivity in this context should exhibit high variances when documents' key passages determining their relevance to queries have been modified, while maintaining low variances for other changes in irrelevant passages. This sensitivity allows a dense retrieval model to produce robust results with respect to attacks that try to promote documents without actually increasing their relevance. It also makes it possible to analyze which part of a document is actually relevant to a query, and thus improve the explainability of the retrieval model. Motivated by causality and counterfactual analysis, we propose a series of counterfactual regularization methods based on game theory and unsupervised learning with counterfactual passages. Experiments show that, our method can extract key passages without reliance on the passage-level relevance annotations. Moreover, the regularized dense retrieval models exhibit heightened robustness against adversarial attacks, surpassing the state-of-the-art anti-attack methods.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Haitian Chen",
        "Qingyao Ai",
        "Xiao Wang",
        "Yiqun Liu",
        "Fen Lin",
        "Qin Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/22523649a450f3f406d2cbdc95a843bcf20cb6f4",
      "pdf_url": "",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e78f251470605a720abba89d34716f9df2ec83b1",
      "title": "Frequency-Aware GAN for Imperceptible Transfer Attack on 3D Point Clouds",
      "abstract": "With the development of depth sensors and 3D vision, the vulnerability of 3D point cloud models has garnered heightened concern. Almost all existing 3D attackers are deployed in the white-box setting, where they access the model details and directly optimize coordinate-wise noises to perturb 3D objects. However, realistic 3D applications would not share any model information (model parameters, gradients, etc.) with users. Although a few recent works try to explore the black-box attack, they still achieve limited attack success rates (ASR) and fail to generate high-quality adversarial samples. In this paper, we focus on designing a transfer-based black-box attack method, called Transferable Frequency-aware 3D GAN, to delve into achieving a high black-box ASR by improving the adversarial transferability while making the adversarial samples more imperceptible. Considering that the 3D imperceptibility depends on whether the shape of the object is distorted, we utilize the spectral tool with the GAN design to explicitly perceive and preserve the 3D geometric structures. Specifically, we design the Graph Fourier Transform (GFT) encoding layer in the GAN generator to extract the geometries as guidance, and develop a corresponding Inverse-GFT decoding layer to decode latent features with this guidance to reconstruct high-quality adversarial samples. To further improve the transferability, we develop a dual learning scheme of discriminator from both frequency and feature perspectives to constrain the generator via adversarial learning. Finally, imperceptible and transferable perturbations are rapidly generated by our proposed attack. Experimental results demonstrate that our attack method achieves the highest transfer ASR while exhibiting stronger imperceptibility.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Xiaowen Cai",
        "Yunbo Tao",
        "Daizong Liu",
        "Pan Zhou",
        "Xiaoye Qu",
        "Jianfeng Dong",
        "Keke Tang",
        "Lichao Sun"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/e78f251470605a720abba89d34716f9df2ec83b1",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b838395165ee6306b5a60e19e9a3422e21d55833",
      "title": "Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification",
      "abstract": "Although many efforts have been made into attack and defense on the 2D image domain in recent years, few methods explore the vulnerability of 3D models. Existing 3D attackers generally perform point-wise perturbation over point clouds, resulting in deformed structures or outliers, which is easily perceivable by humans. Moreover, their adversarial examples are generated under the white-box setting, which frequently suffers from low success rates when transferred to attack remote black-box models. In this article, we study 3D point cloud attacks from two new and challenging perspectives by proposing a novel Imperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the perturbation direction of each point along its normal vector of the neighborhood surface, leading to generated examples with similar geometric properties and thus enhancing the imperceptibility. 2) Transferability: we develop an adversarial transformation model to generate the most harmful distortions and enforce the adversarial examples to resist it, improving their transferability to unknown black-box models. Further, we propose to train more robust black-box 3D models to defend against such ITA attacks by learning more discriminative point cloud representations. Extensive evaluations demonstrate that our ITA attack is more imperceptible and transferable than state-of-the-arts and validate the superiority of our defense strategy.",
      "year": 2021,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Daizong Liu",
        "Wei Hu"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/b838395165ee6306b5a60e19e9a3422e21d55833",
      "pdf_url": "https://arxiv.org/pdf/2111.10990",
      "publication_date": "2021-11-22",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e8f37427638bdf6892cc059b36570475bebdb62d",
      "title": "Ensemble transfer attack targeting text classification systems",
      "abstract": null,
      "year": 2022,
      "venue": "Computers & security",
      "authors": [
        "Hyung-Min Kwon",
        "Sanghyun Lee"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/e8f37427638bdf6892cc059b36570475bebdb62d",
      "pdf_url": "",
      "publication_date": "2022-03-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7edecc82adc33fed9664947c4cda639de647ce9b",
      "title": "Speaker-Specific Utterance Ensemble based Transfer Attack on Speaker Identification",
      "abstract": "While speaker identification (SI) systems based on deep neural network (DNN) have been widely applied in security-related practical tasks, more and more attention has been at-tracted to the robustness of SI systems against potential malicious threats. Existing works have shown that white-box attacks can greatly threaten the current SI systems, but white-box attacks require complete knowledge of the target model, which is almost impractical in many applications. As far as we know, only a few works have studied the more practical black-box attacks, while these attacks are mostly ported from computer vision task and lack the adaptability to speech data. In this work, we propose a novel black-box attack, called speaker-specific utterance ensemble based transfer attack (SUETA). SUETA utilizes the unique characteristic of speech data that different utterances of one specific speaker share the same voiceprint to attack on SI systems. To the best of our knowledge, SUETA is the first black-box attack on SI systems that utilizes the unique characteristic of speech data. Experimental results on three representative SI models show that SUETA can achieve better transfer success rate (TSR) than speaker-unrelated baselines. Furthermore, SUETA can even improve the attack success rate (ASR) of white-box attacks on local substitute model, which is the first step to perform the transfer based black-box attack.",
      "year": 2022,
      "venue": "Interspeech",
      "authors": [
        "Chu-Xiao Zuo",
        "Jia-Yi Leng",
        "Wu-Jun Li"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/7edecc82adc33fed9664947c4cda639de647ce9b",
      "pdf_url": "",
      "publication_date": "2022-09-18",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d41f11b7820c06aa6d16cf8bd13188730b8216f7",
      "title": "Domain Adaptive Transfer Attack-Based Segmentation Networks for Building Extraction From Aerial Images",
      "abstract": "Semantic segmentation models based on convolutional neural networks (CNNs) have gained much attention in relation to remote sensing and have achieved remarkable performance for the extraction of buildings from high-resolution aerial images. However, the issue of limited generalization for unseen images remains. When there is a domain gap between the training and test data sets, the CNN-based segmentation models trained by a training data set fail to segment buildings for the test data set. In this article, we propose segmentation networks based on a domain adaptive transfer attack (DATA) scheme for building extraction from aerial images. The proposed system combines the domain transfer and the adversarial attack concepts. Based on the DATA scheme, the distribution of the input images can be shifted to that of the target images while turning images into adversarial examples against a target network. Defending adversarial examples adapted to the target domain can overcome the performance degradation due to the domain gap and increase the robustness of the segmentation model. Cross-data set experiments and ablation study are conducted for three different data sets: the Inria aerial image labeling data set, the Massachusetts building data set, and the WHU East Asia data set. Compared with the performance of the segmentation network without the DATA scheme, the proposed method shows improvements in the overall intersection over union (IoU). Moreover, it is verified that the proposed method outperforms even when compared with feature adaptation (FA) and output space adaptation (OSA).",
      "year": 2020,
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "authors": [
        "Younghwan Na",
        "Jun Hee Kim",
        "Kyungsu Lee",
        "Juhum Park",
        "J. Hwang",
        "Jihwan P. Choi"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/d41f11b7820c06aa6d16cf8bd13188730b8216f7",
      "pdf_url": "http://arxiv.org/pdf/2004.11819",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b1d3120f0fe4f77016c93d71d48837c5bfbda9b8",
      "title": "Robust DDoS attack detection with adaptive transfer learning",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & security",
      "authors": [
        "Mulualem Bitew Anley",
        "A. Genovese",
        "Davide Agostinello",
        "Vincenzo Piuri"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/b1d3120f0fe4f77016c93d71d48837c5bfbda9b8",
      "pdf_url": "https://doi.org/10.1016/j.cose.2024.103962",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d199d7236a0639b7ecf97f265a8671eb041d5e72",
      "title": "A Transfer Learning-Based Method for Cyber-Attack Tolerance in Distributed Control of Microgrids",
      "abstract": "Distributed secondary control of a microgrid highly relies on information exchange via communication networks, which may be vulnerable to cyber-attacks. This paper aims to enhance the cyber-attack tolerance against various types of cyber-attacks, including false data injection (FDI) attack, denial of service (DoS) attack, latency attack. To this end, a data-driven signal estimator is developed based on a deep neural network (DNN), where the inputs are local sampled time-series measurements (frequency, voltage, active/reactive power) and the outputs are the sums of communicated signals from other units. To achieve a better generalization ability, a representation subspace distance (RSD)-based transfer learning model is designed for offline training. As an advantage, the proposed estimator can work for various working conditions without multiple training processes. For online stage, a reference tracking scheme is designed to recover the attacked signal based on the estimated signal, which mitigates or eliminates the effect of cyber-attacks. Finally, numerical case studies are presented to validate the effectiveness and advantages of the method.",
      "year": 2024,
      "venue": "IEEE Transactions on Smart Grid",
      "authors": [
        "Yang Xia",
        "Yan Xu",
        "S. Mondal",
        "A. Gupta"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/d199d7236a0639b7ecf97f265a8671eb041d5e72",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2728a6d556138d2e2884e651fce2bd25eccbc704",
      "title": "A transfer learning-based intrusion detection system for zero-day attack in communication-based train control system",
      "abstract": null,
      "year": 2024,
      "venue": "Cluster Computing",
      "authors": [
        "He Lu",
        "Yanan Zhao",
        "Yajing Song",
        "Yang Yang",
        "Guanjie He",
        "Haiyang Yu",
        "Yilong Ren"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/2728a6d556138d2e2884e651fce2bd25eccbc704",
      "pdf_url": "",
      "publication_date": "2024-04-12",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0886c053462bce6c285b9149a28e213aaeeca5d8",
      "title": "TASAR: Transfer-based Attack on Skeletal Action Recognition",
      "abstract": "Skeletal sequence data, as a widely employed representation of human actions, are crucial in Human Activity Recognition (HAR). Recently, adversarial attacks have been proposed in this area, which exposes potential security concerns, and more importantly provides a good tool for model robustness test. Within this research, transfer-based attack is an important tool as it mimics the real-world scenario where an attacker has no knowledge of the target model, but is under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR attacks exhibit weak adversarial transferability and the reason remains largely unknown. In this paper, we investigate this phenomenon via the characterization of the loss function. We find that one prominent indicator of poor transferability is the low smoothness of the loss function. Led by this observation, we improve the transferability by properly smoothening the loss when computing the adversarial examples. This leads to the first Transfer-based Attack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened model posterior of pre-trained surrogates, which is achieved by a new post-train Dual Bayesian optimization strategy. Furthermore, unlike existing transfer-based methods which overlook the temporal coherence within sequences, TASAR incorporates motion dynamics into the Bayesian attack, effectively disrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation, we build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive results demonstrate the superiority of TASAR. Our benchmark enables easy comparisons for future studies, with the code available in the https://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Yunfeng Diao",
        "Baiqi Wu",
        "Ruixuan Zhang",
        "Ajian Liu",
        "Xingxing Wei",
        "Meng Wang",
        "He Wang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/0886c053462bce6c285b9149a28e213aaeeca5d8",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f90e535758ba262ebbdd1e7966f6645eff535604",
      "title": "T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection",
      "abstract": "Compared to query-based black-box attacks, transferbased black-box attacks do not require any information of the attacked models, which ensures their secrecy. However, most existing transfer-based approaches rely on ensembling multiple models to boost the attack transferability, which is time- and resource-intensive, not to mention the difficulty of obtaining diverse models on the same task. To address this limitation, in this work, we focus on the single-model transfer-based black-box attack on object detection, utilizing only one model to achieve a high-transferability adversarial attack on multiple black-box detectors. Specifically, we first make observations on the patch optimization process of the existing method and propose an enhanced attack framework by slightly adjusting its training strategies. Then, we analogize patch optimization with regular model optimization, proposing a series of self-ensemble approaches on the input data, the attacked model, and the adversarial patch to efficiently make use of the limited information and prevent the patch from overfitting. The experimental results show that the proposed framework can be applied with multiple classical base attack methods (e.g., PGD and MIM) to greatly improve the black-box transferability of the well-optimized patch on multiple mainstream detectors, meanwhile boosting white-box performance. Our code is available at https://github.com/VDIGPKU/TSEA.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Hao Huang",
        "Ziyan Chen",
        "Huanran Chen",
        "Yongtao Wang",
        "K. Zhang"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/f90e535758ba262ebbdd1e7966f6645eff535604",
      "pdf_url": "https://arxiv.org/pdf/2211.09773",
      "publication_date": "2022-11-16",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5206aa1f39cdbc90245bab8761375f9159d913a0",
      "title": "CDTA: A Cross-Domain Transfer-Based Attack with Contrastive Learning",
      "abstract": "Despite the excellent performance, deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Besides, these examples are often transferable among different models. In other words, the same adversarial example can fool multiple models with different architectures at the same time. Based on this property, many black-box transfer-based attack techniques have been developed. However, current transfer-based attacks generally focus on the cross-architecture setting, where the attacker has access to the training data of the target model, which is not guaranteed in realistic situations. In this paper, we design a Cross-Domain Transfer-Based Attack (CDTA), which works in the cross-domain scenario. In this setting, attackers have no information about the target model, such as its architecture and training data. Specifically, we propose a contrastive spectral training method to train a feature extractor on a source domain (e.g., ImageNet) and use it to craft adversarial examples on target domains (e.g., Oxford 102 Flower). Our method corrupts the semantic information of the benign image by scrambling the outputs of both the intermediate feature layers and the final layer of the feature extractor. We evaluate CDTA with 16 target deep models on four datasets with widely varying styles. The results confirm that, in terms of the attack success rate, our approach can consistently outperform the state-of-the-art baselines by an average of 11.45% across all target models. Our code is available at https://github.com/LiulietLee/CDTA.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Zihan Li",
        "Weibin Wu",
        "Yuxin Su",
        "Zibin Zheng",
        "Michael R. Lyu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5206aa1f39cdbc90245bab8761375f9159d913a0",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/25239/25011",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f36cd25292a9fdfcfcd4d49e1741a73fc520a1da",
      "title": "Vulnerability in Deep Transfer Learning Models to Adversarial Fast Gradient Sign Attack for COVID-19 Prediction from Chest Radiography Images",
      "abstract": "The COVID-19 pandemic requires the rapid isolation of infected patients. Thus, high-sensitivity radiology images could be a key technique to diagnose patients besides the polymerase chain reaction approach. Deep learning algorithms are proposed in several studies to detect COVID-19 symptoms due to the success in chest radiography image classification, cost efficiency, lack of expert radiologists, and the need for faster processing in the pandemic area. Most of the promising algorithms proposed in different studies are based on pre-trained deep learning models. Such open-source models and lack of variation in the radiology image-capturing environment make the diagnosis system vulnerable to adversarial attacks such as fast gradient sign method (FGSM) attack. This study therefore explored the potential vulnerability of pre-trained convolutional neural network algorithms to the FGSM attack in terms of two frequently used models, VGG16 and Inception-v3. Firstly, we developed two transfer learning models for X-ray and CT image-based COVID-19 classification and analyzed the performance extensively in terms of accuracy, precision, recall, and AUC. Secondly, our study illustrates that misclassification can occur with a very minor perturbation magnitude, such as 0.009 and 0.003 for the FGSM attack in these models for X-ray and CT images, respectively, without any effect on the visual perceptibility of the perturbation. In addition, we demonstrated that successful FGSM attack can decrease the classification performance to 16.67% and 55.56% for X-ray images, as well as 36% and 40% in the case of CT images for VGG16 and Inception-v3, respectively, without any human-recognizable perturbation effects in the adversarial images. Finally, we analyzed that correct class probability of any test image which is supposed to be 1, can drop for both considered models and with increased perturbation; it can drop to 0.24 and 0.17 for the VGG16 model in cases of X-ray and CT images, respectively. Thus, despite the need for data sharing and automated diagnosis, practical deployment of such program requires more robustness.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "Biprodip Pal",
        "Debashis Gupta",
        "Md. Rashed-Al-Mahfuz",
        "S. Alyami",
        "M. A. Moni"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/f36cd25292a9fdfcfcd4d49e1741a73fc520a1da",
      "pdf_url": "https://doi.org/10.3390/app11094233",
      "publication_date": "2021-05-07",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4adee130b8da7677f151b4a6ba42a1a6312ac3df",
      "title": "Adaptive Image Transformations for Transfer-based Adversarial Attack",
      "abstract": ". Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into considera-tion. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Zheng Yuan",
        "J. Zhang",
        "S. Shan"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/4adee130b8da7677f151b4a6ba42a1a6312ac3df",
      "pdf_url": "",
      "publication_date": "2021-11-27",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04deb24455e327d6a094a5d96939a333a3034a66",
      "title": "Deep Transfer Learning for IoT Attack Detection",
      "abstract": "The digital revolution has substantially changed our lives in which Internet-of-Things (IoT) plays a prominent role. The rapid development of IoT to most corners of life, however, leads to various emerging cybersecurity threats. Therefore, detecting and preventing potential attacks in IoT networks have recently attracted paramount interest from both academia and industry. Among various attack detection approaches, machine learning-based methods, especially deep learning, have demonstrated great potential thanks to their early detecting capability. However, these machine learning techniques only work well when a huge volume of data from IoT devices with label information can be collected. Nevertheless, the labeling process is usually time consuming and expensive, thus, it may not be able to adapt with quick evolving IoT attacks in reality. In this paper, we propose a novel deep transfer learning (DTL) method that allows to learn from data collected from multiple IoT devices in which not all of them are labeled. Specifically, we develop a DTL model based on two AutoEncoders (AEs). The first AE (AE1) is trained on the source datasets (source domains) in the supervised mode using the label information and the second AE (AE2) is trained on the target datasets (target domains) in an unsupervised manner without label information. The transfer learning process attempts to force the latent representation (the bottleneck layer) of AE2 similarly to the latent representation of AE1. After that, the latent representation of AE2 is used to detect attacks in the incoming samples in the target domain. We carry out intensive experiments on nine recent IoT datasets to evaluate the performance of the proposed model. The experimental results demonstrate that the proposed DTL model significantly improves the accuracy in detecting IoT attacks compared to the baseline deep learning technique and two recent DTL approaches.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Ly Vu",
        "Quang Uy Nguyen",
        "Diep N. Nguyen",
        "D. Hoang",
        "E. Dutkiewicz"
      ],
      "citation_count": 99,
      "url": "https://www.semanticscholar.org/paper/04deb24455e327d6a094a5d96939a333a3034a66",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09110582.pdf",
      "publication_date": "2020-06-08",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d6367d42bad9b934ee6a31a3164b100acfdf202",
      "title": "Deep transductive transfer learning framework for zero-day attack detection",
      "abstract": null,
      "year": 2020,
      "venue": "ICT express",
      "authors": [
        "N. Sameera",
        "M. Shashi"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/2d6367d42bad9b934ee6a31a3164b100acfdf202",
      "pdf_url": "https://doi.org/10.1016/j.icte.2020.03.003",
      "publication_date": "2020-03-13",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2219915bc82c17d6aad676500e68c04d946cf233",
      "title": "A Small Sample DDoS Attack Detection Method Based on Deep Transfer Learning",
      "abstract": "When using deep learning for DDoS attack detection, there is a general degradation in detection performance due to small sample size. This paper proposes a small-sample DDoS attack detection method based on deep transfer learning. First, deep learning techniques are used to train several neural networks that can be used for transfer in DDoS attacks with sufficient samples. Then we design a transferability metric to compare the transfer performance of different networks. With this metric, the network with the best transfer performance can be selected among the four networks. Then for a small sample of DDoS attacks, this paper demonstrates that the deep learning detection technique brings deterioration in performance, with the detection performance dropping from 99.28% to 67%. Finally, we end up with a 20.8% improvement in detection performance by deep transfer of the 8LANN network in the target domain. The experiment shows that the detection method based on deep transfer learning proposed in this paper can well improve the performance deterioration of deep learning techniques for small sample DDoS attack detection.",
      "year": 2020,
      "venue": "2020 International Conference on Computer Communication and Network Security (CCNS)",
      "authors": [
        "Jiawei He",
        "Yejin Tan",
        "Wangshu Guo",
        "Ming Xian"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/2219915bc82c17d6aad676500e68c04d946cf233",
      "pdf_url": "",
      "publication_date": "2020-08-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f8882528877aca9fd9060822ea1dfcd116d16087",
      "title": "Adversarial Attack and Defense on Deep Learning for Air Transportation Communication Jamming",
      "abstract": "Air transportation communication jamming recognition model based on deep learning (DL) can quickly and accurately identify and classify communication jamming, to improve the safety and reliability of air traffic. However, due to the vulnerability of deep learning, the jamming recognition model can be easily attacked by the attacker\u2019s carefully designed adversarial examples. Although some defense methods have been proposed, they have strong pertinence to attacks. Thus, new attack methods are needed to improve the defense performance of the model. In this work, we improve the existing attack methods and propose a double level attack method. By constructing the dynamic iterative step size and analyzing the class characteristics of the signals, this method can use the adversarial losses of feature layer and decision layer to generate adversarial examples with stronger attack performance. In order to improve the robustness of the recognition model, we use adversarial examples to train the model, and transfer the knowledge learned from the model to the jamming recognition models in other wireless communication environments by transfer learning. Simulation results show that the proposed attack and defense methods have good performance.",
      "year": 2024,
      "venue": "IEEE transactions on intelligent transportation systems (Print)",
      "authors": [
        "Mingqian Liu",
        "Zhenju Zhang",
        "Yunfei Chen",
        "J. Ge",
        "Nan Zhao"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/f8882528877aca9fd9060822ea1dfcd116d16087",
      "pdf_url": "https://dro.dur.ac.uk/38208/1/38208.pdf",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack",
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9286e79ee70745c3065ace3b04deef0fa2885486",
      "title": "A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning",
      "abstract": "Due to insufficient training data and the high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications. A commonly used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained model used in transfer learning is usually publicly available, including to potential attackers. In this paper, we show that without any additional knowledge other than the pre-trained model, an attacker can launch an effective and efficient brute force attack that can craft instances of input to trigger each target class with high confidence. We assume that the attacker has no access to any target-specific information, including samples from target classes, re-trained model, and probabilities assigned by Softmax to each class, and thus making the attack target-agnostic. These assumptions render all previous attack models inapplicable, to the best of our knowledge. To evaluate the proposed attack, we perform a set of experiments on face recognition and speech recognition tasks and show the effectiveness of the attack. Our work reveals a fundamental security weakness of the Softmax layer when used in transfer learning settings.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shahbaz Rezaei",
        "Xin Liu"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/9286e79ee70745c3065ace3b04deef0fa2885486",
      "pdf_url": "",
      "publication_date": "2019-04-08",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "231578791e1035fe9e4e7f4437d8ad27c131e236",
      "title": "The implications of shedder status and background DNA on direct and secondary transfer in an attack scenario.",
      "abstract": null,
      "year": 2017,
      "venue": "Forensic Science International: Genetics",
      "authors": [
        "A. E. Fonnel\u00f8p",
        "Merete Ramse",
        "T. Egeland",
        "P. Gill"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/231578791e1035fe9e4e7f4437d8ad27c131e236",
      "pdf_url": "",
      "publication_date": "2017-07-01",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33b93108f16648f0c4cdbcf325ca8624b888fb7c",
      "title": "SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation",
      "abstract": "Current Visual-Language Pre-training (VLP) models are vulnerable to adversarial examples. These adversarial examples present substantial security risks to VLP models, as they can leverage inherent weaknesses in the models, resulting in incorrect predictions. In contrast to white-box adversarial attacks, transfer attacks (where the adversary crafts adversarial examples on a white-box model to fool another black-box model) are more reflective of real-world scenarios, thus making them more meaningful for research. By summarizing and analyzing existing research, we identified two factors that can influence the efficacy of transfer attacks on VLP models: inter-modal interaction and data diversity. Based on these insights, we propose a self-augment-based transfer attack method, termed SA-Attack. Specifically, during the generation of adversarial images and adversarial texts, we apply different data augmentation methods to the image modality and text modality, respectively, with the aim of improving the adversarial transferability of the generated adversarial images and texts. Experiments conducted on the FLickr30K and COCO datasets have validated the effectiveness of our method. Our code will be available after this paper is accepted.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Bangyan He",
        "Xiaojun Jia",
        "Siyuan Liang",
        "Tianrui Lou",
        "Yang Liu",
        "Xiaochun Cao"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/33b93108f16648f0c4cdbcf325ca8624b888fb7c",
      "pdf_url": "",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "94b94c545cbb90e2d1ab316758a910c53ed95b66",
      "title": "Rethinking Model Ensemble in Transfer-based Adversarial Attacks",
      "abstract": "It is widely recognized that deep learning models lack robustness to adversarial examples. An intriguing property of adversarial examples is that they can transfer across different models, which enables black-box attacks without any knowledge of the victim model. An effective strategy to improve the transferability is attacking an ensemble of models. However, previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble methods can strongly improve the transferability. In this paper, we rethink the ensemble in adversarial attacks and define the common weakness of model ensemble with two properties: 1) the flatness of loss landscape; and 2) the closeness to the local optimum of each model. We empirically and theoretically show that both properties are strongly correlated with the transferability and propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of our approach to improving the adversarial transferability, especially when attacking adversarially trained models. We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness. Code is available at \\url{https://github.com/huanranchen/AdversarialAttacks}.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Huanran Chen",
        "Yichi Zhang",
        "Yinpeng Dong",
        "Junyi Zhu"
      ],
      "citation_count": 93,
      "url": "https://www.semanticscholar.org/paper/94b94c545cbb90e2d1ab316758a910c53ed95b66",
      "pdf_url": "http://arxiv.org/pdf/2303.09105",
      "publication_date": "2023-03-16",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af43f406749e928ceee60434425b84686a5603b1",
      "title": "Intelligent AI-based Healthcare Cyber Security System using Multi-Source Transfer Learning Method",
      "abstract": "Cyber-security intelligence have made a great impact over healthcare industry where several researchers are developing new techniques to improve security for healthcare systems. Besides, Artificial Intelligence (AI) become the tremendous technology in recent decades to improve the existing methods to be more intelligent. In this paper, we proposed cyber attack detection system for healthcare sector with centralized and federated transfer learning mode. Edge of Things (EoT) framework is developed in connection with cloud and healthcare sectors to transmit the data efficiently and the proposed Centralized with Multi-Source Transfer Learning (CMTL) algorithm which is used for detection and classification of various threats such as information gathering, DoS/DDoS attacks, Malware attacks, Injection attacks, and Man in the Middle attacks. Performance of the proposed framework is evaluated using various datasets such as EMNIST, X-IIoTID, and Federated TON_IoT. Our framework outperforms with the analysis of execution time and obtains high level accuracy when compared with different algorithms.",
      "year": 2023,
      "venue": "ACM transactions on sensor networks",
      "authors": [
        "Chinmay Chakraborty",
        "Senthil Murugan Nagarajan",
        "G. Devarajan",
        "T. V. Ramana",
        "R. Mohanty"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/af43f406749e928ceee60434425b84686a5603b1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3597210",
      "publication_date": "2023-05-15",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "title": "SCMA: A Scattering Center Model Attack on CNN-SAR Target Recognition",
      "abstract": "Convolutional neural networks (CNNs) have been widely used in synthetic aperture radar (SAR) target recognition, which can extract feature automatically. However, due to its own structural flaws, CNNs are easy to be fooled by adversarial examples, even if they have excellent performance. In this letter, a novel attack named scattering center model attack (SCMA) is designed, and its generation process does not rely on the prior knowledge of any neural network. Therefore, we can get a stable way which can be applied to any neural network. In addition, an improved scattering center model extraction method, which is the pre-part of SCMA, can filter out the useless noise to optimize the stability of attack. In the experiment, SCMA is compared with advanced attack algorithms. From the experimental results, it is clear to find that SCMA has excellent performance in terms of transfer attack success rate. Furthermore, visualization and interpretability analysis underpin the theoretical feasibility of SCMA.",
      "year": 2023,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Weibo Qin",
        "Bo Long",
        "Feng Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a3e84a25ab3d90ff1fba789263d81fef0904f6c9",
      "title": "A Transferable Adversarial Belief Attack With Salient Region Perturbation Restriction",
      "abstract": "Deep neural networks are vulnerable to adversarial examples which are crafted by adding small perturbations on benign examples. However, most existing attack methods often perform a poor transferability to attack black-box models, especially to attack defense methods. In addition, perturbations added to semantically irrelevant regions of benign examples are usually inefficient for attacks. To address these issues, we propose a transferable adversarial belief attack with salient region perturbation restriction method, which improves transferability of adversarial examples and decreases the amount of perturbations significantly. Specifically, we first design a salient-region-based perturbation restriction strategy to restrict the range of perturbations into a salient region. After that, we present a transferable belief attack method to generate the adversarial examples iteratively. Besides, our method can be easily integrated with other gradient-based transfer attack methods to further enhance the transferability of adversarial examples. Extensive experiments on the ImageNet dataset show that our method achieves higher transferability with lower perturbations than the state-of-the-art attack methods.",
      "year": 2023,
      "venue": "IEEE transactions on multimedia",
      "authors": [
        "Shihui Zhang",
        "Dongxu Zuo",
        "Yongliang Yang",
        "Xiaowei Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a3e84a25ab3d90ff1fba789263d81fef0904f6c9",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83",
      "title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer",
      "abstract": "Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer\u2014the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.",
      "year": 2021,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Fanchao Qi",
        "Yangyi Chen",
        "Xurui Zhang",
        "Mukai Li",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "citation_count": 224,
      "url": "https://www.semanticscholar.org/paper/342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83",
      "pdf_url": "https://aclanthology.org/2021.emnlp-main.374.pdf",
      "publication_date": "2021-10-14",
      "keywords_matched": [
        "transfer attack",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d3a99a0b24c740cc8bce8c3856032c468d1c3e3b",
      "title": "Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer",
      "abstract": "Deep neural networks are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on clean inputs. Although many attack methods can achieve high success rates in the white-box setting, they also exhibit weak transferability in the black-box setting. Recently, various methods have been proposed to improve adversarial transferability, in which the input transformation is one of the most effective methods. In this work, we notice that existing input transformation-based works mainly adopt the transformed data in the same domain for augmentation. Inspired by domain generalization, we aim to further improve the transferability using the data augmented from different domains. Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Zhijin Ge",
        "Fanhua Shang",
        "Hongying Liu",
        "Yuanyuan Liu",
        "Liang Wan",
        "Wei Feng",
        "Xiaosen Wang"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/d3a99a0b24c740cc8bce8c3856032c468d1c3e3b",
      "pdf_url": "https://arxiv.org/pdf/2308.10601",
      "publication_date": "2023-08-21",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "115ae8b0ace627ea6cd23170f4b6173b2dbd77a6",
      "title": "QFA2SR: Query-Free Adversarial Transfer Attacks to Speaker Recognition Systems",
      "abstract": "Current adversarial attacks against speaker recognition systems (SRSs) require either white-box access or heavy black-box queries to the target SRS, thus still falling behind practical attacks against proprietary commercial APIs and voice-controlled devices. To fill this gap, we propose QFA2SR, an effective and imperceptible query-free black-box attack, by leveraging the transferability of adversarial voices. To improve transferability, we present three novel methods, tailored loss functions, SRS ensemble, and time-freq corrosion. The first one tailors loss functions to different attack scenarios. The latter two augment surrogate SRSs in two different ways. SRS ensemble combines diverse surrogate SRSs with new strategies, amenable to the unique scoring characteristics of SRSs. Time-freq corrosion augments surrogate SRSs by incorporating well-designed time-/frequency-domain modification functions, which simulate and approximate the decision boundary of the target SRS and distortions introduced during over-the-air attacks. QFA2SR boosts the targeted transferability by 20.9%-70.7% on four popular commercial APIs (Microsoft Azure, iFlytek, Jingdong, and TalentedSoft), significantly outperforming existing attacks in query-free setting, with negligible effect on the imperceptibility. QFA2SR is also highly effective when launched over the air against three wide-spread voice assistants (Google Assistant, Apple Siri, and TMall Genie) with 60%, 46%, and 70% targeted transferability, respectively.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Guangke Chen",
        "Yedi Zhang",
        "Zhe Zhao",
        "Fu Song"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/115ae8b0ace627ea6cd23170f4b6173b2dbd77a6",
      "pdf_url": "https://arxiv.org/pdf/2305.14097",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dbd34a8b68a6a32b93a9ada0217fbdc51bdeaf56",
      "title": "A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles",
      "abstract": "Modern vehicles, including autonomous vehicles and connected vehicles, are increasingly connected to the external world, which enables various functionalities and services. However, the improving connectivity also increases the attack surfaces of the Internet of Vehicles (IoV), causing its vulnerabilities to cyber-threats. Due to the lack of authentication and encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs) are essential approaches to protect modern vehicle systems from network attacks. In this paper, a transfer learning and ensemble learning-based IDS is proposed for IoV systems using convolutional neural networks (CNNs) and hyper-parameter optimization techniques. In the experiments, the proposed IDS has demonstrated over 99.25% detection rates and F1-scores on two well-known public benchmark IoV security datasets: the Car-Hacking dataset and the CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for cyber-attack detection in both intra-vehicle and external vehicular networks.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Li Yang",
        "A. Shami"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/dbd34a8b68a6a32b93a9ada0217fbdc51bdeaf56",
      "pdf_url": "https://arxiv.org/pdf/2201.11812",
      "publication_date": "2022-01-27",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "74dc649d6535ba3f5190061d8fefe2524093c49c",
      "title": "Transferable Attack for Semantic Segmentation",
      "abstract": "We analysis performance of semantic segmentation models wrt. adversarial attacks, and observe that the adversarial examples generated from a source model fail to attack the target models. i.e The conventional attack methods, such as PGD and FGSM, do not transfer well to target models, making it necessary to study the transferable attacks, especially transferable attacks for semantic segmentation. We find two main factors to achieve transferable attack. Firstly, the attack should come with effective data augmentation and translation-invariant features to deal with unseen models. Secondly, stabilized optimization strategies are needed to find the optimal attack direction. Based on the above observations, we propose an ensemble attack for semantic segmentation to achieve more effective attacks with higher transferability. The source code and experimental results are publicly available via our project page: https://github.com/anucvers/TASS.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Mengqi He",
        "Jing Zhang",
        "Zhaoyuan Yang",
        "Mingyi He",
        "Nick Barnes",
        "Yuchao Dai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/74dc649d6535ba3f5190061d8fefe2524093c49c",
      "pdf_url": "https://arxiv.org/pdf/2307.16572",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ea1710f4d34950560663210781d11b87225c5d55",
      "title": "AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning",
      "abstract": "Multimodal contrastive learning aims to train a general-purpose feature extractor, such as CLIP, on vast amounts of raw, unlabeled paired image-text data. This can greatly benefit various complex downstream tasks, including cross-modal image-text retrieval and image classification. Despite its promising prospect, the security issue of cross-modal pre-trained encoder has not been fully explored yet, especially when the pre-trained encoder is publicly available for commercial use. In this work, we propose AdvCLIP, the first attack framework for generating downstream-agnostic adversarial examples based on cross-modal pre-trained encoders. AdvCLIP aims to construct a universal adversarial patch for a set of natural images that can fool all the downstream tasks inheriting the victim cross-modal pre-trained encoder. To address the challenges of heterogeneity between different modalities and unknown downstream tasks, we first build a topological graph structure to capture the relevant positions between target samples and their neighbors. Then, we design a topology-deviation based generative adversarial network to generate a universal adversarial patch. By adding the patch to images, we minimize their embeddings similarity to different modality and perturb the sample distribution in the feature space, achieving unviersal non-targeted attacks. Our results demonstrate the excellent attack performance of AdvCLIP on two types of downstream tasks across eight datasets. We also tailor three popular defenses to mitigate AdvCLIP, highlighting the need for new defense mechanisms to defend cross-modal pre-trained encoders. Our codes are available at: https://github.com/CGCL-codes/AdvCLIP.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Ziqi Zhou",
        "Shengshan Hu",
        "Minghui Li",
        "Hangtao Zhang",
        "Yechao Zhang",
        "Hai Jin"
      ],
      "citation_count": 102,
      "url": "https://www.semanticscholar.org/paper/ea1710f4d34950560663210781d11b87225c5d55",
      "pdf_url": "https://arxiv.org/pdf/2308.07026",
      "publication_date": "2023-08-14",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca7c9007635a599e3307d2ba40002da24425366f",
      "title": "Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers",
      "abstract": null,
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Shengyuan Yang",
        "Jiawang Bai",
        "Kuofeng Gao",
        "Yong Yang",
        "Yiming Li",
        "Shu-Tao Xia"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/ca7c9007635a599e3307d2ba40002da24425366f",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95ab040c47adfcb6cd1fe856f489c44273d324b0",
      "title": "Toward Transferable Attack via Adversarial Diffusion in Face Recognition",
      "abstract": "Modern face recognition systems widely use deep convolutional neural networks (DCNNs). However, DCNNs are susceptible to adversarial examples, posing security risks to these systems. Transferable adversarial examples that can be transferred from surrogate to target models greatly undermine the robustness of DCNNs. Numerous attempts have been made to generate transferable adversarial examples, but the existing methods often suffer from limited transferability or produce adversarial examples with poor image perceptual quality. Recently, diffusion models have shown remarkable success in image generation and have excelled in various downstream tasks. However, their potential in adversarial attacks remains largely unexplored. To bridge this gap, we propose a novel approach, namely Adversarial Diffusion Attack (ADA), in generation of transferable adversarial facial examples. ADA employs a dynamic game-like strategy between injection and denoising that progressively reinforces the robustness of adversarial perturbation in the reverse process of diffusion model. Additionally, both adversarial perturbation and residual image are embedded to drift benign distribution towards adversarial distribution, crafting adversarial examples with high image quality. Extensive experimental results obtained on two benchmarking datasets, LFW and CelebA-HQ, demonstrate that ADA achieves higher attack success rates and produces adversarial examples with superior image quality compared to the state-of-the-art methods.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Cong Hu",
        "Yuan-bo Li",
        "Zhenhua Feng",
        "Xiao-jun Wu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/95ab040c47adfcb6cd1fe856f489c44273d324b0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5fbf503e2bd3b6930990ecd3851cf2c224409dff",
      "title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Xingyi Zhao",
        "Depeng Xu",
        "Shuhan Yuan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/5fbf503e2bd3b6930990ecd3851cf2c224409dff",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ce42b2f6e9a6178d1c40ed2f1b736cb5fa13559",
      "title": "Transferable Adversarial Attacks on SAM and Its Downstream Models",
      "abstract": "The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage. This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model. To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations. Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM. Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability. Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models. Code is available at https://github.com/xiasong0501/GRAT.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Song Xia",
        "Wenhan Yang",
        "Yi Yu",
        "Xun Lin",
        "Henghui Ding",
        "Lingyu Duan",
        "Xudong Jiang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6ce42b2f6e9a6178d1c40ed2f1b736cb5fa13559",
      "pdf_url": "",
      "publication_date": "2024-10-26",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "64679845467dce0fb3669e359a9c3dc173fe723f",
      "title": "As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?",
      "abstract": "Foundation models pre-trained on web-scale vision-language data, such as CLIP, are widely used as cornerstones of powerful machine learning systems. While pre-training offers clear advantages for downstream learning, it also endows downstream models with shared adversarial vulnerabilities that can be easily identified through the open-sourced foundation model. In this work, we expose such vulnerabilities in CLIP's downstream models and show that foundation models can serve as a basis for attacking their downstream systems. In particular, we propose a simple yet effective adversarial attack strategy termed Patch Representation Misalignment (PRM). Solely based on open-sourced CLIP vision encoders, this method produces adversaries that simultaneously fool more than 20 downstream models spanning 4 common vision-language tasks (semantic segmentation, object detection, image captioning and visual question-answering). Our findings highlight the concerning safety risks introduced by the extensive usage of public foundational models in the development of downstream systems, calling for extra caution in these scenarios.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Anjun Hu",
        "Jindong Gu",
        "Francesco Pinto",
        "Konstantinos Kamnitsas",
        "Philip H. S. Torr"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/64679845467dce0fb3669e359a9c3dc173fe723f",
      "pdf_url": "",
      "publication_date": "2024-03-19",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "315ede013af8b61506d0e60fab4bc57def64cf50",
      "title": "PAIF: Perception-Aware Infrared-Visible Image Fusion for Attack-Tolerant Semantic Segmentation",
      "abstract": "Infrared and visible image fusion is a powerful technique that combines complementary information from different modalities for downstream semantic perception tasks. Existing learning-based methods show remarkable performance, but are suffering from the inherent vulnerability of adversarial attacks, causing a significant decrease in accuracy. In this work, a perception-aware fusion framework is proposed to promote segmentation robustness in adversarial scenes. We first conduct systematic analyses about the components of image fusion, investigating the correlation with segmentation robustness under adversarial perturbations. Based on these analyses, we propose a harmonized architecture search with a decomposition-based structure to balance standard accuracy and robustness. We also propose an adaptive learning strategy to improve the parameter robustness of image fusion, which can learn effective feature extraction under diverse adversarial perturbations. Thus, the goals of image fusion (i.e., extracting complementary features from source modalities and defending attack) can be realized from the perspectives of architectural and learning strategies. Extensive experimental results demonstrate that our scheme substantially enhances the robustness, with gains of 15.3% mIOU of segmentation in the adversarial scene, compared with advanced competitors. The source codes are available at https://github.com/LiuZhu-CV/PAIF.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Zhu Liu",
        "Jinyuan Liu",
        "Ben-xi Zhang",
        "Long Ma",
        "Xin-Yue Fan",
        "Risheng Liu"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/315ede013af8b61506d0e60fab4bc57def64cf50",
      "pdf_url": "https://arxiv.org/pdf/2308.03979",
      "publication_date": "2023-08-08",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e82483340ba0811b6b5d6521fcd30a90ec267f8b",
      "title": "Highly Transferable Diffusion-based Unrestricted Adversarial Attack on Pre-trained Vision-Language Models",
      "abstract": "Pre-trained Vision-Language Models (VLMs) have shown great ability in various Vision-Language tasks. However, these VLMs exhibit inherent vulnerabilities to transferable adversarial examples, which could potentially undermine their performance and reliability in real-world applications. Cross-modal interactions have been demonstrated to be the key point to boosting adversarial transferability, but the utilization of them is limited in existing multimodal adversarial attacks. Stable Diffusion, which contains multiple cross-attention modules, possesses great potential in facilitating adversarial transferability by leveraging abundant cross-modal interactions. Therefore, We propose a Multimodal Diffusion-based Attack (MDA), which conducts adversarial attacks against VLMs using Stable Diffusion. Specifically, MDA initially generates adversarial text, which is subsequently utilized to optimize the adversarial image during the diffusion process. Besides leveraging adversarial text in calculating downstream loss, MDA also takes it as the guiding prompt in adversarial image generation during the denoising process, which enriches the ways of cross-modal interactions, thus strengthening the adversarial transferability. Compared with pixel-based attacks, MDA introduces perturbations in the latent space rather than pixel space to manipulate high-level semantics, which is also beneficial to improving adversarial transferability. Experimental results demonstrate that the adversarial examples generated by MDA are highly transferable across different VLMs on different downstream tasks, surpassing state-of-the-art methods by a large margin.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Wenzhuo Xu",
        "Kai Chen",
        "Ziyi Gao",
        "Zhipeng Wei",
        "Jingjing Chen",
        "Yu-Gang Jiang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e82483340ba0811b6b5d6521fcd30a90ec267f8b",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "48b6a5842b856bf2fe19e698b8c2d4dd6622cc01",
      "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
      "abstract": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models.In this paper, we propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model), a high-performance and effective black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "CheolWon Na",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/48b6a5842b856bf2fe19e698b8c2d4dd6622cc01",
      "pdf_url": "https://aclanthology.org/2023.acl-long.430.pdf",
      "publication_date": null,
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "92bcd3760fc07f19922496c87ea3912917a2a5e5",
      "title": "CodeBERT\u2010Attack: Adversarial attack against source code deep learning models via pre\u2010trained model",
      "abstract": "Over the past few years, the software engineering (SE) community has widely employed deep learning (DL) techniques in many source code processing tasks. Similar to other domains like computer vision and natural language processing (NLP), the state\u2010of\u2010the\u2010art DL techniques for source code processing can still suffer from adversarial vulnerability, where minor code perturbations can mislead a DL model's inference. Efficiently detecting such vulnerability to expose the risks at an early stage is an essential step and of great importance for further enhancement. This paper proposes a novel black\u2010box effective and high\u2010quality adversarial attack method, namely CodeBERT\u2010Attack (CBA), based on the powerful large pre\u2010trained model (i.e., CodeBERT) for DL models of source code processing. CBA locates the vulnerable positions through masking and leverages the power of CodeBERT to generate textual preserving perturbations. We turn CodeBERT against DL models and further fine\u2010tuned CodeBERT models for specific downstream tasks, and successfully mislead these victim models to erroneous outputs. In addition, taking the power of CodeBERT, CBA is capable of effectively generating adversarial examples that are less perceptible to programmers. Our in\u2010depth evaluation on two typical source code classification tasks (i.e., functionality classification and code clone detection) against the most widely adopted LSTM and the powerful fine\u2010tuned CodeBERT models demonstrate the advantages of our proposed technique in terms of both effectiveness and efficiency. Furthermore, our results also show (1) that pre\u2010training may help CodeBERT gain resilience against perturbations further, and (2) certain pre\u2010training tasks may be beneficial for adversarial robustness.",
      "year": 2023,
      "venue": "J. Softw. Evol. Process.",
      "authors": [
        "Huangzhao Zhang",
        "Shuai Lu",
        "Zhuo Li",
        "Zhi Jin",
        "Lei Ma",
        "Yang Liu",
        "Ge Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/92bcd3760fc07f19922496c87ea3912917a2a5e5",
      "pdf_url": "",
      "publication_date": "2023-05-15",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4697a189986951bc74aa9c3fd609c4cffabaa45e",
      "title": "Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models",
      "abstract": "Denoising probabilistic diffusion models have shown breakthrough performance to generate more photo-realistic images or human-level illustrations than the prior models such as GANs. This high image-generation capability has stimulated the creation of many downstream applications in various areas. However, we find that this technology is actually a double-edged sword: we identify a new type of attack, called the Natural Denoising Diffusion (NDD) attack based on the finding that state-of-the-art deep neural network (DNN) models still hold their prediction even if we intentionally remove their robust features, which are essential to the human visual system (HVS), through text prompts. The NDD attack shows a significantly high capability to generate low-cost, model-agnostic, and transferable adversarial attacks by exploiting the natural attack capability in diffusion models. To systematically evaluate the risk of the NDD attack, we perform a large-scale empirical study with our newly created dataset, the Natural Denoising Diffusion Attack (NDDA) dataset. We evaluate the natural attack capability by answering 6 research questions. Through a user study, we find that it can achieve an 88% detection rate while being stealthy to 93% of human subjects; we also find that the non-robust features embedded by diffusion models contribute to the natural attack capability. To confirm the model-agnostic and transferable attack capability, we perform the NDD attack against the Tesla Model 3 and find that 73% of the physically printed attacks can be detected as stop signs. Our hope is that the study and dataset can help our community be aware of the risks in diffusion models and facilitate further research toward robust DNN models.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Takami Sato",
        "Justin Yue",
        "Nanze Chen",
        "Ningfei Wang",
        "Qi Alfred Chen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4697a189986951bc74aa9c3fd609c4cffabaa45e",
      "pdf_url": "https://arxiv.org/pdf/2308.15692",
      "publication_date": "2023-08-30",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37aa3ea43fc0c1fab03bf3db6188a3236bd6394c",
      "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
      "abstract": "Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step. In this paper, we formally model the adversarial attack task on PLMs as a sequential decision-making problem, where the whole attack process is sequential with two decision-making problems, i.e., word finder and word substitution. Considering the attack process can only receive the final state without any direct intermediate signals, we propose to use reinforcement learning to find an appropriate sequential attack path to generate adversaries, named SDM-Attack. Extensive experimental results show that SDM-Attack achieves the highest attack success rate with a comparable modification rate and semantic similarity to attack fine-tuned BERT. Furthermore, our analyses demonstrate the generalization and transferability of SDM-Attack. The code is available at https://github.com/fduxuan/SDM-Attack.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Xuanjie Fang",
        "Sijie Cheng",
        "Yang Liu",
        "Wen Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/37aa3ea43fc0c1fab03bf3db6188a3236bd6394c",
      "pdf_url": "http://arxiv.org/pdf/2305.17440",
      "publication_date": "2023-05-27",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37ac159e066d9a8f742c2c6bef271d243917ecc3",
      "title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4",
      "abstract": "Prompt-based learning has been widely applied in many low-resource NLP tasks such as few-shot scenarios. However, this paradigm has been shown to be vulnerable to backdoor attacks. Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates. In this work, we propose a novel approach of TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models via GPT4), which is a data-independent attack method. Specifically, we first utilize GPT4 to reformulate manual templates to generate tone-strong and normal templates, and the former are injected into the model as a backdoor trigger in the pre-training phase. Then, we not only directly employ the above templates in the downstream task, but also use GPT4 to generate templates with similar tone to the above templates to carry out transferable attacks. Finally we have conducted extensive experiments on five NLP datasets and three BERT series models, with experimental results justifying that our TARGET method has better attack performance and stealthiness compared to the two-external baseline methods on direct attacks, and in addition achieves satisfactory attack capability in the unseen tone-similar templates.",
      "year": 2023,
      "venue": "Natural Language Processing and Chinese Computing",
      "authors": [
        "Zihao Tan",
        "Qingliang Chen",
        "Yongjian Huang",
        "Chen Liang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/37ac159e066d9a8f742c2c6bef271d243917ecc3",
      "pdf_url": "",
      "publication_date": "2023-11-29",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c85f7d1e65be9465ac76ea2e1711dbb57d733285",
      "title": "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios",
      "abstract": "Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when the defender is, e.g., a downstream user without access to the classifier's training set. This challenge is addressed here by a reverse-engineering defense (RED), which has been shown to yield state-of-the-art performance in several domains. However, existing REDs are not applicable when there are only {\\it two classes} or when {\\it multiple attacks} are present. These scenarios are first studied in the current paper, under the practical constraints that the defender neither has access to the classifier's training set nor to supervision from clean reference classifiers trained for the same domain. We propose a detection framework based on BP reverse-engineering and a novel {\\it expected transferability} (ET) statistic. We show that our ET statistic is effective {\\it using the same detection threshold}, irrespective of the classification domain, the attack configuration, and the BP reverse-engineering algorithm that is used. The excellent performance of our method is demonstrated on six benchmark datasets. Notably, our detection framework is also applicable to multi-class scenarios with multiple attacks. Code is available at https://github.com/zhenxianglance/2ClassBADetection.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Zhen Xiang",
        "David J. Miller",
        "G. Kesidis"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/c85f7d1e65be9465ac76ea2e1711dbb57d733285",
      "pdf_url": "",
      "publication_date": "2022-01-20",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "22fc2178937efb12677347f7b8057cb0291eff46",
      "title": "FoundPAD: Foundation Models Reloaded for Face Presentation Attack Detection",
      "abstract": "Although face recognition systems have seen a massive performance enhancement in recent years, they are still targeted by threats such as presentation attacks, leading to the need for generalizable presentation attack detection (PAD) algorithms. Current PAD solutions suffer from two main problems: low generalization to unknown scenarios and large training data requirements. Foundation models (FM) are pretrained on extensive datasets, achieving remarkable results when generalizing to unseen domains and allowing for efficient task-specific adaption even in low data availability settings. This is one of the first works to recognize the potential of FMs and adapt them for the downstream task of PAD. The FM under consideration is adapted with LoRA weights while simultaneously training a classification header. The resultant architecture, FoundPAD, is highly generalizable to unseen domains, achieving competitive results in several settings under different data availability scenarios and even when using synthetic training data. To encourage reproducibility and facilitate further research in PAD, we publicly release the implementation of FoundPAD at https://github.com/gurayozgur/FoundPAD.",
      "year": 2025,
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "authors": [
        "Guray Ozgur",
        "Eduarda Caldeira",
        "Tahar Chettaoui",
        "Fadi Boutros",
        "Raghavendra Ramachandra",
        "Naser Damer"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/22fc2178937efb12677347f7b8057cb0291eff46",
      "pdf_url": "http://arxiv.org/pdf/2501.02892",
      "publication_date": "2025-01-06",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1bd00ac280edd2608f415c7f6396e946ecf0864",
      "title": "MADation: Face Morphing Attack Detection with Foundation Models",
      "abstract": "Despite the considerable performance improvements of face recognition algorithms in recent years, the same scientific advances responsible for this progress can also be used to create efficient ways to attack them, posing a threat to their secure deployment. Morphing attack detection (MAD) systems aim to detect a specific type of threat, morphing attacks, at an early stage, preventing them from being considered for verification in critical processes. Foundation models (FM) learn from extensive amounts of unlabelled data, achieving remarkable zero-shot generalization to unseen domains. Although this generalization capacity might be weak when dealing with domain-specific downstream tasks such as MAD, FMs can easily adapt to these settings while retaining the built-in knowledge acquired during pretraining. In this work, we recognize the potential of FMs to perform well in the MAD task when properly adapted to its specificities. To this end, we adapt FM CLIP architectures with LoRA weights while simultaneously training a classification header. The proposed framework, MADation surpasses our alternative FM and transformer-based frameworks and constitutes the first adaption of FMs to the MAD task. MADation presents competitive results with current MAD solutions in the literature and even surpasses them in several evaluation scenarios. To encourage reproducibility and facilitate further research in MAD, we publicly release the implementation of MADation at https://github.com/gurayozgur/MADation.",
      "year": 2025,
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "authors": [
        "Eduarda Caldeira",
        "Guray Ozgur",
        "Tahar Chettaoui",
        "Marija Ivanovska",
        "Fadi Boutros",
        "Vitomir \u0160truc",
        "Naser Damer"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/f1bd00ac280edd2608f415c7f6396e946ecf0864",
      "pdf_url": "http://arxiv.org/pdf/2501.03800",
      "publication_date": "2025-01-07",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "12b520a740d47b7862f1bd78f4886a676436e1e4",
      "title": "Eclipse Attack Detection for Blockchain Network Layer Based on Deep Feature Extraction",
      "abstract": "An eclipse attack is a common method used to attack the blockchain network layer; however, detecting eclipse attacks is challenging, and the performance of existing methods is inadequate due to uneven sample distribution, incomplete definition of discriminating features, and weak feature perception. Thus, this paper proposes an eclipse attack traffic detection method based in a custom combination of features and deep learning. To describe the behavior characteristics of attack traffic more accurately, traffic attribute features in there levels are defined in combination with the eclipse attack method. Here, the downstream traffic behavior feature of the eclipse attack is described from the conventional traffic feature, and the frequency distribution characteristics of eclipse attack traffic is by introducing the \u03c6-entropy divergence algorithm. In addition, the structural characteristics of eclipse attack traffic are mapped from the rate of changes in traffic communication and load features. Then, the improved synthetic minority oversampling technique (ISMOTE) up-sampling algorithm is employed to eliminate interference caused by the uneven distribution of eclipse attack traffic samples on the detection results. In addition, the ISMOTE algorithm adjusts the sampling weight of minority class samples, supports automatic clustering and efficient up-sampling of samples, and improves the detection accuracy performance of eclipse attack samples by calculating the local cluster density. Then, deep feature mining is performed on the eclipse attack traffic from the distribution characteristics of space and time series using a CNN and Bi-LSTM. Simultaneously, mining features are fully integrated into mixed feature using the multihead attention mechanism such that the relevance and complementarity of the two feature distributions can be utilized to enhance the model\u2019s ability to perceive the spatiotemporal relationship of the eclipse attack traffic. Finally, the generated multihead attention items are detected for binary classification, and the results are output. Experimental results demonstrate that the proposed method can comprehensively enhance detection performance and sufficiently detect and classify eclipse attack traffic in the blockchain network layer.",
      "year": 2022,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Qianyi Dai",
        "Bin Zhang",
        "Shuqin Dong"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/12b520a740d47b7862f1bd78f4886a676436e1e4",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2022/1451813.pdf",
      "publication_date": "2022-04-13",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2e5047b2e3e8e9b36c1d365e7bfeba6d34bada56",
      "title": "Self-Supervised Face Presentation Attack Detection with Dynamic Grayscale Snippets",
      "abstract": "Face presentation attack detection (PAD) plays an important role in defending face recognition systems against presentation attacks. The success of PAD largely relies on supervised learning that requires a huge number of labeled data, which is especially challenging for videos and often requires expert knowledge. To avoid the costly collection of labeled data, this paper presents a novel method for self-supervised video representation learning via motion prediction. To achieve this, we exploit the temporal consistency based on three RGB frames which are acquired at three different times in the video sequence. The obtained frames are then transformed into grayscale images where each image is specified to three different channels such as R(red), G(green), and B(blue) to form a dynamic grayscale snippet (DGS). Motivated by this, the labels are automatically generated to increase the temporal diversity based on DGS by using the different temporal lengths of the videos, which prove to be very helpful for the downstream task. Benefiting from the self-supervised nature of our method, we report the results that outperform existing methods on four public benchmarks, namely, Replay-Attack, MSU-MFSD, CASIA-FASD, and OULU-NPU. Explainability analysis has been carried out through LIME and Grad-CAM techniques to visualize the most important features used in the DGS.",
      "year": 2022,
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
      "authors": [
        "Usman Muhammad",
        "M. Oussalah"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/2e5047b2e3e8e9b36c1d365e7bfeba6d34bada56",
      "pdf_url": "https://oulurepo.oulu.fi/bitstream/10024/45414/1/nbnfi-fe2023081697145.pdf",
      "publication_date": "2022-08-27",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a0755a1dc1fd8314fa3c669e54edbf1adc2f1b68",
      "title": "Task and Model Agnostic Adversarial Attack on Graph Neural Networks",
      "abstract": "Adversarial attacks on Graph Neural Networks (GNNs) reveal their security vulnerabilities, limiting their adoption in safety-critical applications. However, existing attack strategies rely on the knowledge of either the GNN model being used or the predictive task being attacked. Is this knowledge necessary? For example, a graph may be used for multiple downstream tasks unknown to a practical attacker. It is thus important to test the vulnerability of GNNs to adversarial perturbations in a model and task-agnostic setting. In this work, we study this problem and show that Gnns remain vulnerable even when the downstream task and model are unknown. The proposed algorithm, TANDIS (Targeted Attack via Neighborhood DIStortion) shows that distortion of node neighborhoods is effective in drastically compromising prediction performance. Although neighborhood distortion is an NP-hard problem, TANDIS designs an effective heuristic through a novel combination of Graph Isomorphism Network with deep Q-learning. Extensive experiments on real datasets show that, on average, TANDIS is up to 50% more effective than state-of-the-art techniques, while being more than 1000 times faster.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Kartik Sharma",
        "S. Verma",
        "Sourav Medya",
        "Sayan Ranu",
        "Arnab Bhattacharya"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/a0755a1dc1fd8314fa3c669e54edbf1adc2f1b68",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26761/26533",
      "publication_date": "2021-12-25",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a377b6c84cb988569227c073a5defea06dc8f117",
      "title": "Downstream Task-agnostic Transferable Attacks on Language-Image Pre-training Models",
      "abstract": "Vision-language pre-trained models (e.g., CLIP) trained on large-scale datasets via self-supervised learning, are drawing increasing research attention since they can achieve superior performances on multi-modal downstream tasks. Nevertheless, we find that the adversarial perturbations crafted on vision-language pre-trained models can be used to attack different corresponding downstream task models. Specifically, to investigate such adversarial transferability, we introduce a task-agnostic method named Global and Local Augmentation (GLA) attack to generate highly transferable adversarial examples on CLIP, to attack black-box downstream task models. GLA adopts random crop and resize at both global and local patch levels, to create more diversity and make adversarial noises robust. Then GLA generates the adversarial perturbations by minimizing the cosine similarity between intermediate features from augmented adversarial and benign examples. Extensive experiments on three CLIP image encoders with different backbones and three different downstream tasks demonstrate the superiority of our method compared with other strong baselines. The code is available at https://github.com/yqlvcoding/GLAattack.",
      "year": 2023,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yiqiang Lv",
        "Jingjing Chen",
        "Zhipeng Wei",
        "Kai Chen",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a377b6c84cb988569227c073a5defea06dc8f117",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "58beee0db34e6f3bd2e2363267a7f728a3470692",
      "title": "SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer",
      "abstract": "Although pre-training achieves remarkable performance, it suffers from task-agnostic backdoor attacks due to vulnerabilities in data and training mechanisms. These attacks can transfer backdoors to various downstream tasks. In this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning filter that mitigates such risks. To overcome the limitations of manual target setting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and universal task-agnostic backdoor attack via syntactic transfer, further exposing vulnerabilities in pre-trained language models (PLMs). Specifically, $\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training space through corpus poisoning, while preserving the PLM's pre-training capabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets based on contrastive learning, creating a uniform distribution in the pre-training space. To identify syntactic differences, we also introduce an awareness module to minimize interference between backdoors. Experiments show that $\\mathtt{SynGhost}$ poses significant threats and can transfer to various downstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on perplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at https://github.com/Zhou-CyberSecurity-AI/SynGhost.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Pengzhou Cheng",
        "Wei Du",
        "Zongru Wu",
        "Fengwei Zhang",
        "Libo Chen",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/58beee0db34e6f3bd2e2363267a7f728a3470692",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "downstream task attack",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2e8a8e28c9880c85b44b8dae619eab2cc6e5a4cb",
      "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models",
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding and generation, yet their vulnerability to adversarial attacks raises significant robustness concerns. While existing effective attacks always focus on task-specific white-box settings, these approaches are limited in the context of LVLMs, which are designed for diverse downstream tasks and require expensive full-model gradient computations. Motivated by the pivotal role and wide adoption of the vision encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack (VEAttack), which targets the vision encoder of LVLMs only. Specifically, we propose to generate adversarial examples by minimizing the cosine similarity between the clean and perturbed visual features, without accessing the following large language models, task information, and labels. It significantly reduces the computational overhead while eliminating the task and label dependence of traditional white-box attacks in LVLMs. To make this simple attack effective, we propose to perturb images by optimizing image tokens instead of the classification token. We provide both empirical and theoretical evidence that VEAttack can easily generalize to various tasks. VEAttack has achieved a performance degradation of 94.5% on image caption task and 75.7% on visual question answering task. We also reveal some key observations to provide insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token attention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity to attack steps. The code is available at https://github.com/hfmei/VEAttack-LVLM",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hefei Mei",
        "Zirui Wang",
        "Shengjie You",
        "Minjing Dong",
        "Chang Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2e8a8e28c9880c85b44b8dae619eab2cc6e5a4cb",
      "pdf_url": "",
      "publication_date": "2025-05-23",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd424df552b3759a4a6ba24e0ce4169047c5cbe7",
      "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models",
      "abstract": "Unified vision-language models(VLMs) have recently shown remarkable progress, enabling a single model to flexibly address diverse tasks through different instructions within a shared computational architecture. This instruction-based control mechanism creates unique security challenges, as adversarial inputs must remain effective across multiple task instructions that may be unpredictably applied to process the same malicious content. In this paper, we introduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with GPT-4-assisted annotations for systematically evaluating cross-task adversarial attacks on unified VLMs. CrossVLAD centers on the object-change objective-consistently manipulating a target object's classification across four downstream tasks-and proposes a novel success rate metric that measures simultaneous misclassification across all tasks, providing a rigorous evaluation of adversarial transferability. To tackle this challenge, we present CRAFT (Cross-task Region-based Attack Framework with Token-alignment), an efficient region-centric attack method. Extensive experiments on Florence-2 and other popular unified VLMs demonstrate that our method outperforms existing approaches in both overall cross-task attack performance and targeted object-change success rates, highlighting its effectiveness in adversarially influencing unified VLMs across diverse tasks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiale Zhao",
        "Xinyang Jiang",
        "Junyao Gao",
        "Yuhao Xue",
        "Cairong Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fd424df552b3759a4a6ba24e0ce4169047c5cbe7",
      "pdf_url": "",
      "publication_date": "2025-07-10",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e71cd563f3e97f58803e2c871a8ab179994cc23e",
      "title": "Mimic and Fool: A Task-Agnostic Adversarial Attack",
      "abstract": "At present, adversarial attacks are designed in a task-specific fashion. However, for downstream computer vision tasks such as image captioning and image segmentation, the current deep-learning systems use an image classifier such as VGG16, ResNet50, and Inception-v3 as a feature extractor. Keeping this in mind, we propose Mimic and Fool (MaF), a task-agnostic adversarial attack. Given a feature extractor, the proposed attack finds an adversarial image, which can mimic the image feature of the original image. This ensures that the two images give the same (or similar) output regardless of the task. We randomly select 1000 MSCOCO validation images for experimentation. We perform experiments on two image captioning models, Show and Tell, Show Attend and Tell, and one visual question answering (VQA) model, namely, end-to-end neural module network (N2NMN). The proposed attack achieves a success rate of 74.0%, 81.0%, and 87.1% for Show and Tell, Show Attend and Tell, and N2NMN, respectively. We also propose a slight modification to our attack to generate natural-looking adversarial images. In addition, we also show the applicability of the proposed attack for invertible architecture. Since MaF only requires information about the feature extractor of the model, it can be considered as a gray-box attack.",
      "year": 2019,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Akshay Chaturvedi",
        "Utpal Garain"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/e71cd563f3e97f58803e2c871a8ab179994cc23e",
      "pdf_url": "https://arxiv.org/pdf/1906.04606",
      "publication_date": "2019-06-11",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bccc89fd7e000cb5519e2c675a7b0f8db05a85a8",
      "title": "Faster Mimic and Fool: An Adversarial Attack",
      "abstract": "Currently, adversarial attacks are typically designed for specific tasks, and although there are some task-agnostic attacks are generally less effective than task-specific ones. These attacks exploit the fact that CNN-based feature extractors cannot be reversed or inverted, making the downstream models vulnerable to these attacks. However, they are not optimally designed because they use the entire CNN to generate an adversarial example. This paper proposes a modified version of this approach called Faster Mimic and Fool (MaF), which requires less time and fewer resources to create an adversarial image. The experiment involved selecting 100 random FlickR 8K images and testing the attack on an Inception-V3-based captioning model. The results showed that Faster MaF achieved a Bleu-4 score that is 13.5% and 31.1% better than MaF and OIMO, respectively. Since Faster MaF requires knowledge of the CNN, it can be considered a grey-box attack.",
      "year": 2024,
      "venue": "2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)",
      "authors": [
        "Madhvi Patel",
        "Rohan Patel",
        "Dhirendra Pratap Singh",
        "Jaytrilok Choudhary",
        "Nikhil Nigam"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bccc89fd7e000cb5519e2c675a7b0f8db05a85a8",
      "pdf_url": "",
      "publication_date": "2024-02-24",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "65768dd5531295e2e9a872ad7c8d99c07b8747a4",
      "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
      "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Chengkun Wei",
        "Wenlong Meng",
        "Zhikun Zhang",
        "M. Chen",
        "Ming-Hui Zhao",
        "Wenjing Fang",
        "Lei Wang",
        "Zihui Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/65768dd5531295e2e9a872ad7c8d99c07b8747a4",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.23238",
      "publication_date": "2023-08-26",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2badd0a9b70754dcf729cf1b987f601c22478da8",
      "title": "Attacking Attention of Foundation Models Disrupts Downstream Tasks",
      "abstract": "Foundation models represent the most prominent and recent paradigm shift in artificial intelligence. Foundation models are large models, trained on broad data that deliver high accuracy in many downstream tasks, often without finetuning. For this reason, models such as CLIP [36], DINO [35] or Vision Transfomers (ViT) [10], are becoming the bedrock of many industrial AI-powered applications. However, the reliance on pre-trained foundation models also introduces significant security concerns, as these models are vulnerable to adversarial attacks. Such attacks involve deliberately crafted inputs designed to deceive AI systems, jeopardizing their reliability. This paper studies the vulnerabilities of vision foundation models, focusing specifically on CLIP and ViTs, and explores the transferability of adversarial attacks to downstream tasks. We introduce a novel attack, targeting the structure of transformer-based architectures in a task-agnostic fashion. We demonstrate the effectiveness of our attack on several downstream tasks: classification, captioning, image/text retrieval, segmentation and depth estimation.",
      "year": 2025,
      "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Hondamunige Prasanna Silva",
        "Federico Becattini",
        "Lorenzo Seidenari"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/2badd0a9b70754dcf729cf1b987f601c22478da8",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "downstream task attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ef22844863ea3e14c4b77f3fadd6a11dc9da0f76",
      "title": "BASK: Backdoor Attack for Self-Supervised Encoders with Knowledge Distillation Survivability",
      "abstract": "Backdoor attacks in self-supervised learning pose an increasing threat. Recent studies have shown that knowledge distillation can mitigate these attacks by altering feature representations. In response, we propose BASK, a novel backdoor attack that remains effective after distillation. BASK uses feature weighting and representation alignment strategies to implant persistent backdoors into the encoder\u2019s feature space. This enables transferability to student models. We evaluated BASK on the CIFAR-10 and STL-10 datasets and compared it with existing self-supervised backdoor attacks under four advanced defenses: SEED, MKD, Neural Cleanse, and MiMiC. Our experimental results demonstrate that BASK maintains high attack success rates while preserving downstream task performance. This highlights the robustness of BASK and the limitations of current defense mechanisms.",
      "year": 2025,
      "venue": "Electronics",
      "authors": [
        "Yihong Zhang",
        "Guojia Li",
        "Yihui Zhang",
        "Yan Cao",
        "Mingyue Cao",
        "Chengyao Xue"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ef22844863ea3e14c4b77f3fadd6a11dc9da0f76",
      "pdf_url": "",
      "publication_date": "2025-07-06",
      "keywords_matched": [
        "neural cleanse",
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1a4708d7651a918ce53c0d67019def41a9bc5b14",
      "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation",
      "abstract": "Face Morphing Attack Detection (MAD) is a critical challenge in face recognition security, where attackers can fool systems by interpolating the identity information of two or more individuals into a single face image, resulting in samples that can be verified as belonging to multiple identities by face recognition systems. While multimodal foundation models (FMs) like CLIP offer strong zero-shot capabilities by jointly modeling images and text, most prior works on FMs for biometric recognition have relied on fine-tuning for specific downstream tasks, neglecting their potential for direct, generalizable deployment. This work explores a pure zero-shot approach to MAD by leveraging CLIP without any additional training or fine-tuning, focusing instead on the design and aggregation of multiple textual prompts per class. By aggregating the embeddings of diverse prompts, we better align the model's internal representations with the MAD task, capturing richer and more varied cues indicative of bona-fide or attack samples. Our results show that prompt aggregation substantially improves zero-shot detection performance, demonstrating the effectiveness of exploiting foundation models' built-in multimodal knowledge through efficient prompt engineering.",
      "year": 2025,
      "venue": "Proceedings of the 1st International Workshop &amp; Challenge on Subtle Visual Computing",
      "authors": [
        "Eduarda Caldeira",
        "Fadi Boutros",
        "Naser Damer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1a4708d7651a918ce53c0d67019def41a9bc5b14",
      "pdf_url": "",
      "publication_date": "2025-08-12",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa6c94b64f6b3e8e1aa6cf6796e76b692e737282",
      "title": "BADTV: Unveiling Backdoor Threats in Third-Party Task Vectors",
      "abstract": "Task arithmetic in large-scale pre-trained models enables agile adaptation to diverse downstream tasks without extensive retraining. By leveraging task vectors (TVs), users can perform modular updates through simple arithmetic operations like addition and subtraction. Yet, this flexibility presents new security challenges. In this paper, we investigate how TVs are vulnerable to backdoor attacks, revealing how malicious actors can exploit them to compromise model integrity. By creating composite backdoors that are designed asymmetrically, we introduce BadTV, a backdoor attack specifically crafted to remain effective simultaneously under task learning, forgetting, and analogy operations. Extensive experiments show that BadTV achieves near-perfect attack success rates across diverse scenarios, posing a serious threat to models relying on task arithmetic. We also evaluate current defenses, finding they fail to detect or mitigate BadTV. Our results highlight the urgent need for robust countermeasures to secure TVs in real-world deployments.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chia-Yi Hsu",
        "Yu-Lin Tsai",
        "Yu Zhe",
        "Yan-Lun Chen",
        "Chih-Hsun Lin",
        "Chia-Mu Yu",
        "Yang Zhang",
        "Chun-ying Huang",
        "Jun Sakuma"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fa6c94b64f6b3e8e1aa6cf6796e76b692e737282",
      "pdf_url": "",
      "publication_date": "2025-01-04",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "17dabb7cc4e74eb05bac5ffc595b40f888aa87e4",
      "title": "From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge",
      "abstract": "Large-scale Video Foundation Models (VFMs) has significantly advanced various video-related tasks, either through task-specific models or Multi-modal Large Language Models (MLLMs). However, the open accessibility of VFMs also introduces critical security risks, as adversaries can exploit full knowledge of the VFMs to launch potent attacks. This paper investigates a novel and practical adversarial threat scenario: attacking downstream models or MLLMs fine-tuned from open-source VFMs, without requiring access to the victim task, training data, model query, and architecture. In contrast to conventional transfer-based attacks that rely on task-aligned surrogate models, we demonstrate that adversarial vulnerabilities can be exploited directly from the VFMs. To this end, we propose the Transferable Video Attack (TVA), a temporal-aware adversarial attack method that leverages the temporal representation dynamics of VFMs to craft effective perturbations. TVA integrates a bidirectional contrastive learning mechanism to maximize the discrepancy between the clean and adversarial features, and introduces a temporal consistency loss that exploits motion cues to enhance the sequential impact of perturbations. TVA avoids the need to train expensive surrogate models or access to domain-specific data, thereby offering a more practical and efficient attack strategy. Extensive experiments across 24 video-related tasks demonstrate the efficacy of TVA against downstream models and MLLMs, revealing a previously underexplored security vulnerability in the deployment of video models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hui Lu",
        "Yi Yu",
        "Song Xia",
        "Yiming Yang",
        "Deepu Rajan",
        "Boon Poh Ng",
        "A. Kot",
        "Xudong Jiang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/17dabb7cc4e74eb05bac5ffc595b40f888aa87e4",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786",
      "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
      "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kangjie Chen",
        "Yuxian Meng",
        "Xiaofei Sun",
        "Shangwei Guo",
        "Tianwei Zhang",
        "Jiwei Li",
        "Chun Fan"
      ],
      "citation_count": 127,
      "url": "https://www.semanticscholar.org/paper/6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786",
      "pdf_url": "",
      "publication_date": "2021-10-06",
      "keywords_matched": [
        "downstream task attack",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "362fd372587399603ca1a3bdfe9bb239eca90923",
      "title": "Task-Agnostic Attacks Against Vision Foundation Models",
      "abstract": "The study of security in machine learning mainly focuses on downstream task-specific attacks, where the adversarial example is obtained by optimizing a loss function specific to the downstream task. At the same time, it has become standard practice for machine learning practitioners to adopt publicly available pre-trained vision foundation models, effectively sharing a common backbone architecture across a multitude of applications such as classification, segmentation, depth estimation, retrieval, questionanswering and more. The study of attacks on such foundation models and their impact to multiple downstream tasks remains vastly unexplored. This work proposes a general framework that forges task-agnostic adversarial examples by maximally disrupting the feature representation obtained with foundation models. We extensively evaluate the security of the feature representations obtained by popular vision foundation models by measuring the impact of this attack on multiple downstream tasks and its transferability between models.",
      "year": 2025,
      "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Brian Pulfer",
        "Yury Belousov",
        "Vitaliy Kinakh",
        "Teddy Furon",
        "S. Voloshynovskiy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/362fd372587399603ca1a3bdfe9bb239eca90923",
      "pdf_url": "",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "347ae892b65d779e3267354e615c43d95e14737b",
      "title": "Pre-training transformer-based framework on large-scale pediatric claims data for downstream population-specific tasks",
      "abstract": "The adoption of electronic health records (EHR) has become universal during the past decade, which has afforded in-depth data-based research. By learning from the large amount of healthcare data, various data-driven models have been built to predict future events for different medical tasks, such as auto diagnosis and heart-attack prediction. Although EHR is abundant, the population that satisfies specific criteria for learning population-specific tasks is scarce, making it challenging to train data-hungry deep learning models. This study presents the Claim Pre-Training (Claim-PT) framework, a generic pre-training model that first trains on the entire pediatric claims dataset, followed by a discriminative fine-tuning on each population-specific task. The semantic meaning of medical events can be captured in the pre-training stage, and the effective knowledge transfer is completed through the task-aware fine-tuning stage. The fine-tuning process requires minimal parameter modification without changing the model architecture, which mitigates the data scarcity issue and helps train the deep learning model adequately on small patient cohorts. We conducted experiments on a real-world claims dataset with more than one million patient records. Experimental results on two downstream tasks demonstrated the effectiveness of our method: our general task-agnostic pre-training framework outperformed tailored task-specific models, achieving more than 10\\% higher in model performance as compared to baselines. In addition, our framework showed a great generalizability potential to transfer learned knowledge from one institution to another, paving the way for future healthcare model pre-training across institutions.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xianlong Zeng",
        "Simon M. Lin",
        "Chang Liu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/347ae892b65d779e3267354e615c43d95e14737b",
      "pdf_url": "",
      "publication_date": "2021-06-24",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "daaff30d9c716cd5ce747774f2057eb248185615",
      "title": "BadMerging: Backdoor Attacks Against Model Merging",
      "abstract": "Fine-tuning pre-trained models for downstream tasks has led to a proliferation of open-sourced task-specific models. Recently, Model Merging (MM) has emerged as an effective approach to facilitate knowledge transfer among these independently fine-tuned models. MM directly combines multiple fine-tuned task-specific models into a merged model without additional training, and the resulting model shows enhanced capabilities in multiple tasks. Although MM provides great utility, it may come with security risks because an adversary can exploit MM to affect multiple downstream tasks. However, the security risks of MM have barely been studied. In this paper, we first find that MM, as a new learning paradigm, introduces unique challenges for existing backdoor attacks due to the merging process. To address these challenges, we introduce BadMerging, the first backdoor attack specifically designed for MM. Notably, BadMerging allows an adversary to compromise the entire merged model by contributing as few as one backdoored task-specific model. BadMerging comprises a two-stage attack mechanism and a novel feature-interpolation-based loss to enhance the robustness of embedded backdoors against the changes of different merging parameters. Considering that a merged model may incorporate tasks from different domains, BadMerging can jointly compromise the tasks provided by the adversary (on-task attack) and other contributors (off-task attack) and solve the corresponding unique challenges with novel attack designs. Extensive experiments show that BadMerging achieves remarkable attacks against various MM algorithms. Our ablation study demonstrates that the proposed attack designs can progressively contribute to the attack performance. Finally, we show that prior defense mechanisms fail to defend against our attacks, highlighting the need for more advanced defense. Our code is available at: https://github.com/jzhang538/BadMerging.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jinghuai Zhang",
        "Jianfeng Chi",
        "Zhengliang Li",
        "Kunlin Cai",
        "Yang Zhang",
        "Yuan Tian"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/daaff30d9c716cd5ce747774f2057eb248185615",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690284",
      "publication_date": "2024-08-14",
      "keywords_matched": [
        "downstream task attack",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8dd9605fbc9702f08a295cba5ae263f625781856",
      "title": "VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models",
      "abstract": "Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multimodal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level. We conduct extensive experiments to attack three widely-used VL pretrained models for six tasks on eight datasets. Experimental results show that the proposed VLAttack framework achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a significant blind spot in the deployment of pre-trained VL models. Codes will be released soon.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Ziyi Yin",
        "Muchao Ye",
        "Tianrong Zhang",
        "Tianyu Du",
        "Jinguo Zhu",
        "Han Liu",
        "Jinghui Chen",
        "Ting Wang",
        "Fenglong Ma"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/8dd9605fbc9702f08a295cba5ae263f625781856",
      "pdf_url": "https://arxiv.org/pdf/2310.04655",
      "publication_date": "2023-10-07",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "06153c465e11d4a3872347444d9162dd4ce0005d",
      "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP",
      "abstract": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \\textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \\textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Hanxun Huang",
        "Sarah Erfani",
        "Yige Li",
        "Xingjun Ma",
        "James Bailey"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/06153c465e11d4a3872347444d9162dd4ce0005d",
      "pdf_url": "",
      "publication_date": "2025-05-08",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c986407970c56ea7bd7ce650d95573d8bf688023",
      "title": "Robust and Transferable Backdoor Attacks Against Deep Image Compression With Selective Frequency Prior",
      "abstract": "Recent advancements in deep learning-based compression techniques have demonstrated remarkable performance surpassing traditional methods. Nevertheless, deep neural networks have been observed to be vulnerable to backdoor attacks, where an added pre-defined trigger pattern can induce the malicious behavior of the models. In this paper, we propose a novel approach to launch a backdoor attack with multiple triggers against learned image compression models. Drawing inspiration from the widely used discrete cosine transform (DCT) in existing compression codecs and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives that are adapted for a series of diverse scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as face recognition and semantic segmentation in downstream applications. To facilitate more efficient training, we develop a dynamic loss function that dynamically balances the impact of different loss terms with fewer hyper-parameters, which also results in more effective optimization of the attack objectives with improved performance. Furthermore, we consider several advanced scenarios. We evaluate the resistance of the proposed backdoor attack to the defensive pre-processing methods and then propose a two-stage training schedule along with the design of robust frequency selection to further improve resistance. To strengthen both the cross-model and cross-domain transferability on attacking downstream CV tasks, we propose to shift the classification boundary in the attack loss during training. Extensive experiments also demonstrate that by employing our trained trigger injection models and making slight modifications to the encoder parameters of the compression model, our proposed attack can successfully inject multiple backdoors accompanied by their corresponding triggers into a single image compression model.",
      "year": 2024,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Yi Yu",
        "Yufei Wang",
        "Wenhan Yang",
        "Lanqing Guo",
        "Shijian Lu",
        "Lingyu Duan",
        "Yap-Peng Tan",
        "A.C. Kot"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c986407970c56ea7bd7ce650d95573d8bf688023",
      "pdf_url": "",
      "publication_date": "2024-11-28",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eebecd8e91deed0bc12155264cc01a4934844228",
      "title": "Exploiting the Adversarial Example Vulnerability of Transfer Learning of Source Code",
      "abstract": "State-of-the-art source code classification models exhibit excellent task transferability, in which the source code encoders are first pre-trained on a source domain dataset in a self-supervised manner and then fine-tuned on a supervised downstream dataset. Recent studies reveal that source code models are vulnerable to adversarial examples, which are crafted by applying semantic-preserving transformations that can mislead the prediction of the victim model. While existing research has introduced practical black-box adversarial attacks, these are often designed for transfer-based or query-based scenarios, necessitating access to the victim domain dataset or the query feedback of the victim system. These attack resources are very challenging or expensive to obtain in real-world situations. This paper proposes the cross-domain attack threat model against the transfer learning of source code where the adversary has only access to an open-sourced pre-trained code encoder. To achieve such realistic attacks, this paper designs the Code Transfer learning Adversarial Example (CodeTAE) method. CodeTAE applies various semantic-preserving transformations and utilizes a genetic algorithm to generate powerful identifiers, thereby enhancing the transferability of the generated adversarial examples. Experimental results on three code classification tasks show that the CodeTAE attack can achieve 30% $\\sim ~80$ % attack success rates under the cross-domain cross-architecture setting. Besides, the generated CodeTAE adversarial examples can be used in adversarial fine-tuning to enhance both the clean accuracy and the robustness of the code model. Our code is available at https://github.com/yyl-github-1896/CodeTAE/.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yulong Yang",
        "Haoran Fan",
        "Chenhao Lin",
        "Qian Li",
        "Zhengyu Zhao",
        "Chao Shen"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/eebecd8e91deed0bc12155264cc01a4934844228",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2e7f942981324e4d9be9467f6f464f32bde236cd",
      "title": "Pre-trained Trojan Attacks for Visual Recognition",
      "abstract": "Pre-trained vision models (PVMs) have become a dominant component due to their exceptional performance when fine-tuned for downstream tasks. However, the presence of backdoors within PVMs poses significant threats. Unfortunately, existing studies primarily focus on backdooring PVMs for the classification task, neglecting potential inherited backdoors in downstream tasks such as detection and segmentation. In this paper, we propose the Pre-trained Trojan attack, which embeds backdoors into a PVM, enabling attacks across various downstream vision tasks. We highlight the challenges posed by cross-task activation and shortcut connections in successful backdoor attacks. To achieve effective trigger activation in diverse tasks, we stylize the backdoor trigger patterns with class-specific textures, enhancing the recognition of task-irrelevant low-level features associated with the target class in the trigger pattern. Moreover, we address the issue of shortcut connections by introducing a context-free learning pipeline for poison training. In this approach, triggers without contextual backgrounds are directly utilized as training data, diverging from the conventional use of clean images. Consequently, we establish a direct shortcut from the trigger to the target class, mitigating the shortcut connection issue. We conducted extensive experiments to thoroughly validate the effectiveness of our attacks on downstream detection and segmentation tasks. Additionally, we showcase the potential of our approach in more practical scenarios, including large vision models and 3D object detection in autonomous driving. This paper aims to raise awareness of the potential threats associated with applying PVMs in practical scenarios. Our codes are available at https://github.com/Veee9/Pre-trained-Trojan.",
      "year": 2023,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Aishan Liu",
        "Xianglong Liu",
        "Xinwei Zhang",
        "Yisong Xiao",
        "Yuguang Zhou",
        "Siyuan Liang",
        "Jiakai Wang",
        "Xiaochun Cao",
        "Dacheng Tao"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/2e7f942981324e4d9be9467f6f464f32bde236cd",
      "pdf_url": "https://arxiv.org/pdf/2312.15172",
      "publication_date": "2023-12-23",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "79d945fb74767a090dc971c5c0d2df78c1af8b14",
      "title": "MakeupAttack: Feature Space Black-box Backdoor Attack on Face Recognition via Makeup Transfer",
      "abstract": "Backdoor attacks pose a significant threat to the training process of deep neural networks (DNNs). As a widely-used DNN-based application in real-world scenarios, face recognition systems once implanted into the backdoor, may cause serious consequences. Backdoor research on face recognition is still in its early stages, and the existing backdoor triggers are relatively simple and visible. Furthermore, due to the perceptibility, diversity, and similarity of facial datasets, many state-of-the-art backdoor attacks lose effectiveness on face recognition tasks. In this work, we propose a novel feature space backdoor attack against face recognition via makeup transfer, dubbed MakeupAttack. In contrast to many feature space attacks that demand full access to target models, our method only requires model queries, adhering to black-box attack principles. In our attack, we design an iterative training paradigm to learn the subtle features of the proposed makeup-style trigger. Additionally, MakeupAttack promotes trigger diversity using the adaptive selection method, dispersing the feature distribution of malicious samples to bypass existing defense methods. Extensive experiments were conducted on two widely-used facial datasets targeting multiple models. The results demonstrate that our proposed attack method can bypass existing state-of-the-art defenses while maintaining effectiveness, robustness, naturalness, and stealthiness, without compromising model performance.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Mingze Sun",
        "Lihua Jing",
        "Zixuan Zhu",
        "Rui Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/79d945fb74767a090dc971c5c0d2df78c1af8b14",
      "pdf_url": "",
      "publication_date": "2024-08-22",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d7720b2ba68a637d45d0d67a7bb96408bcda8e3",
      "title": "Backdoor Attacks on Deep Neural Networks via Transfer Learning from Natural Images",
      "abstract": "Backdoor attacks are a serious security threat to open-source and outsourced development of computational systems based on deep neural networks (DNNs). In particular, the transferability of backdoors is remarkable; that is, they can remain effective after transfer learning is performed. Given that transfer learning from natural images is widely used in real-world applications, the question of whether backdoors can be transferred from neural models pretrained on natural images involves considerable security implications. However, this topic has not been evaluated rigorously in prior studies. Hence, in this study, we configured backdoors in 10 representative DNN models pretrained on a natural image dataset, and then fine-tuned the backdoored models via transfer learning for four real-world applications, including pneumonia classification from chest X-ray images, emergency response monitoring from aerial images, facial recognition, and age classification from images of faces. Our experimental results show that the backdoors generally remained effective after transfer learning from natural images, except for small DNN models. Moreover, the backdoors were difficult to detect using a common method. Our findings indicate that backdoor attacks can exhibit remarkable transferability in more realistic transfer learning processes, and highlight the need for the development of more advanced security countermeasures in developing systems using DNN models for sensitive or mission-critical applications.",
      "year": 2022,
      "venue": "Applied Sciences",
      "authors": [
        "Yuki Matsuo",
        "Kazuhiro Takemoto"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8d7720b2ba68a637d45d0d67a7bb96408bcda8e3",
      "pdf_url": "https://www.mdpi.com/2076-3417/12/24/12564/pdf?version=1670480744",
      "publication_date": "2022-12-08",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e9ff3d0dc20a16f57851b2980676d60a5fa6dba",
      "title": "A Novel Backdoor Attack Adapted to Transfer Learning",
      "abstract": "Recently, backdoor attacks pose a major security threat to deep neural networks. The attacker embeds hidden malicious behaviors into deep learning models that is activated only when the input contains specific triggers. Although backdoor attacks have a high success rate and are very stealthy, existing backdoor models often lose their attack capabilities after transfer learning. A critical question at present is how to ensure that the malicious behavior of the backdoor model is not disturbed by transfer learning. In this paper, we conducted a detailed empirical study of multiple existing backdoor defenses, and found that the existing defenses are generally based on the different recognition mechanisms of backdoor triggers and clean samples in the model. In order to resist existing defenses, we propose a method for generating backdoor triggers inversely based on the gradient information of model. In addition, for preventing the model from being disturbed by transfer learning, we use the modified Triplets-Loss to inject the backdoor only in the convolutional layer, and erase the backdoor information of the fully connected layer under the premise of ensuring the effect of the backdoor attack. Finally, we evaluate the attack and 4 potential defenses in several benchmark datasets, including MNIST, CIFAR10, GTSRB. The proposed attack method achieves 100% success rates while circumventing the interference of existing backdoor defenses.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Peihao Li",
        "Jie Huang",
        "Shuaishuai Zhang",
        "Chunyang Qi",
        "Chuangrun Liang",
        "Yang Peng"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9e9ff3d0dc20a16f57851b2980676d60a5fa6dba",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c41c30882161181bd03c55e23997479d8ad5f1b1",
      "title": "Secure Transfer Learning: Training Clean Model Against Backdoor in Pre-Trained Encoder and Downstream Dataset",
      "abstract": "Transfer learning from pre-trained encoders has become essential in modern machine learning, enabling efficient model adaptation across diverse tasks. However, this combination of pre-training and downstream adaptation creates an expanded attack surface, exposing models to sophisticated backdoor embedding at both the encoder and dataset levels\u2014an area often overlooked in prior research. Additionally, the limited computational resources typically available to users of pre-trained encoders constrain the effectiveness of generic backdoor defenses compared to end-to-end training from scratch. In this work, we investigate how to mitigate potential backdoor risks in resource-constrained transfer learning scenarios. Specifically, we first conduct an exhaustive analysis of existing defense strategies, revealing that many follow a reactive workflow based on assumptions that do not scale to unknown threats, novel attack types, or different training paradigms. In response, we introduce a proactive mindset focused on identifying clean elements and propose the Trusted Core (T-Core) Bootstrapping framework, which emphasizes the importance of pinpointing trustworthy data and neurons to enhance model security. Our empirical evaluations demonstrate the effectiveness and superiority of T-Core, specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses across 5 benchmark datasets, addressing 4 scenarios of 3 potential backdoor threats.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yechao Zhang",
        "Yuxuan Zhou",
        "Tianyu Li",
        "Minghui Li",
        "Shengshan Hu",
        "Wei Luo",
        "Leo Yu Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c41c30882161181bd03c55e23997479d8ad5f1b1",
      "pdf_url": "",
      "publication_date": "2025-04-16",
      "keywords_matched": [
        "transfer backdoor",
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "781c4027e9efcee90899dcc53d8740b2cc28631f",
      "title": "Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models",
      "abstract": "Transfer learning provides an effective solution for feasibly and fast customize accurate <italic>Student</italic> models, by transferring the learned knowledge of pre-trained <italic>Teacher</italic> models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this article, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly adopted defenses: <italic>pruning-based</italic>, <italic>retraining-based</italic> and <italic>input pre-processing-based defenses</italic>. Specifically, (<inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {A}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">A</mml:mi></mml:math><inline-graphic xlink:href=\"wang-ieq1-3000900.gif\"/></alternatives></inline-formula>) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating <italic>pruning-based</italic> and/or <italic>retraining-based defenses</italic>. (<inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {B}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">B</mml:mi></mml:math><inline-graphic xlink:href=\"wang-ieq2-3000900.gif\"/></alternatives></inline-formula>) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the <italic>input pre-processing-based defense</italic>, while guaranteeing that selected neuron(s) can be significantly activated. (<inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {C}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"script\">C</mml:mi></mml:math><inline-graphic xlink:href=\"wang-ieq3-3000900.gif\"/></alternatives></inline-formula>) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the 98.4 and 97.2 percent classification accuracy as the genuine model on clean image and time series inputs while improving <inline-formula><tex-math notation=\"LaTeX\">$27.9\\%-100\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>27</mml:mn><mml:mo>.</mml:mo><mml:mn>9</mml:mn><mml:mo>%</mml:mo><mml:mo>-</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"wang-ieq4-3000900.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$27.1\\%-56.1\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>27</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo><mml:mo>-</mml:mo><mml:mn>56</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"wang-ieq5-3000900.gif\"/></alternatives></inline-formula> attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.",
      "year": 2020,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Shuo Wang",
        "S. Nepal",
        "C. Rudolph",
        "M. Grobler",
        "Shangyu Chen",
        "Tianle Chen"
      ],
      "citation_count": 117,
      "url": "https://www.semanticscholar.org/paper/781c4027e9efcee90899dcc53d8740b2cc28631f",
      "pdf_url": "https://arxiv.org/pdf/2001.03274",
      "publication_date": "2020-01-10",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "032a011837d303e2e9ed35f4cbc0afa409cf17dd",
      "title": "A Hidden Backdoor Attack via Formal Text Style Transfer in Language Models",
      "abstract": "Natural language processing (NLP) systems have been demonstrated to be vulnerable to backdoor attacks. Specifically, attackers embed the backdoor into the model by poisoning training data, producing the desired results when the input contains pre-defined triggers. Typical textual backdoor attacks adopt static triggers such as words or phrases, which make them detectable by existing defense methods. To enhance stealthiness, this paper introduces a hidden backdoor attack method utilizing formal text style transfer (FTST). Specifically, we adopt a formal text style transfer model to convert part of the benign training samples into formal samples, which serve as the backdoor samples. Compared to static textual triggers, FTST-based triggers can maintain original semantics while evading common defenses and human detections. We conduct extensive experiments on typical NLP tasks, including topic and sentiment classification tasks utilizing three prominent pre-trained language models and four datasets. The results show that our approach achieves the desired attack performance while preserving the normal-functionality of the model. Furthermore, compared to common word-level triggers and sentence-level triggers, our approach has been demonstrated to be more stealthy under GPT-2-based perplexity detection and more robust under backdoor defense methods.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zihan Wang",
        "Hongwei Li",
        "Wenbo Jiang",
        "Rui Zhang",
        "Jiaming He",
        "Hanxiao Chen",
        "Guowen Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/032a011837d303e2e9ed35f4cbc0afa409cf17dd",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "19c2259d208a30fddd252810d6dcbe4f13420f1a",
      "title": "Cross-Context Backdoor Attacks against Graph Prompt Learning",
      "abstract": "Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL's vulnerability to backdoor attacks. We introduce CrossBA, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications.Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that CrossBA consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against CrossBA and conclude that current defenses are insufficient to mitigate CrossBA. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.",
      "year": 2024,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Xiaoting Lyu",
        "Yufei Han",
        "Wei Wang",
        "Hangwei Qian",
        "Ivor W. Tsang",
        "Xiangliang Zhang"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/19c2259d208a30fddd252810d6dcbe4f13420f1a",
      "pdf_url": "https://arxiv.org/pdf/2405.17984",
      "publication_date": "2024-05-28",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4b38fca91ffd545a9849f81ac26815ad02618f36",
      "title": "Incremental Learning, Incremental Backdoor Threats",
      "abstract": "Class incremental learning from a pre-trained DNN model is gaining lots of popularity. Unfortunately, the pre-trained model also introduces a new attack vector, which enables an adversary to inject a backdoor into it and further compromise the downstream models learned from it. Prior works proposed backdoor attacks against the pre-trained models in the transfer learning scenario. However, they become less effective when the adversary does not have the knowledge of the downstream tasks or new data, which is more practical and considered in this paper. To this end, we design the first latent backdoor attacks against incremental learning. We propose two novel techniques, which can effectively and stealthily embed a backdoor into the pre-trained model. Such backdoor can only be activated when the pre-trained model is extended to a downstream model with incremental learning. It has a very high attack success rate, and is able to bypass existing backdoor detection approaches. Extensive experiments confirm the effectiveness of our attacks over different datasets and incremental learning methods, as well as strong robustness against state-of-the-art backdoor defense mechanisms including Neural Cleanse, Fine-Pruning and STRIP.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Tianwei Zhang",
        "Han Qiu",
        "Hongwei Li",
        "Guowen Xu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/4b38fca91ffd545a9849f81ac26815ad02618f36",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "28d394d1707c21b039db75af9542f41a5782441f",
      "title": "ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms",
      "abstract": "Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. We also provide procedures to adaptively select the number of suspicious points to remove. In the end-to-end SL setting, ASSET is superior to existing methods in terms of consistency of defensive performance across different attacks and robustness to changes in poison ratios; in particular, it is the only method that can detect the state-of-the-art clean-label attack. Moreover, ASSET's average detection rates are higher than the best existing methods in SSL and TL, respectively, by 69.3% and 33.2%, thus providing the first practical backdoor defense for these new DL settings. We open-source the project to drive further development and encourage engagement: https://github.com/ruoxi-jia-group/ASSET.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minzhou Pan",
        "Yi Zeng",
        "L. Lyu",
        "X. Lin",
        "R. Jia"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/28d394d1707c21b039db75af9542f41a5782441f",
      "pdf_url": "https://arxiv.org/pdf/2302.11408",
      "publication_date": "2023-02-22",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d477457f8ea5b7169ff370751653a6244c1c96c4",
      "title": "Latent Backdoor Attacks on Deep Neural Networks",
      "abstract": "Recent work proposed the concept of backdoor attacks on deep neural networks (DNNs), where misclassification rules are hidden inside normal models, only to be triggered by very specific inputs. However, these \"traditional\" backdoors assume a context where users train their own models from scratch, which rarely occurs in practice. Instead, users typically customize \"Teacher\" models already pretrained by providers like Google, through a process called transfer learning. This customization process introduces significant changes to models and disrupts hidden backdoors, greatly reducing the actual impact of backdoors in practice. In this paper, we describe latent backdoors, a more powerful and stealthy variant of backdoor attacks that functions under transfer learning. Latent backdoors are incomplete backdoors embedded into a \"Teacher\" model, and automatically inherited by multiple \"Student\" models through transfer learning. If any Student models include the label targeted by the backdoor, then its customization process completes the backdoor and makes it active. We show that latent backdoors can be quite effective in a variety of application contexts, and validate its practicality through real-world attacks against traffic sign recognition, iris identification of volunteers, and facial recognition of public figures (politicians). Finally, we evaluate 4 potential defenses, and find that only one is effective in disrupting latent backdoors, but might incur a cost in classification accuracy as tradeoff.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yuanshun Yao",
        "Huiying Li",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 449,
      "url": "https://www.semanticscholar.org/paper/d477457f8ea5b7169ff370751653a6244c1c96c4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3319535.3354209",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c279ce37bf56bd4d0b8b144fa3c6e6382dafcbc9",
      "title": "A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification",
      "abstract": "Deep learning achieves outstanding results in many machine learning tasks. Nevertheless, it is vulnerable to backdoor attacks that modify the training set to embed a secret functionality in the trained model. The modified training samples have a secret property, i. e., a trigger. At inference time, the secret functionality is activated when the input contains the trigger, while the model functions correctly in other cases. While there are many known backdoor attacks (and defenses), deploying a stealthy attack is still far from trivial. Successfully creating backdoor triggers depends on numerous parameters. Unfortunately, research has not yet determined which parameters contribute most to the attack performance. This paper systematically analyzes the most relevant parameters for the backdoor attacks, i.e., trigger size, position, color, and poisoning rate. Using transfer learning, which is very common in computer vision, we evaluate the attack on state-of-the-art models (ResNet, VGG, AlexNet, and GoogLeNet) and datasets (MNIST, CIFAR10, and TinyImageNet). Our attacks cover the majority of backdoor settings in research, providing concrete directions for future works. Our code is publicly available to facilitate the reproducibility of our results.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Gorka Abad",
        "Jing Xu",
        "Stefanos Koffas",
        "Behrad Tajalli",
        "S. Picek"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c279ce37bf56bd4d0b8b144fa3c6e6382dafcbc9",
      "pdf_url": "http://arxiv.org/pdf/2302.01740",
      "publication_date": "2023-02-03",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a33bfae6bfacb53075bf3e54f4383740e61bed84",
      "title": "Invisible Encoded Backdoor attack on DNNs using Conditional GAN",
      "abstract": "Deep Learning (DL) models deliver superior performance and have achieved remarkable results for classification and vision tasks. However, recent research focuses on exploring these Deep Neural Networks (DNNs) weaknesses as these can be vulnerable due to transfer learning and outsourced training data. This paper investigates the feasibility of generating a stealthy invisible backdoor attack during the training phase of deep learning models. For developing the poison dataset, an interpolation technique is used to corrupt the sub-feature space of the conditional generative adversarial network. Then, the generated poison dataset is mixed with the clean dataset to corrupt the training images dataset. The experiment results show that by injecting a 3% poison dataset combined with the clean dataset, the DL models can effectively fool with a high degree of model accuracy.",
      "year": 2023,
      "venue": "IEEE International Conference on Consumer Electronics",
      "authors": [
        "Iram Arshad",
        "Yuansong Qiao",
        "Brian Lee",
        "Yuhang Ye"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a33bfae6bfacb53075bf3e54f4383740e61bed84",
      "pdf_url": "",
      "publication_date": "2023-01-06",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f895931477a93ba9c908116f66e78206f95fe26e",
      "title": "Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks",
      "abstract": "This paper highlights vulnerabilities of deep learning-driven semantic communications to backdoor (Trojan) attacks. Semantic communications aims to convey a desired meaning while transferring information from a transmitter to its receiver. The encoder-decoder pair of an autoencoder that is represented by deep neural networks (DNNs) is trained to reconstruct signals such as images at the receiver by transmitting latent features of small size over a limited number of channel uses. In the meantime, the DNN of a semantic task classifier at the receiver is jointly trained with the autoencoder to check the meaning conveyed to the receiver. The complex decision space of the DNNs makes semantic communications susceptible to adversarial manipulations. In a backdoor (Trojan) attack, the adversary adds triggers to a small portion of training samples and changes the label to a target label. When the transfer of images is considered, the triggers can be added to the images or equivalently to the corresponding transmitted or received signals. In test time, the adversary activates these triggers by providing poisoned samples as input to the encoder (or decoder) of semantic communications. The backdoor attack can effectively change the semantic information transferred for the poisoned input samples to a target meaning. As the performance of semantic communications improves with the signal-to-noise ratio and the number of channel uses, the success of the backdoor attack increases as well. Also, increasing the Trojan ratio in training data makes the attack more successful. On the other hand, the attack is selective and its effect on the unpoisoned input samples remains small. Overall, this paper shows that the backdoor attack poses a serious threat to semantic communications and presents novel design guidelines to preserve the meaning of transferred information in the presence of backdoor attacks.",
      "year": 2022,
      "venue": "Annual Conference on Information Sciences and Systems",
      "authors": [
        "Y. Sagduyu",
        "T. Erpek",
        "S. Ulukus",
        "A. Yener"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/f895931477a93ba9c908116f66e78206f95fe26e",
      "pdf_url": "https://arxiv.org/pdf/2212.11205",
      "publication_date": "2022-12-21",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fae14f7cfb073a19e51f614a2abc2dcfae61c898",
      "title": "Backdoor Attacks by Leveraging Latent Representation in Competitive Learning for Resistance to Removal",
      "abstract": "SUMMARY Backdoor attacks on machine learning are a kind of attack whereby an adversary obtains the expected output for a particular input called a trigger, and the existing work, called latent backdoor attack (Yao et al., CCS 2019), can resist backdoor removal as countermeasures to the attacks, i.e., pruning and transfer learning. In this paper, we present a novel backdoor attack, TALPA, which outperforms the latent backdoor attack with respect to the attack success rate of backdoors as well as keeping the same-level accuracy. The key idea of TALPA is to directly overrides parameters of latent representations in competitive learning between a generative model for triggers and a victim model, and hence can more optimize modelparametersandtriggergenerationthanthelatentbackdoorattack. We experimentally demonstrate that TALPA outperforms the latent backdoor attack with respect to the attack success rate and also show that TALPA can resistbothpruningandtransferlearningthroughextensiveexperiments. We also show various discussions, such as the impact of hyperparameters and extensions to other layers from the latent representation, to shed light on the properties of TALPA. Our code is publicly available (https://github.com/ fseclab-osaka/talpa).",
      "year": 2025,
      "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences",
      "authors": [
        "Kazuki Iwahana",
        "Naoto Yanai",
        "A. Inomata",
        "Toru Fujiwara"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fae14f7cfb073a19e51f614a2abc2dcfae61c898",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "backdoor removal",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4b793bb5a6c30d4f05c3d6fecf22be9191200b3b",
      "title": "Knowledge-Aided Generative Adversarial Network: A Transfer Gradient-Less Adversarial Attack for Deep Learning-Based Soft Sensors",
      "abstract": null,
      "year": 2024,
      "venue": "Asian Control Conference",
      "authors": [
        "Runyuan Guo",
        "Qingyuan Chen",
        "Shuo Tong",
        "Han Liu"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/4b793bb5a6c30d4f05c3d6fecf22be9191200b3b",
      "pdf_url": "",
      "publication_date": "2024-07-05",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7b9bb9fa896cff65a076ef09fe3d2a8ea43bf9a1",
      "title": "PTTS: Zero-Knowledge Proof-based Private Token Transfer System on Ethereum Blockchain and its Network Flow Based Balance Range Privacy Attack Analysis",
      "abstract": "Blockchains are decentralized and immutable databases that are shared among the nodes of the network. Although blockchains have attracted a great scale of attention in the recent years by disrupting the traditional financial systems, the transaction privacy is still a challenging issue that needs to be addressed and analysed. We propose a Private Token Transfer System (PTTS) for the Ethereum public blockchain in the first part of this paper. For the proposed framework, zero-knowledge based protocol has been designed using Zokrates and integrated into our private token smart contract. With the help of web user interface designed, the end users can interact with the smart contract without any third-party setup. In the second part of the paper, we provide security and privacy analysis including the replay attack and the balance range privacy attack which has been modelled as a network flow problem. It is shown that in case some balance ranges are deliberately leaked out to particular organizations or adversial entities, it is possible to extract meaningful information about the user balances by employing minimum cost flow network algorithms that have polynomial complexity. The experimental study reports the Ethereum gas consumption and proof generation times for the proposed framework. It also reports network solution times and goodness rates for a subset of addresses under the balance range privacy attack with respect to number of addresses, number of transactions and ratio of leaked transfer transaction amounts.",
      "year": 2023,
      "venue": "Journal of Network and Computer Applications",
      "authors": [
        "Goshgar Ismayilov",
        "Can C. \u00d6zturan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7b9bb9fa896cff65a076ef09fe3d2a8ea43bf9a1",
      "pdf_url": "https://arxiv.org/pdf/2308.15139",
      "publication_date": "2023-08-29",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f118a713952c49f96382d9b8fdc153d9f54bcd3c",
      "title": "Multimodal Hate Speech Detection via Cross-Domain Knowledge Transfer",
      "abstract": "Nowadays, the hate speech diffusion of texts and images in social network has become the mainstream compared with the diffusion of texts-only, raising the pressing needs of multimodal hate speech detection task. Current research on this task mainly focuses on the construction of multimodal models without considering the influence of the unbalanced and widely distributed samples for various attacks in hate speech. In this situation, introducing enhanced knowledge is necessary for understanding the attack category of hate speech comprehensively. Due to the high correlation between hate speech detection and sarcasm detection tasks, this paper makes an initial attempt of common knowledge transfer based on the above two tasks, where hate speech detection and sarcasm detection are defined as primary and auxiliary tasks, respectively. A scalable cross-domain knowledge transfer (CDKT) framework is proposed, where the mainstream vision-language transformer could be employed as backbone flexibly. Three modules are included, bridging the semantic, definition and domain gaps simultaneously between primary and auxiliary tasks. Specifically, semantic adaptation module formulates the irrelevant parts between image and text in primary and auxiliary tasks, and disentangles with the text representation to align the visual and word tokens. Definition adaptation module assigns different weights to the training samples of auxiliary task by measuring the correlation between samples of the auxiliary and primary task. Domain adaptation module minimizes the feature distribution gap of samples in two tasks. Extensive experiments show that the proposed CDKT provides a stable improvement compared with baselines and produces a competitive performance compared with some existing multimodal hate speech detection methods.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Chuanpeng Yang",
        "Fuqing Zhu",
        "Guihua Liu",
        "Jizhong Han",
        "Songiln Hu"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/f118a713952c49f96382d9b8fdc153d9f54bcd3c",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3503161.3548255",
      "publication_date": "2022-10-10",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a5889ed442ad68c305e06c85112541c2144d25b1",
      "title": "A hybrid style transfer with whale optimization algorithm model for textual adversarial attack",
      "abstract": null,
      "year": 2023,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "Yan Kang",
        "Jianjun Zhao",
        "Xuekun Yang",
        "Baochen Fan",
        "Wentao Xie"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a5889ed442ad68c305e06c85112541c2144d25b1",
      "pdf_url": "",
      "publication_date": "2023-12-11",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a04c95f394c1d673ff622f33719abaa7d00e9cf",
      "title": "Model Mimic Attack: Knowledge Distillation for Provably Transferable Adversarial Examples",
      "abstract": "The vulnerability of artificial neural networks to adversarial perturbations in the black-box setting is widely studied in the literature. The majority of attack methods to construct these perturbations suffer from an impractically large number of queries required to find an adversarial example. In this work, we focus on knowledge distillation as an approach to conduct transfer-based black-box adversarial attacks and propose an iterative training of the surrogate model on an expanding dataset. This work is the first, to our knowledge, to provide provable guarantees on the success of knowledge distillation-based attack on classification neural networks: we prove that if the student model has enough learning capabilities, the attack on the teacher model is guaranteed to be found within the finite number of distillation iterations.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "K. Lukyanov",
        "Andrew I. Perminov",
        "Denis Turdakov",
        "Mikhail Pautov"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0a04c95f394c1d673ff622f33719abaa7d00e9cf",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6aa4275360e94540538ce72a84e8b7292dc4d38c",
      "title": "Deep Learning-Enabled Heterogeneous Transfer Learning for Improved Network Attack Detection in Internal Networks",
      "abstract": "Cybersecurity faces constant challenges from increasingly sophisticated network attacks. Recent research shows machine learning can improve attack detection by training models on large labeled datasets. However, obtaining sufficient labeled data is difficult for internal networks. We propose a deep transfer learning model to learn common knowledge from domains with different features and distributions. The model has two feature projection networks to transform heterogeneous features into a common space, and a classification network then predicts transformed features into labels. To align probability distributions for two domains, maximum mean discrepancy (MMD) is used to compute distribution distance alongside classification loss. Though the target domain only has a few labeled samples, unlabeled samples are adequate for computing MMD to align unconditional distributions. In addition, we apply a soft classification scheme on unlabeled data to compute MMD over classes to further align conditional distributions. Experiments between NSL-KDD, UNSW-NB15, and CICIDS2017 validate that the method substantially improves cross-domain network attack detection accuracy.",
      "year": 2023,
      "venue": "Applied Sciences",
      "authors": [
        "Gang Wang",
        "Dong Liu",
        "Chunrui Zhang",
        "Teng Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/6aa4275360e94540538ce72a84e8b7292dc4d38c",
      "pdf_url": "https://www.mdpi.com/2076-3417/13/21/12033/pdf?version=1699087474",
      "publication_date": "2023-11-04",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9cccbf9dce5fe8084a92ce5247c74e1855193bbb",
      "title": "A Practical Website Fingerprinting Attack via CNN-Based Transfer Learning",
      "abstract": "Website fingerprinting attacks attempt to apply deep learning technology to identify websites corresponding to encrypted traffic data. Unfortunately, to the best of our knowledge, once the total number of encrypted traffic data becomes insufficient, the identification accuracy in most existing works will drop dramatically. This phenomenon grows worse because the statistical features of the encrypted traffic data are not always stable but irregularly varying in different time periods. Even a deep learning model requires good performance to capture the statistical features, its accuracy usually diminishes in a short period of time because the changes of the statistical features technically put the training and testing data into two non-identical distributions. In this paper, we first propose a convolutional neural network-based website fingerprinting attack (CWFA) scheme. This scheme integrates packet direction with the timing sequence from the encrypted traffic data to improve the accuracy of analysis as much as possible on few data samples. We then design a new fine-tuning mechanism for the CWFA (FM-CWFA) scheme based on transfer learning. This mechanism enables the proposed FM-CWFA scheme to support the changes in the statistical patterns. The experimental results in closed-world and open-world settings show that the effectiveness of the CWFA scheme is better than previous researches, with the slowest performance degradation when the number of data decreases, and the FM-CWFA scheme can remain effective when the statistical features change.",
      "year": 2023,
      "venue": "Mathematics",
      "authors": [
        "Tianyao Pan",
        "Zejia Tang",
        "Dawei Xu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9cccbf9dce5fe8084a92ce5247c74e1855193bbb",
      "pdf_url": "https://www.mdpi.com/2227-7390/11/19/4078/pdf?version=1695718331",
      "publication_date": "2023-09-26",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f483fbc84a129871a25b06137689aade401e1679",
      "title": "SALT: transfer learning-based threat model for attack detection in smart home",
      "abstract": "The next whooping revolution after the Internet is its scion, the Internet of Things (IoT), which has facilitated every entity the power to connect to the web. However, this magnifying depth of the digital pool oil the wheels for the attackers to penetrate. Thus, these threats and attacks have become a prime concern among researchers. With promising features, Machine Learning (ML) has been the solution throughout to detect these threats. But, the general ML-based solutions have been declining with the practical implementation to detect unknown threats due to changes in domains, different distributions, long training time, and lack of labelled data. To tackle the aforementioned issues, Transfer Learning (TL) has emerged as a viable solution. Motivated by the facts, this article aims to leverage TL-based strategies to get better the learning classifiers to detect known and unknown threats targeting IoT systems. TL transfers the knowledge attained while learning a task to expedite the learning of new similar tasks/problems. This article proposes a learning-based threat model for attack detection in the Smart Home environment (SALT). It uses the knowledge of known threats in the source domain (labelled data) to detect the unknown threats in the target domain (unlabelled data). The proposed scheme addresses the workable differences in feature space distribution or the ratio of attack instances to a normal one, or both. The proposed threat model would show the implying competence of ML with the TL scheme to improve the robustness of learning classifiers besides the threat variants to detect known and unknown threats. The performance analysis shows that traditional schemes underperform for unknown threat variants with accuracy dropping to 39% and recall to 56.",
      "year": 2022,
      "venue": "Scientific Reports",
      "authors": [
        "Pooja Anand",
        "Yashwant Singh",
        "Harvinder Singh",
        "M. Alshehri",
        "Sudeep Tanwar"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/f483fbc84a129871a25b06137689aade401e1679",
      "pdf_url": "https://www.nature.com/articles/s41598-022-16261-9.pdf",
      "publication_date": "2022-07-18",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4c86ffcc12bf728706152ba8a36181374cccfd1e",
      "title": "Attack Tactic Identification by Transfer Learning of Language Model",
      "abstract": "Cybersecurity has become a primary global concern with the rapid increase in security attacks and data breaches. Arti\ufb01cial intelligence is promising to help humans analyzing and identifying attacks. However, labeling millions of packets for supervised learning is never easy. This study aims to leverage transfer learning technique that stores the knowledge gained from well-de\ufb01ned attack lifecycle documents and applies it to hundred thousands of unlabeled attacks (packets) for identifying their attack tactics. We anticipate the knowledge of an attack is well-described in the documents, and the cutting edge transformer-based language model can embed the knowledge into a high-dimensional latent space. Then, reusing the information from the language model for the learning of attack tactic carried by packets to improve the learning e \ufb03 ciency. We propose a system, PELAT, that \ufb01ne-tunes BERT model with 1,417 articles from MITRE ATT&CK lifecycle framework to enhance its attack knowledge (including syntax used and semantic meanings embedded). PELAT then transfers its knowledge to perform semi-supervised learning for unlabeled packets to generate their tactic labels. Further, when a new attack packet arrives, the packet payload will be processed by the PELAT language model with a downstream classi\ufb01er to predict its tactics. In this way, we can e \ufb00 ectively reduce the burden of manually labeling big datasets. In a one-week honeypot attack dataset (227 thousand packets per day), PELAT performs 99% of precision, recall, and F1 on testing dataset. PELAT can infer over 99% of tactics on two other testing datasets (while nearly 90% of tactics are identi\ufb01ed).",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Lily Lin",
        "Shun-Wen Hsiao"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4c86ffcc12bf728706152ba8a36181374cccfd1e",
      "pdf_url": "http://arxiv.org/pdf/2209.00263",
      "publication_date": "2022-09-01",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c802746104ed482b8d1343cbd732d63b2fbb28c",
      "title": "Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors",
      "abstract": "Despite being widely used in network intrusion detection systems (NIDSs), machine learning (ML) has proven to be vulnerable to adversarial attacks. Transparent-box and opaque-box adversarial ML attacks of NIDS have been explored in several studies. However, transparent-box attacks unrealistically assume that the attackers have full knowledge of the target NIDSs. Meanwhile, existing opaque-box attacks can not achieve high attack success rate due to the weak adversarial transferability between models (e.g., neural networks and tree models). Additionally, neither of them explains why adversarial examples exist and why they can transfer across models. To address these challenges, this article introduces ETA, an Explainable Transfer-based Opaque-Box Adversarial Attack framework. ETA aims to achieve two primary objectives: 1) create transferable adversarial examples applicable to various ML detectors and 2) provide insights into the existence of adversarial examples and their transferability within NIDSs. Specifically, we first provide a general transfer-based adversarial attack method applicable across the entire ML space. Following that, we exploit a unique insight based on cooperative game theory and perturbation interpretations to explain adversarial examples and adversarial transferability. On this basis, we propose an Important-Sensitive Feature Selection (ISFS) method to guide the search for adversarial examples, achieving stronger transferability and ensuring traffic-space constraints. Finally, the experimental results on three NIDSs datasets show that our method performs significantly effectively against several classical and state-of-the-art ML classifiers, outperforming the latest baselines. We conduct three interpretation experiments and two cases to verify our interpretation method\u2019s correctness. Meanwhile, we uncover two major misconceptions about applying machine learning to NIDSs systems.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hangsheng Zhang",
        "Dongqi Han",
        "Shangyuan Zhuang",
        "Zhiliang Wang",
        "Jiyan Sun",
        "Yinlong Liu",
        "Jiqiang Liu",
        "Jinsong Dong"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/7c802746104ed482b8d1343cbd732d63b2fbb28c",
      "pdf_url": "",
      "publication_date": "2024-01-19",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1780e81674c49c8b4f5452530066eeca7e9040db",
      "title": "AAMT: Adversarial Attack-Driven Mutual Teaching for Source-Free Domain-Adaptive Person Reidentification",
      "abstract": "Conventional domain adaptive (DA) methods for person re-identification (ReID) face knowledge transfer challenges when labeled data from the source domain cannot be accessed due to privacy constraints. Although the methods operating under source-absent DA settings attempt to address this challenge by using models pretrained on the source domain in their mutual teaching frameworks, failing to capture domain divergence in scenarios in which the source data are completely inaccessible can simultaneously introduce issues related to mutual convergence. In response, we introduce an adversarial attacks-driven mutual teaching (AAMT) framework as an innovative and applicable source-free DA person ReID scheme. Specifically, we first carefully develop a perturbation generator to generate source-style adversarial examples by leveraging a pretrained source model. Then, these diverse adversarial examples are employed to attack the mutual teaching model, implicitly measuring the domain divergence. Accordingly, we design a contrastive learning loss to enlarge the differences between the training pairs and further mitigate the mutual convergence issue. Extensive experiments demonstrate that AAMT outperforms the existing methods under both conventional and source-absent DA settings, achieving state-of-the-art performance.",
      "year": 2024,
      "venue": "IEEE transactions on multimedia",
      "authors": [
        "Xiaofeng Qu",
        "Huaxiang Zhang",
        "Lei Zhu",
        "Liqiang Nie",
        "Li Liu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1780e81674c49c8b4f5452530066eeca7e9040db",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "072765f93c54ad5a3592b1a56285f373f75b523b",
      "title": "Peak-controlled logits poisoning attack in federated distillation",
      "abstract": "Federated Distillation (FD) is an innovative distributed machine learning paradigm that enables efficient and flexible cross-device knowledge transfer through knowledge distillation, without the need to upload large-scale model parameters to a central server. Although FD has attracted increasing attention in recent years, its security aspects remain relatively underexplored. Existing attack methods targeting traditional federated learning mainly focus on the transmission of model parameters and gradients, while attacks specifically designed for the unnormalized outputs (logits) in the emerging FD paradigm are still lacking. To fill this research gap and contribute to the enhancement of FD\u2019s security, we previously proposed the Federated Distillation Logits Attack (FDLA), which manipulates the logits transmitted during communication to mislead and degrade the performance of client models. However, FDLA has limitations in controlling its impact on participants with different roles or identities and lacks a systematic investigation into the effects of malicious interventions at various stages of knowledge transfer. To overcome these limitations, we propose a more advanced and controllable logits poisoning method\u2014Peak-Controlled Federated Distillation Logits Attack (PCFDLA). PCFDLA enhances the effectiveness of FDLA by precisely controlling the peak values of logits to adjust the intensity of the attack. This method generates highly misleading perturbations that achieve stronger attack performance while maintaining a similar level of stealthiness to FDLA when detection is based on differences in model parameters. Moreover, we introduce a novel evaluation metric to more comprehensively assess the performance of such attacks. Experimental results show that PCFDLA significantly increases the destructive impact on victim models while maintaining high stealth. It consistently achieves superior performance across multiple datasets, highlighting its potential threat to the security of federated distillation systems.",
      "year": 2024,
      "venue": "Discover Computing",
      "authors": [
        "Yuhan Tang",
        "Aoxu Zhang",
        "Zhiyuan Wu",
        "Bo Gao",
        "Tian Wen",
        "Yuwei Wang",
        "Sheng Sun"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/072765f93c54ad5a3592b1a56285f373f75b523b",
      "pdf_url": "",
      "publication_date": "2024-07-25",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "78791e2a4f93f3c3dbb5943bb67d524c02247848",
      "title": "Improving the Transferability of 3D Point Cloud Attack via Spectral-aware Admix and Optimization Designs",
      "abstract": "Deep learning models for point clouds have shown to be vulnerable to adversarial attacks, which have received increasing attention in various safety-critical applications such as autonomous driving, robotics, and surveillance. Existing 3D attackers generally design various attack strategies in the white-box setting, requiring the prior knowledge of 3D model details. However, real-world 3D applications are in the black-box setting, where we can only acquire the outputs of the target classifier. Although few recent works try to explore the black-box attack, they still achieve limited attack success rates (ASR). To alleviate this issue, this paper focuses on attacking the 3D models in a transfer-based black-box setting, where we first carefully design adversarial examples in a white-box surrogate model and then transfer them to attack other black-box victim models. Specifically, we propose a novel Spectral-aware Admix with Augmented Optimization method (SAAO) to improve the adversarial transferability. In particular, since traditional Admix strategy are deployed in the 2D domain that adds pixel-wise images for perturbing, we can not directly follow it to merge point clouds in coordinate domain as it will destroy the geometric shapes. Therefore, we design spectral-aware fusion that performs Graph Fourier Transform (GFT) to get spectral features of the point clouds and add them in the spectral domain. Afterward, we run a few steps with spectral-aware weighted Admix to select better optimization paths as well as to adjust corresponding learning weights. At last, we run more steps to generate adversarial spectral feature along the optimization path and perform Inverse-GFT on the adversarial spectral feature to obtain the adversarial example in the data domain. Experiments show that our SAAO achieves better transferability compared to existing 3D attack methods.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shiyu Hu",
        "Daizong Liu",
        "Wei Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/78791e2a4f93f3c3dbb5943bb67d524c02247848",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b93c78ab37670e34d7145f2f3552d71eb3960188",
      "title": "GNP Attack: Transferable Adversarial Examples Via Gradient Norm Penalty",
      "abstract": "Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Tao Wu",
        "Tie Luo",
        "D. Wunsch"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/b93c78ab37670e34d7145f2f3552d71eb3960188",
      "pdf_url": "https://arxiv.org/pdf/2307.04099",
      "publication_date": "2023-07-09",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aabe24bc6848e2f51eb3a1f186a20ce6cca6015e",
      "title": "Transferable Structure-based Adversarial Attack of Heterogeneous Graph Neural Network",
      "abstract": "Heterogeneous graph neural networks (HGNNs) have achieved remarkable development recently and exhibited superior performance in various tasks. However, recently HGNNs have been shown to have robustness weakness towards adversarial perturbations, which brings critical pitfalls for real applications, e.g. node classification and recommender systems. In particular, the transfer-based black-box attack is the most practical method to attack unknown models and poses a great threat to the reliability of HGNNs. In this work, we take the first step to explore the transferability of adversarial examples of HGNNs. Due to the overfitting of the source model, the adversarial perturbations generated by traditional methods usually exhibit unpromising transferability. To address this problem and boost adversarial transferability, we expect to seek common vulnerable directions of different models to attack. Inspired by the observation of the notable commonality of edge attention distribution between different HGNNs, we propose to guide the perturbation generation toward disrupting edge attention distribution. This edge attention-guided attack prioritizes the perturbation on edges that are more likely to be given common attention by different models, which benefits the transferability of adversarial perturbations. Finally, we develop two edge attention-guided attack methods towards heterogeneous relations tailored for HGNNs, called EA-FGSM and EA-PGD. Extensive experiments on six representative models and two datasets verify the effectiveness of our methods and form an unprecedented transfer robustness benchmark for HGNNs.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Yu Shang",
        "Yudong Zhang",
        "Jiansheng Chen",
        "Depeng Jin",
        "Yong Li"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/aabe24bc6848e2f51eb3a1f186a20ce6cca6015e",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3583780.3615095",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d6272f575585c90fd79490318261701a0217b7c8",
      "title": "Efficient malware detection using hybrid approach of transfer learning and generative adversarial examples with image representation",
      "abstract": "Identifying malicious intent within a program, also known as malware, is a critical security task. Many detection systems remain ineffective due to the persistent emergence of zero\u2010day variants, despite the pervasive use of antivirus tools for malware detection. The application of generative AI in the realm of malware visualization, particularly when binaries are depicted as colour visuals, represents a significant advancement over traditional machine\u2010learning approaches. Generative AI generates various samples, minimizing the need for specialized knowledge and time\u2010consuming analysis, hence boosting zero\u2010day attack detection and mitigation. This paper introduces the Deep Convolutional Generative Adversarial Network for Zero\u2010Shot Learning (DCGAN\u2010ZSL), leveraging transfer learning and generative adversarial examples for efficient malware classification. First, a normalization method is proposed, resizing malicious images to 128\u2009\u00d7\u2009128 or 300\u2009\u00d7\u2009300 for standardized input, enhancing feature transformation for improved malware pattern recognition. Second, greyscale representations are converted into colour images to augment feature extraction, providing a richer input for enhanced model performance in malware classification. Third, a novel DCGAN with progressive training improves model stability, mode collapse, and image quality, thus advancing generative model training. We apply the Attention ResNet\u2010based transfer learning method to extract texture features from generated samples, which increases security evaluation performance. Finally, the ZSL for zero\u2010day malware presents a novel method for identifying previously unknown threats, indicating a significant advancement in cybersecurity. The proposed approach is evaluated using two standard datasets, namely dumpware and malimg, achieving malware classification accuracies of 96.21% and 98.91%, respectively.",
      "year": 2024,
      "venue": "Expert Syst. J. Knowl. Eng.",
      "authors": [
        "Yue Zhao",
        "Farhan Ullah",
        "Chien-Ming Chen",
        "Mohammed Amoon",
        "Saru Kumari"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/d6272f575585c90fd79490318261701a0217b7c8",
      "pdf_url": "",
      "publication_date": "2024-08-13",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aee47284640c966e46ebd040e2a9a72674cd2785",
      "title": "Jamming Attacks and Mitigation in Transfer Learning Enabled 5G RAN Slicing",
      "abstract": "Radio access technology is crucial in both 5G and 6G cellular networks, providing differentiated services that demand reliability, low latency, and high throughput. To meet these requirements, machine learning (ML) has demonstrated considerable progress by facilitating resource allocation. However, these ML techniques can be susceptible to attacks, and the jamming attack is one of the most considered attacks in the literature, disrupting network functionality by sending interference signals. This paper, to the best of our knowledge for the first time, examines the vulnerability of radio access networks (RANs) to jamming attacks on resource allocation of a transfer reinforcement learning (TRL) based system and provides a mitigation approach to such attacks. A system model is presented for RAN slicing, followed by an introduction of the TRL algorithm for resource allocation. Afterward, we investigate covert patterned jamming attack (CPJA) on the TRL algorithm in downlink communication which decreases system throughput by 17% and 38.14% in the expert and learner agents and increases latency by 7.36% and 9.37% respectively. In addition, we propose a neural network (NN) solution to mitigate the CPJA trained on the network side and provide the trained NN model to the users' equipment (UEs) to eliminate interference from the signal by the filter. The trained NN is applied to predict the future activity of the interference generated by the attacker. Attack mitigation reduces the impact of the attack while the system's throughput suffers a 6% and 1.8% degradation, and its latency increases by 6.5% and 3.83% compared to the original system for expert and learner agents, respectively.",
      "year": 2024,
      "venue": "ICC 2024 - IEEE International Conference on Communications",
      "authors": [
        "Shavbo Salehi",
        "Hao Zhou",
        "Medhat H. M. Elsayed",
        "Majid Bavand",
        "Raimundas Gaigalas",
        "Yigit Ozcan",
        "Melike Erol-Kantarci"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/aee47284640c966e46ebd040e2a9a72674cd2785",
      "pdf_url": "",
      "publication_date": "2024-06-09",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fac0a720c53bd05d908d1c1a7d9a092bf96c5abc",
      "title": "Prompt as a Double-Edged Sword: A Dynamic Equilibrium Gradient-Assigned Attack against Graph Prompt Learning",
      "abstract": "Graph prompt learning (GPL) is designed to bridge the gap between graph pretraining models and downstream graph tasks, providing advantages in terms of graph knowledge transfer. However, GPL is vulnerable to poisoned graph attacks that induce abnormal training via adversarial malicious perturbations. We observe that the prevalent meta-gradient attacks, which heavily rely on the training of surrogate graph neural networks (GNNs), fail to account for the impact of perturbations on GPL where the pretrained GNN remains frozen and graph prompt tokens are tuned. Moreover, their gradient-assigned strategies tend to corrupt the topological semantics on a few influential labeled graphs, which in turn diminishes the trustworthiness of the surrogate training. To address this issue, we propose a dynamic equilibrium gradient-assigned attack against GPL, named MetaGpro. To guarantee the transferability of MetaGpro, the surrogate GPL is utilized in our simulation across various downstream tasks. To dynamically equilibrate the relationships between the reliability of surrogate models and instable structures, the over-robust contrastive learning is integrated into the surrogate training. In this way, the gradient bias caused by excessive perturbations of labeled nodes can be effectively mitigated. Subsequently, the topology perturbation generation is exploited to assign more gradient weights to nodes that are closer to the misclassification area. The experimental results reveal that the surrogate GPL outperforms the surrogate GNN in 96% of downstream evaluations, and our MetaGpro reduces the accuracy of GPL by 2%\u223c20% compared to the state-of-the-art (SOTA) works mostly. The code for our MetaGpro is available here.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Ju Jia",
        "Jingxuan Yu",
        "Di Wu",
        "Cong Wu",
        "Hengjie Zhu",
        "Lina Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/fac0a720c53bd05d908d1c1a7d9a092bf96c5abc",
      "pdf_url": "",
      "publication_date": "2025-08-03",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f142b58998a4045da65ebfa6273689c3f875c5a8",
      "title": "The Efficacy of Transfer-based No-box Attacks on Image Watermarking: A Pragmatic Analysis",
      "abstract": "Watermarking approaches are widely used to identify if images being circulated are authentic or AI-generated. Determining the robustness of image watermarking methods in the ``no-box'' setting, where the attacker is assumed to have no knowledge about the watermarking model, is an interesting problem. Our main finding is that evading the no-box setting is challenging: the success of optimization-based transfer attacks (involving training surrogate models) proposed in prior work~\\cite{hu2024transfer} depends on impractical assumptions, including (i) aligning the architecture and training configurations of both the victim and attacker's surrogate watermarking models, as well as (ii) a large number of surrogate models with potentially large computational requirements. Relaxing these assumptions i.e., moving to a more pragmatic threat model results in a failed attack, with an evasion rate at most $21.1\\%$. We show that when the configuration is mostly aligned, a simple non-optimization attack we propose, OFT, with one single surrogate model can already exceed the success of optimization-based efforts. Under the same $\\ell_\\infty$ norm perturbation budget of $0.25$, prior work~\\citet{hu2024transfer} is comparable to or worse than OFT in $11$ out of $12$ configurations and has a limited advantage on the remaining one. The code used for all our experiments is available at \\url{https://github.com/Ardor-Wu/transfer}.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qilong Wu",
        "Varun Chandrasekaran"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f142b58998a4045da65ebfa6273689c3f875c5a8",
      "pdf_url": "",
      "publication_date": "2024-12-03",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45e175a1bde8553b02d1675a3467112c2dcb535f",
      "title": "Reducing Sentiment Bias in Pre-trained Sentiment Classification via Adaptive Gumbel Attack",
      "abstract": "Pre-trained language models (PLMs) have recently enabled rapid progress on sentiment classification under the pre-train and fine-tune paradigm, where the fine-tuning phase aims to transfer the factual knowledge learned by PLMs to sentiment classification. However, current fine-tuning methods ignore the risk that PLMs cause the problem of sentiment bias, that is, PLMs tend to inject positive or negative sentiment from the contextual information of certain entities (or aspects) into their word embeddings, leading them to establish spurious correlations with labels. In this paper, we propose an adaptive Gumbel-attacked classifier that immunes sentiment bias from an adversarial-attack perspective. Due to the complexity and diversity of sentiment bias, we construct multiple Gumbel-attack expert networks to generate various noises from mixed Gumbel distribution constrained by mutual information minimization, and design an adaptive training framework to synthesize complex noise by confidence-guided controlling the number of expert networks. Finally, we capture these noises that effectively simulate sentiment bias based on the feedback of the classifier, and then propose a multi-channel parameter updating algorithm to strengthen the classifier to recognize these noises by fusing the parameters between the classifier and each expert network. Experimental results illustrate that our method significantly reduced sentiment bias and improved the performance of sentiment classification.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jiachen Tian",
        "Shizhan Chen",
        "Xiaowang Zhang",
        "Xin Wang",
        "Zhiyong Feng"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/45e175a1bde8553b02d1675a3467112c2dcb535f",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26599/26371",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ebbaf455aff9d516dc1f0dcfa949e4111756526b",
      "title": "Hybrid IoT security model with integration of LSTM, BERT, ROBERTA and transform learning for attack classification",
      "abstract": null,
      "year": 2025,
      "venue": "International journal of information technology",
      "authors": [
        "Ankur Gupta",
        "Dr. Dinesh Chandra Misra"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebbaf455aff9d516dc1f0dcfa949e4111756526b",
      "pdf_url": "",
      "publication_date": "2025-08-02",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd59c78825ae11c117b57e08e8003250343362a3",
      "title": "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models",
      "abstract": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose Safe LoRA, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that Safe LoRA is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data, Safe LoRA mitigates the negative effect made by malicious data while preserving performance on downstream tasks. Our codes are available at \\url{https://github.com/IBM/SafeLoRA}.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Chia-Yi Hsu",
        "Yu-Lin Tsai",
        "Chih-Hsun Lin",
        "Pin-Yu Chen",
        "Chia-Mu Yu",
        "Chun-ying Huang"
      ],
      "citation_count": 95,
      "url": "https://www.semanticscholar.org/paper/fd59c78825ae11c117b57e08e8003250343362a3",
      "pdf_url": "",
      "publication_date": "2024-05-27",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e1f2d6b7b1c3e69e3781dcd5693548d2e69fca55",
      "title": "FOSP: Fine-tuning Offline Safe Policy through World Models",
      "abstract": "Offline Safe Reinforcement Learning (RL) seeks to address safety constraints by learning from static datasets and restricting exploration. However, these approaches heavily rely on the dataset and struggle to generalize to unseen scenarios safely. In this paper, we aim to improve safety during the deployment of vision-based robotic tasks through online fine-tuning an offline pretrained policy. To facilitate effective fine-tuning, we introduce model-based RL, which is known for its data efficiency. Specifically, our method employs in-sample optimization to improve offline training efficiency while incorporating reachability guidance to ensure safety. After obtaining an offline safe policy, a safe policy expansion approach is leveraged for online fine-tuning. The performance of our method is validated on simulation benchmarks with five vision-only tasks and through real-world robot deployment using limited data. It demonstrates that our approach significantly improves the generalization of offline policies to unseen safety-constrained scenarios. To the best of our knowledge, this is the first work to explore offline-to-online RL for safe generalization tasks.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Chenyang Cao",
        "Yucheng Xin",
        "Silang Wu",
        "Longxiang He",
        "Zichen Yan",
        "Junbo Tan",
        "Xueqian Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e1f2d6b7b1c3e69e3781dcd5693548d2e69fca55",
      "pdf_url": "",
      "publication_date": "2024-07-06",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d83b615f989c8d1e9860a8a2d82628c95e40d22",
      "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
      "abstract": "Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset are available at https://github.com/ys-zong/VLGuard.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Yongshuo Zong",
        "Ondrej Bohdal",
        "Tingyang Yu",
        "Yongxin Yang",
        "Timothy M. Hospedales"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/2d83b615f989c8d1e9860a8a2d82628c95e40d22",
      "pdf_url": "",
      "publication_date": "2024-02-03",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6a4753b8843d2fb87b9c1750b6004e0a2de34946",
      "title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection",
      "abstract": "Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, previous studies have demonstrated that fine-tuning the models on several adversarial samples or even benign data can greatly comprise the model's pre-equipped alignment and safety capabilities. In this work, we propose SEAL, a novel framework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based on the bilevel optimization to up rank the safe and high-quality fine-tuning data and down rank the unsafe or low-quality ones. Models trained with SEAL demonstrate superior quality over multiple baselines, with 8.5% and 9.7% win rate increase compared to random selection respectively on Llama-3-8b-Instruct and Merlinite-7b models. Our code is available on github https://github.com/hanshen95/SEAL.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Han Shen",
        "Pin-Yu Chen",
        "Payel Das",
        "Tianyi Chen"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/6a4753b8843d2fb87b9c1750b6004e0a2de34946",
      "pdf_url": "",
      "publication_date": "2024-10-09",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8e624e215908934a38044500f8434a0f88c69059",
      "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
      "abstract": "The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Eliahu Horwitz",
        "Jonathan Kahana",
        "Yedid Hoshen"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/8e624e215908934a38044500f8434a0f88c69059",
      "pdf_url": "",
      "publication_date": "2024-02-15",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2f03b07c069259730f119f10737a1e033440a0c",
      "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
      "abstract": "Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model's alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Ning Lu",
        "Shengcai Liu",
        "Jiahao Wu",
        "Weiyu Chen",
        "Zhirui Zhang",
        "Y. Ong",
        "Qi Wang",
        "Ke Tang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c2f03b07c069259730f119f10737a1e033440a0c",
      "pdf_url": "",
      "publication_date": "2025-05-17",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e25a29872de8bfd1977596a56f30cae716846a92",
      "title": "Deep Learning-Based Lane-Keeping Assist System for Self-Driving Cars Using Transfer Learning and Fine Tuning",
      "abstract": "\u2014This paper presents an advanced lane-keeping assistance system specifically designed for self-driving cars. The proposed model combines the powerful Xception network with transfer learning and fine-tuning techniques to accurately predict the steering angle. By analyzing camera-captured images, the model effectively learns from human driving knowledge and provides precise estimations of the steering angle necessary for safe lane-keeping. The transfer learning technique allows the model to leverage the extensive knowledge acquired from the ImageNet dataset, while the fine-tuning technique is utilized to tailor the pre-trained model to the specific task of steering angle prediction based on input images, enabling optimal performance. Fine-tuning was initiated by initially freezing the pre-trained model and training only the Fully Connected (FC) layer for the first 10 epochs. Subsequently, the entire model, encompassing both the backbone and the FC layer, was unfrozen for further training. To evaluate the system\u2019s effectiveness, a comprehensive comparative analysis is conducted against popular existing models, including Nvidia, MobilenetV2, VGG19, and InceptionV3. The evaluation includes an assessment of the operational accuracy based on the loss function, specifically utilizing the Mean Squared Error (MSE) equation. The proposed model achieves the lowest loss function values for both training and validation, demonstrating its superior predictive performance. Additionally, the model\u2019s performance is further evaluated through extensive real-world testing on pre-designed trajectories and maps, resulting in the minimal deviation of the steering angle from the desired trajectory over time. This practical evaluation provides valuable insights into the mode\u2019s reliability and its potential to effectively assist in lane-keeping tasks.",
      "year": 2024,
      "venue": "Journal of Advances in Information Technology",
      "authors": [
        "Phuc Phan Hong",
        "Huy Hua Khanh",
        "Nghi Nguyen Vinh",
        "Nguyen Nguyen Trung",
        "Anh Nguyen Quoc",
        "Hoang Tran Ngoc"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/e25a29872de8bfd1977596a56f30cae716846a92",
      "pdf_url": "https://www.jait.us/uploadfile/2024/JAIT-V15N3-322.pdf",
      "publication_date": null,
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3cea9013b9b8383c673dd992942a248646743e09",
      "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
      "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such hidden capabilities are relevant leads to sample-efficient 'revival' of the capability, i.e., the model begins reusing these capability after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Samyak Jain",
        "Robert Kirk",
        "E. Lubana",
        "Robert P. Dick",
        "Hidenori Tanaka",
        "Edward Grefenstette",
        "Tim Rocktaschel",
        "D. Krueger"
      ],
      "citation_count": 88,
      "url": "https://www.semanticscholar.org/paper/3cea9013b9b8383c673dd992942a248646743e09",
      "pdf_url": "",
      "publication_date": "2023-11-21",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85d8207536791874b67000d40324597d7d9a7c01",
      "title": "Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning - A Convex Optimization Perspective",
      "abstract": "The post-training of LLMs, which typically consists of the supervised fine-tuning (SFT) stage and the preference learning stage (RLHF or DPO), is crucial to effective and safe LLM applications. The widely adopted approach in post-training popular open-source LLMs is to sequentially perform SFT and RLHF/DPO. However, this is suboptimal in terms of SFT and RLHF/DPO trade-off: the LLM gradually forgets about the first stage's training when undergoing the second stage's training. This sequential paradigm persists largely due to its simplicity and modularity, which make it easier to implement and manage at scale despite its limitations. We theoretically prove the sub-optimality of sequential post-training and propose a practical joint post-training framework which has theoretical convergence guarantees and empirically outperforms sequential post-training framework, with up to 23% overall performance improvement across multiple LLM evaluation benchmarks, while having minimal computational overhead. Our code is available at https://github.com/heshandevaka/XRIGHT.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Heshan Fernando",
        "Han Shen",
        "Parikshit Ram",
        "Yi Zhou",
        "Horst Samulowitz",
        "Nathalie Baracaldo",
        "Tianyi Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/85d8207536791874b67000d40324597d7d9a7c01",
      "pdf_url": "",
      "publication_date": "2024-10-20",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b9c3b144bbc7ff39168b3b756214649cd7b27c3f",
      "title": "Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models",
      "abstract": "Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes to both task performance and resource usage. Motivated by this insight, we propose Selective Adapter FrEezing (SAFE), which gradually freezes less important adapters early to reduce unnecessary resource usage while maintaining performance. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while achieving comparable or better task performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape, which enables the model to generalize better by avoiding sharp minima.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Hyegang Son",
        "Yonglak Son",
        "Changhoon Kim",
        "Young Geun Kim"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b9c3b144bbc7ff39168b3b756214649cd7b27c3f",
      "pdf_url": "",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0efcfd7af01650631d68148d68e7d68ccd96327",
      "title": "Generative Model for Small Molecules with Latent Space RL Fine-Tuning to Protein Targets",
      "abstract": "A specific challenge with deep learning approaches for molecule generation is generating both syntactically valid and chemically plausible molecular string representations. To address this, we propose a novel generative latent-variable transformer model for small molecules that leverages a recently proposed molecular string representation called SAFE. We introduce a modification to SAFE to reduce the number of invalid fragmented molecules generated during training and use this to train our model. Our experiments show that our model can generate novel molecules with a validity rate>90% and a fragmentation rate<1% by sampling from a latent space. By fine-tuning the model using reinforcement learning to improve molecular docking, we significantly increase the number of hit candidates for five specific protein targets compared to the pre-trained model, nearly doubling this number for certain targets. Additionally, our top 5% mean docking scores are comparable to the current state-of-the-art (SOTA), and we marginally outperform SOTA on three of the five targets.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Ulrich A. Mbou Sob",
        "Qiulin Li",
        "Miguel Arbes'u",
        "Oliver Bent",
        "Andries P. Smit",
        "Arnu Pretorius"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d0efcfd7af01650631d68148d68e7d68ccd96327",
      "pdf_url": "",
      "publication_date": "2024-07-02",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59e529bedb04265d3bdc600641289b1583ea17aa",
      "title": "Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances",
      "abstract": "Fine-tuning pretrained language models (PLMs) on downstream tasks has become common practice in natural language processing. However, most of the PLMs are vulnerable, e.g., they are brittle under adversarial attacks or imbalanced data, which hinders the application of the PLMs on some downstream tasks, especially in safe-critical scenarios. In this paper, we propose a simple yet effective fine-tuning method called Match-Tuning to force the PLMs to be more robust. For each instance in a batch, we involve other instances in the same batch to interact with it. To be specific, regarding the instances with other labels as a perturbation, Match-Tuning makes the model more robust to noise at the beginning of training. While nearing the end, Match-Tuning focuses more on performing an interpolation among the instances with the same label for better generalization. Extensive experiments on various tasks in GLUE benchmark show that Match-Tuning consistently outperforms the vanilla fine-tuning by 1.64 scores. Moreover, Match-Tuning exhibits remarkable robustness to adversarial attacks and data imbalance.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Shoujie Tong",
        "Qingxiu Dong",
        "Damai Dai",
        "Yifan Song",
        "Tianyu Liu",
        "Baobao Chang",
        "Zhifang Sui"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/59e529bedb04265d3bdc600641289b1583ea17aa",
      "pdf_url": "http://arxiv.org/pdf/2205.00633",
      "publication_date": "2022-05-02",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a11fe2b7cdd7858f71a3a81d41d0c2f01b70739b",
      "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models",
      "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes: the cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy. Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. In particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes. Extensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shuoyuan Wang",
        "Yixuan Li",
        "Hongxin Wei"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a11fe2b7cdd7858f71a3a81d41d0c2f01b70739b",
      "pdf_url": "",
      "publication_date": "2024-10-03",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1913db292717210a056b912c917e435de45e27c",
      "title": "Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms",
      "abstract": "The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Ehsan Hallaji",
        "R. Razavi-Far",
        "M. Saif"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/f1913db292717210a056b912c917e435de45e27c",
      "pdf_url": "",
      "publication_date": "2022-07-05",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "17135a57e773acc1bd858b6963791e18193ca4a5",
      "title": "Enhanced Network Intrusion Detection System for Internet of Things Security Using Multimodal Big Data Representation with Transfer Learning and Game Theory",
      "abstract": "Internet of Things (IoT) applications and resources are highly vulnerable to flood attacks, including Distributed Denial of Service (DDoS) attacks. These attacks overwhelm the targeted device with numerous network packets, making its resources inaccessible to authorized users. Such attacks may comprise attack references, attack types, sub-categories, host information, malicious scripts, etc. These details assist security professionals in identifying weaknesses, tailoring defense measures, and responding rapidly to possible threats, thereby improving the overall security posture of IoT devices. Developing an intelligent Intrusion Detection System (IDS) is highly complex due to its numerous network features. This study presents an improved IDS for IoT security that employs multimodal big data representation and transfer learning. First, the Packet Capture (PCAP) files are crawled to retrieve the necessary attacks and bytes. Second, Spark-based big data optimization algorithms handle huge volumes of data. Second, a transfer learning approach such as word2vec retrieves semantically-based observed features. Third, an algorithm is developed to convert network bytes into images, and texture features are extracted by configuring an attention-based Residual Network (ResNet). Finally, the trained text and texture features are combined and used as multimodal features to classify various attacks. The proposed method is thoroughly evaluated on three widely used IoT-based datasets: CIC-IoT 2022, CIC-IoT 2023, and Edge-IIoT. The proposed method achieves excellent classification performance, with an accuracy of 98.2%. In addition, we present a game theory-based process to validate the proposed approach formally.",
      "year": 2024,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Farhan Ullah",
        "Ali Turab",
        "Shamsher Ullah",
        "D. Cacciagrano",
        "Yue Zhao"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/17135a57e773acc1bd858b6963791e18193ca4a5",
      "pdf_url": "https://www.mdpi.com/1424-8220/24/13/4152/pdf?version=1719403678",
      "publication_date": "2024-06-26",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f49b785cd3493bc78d8472dbb111d38edb742e3a",
      "title": "Enhancing Gun Detection With Transfer Learning and YAMNet Audio Classification",
      "abstract": "Identification of the type of gun used is essential in several fields, including forensics, the military, and defense. In this research, one of the powerful deep learning architectures is applied to identify several types of firearms based on their gunshot noises. For the purpose of extracting features from the audio data, the suggested technique makes use of YAMNet, an effective deep learning-based classification model. The Mel spectrograms created from the collected features are used for multi-class audio classification, which makes it possible to identify different types of guns. 1174 audio samples from 12 distinct weapons make up the study\u2019s extensive dataset, which offers a varied and representative collection for training and evaluation. We achieve a remarkable accuracy of 94.96% by employing the best hyperparameter changes and optimization methods. The findings of this study make a substantial contribution to the domains of forensics, military, and defense, where precise gun type identification is crucial. Applying deep learning and mel spectrograms to analyze gunshot audio demonstrates itself to be a promising strategy, providing quick and accurate categorization. This research emphasizes the effectiveness and relevance of using YAMNet, an AI-driven model, as a superior answer to the issues of real-world weapon detection.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "N. H. Valliappan",
        "S. Pande",
        "Surendra Reddy Vinta",
        "Surendra Reddy"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/f49b785cd3493bc78d8472dbb111d38edb742e3a",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10506933.pdf",
      "publication_date": null,
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "07d582e498d049d4c87a5f225bc129e2b1cb9a4d",
      "title": "DEFENDIFY: defense amplified with transfer learning for obfuscated malware framework",
      "abstract": null,
      "year": 2025,
      "venue": "Cybersecurity",
      "authors": [
        "Rodrigo Castillo Camargo",
        "Juan Murcia Nieto",
        "Nicol\u00e1s Rojas",
        "Daniel D\u00edaz-L\u00f3pez",
        "Santiago Alf\u00e9rez",
        "\u00c1. L. Perales G\u00f3mez",
        "P. Nespoli",
        "F\u00e9lix G\u00f3mez M\u00e1rmol",
        "Umit Karabiyik"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/07d582e498d049d4c87a5f225bc129e2b1cb9a4d",
      "pdf_url": "",
      "publication_date": "2025-04-29",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e55eaf6a091b01e610f35533b4e3db624c031b10",
      "title": "Mathematical Modeling of Cyberattack Defense Mechanism Using Hybrid Transfer Learning With Snow Ablation Optimization Algorithm in Critical Infrastructures",
      "abstract": "Cybersecurity is a significant topic that has turned into an efficient one at present owing to the increasing dependency on interconnected methods and technology. As digitalization upsurges, the requirement for cybersecurity measures becomes even more vital for numerous networks. To certify the security of critical infrastructures, numerous cyber security solutions must be taken together and the essential infrastructure must be developed. Industrial control methods are one of the most vital aspects of the cybersecurity of critical infrastructures. It is possible to entirely stop these physically located devices\u2019 operations and do substantial destruction with a cyberattack. Whereas AI solutions are being utilized in numerous fields, cyber security has started to become one of the concentrated fields of artificial intelligence (AI) domain. Consequently, there are many studies on identifying cyberattacks by utilizing AI methods. It is probable to utilize AI to help and support cybersecurity solutions to develop cybersecurity of significant infrastructures. This study develops a Cyberattack Defense Mechanism using Hybrid Transfer Learning with Snow Ablation Optimization Algorithm (CDMHTL-SAOA) technique in Critical infrastructures. The main cause of the CDMHTL-SAOA model is to improve the cyber security maturity level of critical infrastructures and inspect both traditional cyberattack and AI approaches. Primarily, the data normalization process can be implemented to scale the raw data into a uniform format. In addition, the snow ablation optimization (SAO) algorithm can be exploited for the optimum choice of feature subsets. For the cybersecurity classification process, the presented CDMHTL-SAOA technique applies the hybrid of convolutional neural network and bi-directional long short-term memory (CNN-BiLSTM) method. Eventually, the parameter choice of the CNN-BiLSTM technique has been implemented by the design of the hippopotamus optimization algorithm (HOA). To represent the better solution of the CDMHTL-SAOA classifier, a simulation validation can be tested on a benchmark database and the solutions are measured for various aspects. The simulation outcomes certified the improved execution of the CDMHTL-SAOA method over other techniques.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Mohamad Khairi Ishak"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e55eaf6a091b01e610f35533b4e3db624c031b10",
      "pdf_url": "https://doi.org/10.1109/access.2025.3530931",
      "publication_date": null,
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "91b6b017ec711d3567aec253414e321a20314b24",
      "title": "An Interpretable Skin Cancer Classification Using Optimized Deep Transfer Learning Method",
      "abstract": "The skin, the largest organ in the body, acts as a vital layer of defense but can also develop several diseases, including skin cancer, one of the top three cancers. There are various forms of skin cancer, each of which poses multiple challenges for identification and treatment. This study investigates the effectiveness of deep learning approaches, namely convolutional neural networks (CNNs), for the rapid and exact diagnosis of skin cancer types. Using the ISIC Skin Cancer Challenge 2019 dataset, our research sheds insight into CNNs ability to distinguish diagnostic accuracy for skin cancer diagnosis. We used 12,295 skin lesion images from this dataset, where three forms of skin cancer: nevus, melanoma, and basal cell carcinoma are focused. During the preprocessing step, the photographs were augmented, normalized, and resized. The MobileNetV2 transfer learning model is evaluated against DenseNet121, InceptionV3, ResNet152V2, CNN and comparative analysis with the the current state of the art. After testing, the model achieved the greatest accuracy at 97.48%. Our MobileNetV2 transfer learning model surpasses earlier models in terms of dependability and robustness. Beyond providing an in-depth assessment of current difficulties, it further promotes critical conversation and collaboration in medical image analysis and healthcare innovation.",
      "year": 2024,
      "venue": "2024 International Conference on Innovations in Science, Engineering and Technology (ICISET)",
      "authors": [
        "Mohammad Naimul Islam Shanto",
        "Md. Sayem Uddin",
        "Arfanul Islam",
        "Taivan Reza Dipta",
        "Md. Sorowar Mahabub Rabby",
        "Md. Khaliluzzaman"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/91b6b017ec711d3567aec253414e321a20314b24",
      "pdf_url": "",
      "publication_date": "2024-10-26",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd62a07e82da0004ee09bf5aa49479d228b2d262",
      "title": "Pixel Map Analysis Adversarial Attack Detection on Transfer Learning Model",
      "abstract": "Adversarial attacks pose a significant threat to the robustness and reliability of deep learning models, particularly in the context of transfer learning where pre-trained models are widely used. In this research, we propose a novel approach for detecting adversarial attacks on transfer learning models using pixel map analysis. By analyzing changes in pixel values at a granular level, our method aims to uncover subtle manipulations that are often overlooked by traditional detection techniques. We demonstrate the effectiveness of our approach through extensive experiments on various benchmark datasets, showcasing its ability to accurately detect adversarial attacks while maintaining high classification performance on clean data. Our findings highlight the importance of incorporating pixel map analysis into the defense mechanisms of transfer learning models to enhance their robustness against sophisticated adversarial threats.",
      "year": 2024,
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "authors": [
        "Soni Kumari",
        "Dr.Sheshang Degadwala"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fd62a07e82da0004ee09bf5aa49479d228b2d262",
      "pdf_url": "https://ijsrcseit.com/index.php/home/article/download/CSEIT2410229/CSEIT2410229",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9f769585b73d6fadcf4c17eec271822fd75ccf1a",
      "title": "Improving SAR ATR using synthetic data via transfer learning",
      "abstract": "Attempts to use synthetic data to augment measured data for improved synthetic aperture radar (SAR) automatic target recognition (ATR) performance have been hampered by domain mismatch between datasets. Past work which leveraged synthetic data in a transfer learning framework has been successful but was primarily focused on transferring generic SAR features. Recently SAMPLE, a paired synthetic and measured dataset was introduced to the SAR community, enabling demonstration of good ATR performance using 100% synthetic data. In this work, we examine how to leverage synthetic data and measured data to boost ATR using transfer learning. The synthetic dataset corresponds to the MSTAR 15o dataset. We demonstrate that high quality synthetic data can enhance ATR performance even when substantial measured data is available, and that synthetic data can reduce measured data requirements by over 50% while maintaining classification accuracy.",
      "year": 2023,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "Brian O. Raeker",
        "Tyler Hill",
        "C. Kreucher",
        "Katherine M. Banas",
        "Kevin Tactac",
        "Kyle Simpson",
        "Kirk L. Weeks"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/9f769585b73d6fadcf4c17eec271822fd75ccf1a",
      "pdf_url": "",
      "publication_date": "2023-06-13",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a368b69fd02e188f3ae3953d10d78a2a1cab2c0e",
      "title": "Deep transductive transfer learning for automatic target recognition",
      "abstract": "One of the major obstacles in designing an automatic target recognition (ATR) algorithm, is that there are often labeled images in one domain (i.e., infrared source domain) but no annotated images in the other target domains (i.e., visible, SAR, LIDAR). Therefore, automatically annotating these images is essential to build a robust classifier in the target domain based on the labeled images of the source domain. Transductive transfer learning is an effective way to adapt a network to a new target domain by utilizing a pretrained ATR network in the source domain. We propose an unpaired transductive transfer learning framework where a CycleGAN model and a well-trained ATR classifier in the source domain are used to construct an ATR classifier in the target domain without having any labeled data in the target domain. We employ a CycleGAN model to transfer the mid-wave infrared (MWIR) images to visible (VIS) domain images (or visible to MWIR domain). To train the transductive CycleGAN, we optimize a cost function consisting of the adversarial, identity, cycle-consistency, and categorical cross-entropy loss for both the source and target classifiers. In this paper, we perform a detailed experimental analysis on the challenging DSIAC ATR dataset. The dataset consists of ten classes of vehicles at different poses and distances ranging from 1-5 kilometers on both the MWIR and VIS domains. In our experiment, we assume that the images in the VIS domain are the unlabeled target dataset. We first detect and crop the vehicles from the raw images and then project them into a common distance of 2 kilometers. Our proposed transductive CycleGAN achieves 71.56% accuracy in classifying the visible domain vehicles in the DSIAC ATR dataset.",
      "year": 2023,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "S. Sami",
        "N. Nasrabadi",
        "R. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a368b69fd02e188f3ae3953d10d78a2a1cab2c0e",
      "pdf_url": "https://arxiv.org/pdf/2305.13886",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be152542b17d368bb814aac1f4514d34b744a33b",
      "title": "Explorations in transfer learning and machine learning architectures utilizing the DSIAC ATR algorithm development data set",
      "abstract": "This paper presents the results of applying several transfer learning front-ends (e.g. Resnet50, Inception, MobileNet) commonly utilized in academia based upon the ImageNet database to perform feature extraction for the DSIAC ATR data set followed by classification layers. This paper describes the performance of a machine learning system (MLS) composed of a feature generating front-end followed by a classification backend trained on electro-optical (EO) and mid-wave infrared (MWIR) imagery from the DSIAC dataset. The baseline MLS architecture achieves over 99 percent accuracy on both the EO and MWIR DSIAC datasets.",
      "year": 2023,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "K. Priddy",
        "Sastry Dhara"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/be152542b17d368bb814aac1f4514d34b744a33b",
      "pdf_url": "",
      "publication_date": "2023-06-13",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c16f57236555aae3f600ef8f1978eff10b410233",
      "title": "Efficient Drones-Birds Classification Using Transfer Learning",
      "abstract": "Transfer learning is one of the recent deep learning technologies that can saves most of the time consumed in training deep networks. It could be performed by using pre-trained deep networks. In this paper, we will propose a new model for drones-bird classification based on transfer learning. We modify and evaluate three popular pre-trained deep models using dataset consists of drones and birds. The performance of each pre-trained network is evaluated and compared. Results show that, these pretrained networks could be adopted efficiently in classification of the addressed dataset. Additionally, the accuracy of the ResNet18 outperforms other evaluated networks. The accuracy and F-Score of ResNet18 exceeds 98% in all cases. Also, the other models have an excellent performance. There are many important applications of the problem addressed especially in security, defense, and surveillance. So, accurate classification of drones and birds is very important.",
      "year": 2023,
      "venue": "2023 4th International Conference on Artificial Intelligence, Robotics and Control (AIRC)",
      "authors": [
        "Taha M. Mohamed",
        "I. Alharbi"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c16f57236555aae3f600ef8f1978eff10b410233",
      "pdf_url": "",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0bccbc194ce107fe003005c1fb5900157e9697ea",
      "title": "A Transfer Learning Approach for Face Liveness Detection",
      "abstract": "In the rapidly advancing domain of digital security, the authenticity of facial recognition systems remains essential. Face spoofing attacks pose a significant challenge, with malicious entities attempting to deceive facial authentication systems using counterfeit techniques. In this study, we present a novel transfer learning methodology leveraging the MoViNet model, renowned for its efficiency in video data processing, to discern genuine from spoofed faces. Our approach addresses the specific challenges of face liveness detection by fine-tuning the MoViNet model on a diverse dataset of both genuine and counterfeit face videos, enhanced by preprocessing steps such as frame extraction, padding, resizing, and normalization. This meticulous adaptation not only improves training but, through our transfer learning technique, equips the model with a robust defense against new and unseen spoofing attempts, ultimately achieving a notable zero False Acceptance Rate (FAR) \u2013 a testament to its high generalizability and uncompromising efficacy against spoofing. Post-training, our MoViNet-based model exhibited a remarkable accuracy of 98%, with a Precision of 1.0, an F1-Score of 0.94, and most notably, an Equal Error Rate (EER) and Half Total Error Rate (HTER) of 0.05%. When compared with several state-of-the-art methods, our approach not only matched but surpassed their efficacy. The findings underscore the potential of MoViNet in fortifying facial recognition systems against spoofing attacks, setting a new benchmark in the field, and offering robust protection for various real-world applications. We believe our transfer learning approach opens exciting possibilities for enhancing security and user experience in diverse domains.",
      "year": 2023,
      "venue": "International Conference on the Internet, Cyber Security and Information Systems",
      "authors": [
        "Mahmoud Omara",
        "Mahmoud Fayez",
        "Heba Khalid",
        "Said Ghoniemy"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0bccbc194ce107fe003005c1fb5900157e9697ea",
      "pdf_url": "",
      "publication_date": "2023-11-21",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eed6a405207839f3935e7d19ad7698c11773d73f",
      "title": "Transfer Learning Innovations in Network Security and Threat Intelligence",
      "abstract": "In Transfer Learning Innovations in Network Security and Threat Intelligence,\u201d we introduce a holistic approach that harnesses transfer learning to fortify network security and elevate threat intelligence. This methodology revolves around three core algorithms, each addressing a specific facet of cybersecurity enhancement. Initially, a deep neural network undergoes pre-training on a diverse source domain dataset rich in cybersecurity information. This process aims to capture generalized features and knowledge pertaining to a broad spectrum of cyber threats and vulnerabilities. The outline these techniques, such as Maximum Mean Discrepancy (MMD) and adversarial training, both conceptually and mathematically. These three algorithms amalgamate to create a robust transfer learning framework, purpose-built for network security and threat intelligence. This comprehensive approach empowers security practitioners to dynamically adapt and reinforce their defense mechanisms.",
      "year": 2023,
      "venue": "2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG)",
      "authors": [
        "Aishwarya Mishra",
        "Sandeep Kumar",
        "D. Pipalia",
        "Rohit Vaish"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eed6a405207839f3935e7d19ad7698c11773d73f",
      "pdf_url": "",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ebccbf14623beb07e276a2eb6288612a300cf128",
      "title": "Dependable Intrusion Detection System for IoT: A Deep Transfer Learning Based Approach",
      "abstract": "Security concerns for Internet of Things (IoT) applications have been alarming because of their widespread use in different enterprise systems. The potential threats to these applications are constantly emerging and changing, and, therefore, sophisticated and dependable defense solutions are necessary against such threats. With the rapid development of IoT networks and evolving threat types, the traditional machine learning based IDS must update to cope with the security requirements of the current sustainable IoT environment. In recent years, deep learning and deep transfer learning have progressed and experienced great success in different fields and have emerged as a potential solution for dependable network intrusion detection. However, new and emerging challenges have arisen related to the accuracy, efficiency, scalability, and dependability of the traditional IDS in a heterogeneous IoT setup. This manuscript proposes a deep transfer learning based dependable IDS model that outperforms several existing approaches. The unique contributions include effective attribute selection, which is best suited to identify normal and attack scenarios for a small amount of labeled data, designing a dependable deep transfer learning based ResNet model and evaluating considering real-world data. To this end, a comprehensive experimental performance evaluation has been conducted. Extensive analysis and performance evaluation show that the proposed model is robust, more efficient, and has demonstrated better performance, ensuring dependability.",
      "year": 2022,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "S. T. Mehedi",
        "A. Anwar",
        "Ziaur Rahman",
        "Kawsar Ahmed",
        "R. Islam"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/ebccbf14623beb07e276a2eb6288612a300cf128",
      "pdf_url": "https://arxiv.org/pdf/2204.04837",
      "publication_date": "2022-04-11",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c9581e5398a7fd0e6d10981759a922fa8ff42806",
      "title": "Skin Cancer Classification: A Transfer Learning Approach Using Inception-v3",
      "abstract": "In the human body, the skin serves as the primary layer of defense for essential organs. However, as a result of ozone layer degradation, exposure to UV radiation, fungal and viral infections.\u00a0 Skin cancer is becoming more common.\nThis study proposes a novel deep learning-based framework for the multi-classification of eight different types of skin cancer. The suggested framework is divided into several steps. The initial phase is the data augmentation of images. In the second step, deep models are fine-tuned. The model is opted, for Inception-v3, and updated their layers. In the third step, The suggested model has been applied to train both fine-tuned on augmented datasets.\nAfter optimization, the pre-trained model performs well for classifying skin tumors, with Inception-v3 having accuracy and an F-score of 81% and 81%, respectively.",
      "year": 2023,
      "venue": "NTU Journal of Engineering and Technology",
      "authors": [
        "Yaarob Younus Al Badrani",
        "Abbas Mgharbel"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c9581e5398a7fd0e6d10981759a922fa8ff42806",
      "pdf_url": "https://doi.org/10.56286/ntujet.v2i2.532",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59df668a90b39c3c911bc923e7685821be9b2f0d",
      "title": "An Ensemble Transfer Learning Spiking Immune System for Adaptive Smart Grid Protection",
      "abstract": "The rate of technical innovation, system interconnection, and advanced communications undoubtedly boost distributed energy networks\u2019 efficiency. However, when an additional attack surface is made available, the possibility of an increase in attacks is an unavoidable result. The energy ecosystem\u2019s significant variety draws attackers with various goals, making any critical infrastructure a threat, regardless of scale. Outdated technology and other antiquated countermeasures that worked years ago cannot address the complexity of current threats. As a result, robust artificial intelligence cyber-defense solutions are more important than ever. Based on the above challenge, this paper proposes an ensemble transfer learning spiking immune system for adaptive smart grid protection. It is an innovative Artificial Immune System (AIS) that uses a swarm of Evolving Izhikevich Neural Networks (EINN) in an Ensemble architecture, which optimally integrates Transfer Learning methodologies. The effectiveness of the proposed innovative system is demonstrated experimentally in multiple complex scenarios that optimally simulate the modern energy environment. The most significant findings of this work are that the transfer learning architecture\u2019s shared learning rate significantly adds to the speed of generalization and convergence approach. In addition, the ensemble combination improves the accuracy of the model because the overall behavior of the numerous models is less noisy than a comparable individual single model. Finally, the Izhikevich Spiking Neural Network used here, due to its dynamic configuration, can reproduce different spikes and triggering behaviors of neurons, which models precisely the problem of digital security of energy infrastructures, as proved experimentally.",
      "year": 2022,
      "venue": "Energies",
      "authors": [
        "Konstantinos Demertzis",
        "Dimitrios Taketzis",
        "Vasiliki Demertzi",
        "C. Skianis"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/59df668a90b39c3c911bc923e7685821be9b2f0d",
      "pdf_url": "https://www.mdpi.com/1996-1073/15/12/4398/pdf?version=1655690320",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf2724fcf2bae6b3e7c51d6e06ca7fd4b1a2eb35",
      "title": "A Novel Method of Intrusion Detection Based on Federated Transfer Learning and Convolutional Neural Network",
      "abstract": "As a network security defense technology, intrusion detection system can effectively protect network security. At present, machine learning is widely used in intrusion detection and has achieved good application results. The detection methods based on traditional machine learning need enough available intrusion detection data samples, and the samples meet the conditions of independent and identically distributed. However, in reality, the intrusion detection data generated by a single institution is insufficient, and various institutions protect users' privacy and data security in the form of islands, which makes it difficult to maintain the same data distribution. In addition, there is the problem of data imbalance. To solve the above problems, this paper proposes a new intrusion detection method FTLCNN, which integrates federal transfer learning and convolutional neural network. FTLCNN constructs a transfer convolution neural network framework to solve the problems of sample scarcity, imbalance and probability adaptation; under the mechanism of federal learning, FTLCNN use the model to learn without sharing training data, protect data privacy and solve the problem of data island. The experimental on UNSW-NB15 shows that compared with the other four benchmark algorithms, FTLCNN has higher detection rate and lower false positive rate, and has significant advantages in solving the problem of scarcity and imbalance of in intrusion detection.",
      "year": 2022,
      "venue": "IEEE Joint International Information Technology and Artificial Intelligence Conference",
      "authors": [
        "Xiang Ji",
        "Hong Zhang",
        "Xu Ma"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/cf2724fcf2bae6b3e7c51d6e06ca7fd4b1a2eb35",
      "pdf_url": "",
      "publication_date": "2022-06-17",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "075af4378b5ed3b3cc154133ee71bdbc264be2db",
      "title": "Fighter Aircraft Detection using CNN and Transfer Learning",
      "abstract": "In this work, Deep learning techniques such as Convolutional Neural networks (CNN) and Transfer Learning are used to detect and identify Fighter aircraft or jets. A dataset consisting of 21 different aircraft with 20000 images is being processed using the above algorithms. CNN works on the principle of \"pooling,\" which progressively reduces the spatial size of the model to decrease the number of parameters and computations in the network. CNN's are widely used for image detection in different domains, including defense, agriculture, business, face recognition technology, etc. Transfer learning is a machine learning method where a model created for a task is reused as the initial point for a model on a second task. Transfer learning is related to issues such as multi-task learning and concept drift and is not only an area of study in deep learning. The dataset is processed and uses python libraries such as pandas, seaborn, sci-kit- learn, etc., to find any pre-trained patterns and insights. Data is separated into train and test datasets with 80-20 percent of total data, respectively. A model is built using the TensorFlow library for CNN. The metric used is \"accuracy.\" A transfer learning model is also built to compare the accuracy results and adopt the best-fitting one",
      "year": 2022,
      "venue": "International Journal of Engineering and Advanced Technology",
      "authors": [
        "Motati Dinesh Reddy",
        "Sai Venkata Rao Kora",
        "Gnana Samhitha Ch"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/075af4378b5ed3b3cc154133ee71bdbc264be2db",
      "pdf_url": "https://doi.org/10.35940/ijeat.a3854.1012122",
      "publication_date": "2022-10-30",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "60a3d59717aaa25289f29222d3677b06608d9765",
      "title": "Detection and Classification in Ballistic Drone Capturing Based on Transfer Learning Networks Using Infrared Images",
      "abstract": "Unmanned aerial vehicles and drones are flying objects carrying small payloads for various applications, including agriculture, photography for weddings & religious ceremonies, e-commerce deliveries, remote sensing, 3D mapping, surveillance and defense. These flying objects also called as drones are available in two variants; 1. Fixed wing propeller type, and 2. Multi-wings propeller type. Nowadays, UAVs and Drones are the primary threat to the national security on the mainland and the line of actual control. Generally, these threat-rising drones can be eradicated by anti-missile techniques, which are not economical however will destroy them. The objective of the work here is to design and develop a ballistic drone capturing system comprising two main steps; 1. classification of UAVs from the flying birds and 2. Capturing the UAVs without causing significant damage to the drone. Here, the methodology uses computer-generated infrared images of the drone in the pretrained network architecture, which acts as the Convolution Neural Network (CNN) for classifying drones and birds. The performance of AlexNet and ResNet-50 performs better and it shows 100 percent efficiency and small distribution with more stability. In the near future, the ballistic capturing system will be designed and developed as a product, which would support acquiring data about malicious drones.",
      "year": 2022,
      "venue": "Conference Information and Communication Technology",
      "authors": [
        "Harsha Vardhan Reddy M",
        "Rajkumar Gothandaraman",
        "Sreekumar Muthuswamy"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/60a3d59717aaa25289f29222d3677b06608d9765",
      "pdf_url": "",
      "publication_date": "2022-11-18",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4e4356c709e47cf1c0c59eacc324d9b43df63d02",
      "title": "Transfer Learning and Domain Adaptation in Hyperspectral Image Processing: an Overview",
      "abstract": "Fine-tuning a pre-trained model is a type of transfer learning: a pre-trained model that works on a large dataset is transferred onto a particular task or a smaller dataset, such as a hyperspectral image. The efficacy of such a technique is most visible in hyperspectral imaging since the data is usually too high-dimensional with a very scarce number of available annotated datasets. By using the pre-trained model, fine-tuning enables spatial and spectral features to be captured with lower computational overheads than fully training models. Domain adaptation approaches, including feature alignment and adversarial training, are also adapted to overcome limitations caused by the domain shift phenomenon due to the variability of the sensor, environmental changes, and temporal changes. Hyperspectral imaging is applied in diverse fields, including agriculture, environmental monitoring, mineral exploration, defense, medical diagnostics, and food quality assessment. Such applications are unique in the sense that HSI can integrate spatial and spectral information to achieve detailed material analysis. Advanced learning techniques will enhance the effectiveness of HSI models in real-world scenarios.",
      "year": 2025,
      "venue": "2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)",
      "authors": [
        "Komal Pokale",
        "S. N. Chaudhri"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4e4356c709e47cf1c0c59eacc324d9b43df63d02",
      "pdf_url": "",
      "publication_date": "2025-03-10",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "98ef0131366a016d726ec5a746d9757743447fd9",
      "title": "CBCTL-IDS: A Transfer Learning-Based Intrusion Detection System Optimized With the Black Kite Algorithm for IoT-Enabled Smart Agriculture",
      "abstract": "Smart Agriculture(SA) stands as a critical application frontier for the evolution of the IoT, catalyzing the transformation of traditional agricultural practices into modern, data-driven, intelligent, and automated systems. This transition not only elevates the levels of automation and intelligence in agricultural production but also introduces a spectrum of cybersecurity challenges. In particular, the widespread deployment of interconnected devices within these systems poses significant risks to the security and integrity of data. As a pivotal defense mechanism against malicious activities and anomalous traffic, Network Intrusion Detection Systems (NIDS) play an indispensable role in ensuring the security of cyberspace. Nevertheless, existing methods for detecting anomalous traffic in IoT networks still exhibit limitations in accuracy, thereby hindering the efficiency of identifying potential threats. To enhance the capability of threat detection, this paper proposes a novel network intrusion detection method, CBCTL-IDS, based on transfer learning. This method integrates three core components: Convolutional Neural Networks (CNN), the Black Kite Algorithm (BKA), and a Confidence Averaging mechanism. By incorporating pre-trained models such as MobileNet, EfficientNet, Xception, VGG19, and Inception into the domain of IoT intrusion detection, CBCTL-IDS leverages transfer learning to achieve efficient feature extraction and classification, addressing the limitations of traditional methods in feature representation. Furthermore, the BKA is utilized to adaptively optimize the hyperparameters of multiple DL models, significantly enhancing their classification performance. Through ensemble learning, the top three performing models are selected, and the Confidence Averaging mechanism is applied to further improve the stability and reliability of detection results. Experimental results demonstrate that the CBCTL-IDS method achieves detection accuracy rates exceeding 99% on three IoT intrusion detection datasets: ToN-IoT, Edge-IIoTset, and WSN-DS significantly outperforming existing mainstream methods. This approach provides robust technical support for the security protection of IoT systems.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Hai Zhou",
        "Haijie Zou",
        "Pinxi Zhou",
        "Yue Shen",
        "Di Li",
        "Wei Li"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/98ef0131366a016d726ec5a746d9757743447fd9",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aead3a16019f85630dddd410f256207dc0e44c10",
      "title": "Detecting malicious code variants using convolutional neural network (CNN) with transfer learning",
      "abstract": "Malware presents a significant threat to computer networks and devices that lack robust defense mechanisms, despite the widespread use of anti-malware solutions. The rapid growth of the Internet has led to an increase in malicious code attacks, making them one of the most critical challenges in network security. Accurate identification and classification of malware variants are crucial for preventing data theft, security breaches, and other cyber risks. However, existing malware detection methods are often inefficient or inaccurate. Prior research has explored converting malicious code into grayscale images, but these approaches are often computationally intensive, especially in binary form. To address these challenges, we propose the Malware Variants Detection System (MVDS), a novel technique that transforms malicious code into color images, enhancing malware detection capabilities compared to traditional methods. Our approach leverages the richer information in color images to achieve higher classification accuracy than grayscale-based methods. We further improve the detection process by employing transfer learning to automatically identify and classify malware images based on their distinctive features. Empirical results demonstrate that MVDS achieves 97.98% accuracy with high detection speed, highlighting its potential for practical implementation in strengthening network security.",
      "year": 2025,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Nazish Younas",
        "Shazia Riaz",
        "Saqib Ali",
        "R. Khan",
        "Farman Ali",
        "Daehan Kwak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/aead3a16019f85630dddd410f256207dc0e44c10",
      "pdf_url": "",
      "publication_date": "2025-04-04",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ce2623944fec43aac632b8caf0e237d8ed015a75",
      "title": "Adversarial defense based on distribution transfer",
      "abstract": "The presence of adversarial examples poses a significant threat to deep learning models and their applications. Existing defense methods provide certain resilience against adversarial examples, but often suffer from decreased accuracy and generalization performance, making it challenging to achieve a trade-off between robustness and generalization. To address this, our paper interprets the adversarial example problem from the perspective of sample distribution and proposes a defense method based on distribution shift, leveraging the distribution transfer capability of a diffusion model for adversarial defense. The core idea is to exploit the discrepancy between normal and adversarial sample distributions to achieve adversarial defense using a pretrained diffusion model. Specifically, an adversarial sample undergoes a forward diffusion process, moving away from the source distribution, followed by a reverse process guided by the protected model (victim model) output to map it back to the normal distribution. Experimental evaluations on CIFAR10 and ImageNet30 datasets are conducted, comparing with adversarial training and input preprocessing methods. For infinite-norm attacks with 8/255 perturbation, accuracy rates of 78.1% and 83.5% are achieved, respectively. For 2-norm attacks with 128/255 perturbation, accuracy rates are 74.3% and 82.5%. Additional experiments considering perturbation amplitude, diffusion iterations, and adaptive attacks also validate the effectiveness of the proposed method. Results demonstrate that even when the attacker has knowledge of the defense, the proposed distribution-based method effectively withstands adversarial examples. It fills the gaps of traditional approaches, restoring high-quality original samples and showcasing superior performance in model robustness and generalization.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jiahao Chen",
        "Diqun Yan",
        "Li Dong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ce2623944fec43aac632b8caf0e237d8ed015a75",
      "pdf_url": "",
      "publication_date": "2023-11-23",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a84d569eded5eec351eb20b6e83b348bd811c3a",
      "title": "A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion",
      "abstract": "Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role in UAV defense systems. Deep learning is widely adopted for UAV object detection whereas researches on this topic are limited by the amount of dataset and small scale of UAV. To tackle these problems, a novel comprehensive approach that combines transfer learning based on simulation data and adaptive fusion is proposed. Firstly, the open-source plugin AirSim proposed by Microsoft is used to generate mass realistic simulation data. Secondly, transfer learning is applied to obtain a pre-trained YOLOv5 model on the simulated dataset and fine-tuned model on the real-world dataset. Finally, an adaptive fusion mechanism is proposed to further improve small object detection performance. Experiment results demonstrate the effectiveness of simulation-based transfer learning which leads to a 2.7% performance increase on UAV object detection. Furthermore, with transfer learning and adaptive fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5 model.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Rui Chen",
        "Youwei Guo",
        "Huafei Zheng",
        "Hongyu Jiang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/3a84d569eded5eec351eb20b6e83b348bd811c3a",
      "pdf_url": "",
      "publication_date": "2021-09-04",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b87a6dabeec34117e06a65c615841dd41ff31b79",
      "title": "Aspects of hyperdimensional computing for robotics: transfer learning, cloning, extraneous sensors, and network topology",
      "abstract": "Hyperdimensional computing (HDC) is a type of machine learning algorithm but is not based on the ubiquitous artificial neural network (ANN) paradigm. Instead of neurons and synapses, HDC implements online learning via very large vectors manipulated to represent correlations among the various vectors, measured by a similarity metric. Yet this approach readily affords one-shot learning, transfer learning, and native error correction, which are standing challenges for traditional ANNs. Further, implementations using binary vectors {0,1} are particularly attractive for size, weight, and power (SWaP) constrained systems, particularly disposable robotics. The paper is the first to identify and formalize a method to completely clone trained hyperdimensional behavior vectors. Using shift maps, d-1 unique clones can be made from a parent vector of length d. Additionally, expeditionary robots with extraneous sensors were trained via HDC to solve a maze even when up to 75% of the sensors fed irrelevant data to the robot. Lastly, we demonstrated the resiliency of this encoding method to random bit flips and how different network topologies contribute to dynamic reprogramming of HDC robots. HDC is presented here though not to replace ANNs but to encourage integration of these complementary ML paradigms.",
      "year": 2021,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "N. McDonald",
        "Richard Davis",
        "Lisa Loomis",
        "Johan Kopra"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b87a6dabeec34117e06a65c615841dd41ff31b79",
      "pdf_url": "https://www.utupub.fi/bitstream/10024/159299/1/2021-01-11%20aspects%20of%20HDC%20for%20robotics%20%28003%29.pdf",
      "publication_date": "2021-04-12",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e204a750c867652a9e2f9af589b5fc9f469620a9",
      "title": "Chest x-ray classification using transfer learning on multi-GPU",
      "abstract": "Since the first quarter of this year, the spread of SARS-CoV-19 virus has been a worldwide health priority. Medical testing consists of Lab studies, PCR tests, CT, PET, which are time-consuming, some countries lack these resources. One medical tool for diagnosis is X-Ray imaging, which is one of the fastest and low-cost resources for physicians to detect and to distinguish among these different diseases. We propose an X-Ray CAD system based on DCNN, using well-known architectures such as DenseNet-201, ResNet-50 and EfficientNet. These architectures are pre-trained on data from Imagenet classification challenge, moreover, using Transfer Learning methods to Fine-Tune the classification stage. The system is capable to visualize the learned recognition patterns applying the GRAD-CAM algorithm aiming to help physicians in seeking hidden features from perceptual vision. The proposed CAD can differentiate between COVID-19, Pneumonia, Nodules and Normal lung X-Ray images.",
      "year": 2021,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "V. Ponomaryov",
        "J. Almaraz-Damian",
        "Rogelio Reyes-Reyes",
        "Clara Cruz-Ramos"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e204a750c867652a9e2f9af589b5fc9f469620a9",
      "pdf_url": "",
      "publication_date": "2021-04-12",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9c333f6a1d142a7d6341caee27be96f0d1044fad",
      "title": "RL-VAEGAN: Adversarial defense for reinforcement learning agents via style transfer",
      "abstract": null,
      "year": 2021,
      "venue": "Knowledge-Based Systems",
      "authors": [
        "Yueyue Hu",
        "Shiliang Sun"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/9c333f6a1d142a7d6341caee27be96f0d1044fad",
      "pdf_url": "",
      "publication_date": "2021-03-17",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e349699badb71b2d9e2a25f70f4549c596af12ea",
      "title": "A Study on Adversarial Sample Resistance and Defense Mechanism for Multimodal Learning-Based Phishing Website Detection",
      "abstract": "Recent advancements in Artificial Intelligence (AI) have greatly impacted cybersecurity, particularly in detecting phishing websites. Traditional methods struggle to address evolving vulnerabilities, but research shows that Machine Learning (ML), Ensemble Learning (EL), and Deep Learning (DL) are effective in developing defenses. However, these methods face challenges with adversarial examples (AEs). The multimodal model (MM) is a promising solution, yet there is a significant lack of research using multimodal techniques specifically for phishing website detection (PWD) against adversarial websites. To tackle this challenge, this paper assesses 15 learning-based models, particularly multimodal ones, for phishing and adversarial detection, aiming to enhance their defense capabilities. Due to the scarcity of adversarial websites, training and testing models are limited. Therefore, this study proposes an innovative attack framework, AWG - Adversarial Website Generation that employs Generative Adversarial Networks (GAN) and transfer-based black box attacks to create AEs. This framework closely mirrors real-world attack scenarios, ensuring high effectiveness and realism. Finally, we present defense strategies with straightforward implementation and high effectiveness to enhance the resistance of models. The models underwent training and testing on a dataset collected from reputable sources such as OpenPhish, PhishTank, Phishing Database, and Alexa. This approach was chosen to ensure the dataset\u2019s diversity and relevance to reflect real-world conditions. Experimental results highlight that the Generator\u2019s effectiveness is demonstrated by a domain structure generation rate exceeding 90%. Moreover, AEs generated by this Generator effectively bypass most state-of-the-art ML, DL, and EL models with an evasion rate of up to 88%. Notably, the Support Vector Machine (SVM) model is the most vulnerable, with a detection rate of only 10.02%. On the other hand, the MM Shark-Eyes demonstrates outstanding resistance against AEs, with a detection rate of up to 99%. Upon applying our defense strategy, the resistance of models is significantly boosted, with all detection rates surpassing 90%. These findings underscore the robustness of our methods and pave the way for further exploration into advanced attack and defense strategies in the context of phishing and adversarial website detection.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Phan The Duy",
        "V. Minh",
        "Bui Tan Hai Dang",
        "Ngo Duc Hoang Son",
        "N. H. Quyen",
        "Van-Hau Pham"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/e349699badb71b2d9e2a25f70f4549c596af12ea",
      "pdf_url": "https://doi.org/10.1109/access.2024.3436812",
      "publication_date": null,
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82e0fcaa033bbf0e3f351da9d72f0d478a1f229e",
      "title": "Convolutional and generative pairing for SAR cross-target transfer learning",
      "abstract": "Machine learning systems are known to require large amounts of data to effectively generalize. When this data isn\u2019t available, synthetically generated data is often used in its place. With synthetic aperture radar (SAR) imagery, the domain shift required to effectively transfer knowledge from simulated to measured imagery is non-trivial. We propose a pairing of convolutional networks (CNNs) with generative adversarial networks (GANs) to learn an effective mapping between the two domains. Classification networks are trained individually on measured and synthetic data, then a mapping between layers of the two CNNs is learned using a GAN.",
      "year": 2021,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "Alexander Jennison",
        "B. Lewis",
        "Ashley DeLuna",
        "Jonathan Garrett"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/82e0fcaa033bbf0e3f351da9d72f0d478a1f229e",
      "pdf_url": "",
      "publication_date": "2021-04-12",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb3d8253ab6c3aaf7cb25b175322f76dd68c2fca",
      "title": "TL-NID: Deep Neural Network with Transfer Learning for Network Intrusion Detection",
      "abstract": "Network intrusion detection systems (NIDSs) play an essential role in the defense of computer networks by identifying a computer networks' unauthorized access and investigating potential security breaches. Traditional NIDSs encounters difficulties to combat newly created sophisticated and unpredictable security attacks. Hence, there is an increasing need for automatic intrusion detection solution that can detect malicious activities more accurately and prevent high false alarm rates (FPR). In this paper, we propose a novel network intrusion detection framework using a deep neural network based on the pretrained VGG-16 architecture. The framework, TL-NID (Transfer Learning for Network Intrusion Detection), is a two-step process where features are extracted in the first step, using VGG-16 pre-trained on ImageNet dataset and in the 2ndstep a deep neural network is applied to the extracted features for classification. We applied TL-NID on NSL-KDD, a benchmark dataset for network intrusion, to evaluate the performance of the proposed framework. The experimental results show that our proposed method can effectively learn from the NSL-KDD dataset with producing a realistic performance in terms of accuracy, precision, recall, and false alarm. This study also aims to motivate security researchers to exploit different state-of-the-art pre-trained models for network intrusion detection problems through valuable knowledge transfer.",
      "year": 2020,
      "venue": "International Conference for Internet Technology and Secured Transactions",
      "authors": [
        "Mohammad Masum",
        "Hossain Shahriar"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/bb3d8253ab6c3aaf7cb25b175322f76dd68c2fca",
      "pdf_url": "",
      "publication_date": "2020-12-08",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "926130929b399deb7b3ddeb8421088c3bf55d2e7",
      "title": "Toward Secure Data Fusion in Industrial IoT Using Transfer Learning",
      "abstract": null,
      "year": 2020,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Hui Lin",
        "Jia Hu",
        "Xiaoding Wang",
        "Mohammed F. Alhamid",
        "Md. Jalil Piran"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/926130929b399deb7b3ddeb8421088c3bf55d2e7",
      "pdf_url": "",
      "publication_date": "2020-11-17",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "362011f81ca3aa1b4942ea7f6c270c739f1eafb6",
      "title": "Transfer Learning-based Mobile-focused Automated COVID-19 Detection from Chest X-ray",
      "abstract": "COVID-19 Pandemic is still a global issue that threatens global health. To combat the pandemic, testing activities has been the first line of defense. However, increasing number of infections resulted in insufficient number of laboratory kits to perform the test. One potential testing method is using transfer learning for automated detection of COVID-19 from chest x-ray image. We create a model used pretrained model of MobileNetV3Large as a feature extractor, and a custom classification layer. We train the model on dataset consisting of chest x-ray image from 10,192 healthy cases, 3,616 COVID-19 cases, 1,345 Viral Pneumonia cases, and 6,012 Lung Opacity cases. The model achieved macro-average accuracy performance of 89.08%, F1 score of 88.10%, Precision of 91.95%, Sensitivity of 85.51%, and Specificity of 95.26%. Comparison with previous models trained on smaller dataset showed that achieved performance is lower and indicates previous research\u2019s model won\u2019t be able to maintain its performance when evaluated on larger sets of data.",
      "year": 2021,
      "venue": "2021 International Conference on Artificial Intelligence and Big Data Analytics",
      "authors": [
        "M. A. Febriantono",
        "R. Herasmara",
        "Anita Rahayu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/362011f81ca3aa1b4942ea7f6c270c739f1eafb6",
      "pdf_url": "",
      "publication_date": "2021-10-27",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bed85da67acef074d8cdff63eeadf97299d2f526",
      "title": "Safeguarding the Smart Home: Heterogeneous Federated Deep Learning for Intrusion Defense",
      "abstract": "This study introduces an advanced federated learning framework tailored for smart home intrusion detection, incorporating knowledge distillation and transfer learning to tackle escalating threats to IoT devices. In light of the rapid expansion of IoT devices and their vulnerability to botnet incursions, our approach specifically addresses the challenges related to the privacy concerns of home device data, heterogeneity of devices, sparse intrusion data, and the dynamic nature of smart home settings. We adaptively select model architectures tailored to the computational capabilities of each device, ranging from simple Neural Networks (NNs) to more complex Convolutional Neural Networks (CNNs) and hybrid CNN-LSTM models, ensuring efficient local training without overburdening the devices. However, it can achieve good performance through collaborative learning, even for devices with lower capacity and sparse data. Our evaluation, conducted using the N-BaIoT dataset, demonstrates the effectiveness of our approach in detecting anomalies across a diverse set of IoT devices infected with real-world botnets such as Mirai and BASHLITE. The results highlight the potential of our framework to provide a robust, privacy-preserving, and adaptable solution for securing smart homes against emerging threats.",
      "year": 2024,
      "venue": "IEEE International Conference on Mobile Cloud Computing, Services, and Engineering",
      "authors": [
        "Mohammed Shalan",
        "Juan Li",
        "Yan Bai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bed85da67acef074d8cdff63eeadf97299d2f526",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "73b4b45b8ece4c125b1e1b2f7964ef131e625d5d",
      "title": "Parametric Cyber Defense: A Sophisticated Machine Learning Architecture for Advanced Intrusion Detection and Threat Classification",
      "abstract": "In contemporary network environments, effective countermeasures against cybersecurity threats require highly effective Intrusion Detection Systems (IDS). This paper proposes an advanced methodology to enhance IDS by employing parametric analysis on network traffic characteristics. Through careful data selection and preprocessing from a comprehensive dataset, data integrity is maintained, with features such as packet size and transfer duration being leveraged. Machine learning algorithms, including Decision Tree, Extra Tree, and xgBoost, are applied to detect and classify various cyberattack categories. In addition, advanced feature selection and dimensionality reduction techniques are utilized to improve model efficiency. Extensive experimentation demonstrates superior accuracy in identifying and classifying cyber threats while reducing the occurrence of false positives. The proposed framework strengthens cybersecurity infrastructures through the integration of advanced machine learning techniques to address evolving threats.",
      "year": 2024,
      "venue": "2024 5th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)",
      "authors": [
        "Lakshmi S",
        "Maalan M R",
        "Kishore Kumar R"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/73b4b45b8ece4c125b1e1b2f7964ef131e625d5d",
      "pdf_url": "",
      "publication_date": "2024-11-18",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a41f555b7d961fc72d10882711159fb3221e857",
      "title": "Adversarial Transfer Attack Against and Adaptive Defense for Intelligent Modulation Recognition",
      "abstract": "Deep learning (DL) can enhance automatic modulation recognition in complex wireless communication environments by greatly improving the efficiency and accuracy of modulation recognition. However, the DL-based intelligent recognition model also has great vulnerability to the adversarial attack. To avoid this, a subtle adversarial perturbation can be added to the signal to disrupt the eavesdropper\u2019s modulation recognition model while ensuring the normal demodulation at the receiving end. However, the transferability of existing attacks is weak, and it is difficult to disrupt the target model while ensuring concealment. This paper proposes a meta adversarial transfer attack method by using meta-training and meta-testing to enhance the transferability of adversarial attacks. In addition, for the transfer attack, this paper proposes an adversarial domain adaptation defense method, which uses the multi-class domain-invariant features of the examples in the source domain and the adversarial domain to improve the robustness of the model against attacks. Simulation results show that, compared with the traditional methods, the proposed attack method has better transferability for the eavesdropper\u2019s model, and that the proposed defense method has better defense against the transfer attack.",
      "year": 2026,
      "venue": "IEEE Transactions on Cognitive Communications and Networking",
      "authors": [
        "Zhenju Zhang",
        "Linru Ma",
        "Mingqian Liu",
        "Yunfei Chen",
        "Nan Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3a41f555b7d961fc72d10882711159fb3221e857",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39421e9ecb90eb321335bd8bf4d4cef9c2701c83",
      "title": "Flexible deep transfer learning by separate feature embeddings and manifold alignment",
      "abstract": "Object recognition is a key enabler across industry and defense. As technology changes, algorithms must keep pace with new requirements and data. New modalities and higher resolution sensors should allow for increased algorithm robustness. Unfortunately, algorithms trained on existing labeled datasets do not directly generalize to new data because the data distributions do not match. Transfer learning (TL) or domain adaptation (DA) methods have established the groundwork for transferring knowledge from existing labeled source data to new unlabeled target datasets. However, current DA approaches assume similar source and target feature spaces and suffer in the case of massive domain shifts or changes in the feature space. Existing methods assume the data are either the same modality, or can be aligned to a common feature space. Therefore, most methods are not designed to support a fundamental domain change such as visual to auditory data. We propose a novel deep learning framework that overcomes this limitation by learning separate feature extractions for each domain while minimizing the distance between the domains in a latent lower-dimensional space. The alignment is achieved by considering the data manifold along with an adversarial training procedure. We demonstrate the effectiveness of the approach versus traditional methods with several ablation experiments on synthetic, measured, and satellite image datasets. We also provide practical guidelines for training the network while overcoming vanishing gradients which inhibit learning in some adversarial training settings.",
      "year": 2020,
      "venue": "Defense + Commercial Sensing",
      "authors": [
        "Samuel Rivera",
        "J. Klipfel",
        "Deborah Weeks"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/39421e9ecb90eb321335bd8bf4d4cef9c2701c83",
      "pdf_url": "https://arxiv.org/pdf/2012.12302",
      "publication_date": "2020-04-24",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40e5672888cfc33f966b950d1993ec6843611189",
      "title": "Improving convolutional neural networks for buried target detection in ground penetrating radar using transfer learning via pretraining",
      "abstract": null,
      "year": 2017,
      "venue": "Defense + Security",
      "authors": [
        "John Bralich",
        "Daniel Reichman",
        "L. Collins",
        "Jordan M. Malof"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/40e5672888cfc33f966b950d1993ec6843611189",
      "pdf_url": "",
      "publication_date": "2017-05-03",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f0c671c15c38ed78dcdc609b724e9087d28659e",
      "title": "Deep Convolutional Neural Networks for plane identification on Satellite imagery by exploiting transfer learning with a different optimizer",
      "abstract": "Object identification is on an available problem. Automating plane identification on Satellite imagery can be applied for activity and traffic patterns to monitoring airports, and including defense intelligence issues. This paper implements Deep Convolutional Neural Networks(CNN) to classify a plane in the planesnet dataset. Pre-trained model and transfer learning are deployed to overcome a limitation of computation resources by adding new top layer consists of a fully-connected layer and softmax layer to identify the new classes and re-train it. Besides, the experimental designs for testing an implementation of a pretrained model with some kinds of the optimizer to comparing a result. There are four types of optimizer. The first two are well-known optimizer namely Stochastic Gradient Descent optimizer and Adam Optimizer, while others are PowerSign and AddSign optimizer. PowerSign and AddSign optimizer are methods to minimize cost, which discover by using Recurrent neural network(RNN) and Reinforcement Learning. A result demonstrates that a plane identification on Satellite imagery can be achieved by implementing the pre-trained model and obtains an exceptional result with Adam optimizer.",
      "year": 2019,
      "venue": "IEEE International Geoscience and Remote Sensing Symposium",
      "authors": [
        "Patcharin Kamsing",
        "Peerapong Torteeka",
        "S. Yooyen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/5f0c671c15c38ed78dcdc609b724e9087d28659e",
      "pdf_url": "",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1168d8a0989f94e801644cfa8ec37787a13da467",
      "title": "Transfer Learning for Aided Target Recognition: Comparing Deep Learning to other Machine Learning Approaches",
      "abstract": "Aided target recognition (AiTR), the problem of classifying objects from sensor data, is an important problem with applications across industry and defense. While classification algorithms continue to improve, they often require more training data than is available or they do not transfer well to settings not represented in the training set. These problems are mitigated by transfer learning (TL), where knowledge gained in a well-understood source domain is transferred to a target domain of interest. In this context, the target domain could represents a poorly-labeled dataset, a different sensor, or an altogether new set of classes to identify. While TL for classification has been an active area of machine learning (ML) research for decades, transfer learning within a deep learning framework remains a relatively new area of research. Although deep learning (DL) provides exceptional modeling flexibility and accuracy on recent real world problems, open questions remain regarding how much transfer benefit is gained by using DL versus other ML architectures. Our goal is to address this shortcoming by comparing transfer learning within a DL framework to other ML approaches across transfer tasks and datasets. Our main contributions are: 1) an empirical analysis of DL and ML algorithms on several transfer tasks and domains including gene expressions and satellite imagery, and 2) a discussion of the limitations and assumptions of TL for aided target recognition -- both for DL and ML in general. We close with a discussion of future directions for DL transfer.",
      "year": 2019,
      "venue": "Automatic Target Recognition XXIX",
      "authors": [
        "Samuel Rivera",
        "O. Mendoza-Schrock",
        "Ashley Diehl"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1168d8a0989f94e801644cfa8ec37787a13da467",
      "pdf_url": "https://arxiv.org/pdf/2011.12762",
      "publication_date": "2019-05-14",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "574afcdf2c4b63f5095136096545f1c624e1a74b",
      "title": "Blending synthetic and measured data using transfer learning for synthetic aperture radar (SAR) target classification",
      "abstract": null,
      "year": 2018,
      "venue": "Defense + Security",
      "authors": [
        "J. M. Arnold",
        "L. Moore",
        "E. Zelnio"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/574afcdf2c4b63f5095136096545f1c624e1a74b",
      "pdf_url": "",
      "publication_date": "2018-04-27",
      "keywords_matched": [
        "transfer learning defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "adb762a645b72fc4605e6fb512ef2684db37cc93",
      "title": "Adversarial and Clean Data Are Not Twins",
      "abstract": "Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into the wrong predictions with very high confidence. Adversarial training, as the most prevailing defense technique, suffers from class-wise unfairness and model-dependent challenges. In this paper, we propose to detect and eliminate adversarial data in databases prior to data processing in supporting robust and secure AI workloads. We empirically show that we can build a binary classifier separating the adversarial apart from the clean data with high accuracy. We also show that the binary classifier is robust to a second-round adversarial attack. In other words, it is difficult to disguise adversarial samples to bypass the binary classifier. Furthermore, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of the intrinsic property of adversarial crafting algorithms. Our experiments ascertain that adversarial and clean data are two different datasets that can be separated with a binary classifier, which can serve as a portable component to detect and eliminate adversarial data in an end-to-end data management pipeline.",
      "year": 2017,
      "venue": "aiDM@SIGMOD",
      "authors": [
        "Zhitao Gong",
        "Wenlu Wang",
        "Wei-Shinn Ku"
      ],
      "citation_count": 164,
      "url": "https://www.semanticscholar.org/paper/adb762a645b72fc4605e6fb512ef2684db37cc93",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3593078.3593935",
      "publication_date": "2017-04-17",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d521ea5f888a6821828f3db345da9a7f1cc9cee3",
      "title": "Adversarial Policy Learning in Two-player Competitive Games",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Wenbo Guo",
        "Xian Wu",
        "Sui Huang",
        "Xinyu Xing"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/d521ea5f888a6821828f3db345da9a7f1cc9cee3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13d796ed06e1dca277ee363efaf2108c56a89801",
      "title": "Mitigating Demographic Bias in Face Recognition via Regularized Score Calibration",
      "abstract": "Demographic bias in deep learning-based face recog-nition systems has led to serious concerns. Several ex-isting works attempt to mitigate bias by incorporating demographic-specific processing during inference, which requires knowledge or learning of demographic attribute with an additional cost. We propose to regularize training of the face recognition CNN, for demographic fairness, by im-posing constraints on the distributions of matching scores. Our regularization term enforces the score distributions from different demographic groups to respect a pre-defined probability distribution, as well as it penalizes misalign-ment of distributions across demographic groups. The pro-posed method improves fairness of face recognition models without compromising the recognition accuracy, and does not require extra resources during inference. Our experi-ments indicate that in a cross-dataset testing, the regular-ized CNN can reduce the variation in accuracies (i.e., more fairness) of different demographic groups up to 25% while slightly improving recognition accuracy over baselines.",
      "year": 2024,
      "venue": "2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "authors": [
        "Ketan Kotwal",
        "S\u00e9bastien Marcel"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/13d796ed06e1dca277ee363efaf2108c56a89801",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d14b893ac4eb35e5fcd6f564821ded32bbad6f5a",
      "title": "Demographic Bias Effects on Face Image Synthesis",
      "abstract": "Face image synthesis has shown remarkable progress in recent years. However, the effect that the demographics of the data used to train synthesizers has on the generation of new face images remains an open question. This paper investigates the effects of the training set demographics in the face image synthesis task. To this end, we propose a strategy that allows synthesizing face images for specific groups of people with a high visual quality. The strategy uses an unsupervised learning approach to discover groups of people in the training set based on Bayesian inference via a probabilistic mixture model. If labels are available to define the groups, our strategy can also exploit such information in lieu of unsupervised learning. Once the groups are defined, our strategy trains a Generative Adversarial Network on each group to generate new face images with specific characteristics. Our results show remarkable performance in terms of image quality compared to several state-of-the-art baselines. More importantly, our strategy allows synthesizing face images with reduced demographic biases.",
      "year": 2024,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Roberto Leyva",
        "Victor Sanchez",
        "Gregory Epiphaniou",
        "Carsten Maple"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d14b893ac4eb35e5fcd6f564821ded32bbad6f5a",
      "pdf_url": "",
      "publication_date": "2024-06-17",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0352145209a3f9c27504cbacb736343b8f18ecae",
      "title": "Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition",
      "abstract": "The increasing amount of applications of Artificial Intelligence (AI) has led researchers to study the social impact of these technologies and evaluate their fairness. Unfortunately, current fairness metrics are hard to apply in multi-class multi-demographic classification problems, such as Facial Expression Recognition (FER). We propose a new set of metrics to approach these problems. Of the three metrics proposed, two focus on the representational and stereotypical bias of the dataset, and the third one on the residual bias of the trained model. These metrics combined can potentially be used to study and compare diverse bias mitigation methods. We demonstrate the usefulness of the metrics by applying them to a FER problem based on the popular Affectnet dataset. Like many other datasets for FER, Affectnet is a large Internet-sourced dataset with 291,651 labeled images. Obtaining images from the Internet raises some concerns over the fairness of any system trained on this data and its ability to generalize properly to diverse populations. We first analyze the dataset and some variants, finding substantial racial bias and gender stereotypes. We then extract several subsets with different demographic properties and train a model on each one, observing the amount of residual bias in the different setups. We also provide a second analysis on a different dataset, FER+.",
      "year": 2022,
      "venue": "AISafety@IJCAI",
      "authors": [
        "Iris Dominguez-Catena",
        "D. Paternain",
        "M. Galar"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0352145209a3f9c27504cbacb736343b8f18ecae",
      "pdf_url": "http://arxiv.org/pdf/2205.10049",
      "publication_date": "2022-05-20",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "629730d6e23ad6e6e49db648ccd1f2af5d6bf7e3",
      "title": "Mitigating Demographic Bias in Facial Datasets with Style-Based Multi-attribute Transfer",
      "abstract": "Deep learning has catalysed progress in tasks such as face recognition and analysis, leading to a quick integration of technological solutions in multiple layers of our society. While such systems have proven to be accurate by standard evaluation metrics and benchmarks, a surge of work has recently exposed the demographic bias that such algorithms exhibit\u2013highlighting that accuracy does not entail fairness. Clearly, deploying biased systems under real-world settings can have grave consequences for affected populations. Indeed, learning methods are prone to inheriting, or even amplifying the bias present in a training set, manifested by uneven representation across demographic groups. In facial datasets, this particularly relates to attributes such as skin tone, gender, and age. In this work, we address the problem of mitigating bias in facial datasets by data augmentation. We propose a multi-attribute framework that can successfully transfer complex, multi-scale facial patterns even if these belong to underrepresented groups in the training set. This is achieved by relaxing the rigid dependence on a single attribute label, and further introducing a tensor-based mixing structure that captures multiplicative interactions between attributes in a multilinear fashion. We evaluate our method with an extensive set of qualitative and quantitative experiments on several datasets, with rigorous comparisons to state-of-the-art methods. We find that the proposed framework can successfully mitigate dataset bias, as evinced by extensive evaluations on established diversity metrics, while significantly improving fairness metrics such as equality of opportunity.",
      "year": 2021,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Markos Georgopoulos",
        "James Oldfield",
        "M. Nicolaou",
        "Yannis Panagakis",
        "M. Pantic"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/629730d6e23ad6e6e49db648ccd1f2af5d6bf7e3",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s11263-021-01448-w.pdf",
      "publication_date": "2021-05-15",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "89d2875a6bca91942d6b26d5c148224bcef63228",
      "title": "Demographic Fairness Transformer for Bias Mitigation in Face Recognition",
      "abstract": "Demographic bias in deep learning-based face recognition systems has led to serious concerns. Often, the biased nature of models is attributed to severely imbalanced datasets used for training. However, several studies have shown that biased models can emerge even when trained on balanced data due to factors in the data acquisition process. Considering the impact of input data on demographic bias, we propose an image to image transformer for demographic fairness (DeFT). This transformer can be applied before the pretrained recognition CNN to selectively enhance the image representation with the goal of reducing the bias through overall recognition pipeline. The multi-head encoders of DeFT provide multiple transformation paths to the input which are then combined based on its demographic information implicitly inferred through soft-attention mechanism applied to intermittent layers of DeFT. We compute probabilistic weights for demographic information, as opposed to conventional hard labels, simplifying the learning process and enhancing the robustness of the DeFT. Our experiments demonstrate that in a cross-dataset testing (pretrained as well as locally trained models), integrating the DeFT leads to fairer models, reducing the variation in accuracies while often slightly improving average recognition accuracy over baselines.",
      "year": 2024,
      "venue": "2024 IEEE International Joint Conference on Biometrics (IJCB)",
      "authors": [
        "Ketan Kotwal",
        "S\u00e9bastien Marcel"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/89d2875a6bca91942d6b26d5c148224bcef63228",
      "pdf_url": "",
      "publication_date": "2024-09-15",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d70c38230b68b6ce52b35ae19a37960cf2254d1e",
      "title": "The Harms of Demographic Bias in Deep Face Recognition Research",
      "abstract": "In this work we demonstrate the existence of demographic bias in the face representations of currently popular deep-learning-based face recognition models, exposing a bad research and development practice that may lead to a systematic discrimination of certain demographic groups in critical scenarios like automated border control. Furthermore, through the simulation of the template morphing attack, we reveal significant security risks that derive from demographic bias in current deep face models. This widely ignored problem poses important questions on fairness and accountability in face recognition.",
      "year": 2019,
      "venue": "International Conference on Biometrics",
      "authors": [
        "R. Vicente-Garcia",
        "Lukasz Wandzik",
        "Louisa Grabner",
        "J. Kr\u00fcger"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d70c38230b68b6ce52b35ae19a37960cf2254d1e",
      "pdf_url": "",
      "publication_date": "2019-06-01",
      "keywords_matched": [
        "demographic bias"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fe3b5fe7b70803d19c758df74c49432ec0b6fdf5",
      "title": "Unravelling the Effect of Image Distortions for Biased Prediction of Pre-trained Face Recognition Models",
      "abstract": "Identifying and mitigating bias in deep learning algorithms has gained significant popularity in the past few years due to its impact on the society. Researchers argue that models trained on balanced datasets with good representation provide equal and unbiased performance across subgroups. However, can seemingly unbiased pre-trained model become biased when input data undergoes certain distortions? For the first time, we attempt to answer this question in the context of face recognition. We provide a systematic analysis to evaluate the performance of four state-of-the-art deep face recognition models in the presence of image distortions across different gender and race subgroups. We have observed that image distortions have a relationship with the performance gap of the model across different subgroups.",
      "year": 2021,
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "authors": [
        "P. Majumdar",
        "S. Mittal",
        "Richa Singh",
        "Mayank Vatsa"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/fe3b5fe7b70803d19c758df74c49432ec0b6fdf5",
      "pdf_url": "https://arxiv.org/pdf/2108.06581",
      "publication_date": "2021-08-14",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b79fe48ae523dc66185aa04df2dac7041afa8683",
      "title": "Learning Not to Learn: Training Deep Neural Networks With Biased Data",
      "abstract": "We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Byungju Kim",
        "Hyunwoo Kim",
        "Kyungsu Kim",
        "Sungjin Kim",
        "Junmo Kim"
      ],
      "citation_count": 442,
      "url": "https://www.semanticscholar.org/paper/b79fe48ae523dc66185aa04df2dac7041afa8683",
      "pdf_url": "https://arxiv.org/pdf/1812.10352",
      "publication_date": "2018-12-26",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec570b827cf0cd132da7ebd37537df4f0bb7f877",
      "title": "Learning De-biased Representations with Biased Representations",
      "abstract": "Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at this https URL.",
      "year": 2019,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Hyojin Bahng",
        "Sanghyuk Chun",
        "Sangdoo Yun",
        "J. Choo",
        "Seong Joon Oh"
      ],
      "citation_count": 310,
      "url": "https://www.semanticscholar.org/paper/ec570b827cf0cd132da7ebd37537df4f0bb7f877",
      "pdf_url": "",
      "publication_date": "2019-10-07",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d23662d2275498f4fa7a937dabd60fbc7faf9c14",
      "title": "ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation",
      "abstract": "Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming way-points in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of Mani-pLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiaoqi Li",
        "Mingxu Zhang",
        "Yiran Geng",
        "Haoran Geng",
        "Yuxing Long",
        "Yan Shen",
        "Renrui Zhang",
        "Jiaming Liu",
        "Hao Dong"
      ],
      "citation_count": 175,
      "url": "https://www.semanticscholar.org/paper/d23662d2275498f4fa7a937dabd60fbc7faf9c14",
      "pdf_url": "https://arxiv.org/pdf/2312.16217",
      "publication_date": "2023-12-24",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "63857fddb2a4bdf9d9cf88f6dd4eceaf09cadef1",
      "title": "Video Prediction Models as Rewards for Reinforcement Learning",
      "abstract": "Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://escontrela.me/viper",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Alejandro Escontrela",
        "Ademi Adeniji",
        "Wilson Yan",
        "Ajay Jain",
        "Xue Bin Peng",
        "Ken Goldberg",
        "Youngwoon Lee",
        "Danijar Hafner",
        "P. Abbeel"
      ],
      "citation_count": 85,
      "url": "https://www.semanticscholar.org/paper/63857fddb2a4bdf9d9cf88f6dd4eceaf09cadef1",
      "pdf_url": "http://arxiv.org/pdf/2305.14343",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "66a0c26cd7529d6e10f2b4ca01f7259fe0ef55da",
      "title": "AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation",
      "abstract": "Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Mayukh Deb",
        "Bjorn Deiseroth",
        "Samuel Weinbach",
        "Manuel Brack",
        "P. Schramowski",
        "K. Kersting"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/66a0c26cd7529d6e10f2b4ca01f7259fe0ef55da",
      "pdf_url": "http://arxiv.org/pdf/2301.08110",
      "publication_date": "2023-01-19",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "673682bcd138ec6705f8c1dc3c8769975e4f0ecb",
      "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
      "abstract": "The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation. Specifically, TrojVLM inserts predetermined target text into output text when encountering poisoned images. Moreover, a novel semantic preserving loss is proposed to ensure the semantic integrity of the original image content. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of TrojVLM in maintaining original semantic content while triggering specific target text outputs. This study not only uncovers a critical security risk in VLMs and image-to-text generation but also sets a foundation for future research on securing multimodal models against such sophisticated threats.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Weimin Lyu",
        "Lu Pang",
        "Teng Ma",
        "Haibin Ling",
        "Chao Chen"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/673682bcd138ec6705f8c1dc3c8769975e4f0ecb",
      "pdf_url": "",
      "publication_date": "2024-09-28",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "db11fd5bd8d1fcfcf8b37e798e21be8e52e3c3f1",
      "title": "Security Attack on Remote Sensing Equipment: PoIs Recognition Based on HW with Bi-LSTM Attention",
      "abstract": "\n Deep learning is an influencer in hardware security applications, which grows up to be an essential tool in hardware security, threats the confidentiality, integrity, and availability of remote sensing equipment. Comparing to traditional physical attack, not only it can greatly reduce the workload of manual selection of POIs (Points of Interests) in security attack and Trojan backdoor, but also replenishes the toolbox for attacking. On account of minute changes between network structure model and hyperparameters constantly affecting the training and attacking effect, literally, deep learning serves as a tool but not key role in hardware security attack, which means it cannot completely replace template attack and other traditional energy attack methods. In this study, we present a method using Bi-LSTM Attention mechanism to focus on the POIs related to Hamming Weight at the last round s-box output. Firstly, it can increase attacking effect and decrease guessing entropy, where attacking FPGA data demonstrates the efficiency of attacking. Secondly, it is different from the traditional template attack and deep learning attack without preprocessing subjecting to raw traces but provides attentional POIs which is the same with artificial selection. Finally, it provides a solution for attacking encrypting equipment running in parallel.\n\u00a0\n",
      "year": 2023,
      "venue": "\u7db2\u969b\u7db2\u8def\u6280\u8853\u5b78\u520a",
      "authors": [
        "Wei Jiang Wei Jiang",
        "Xianhua Zhang Wei Jiang",
        "Yanpeng Li Xianhua Zhang",
        "Chuansheng Chen Yanpeng Li",
        "Jianfeng Zhu Chuansheng Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/db11fd5bd8d1fcfcf8b37e798e21be8e52e3c3f1",
      "pdf_url": "https://jit.ndhu.edu.tw/article/download/2898/2930",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8bbb971305fb765707babfe8f4f67482c59a2d0a",
      "title": "Defending Large Language Models Against Attacks With Residual Stream Activation Analysis",
      "abstract": "The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply a novel methodology for analyzing distinctive activation patterns in the residual streams for attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.",
      "year": 2024,
      "venue": "CAMLIS",
      "authors": [
        "Amelia Kawasaki",
        "Andrew Davis",
        "Houssam Abbas"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/8bbb971305fb765707babfe8f4f67482c59a2d0a",
      "pdf_url": "",
      "publication_date": "2024-06-05",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97a821f4c33bfa3125dd3bbb308915501989d386",
      "title": "Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces",
      "abstract": "Deepfake videos are becoming increasingly realistic, showing few tampering traces on facial areas that vary between frames. Consequently, existing Deepfake detection methods struggle to detect unknown domain Deepfake videos while accurately locating the tampered region. To address this limitation, we propose Delocate, a novel Deepfake detection model that can both recognize and localize unknown domain Deepfake videos. Our method consists of two stages named recovering and localization. In the recovering stage, the model randomly masks regions of interest (ROIs) and reconstructs real faces without tampering traces, leading to a relatively good recovery effect for real faces and a poor recovery effect for fake faces. In the localization stage, the output of the recovery phase and the forgery ground truth mask serve as supervision to guide the forgery localization process. This process strategically emphasizes the recovery phase of fake faces with poor recovery, facilitating the localization of tampered regions. Our extensive experiments on four widely used benchmark datasets demonstrate that Delocate not only excels in localizing tampered areas but also enhances cross-domain detection performance.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Juan Hu",
        "Xin Liao",
        "Difei Gao",
        "Satoshi Tsutsui",
        "Qian Wang",
        "Zheng Qin",
        "Mike Zheng Shou"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/97a821f4c33bfa3125dd3bbb308915501989d386",
      "pdf_url": "",
      "publication_date": "2024-01-24",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0dfbf14af09e514d139c617aa3288ef96268fba",
      "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP",
      "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings. The code is available at https://github.com/jiawangbai/BadCLIP.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Jiawang Bai",
        "Kuofeng Gao",
        "Shaobo Min",
        "Shu-Tao Xia",
        "Zhifeng Li",
        "Wei Liu"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/d0dfbf14af09e514d139c617aa3288ef96268fba",
      "pdf_url": "https://arxiv.org/pdf/2311.16194",
      "publication_date": "2023-11-26",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f131fbeb41270a0df6762425e3bfe398ad8a3f4c",
      "title": "An Approach to Countering Adversarial Attacks on Image Recognition Based on JPEG-Compression and Neural-Cleanse",
      "abstract": "Nowadays, machine learning is becoming a ubiquitous artificial intelligence technology. It is actively being implemented in various fields of science and technology, including protection against cyber attacks, image recognition, computer vision, autonomous vehicles and other difficult problems. However, despite the advantages of machine learning, this technology is becoming the object of close attention of attackers. Cyber attacks against machine learning models, which can be presented in the form of classifiers or artificial neural networks, can significantly distort the results of these models and cause irreparable damage to a decision-making system based on machine learning. The paper proposes a new approach to countering attacks against machine learning systems in the field of image recognition, which is based on the JPEG-compression and Neural-Cleanse methods. The study shows that the proposed approach is able to restore the ability of a machine learning system to recognize images under gradient adversarial attacks. To demonstrate the effectiveness of the proposed approach, experiments were conducted on four classifiers, which confirmed a significant increase in the accuracy of image recognition after a cyber attack. The results of the work indicate the high potential of using this approach in image recognition systems aimed at detecting and combating adversarial attacks.",
      "year": 2024,
      "venue": "2024 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)",
      "authors": [
        "Igor V. Kotenko",
        "I. Saenko",
        "O. Lauta",
        "N. Vasiliev",
        "V. Sadovnikov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f131fbeb41270a0df6762425e3bfe398ad8a3f4c",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7dae104b2d19aa61ed6c3b4c89fc203a2f8ba049",
      "title": "A Noise-Based Approach Augmented with Neural Cleanse and JPEG Compression to Counter Adversarial Attacks on Image Classification Systems",
      "abstract": "Adversarial attacks are now becoming quite a dangerous means of disrupting image processing systems that use machine learning methods for decision making. Therefore, developing effective countermeasures against adversarial attacks is becoming quite an important area of cybersecurity. The paper proposes a noise-based approach to countering adversarial attacks that is augmented with neural-cleanse and jpeg-compression technologies. The idea of the proposed approach is that adding noise distorts the effect of an adversarial attack, and neural cleaning and jpeg compression eliminate the consequences of such an effect. The paper examines the three most well-known types of adversarial attacks: Fast Gradient Sign Method, Zeroth Order Optimization and One Pixel Attack. These attacks manipulate input data, resulting in misclassification or incorrect predictions by exploiting high-frequency components that are undetectable to humans. The research was carried out on two datasets: MNIST-JPG and PC Parts Images. Two types of noise were used: Gaussian and Poisson. During the experiments, optimal parameters for these types of noise were found, ensuring maximum accuracy of image recognition after exposure to adversarial attacks.",
      "year": 2025,
      "venue": "International Euromicro Conference on Parallel, Distributed and Network-Based Processing",
      "authors": [
        "Igor V. Kotenko",
        "I. Saenko",
        "O. Lauta",
        "N. Vasiliev",
        "V. Sadovnikov"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7dae104b2d19aa61ed6c3b4c89fc203a2f8ba049",
      "pdf_url": "",
      "publication_date": "2025-03-12",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
      "title": "Learning both Weights and Connections for Efficient Neural Network",
      "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.",
      "year": 2015,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Song Han",
        "Jeff Pool",
        "J. Tran",
        "W. Dally"
      ],
      "citation_count": 7285,
      "url": "https://www.semanticscholar.org/paper/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
      "pdf_url": "",
      "publication_date": "2015-06-08",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1de68f12db136526116c6ac7064fd13965f2c966",
      "title": "A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects",
      "abstract": "A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN\u2019s applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",
      "year": 2020,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Zewen Li",
        "Fan Liu",
        "Wenjie Yang",
        "Shouheng Peng",
        "Jun Zhou"
      ],
      "citation_count": 3658,
      "url": "https://www.semanticscholar.org/paper/1de68f12db136526116c6ac7064fd13965f2c966",
      "pdf_url": "https://research-repository.griffith.edu.au/bitstreams/b17b55ad-283a-4b00-9c07-8688d8690e1b/download",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45122c8f76a4e2fd0163d1f0522db37e97ea4721",
      "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
      "abstract": "Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Ben Sorscher",
        "Robert Geirhos",
        "Shashank Shekhar",
        "S. Ganguli",
        "Ari S. Morcos"
      ],
      "citation_count": 544,
      "url": "https://www.semanticscholar.org/paper/45122c8f76a4e2fd0163d1f0522db37e97ea4721",
      "pdf_url": "http://arxiv.org/pdf/2206.14486",
      "publication_date": "2022-06-29",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39b492db00faead70bc3f4fb4b0364d94398ffdb",
      "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
      "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.",
      "year": 2021,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "M. Raghu",
        "Thomas Unterthiner",
        "Simon Kornblith",
        "Chiyuan Zhang",
        "Alexey Dosovitskiy"
      ],
      "citation_count": 1193,
      "url": "https://www.semanticscholar.org/paper/39b492db00faead70bc3f4fb4b0364d94398ffdb",
      "pdf_url": "",
      "publication_date": "2021-08-19",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "036e565a459636cb9005909fcfb7dec55b60ff6f",
      "title": "Watermarks for Generative Adversarial Network Based on Steganographic Invisible Backdoor",
      "abstract": "Model watermarking has become an important solution to protect the intellectual property right (IPR) of deep neural networks (DNN) models. However, there are few researches on the IPR protection of generative adversarial networks (GAN), which are widely used to generate photorealistic images. In the current backdoor-based watermarking method for GAN models, the trigger pattern of the watermark is easy to be detected and invalidated by the adversary, which may fail to achieve IPR protection. To address this drawback, we propose a new GAN model watermarking method, where an invisible backdoor based on steganography is injected into the target GAN model as a watermark. Experimentally, the proposed method effectively trades off the performance of GAN on original task and the robustness of watermarking for removal attacks. Moreover, the generated triggers can effectively resist being detected by attackers.",
      "year": 2023,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yuwei Zeng",
        "Jingxuan Tan",
        "Zhengxin You",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/036e565a459636cb9005909fcfb7dec55b60ff6f",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3310f4454af503c6c588d741a4fed8579c9d580d",
      "title": "Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models",
      "abstract": "Transformer models have demonstrated exceptional performance and have become indispensable in computer vision (CV) and natural language processing (NLP) tasks. However, recent studies reveal that transformers are susceptible to backdoor attacks. Prior backdoor attack methods typically rely on retraining with clean data or altering the model architecture, both of which can be resource-intensive and intrusive. In this paper, we propose Head-wise Pruning and Malicious Injection (HPMI), a novel retraining-free backdoor attack on transformers that does not alter the model's architecture. Our approach requires only a small subset of the original data and basic knowledge of the model architecture, eliminating the need for retraining the target transformer. Technically, HPMI works by pruning the least important head and injecting a pre-trained malicious head to establish the backdoor. We provide a rigorous theoretical justification demonstrating that the implanted backdoor resists detection and removal by state-of-the-art defense techniques, under reasonable assumptions. Experimental evaluations across multiple datasets further validate the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four advanced defense mechanisms. Additionally, relative to state-of-the-art retraining-dependent attacks, HPMI achieves greater concealment and robustness against diverse defense strategies, while maintaining minimal impact on clean accuracy.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Taibiao Zhao",
        "Mingxuan Sun",
        "Hao Wang",
        "Xiaobing Chen",
        "Xiangwei Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3310f4454af503c6c588d741a4fed8579c9d580d",
      "pdf_url": "",
      "publication_date": "2025-08-14",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}