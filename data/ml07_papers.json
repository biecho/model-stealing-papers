{
  "updated": "2026-01-06",
  "total": 138,
  "owasp_id": "ML07",
  "owasp_name": "Transfer Learning Attack",
  "description": "Attacks that exploit transfer learning and pre-trained models. This includes\n        attacks on fine-tuning, exploiting pre-trained representations, transferability\n        of adversarial examples across models, and vulnerabilities introduced by\n        using pre-trained models as feature extractors or starting points.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Y. Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "title": "LoRATEE: A Secure and Efficient Inference Framework for Multi-Tenant LoRA LLMs Based on TEE",
      "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning approach that adaptes pre-trained Large Language Models (LLMs) to multi-tenant tasks by generating a variety of LoRA adapters. However, this approach faces significant security challenges and is particularly susceptible to malicious servers stealing model parameters and sensitive data. Existing research on addressing security risks in multi-tenant environments remains constrained and insufficient. This paper explores the security challenges and proposes the LoRATEE framework, which embeds LoRA adapters within a server-side Trusted Execution Environment (TEE) and employs a lightweight One-Time Pad (OTP) encryption mechanism to ensure secure data transmission. Additionally, we design a dynamic LoRA adapter prefetching mechanism to reduce I/O latency. Moreover, a LoRA adapter module equivalence-sharing strategy based on Parameter-Efficient Fine-Tuning (PEFT) and minimalist design principles was introduced to optimize adapters loading. Experimental results show that LoRATEE maintains inference efficiency while securing multi-tenant LoRA LLMs systems.",
      "year": 2025,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Zechao Lin",
        "Sisi Zhang",
        "Xingbin Wang",
        "Yulan Su",
        "Yan Wang",
        "Rui Hou",
        "Dan Meng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6814d707b0e1f550baebde21c6753d0d7e2f6996",
      "pdf_url": "",
      "publication_date": "2025-04-06",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b5caff453174871e8c9b374b72659a7c63fa830",
      "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
      "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains, with well-trained QNNs representing critical intellectual property often deployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has examined QNN model extraction attacks using classical and emerging quantum strategies. These attacks involve adversaries querying QNNaaS platforms to obtain labeled data for training local substitute QNNs that replicate the functionality of cloud-based models. However, existing approaches have largely over-looked the impact of varying quantum noise inherent in noisy intermediate-scale quantum (NISQ) computers, limiting their effectiveness in real-world settings. To address this limitation, we propose the CopyQNN framework, which employs a three-step data cleaning method to eliminate noisy data based on its noise sensitivity. This is followed by the integration of contrastive and transfer learning within the quantum domain, enabling efficient training of substitute QNNs using a limited but cleaned set of queried data. Experimental results on NISQ computers demonstrate that a practical implementation of CopyQNN significantly outperforms state-of-the-art QNN extraction attacks, achieving an average performance improvement of 8.73% across all tasks while reducing the number of required queries by 90\u00d7, with only a modest increase in hardware overhead.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Leyi Zhao",
        "Xuhong Zhang",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1b5caff453174871e8c9b374b72659a7c63fa830",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks",
      "abstract": "The machine learning problem of model extraction was first introduced in 1991 and gained prominence as a cryptanalytic challenge starting with Crypto 2020. For over three decades, research in this field has primarily focused on ReLU-based neural networks. In this work, we take the first step towards the cryptanalytic extraction of PReLU neural networks, which employ more complex nonlinear activation functions than their ReLU counterparts. We propose a raw output-based parameter recovery attack for PReLU networks and extend it to more restrictive scenarios where only the top-m probability scores are accessible. Our attacks are rigorously evaluated through end-to-end experiments on diverse PReLU neural networks, including models trained on the MNIST dataset. To the best of our knowledge, this is the first practical demonstration of PReLU neural network extraction across three distinct attack scenarios.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yi Chen",
        "Xiaoyang Dong",
        "Ruijie Ma",
        "Yan Shen",
        "Anyu Wang",
        "Hongbo Yu",
        "Xiaoyun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "pdf_url": "",
      "publication_date": "2025-09-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b6603145f57b294c2e5dff92efe136568ab8cf52",
      "title": "Securing RFID With GNN: A Real-Time Tag Cloning Attack Detection System",
      "abstract": "In the field of RFID systems, cloning attacks that replicate authentic tags to deceive readers pose a significant threat to corporate security, potentially leading to financial losses and reputational damage. Many existing solutions struggle to mitigate this threat without altering the Medium Access Control (MAC) layer protocols or integrating additional hardware resources, which are impractical adjustments for commercial off-the-shelf (COTS) RFID devices. This paper introduces an innovative system framework that leverages Graph Neural Network to detect RFID tag cloning attacks without the need to change the MAC protocols or add hardware resources. The system can automatically uncover implicit topological structures from RFID signal data and adaptively capture complex inter-signal relationships. By constructing dynamic graph and employing Graph Attention Network, this framework not only captures deep data correlations that traditional detection methods cannot identify but also demonstrates exceptional accuracy and robustness in experiments. Experimental results have proven that the framework maintains stable performance even when the training and testing data distributions are mismatched, as verified in both static and dynamic tag cloning attack scenarios. Furthermore, the framework effectively identifies anomalous behavior by comprehensively considering precision, recall, and F1 scores, especially when dealing with highly imbalanced datasets.",
      "year": 2025,
      "venue": "IEEE Open Journal of the Communications Society",
      "authors": [
        "Bojun Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b6603145f57b294c2e5dff92efe136568ab8cf52",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "90360529f01cf996d62369fd0c47af3b1823c7f4",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attack poses a significant risk to Machine-Learning-as-a-Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying Black-Box (without internal insight) APIs. Despite FL\u2019s privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of the FL-based victim model to two types of model extraction attacks. For various federated clients built under NVFlare platform, we implemented ME attack across two deep-learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for various FL clients, the accuracy and fidelity of the extraction model are closely related to the size of the attack query set. Additionally, we explore a transfer learning-based approach where pre-trained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pre-trained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/90360529f01cf996d62369fd0c47af3b1823c7f4",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "title": "A Method for Extracting Black Box Models Based on Interpretable Attention",
      "abstract": "Deep neural networks have achieved remarkable success in face recognition. However, their vulnerability has attracted considerable attention. Researchers can analyse the weaknesses of face recognition models by extracting their functionality, aiming to enhance the security performance of these models. The findings of the study reveal that current model extraction methods are afflicted with notable drawbacks, namely low similarity in capturing model functionality and insufficient availability of samples. These limitations significantly impede the analysis of model security performance. We propose an interpretable attention\u2010based method for black\u2010box model extraction, enhancing the similarity between substitute and victim model functionality. Our main contributions are summarized as follows: (i) This study addresses the issue of limited sample training caused by the restricted number of black\u2010box hard label queries. (ii) By applying input perturbations, we obtain feedback from deep black\u2010box models, enabling us to identify facial local regions and the distribution of feature weights that positively influence predictions. (iii) By normalizing the feature weight distribution matrix and associating it with the attention weight matrix, the construction of an attention mask for the dataset is achieved, enabling differential attention to features in different regions. (iv) Leveraging a pre\u2010trained base model, we extract relevant knowledge and features, facilitating cross\u2010domain knowledge transfer. Experiments on Emore, PubFig and CASIA\u2010WebFace show that our method outperforms traditional methods by 10%\u201320% in model consistency for the same query budget. Also, our method achieves the highest model stealing consistency on the three datasets: 94.51%, 93.27% and 91.74%, respectively.",
      "year": 2025,
      "venue": "Expert Syst. J. Knowl. Eng.",
      "authors": [
        "Lijun Gao",
        "Huibin Tian",
        "Kai Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/dd898d4226db4c4dca01e46de96fe3acbc6087ec",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "098c756f393dc1c3fafa8348fabc3a97410f4473",
      "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks",
      "abstract": "Neural network model extraction has emerged in recent years as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. A key step in this process is signature extraction, which aims to recover the absolute values of the network's weights layer by layer. Prior work, notably by Carlini et al. (2020), introduced a technique inspired by differential cryptanalysis to extract neural network parameters. However, their method suffers from several limitations that restrict its applicability to networks with a few layers only. Later works focused on improving sign extraction, but largely relied on the assumption that signature extraction itself was feasible. In this work, we revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues, greatly improving the efficiency of signature extraction. Our approach permits the extraction of much deeper networks than was previously possible. We validate our method through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth and accuracy. For instance, our extracted network matches the target network on at least 95% of the input space for each of the eight layers of a neural network trained on the CIFAR-10 dataset, while previous works could barely extract the first three layers. Our results represent a crucial step toward practical attacks on larger and more complex neural network architectures.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haolin Liu",
        "Adrien Siproudhis",
        "Samuel Experton",
        "Peter Lorenz",
        "Christina Boura",
        "Thomas Peyrin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/098c756f393dc1c3fafa8348fabc3a97410f4473",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4934c657457df8eae2b5193b1744f87906d37cfa",
      "title": "Side-channel attacks on convolutional neural networks based on the hybrid attention mechanism",
      "abstract": "In the field of security assessment of password chips, side-channel attacks are an important and effective means of extracting sensitive information by analysing the physical characteristics of the chip during operation, providing an important basis for security assessment. In recent years, deep learning technology has been widely used in the field of side-channel attacks, which can automatically learn and identify the physical leakage characteristics of the chip and improve the efficiency and accuracy of the attack. However, deep learning-based side-channel attacks may be disturbed by environmental noise during the training process, and there are also problems of model overfitting and slow convergence. In order to more effectively extract feature information in the trajectory to implement a side-channel attack, this paper proposes a new attention mechanism convolutional neural network model architecture. The model combines a convolutional neural network with an attention mechanism. It optimises the traditional CNN model by improving the convolutional layer and introducing a fused hybrid attention mechanism, enhancing the model's ability to capture global information to effectively extract relevant leaked information. Experimental results show that the model has good attack results on the ASCAD public dataset. Compared with other models, it requires 74.87% less power consumption for side-channel analysis, and the model accuracy is significantly improved. It solves the problems of overfitting and slow convergence speed, and can meet the requirements of side-channel modeling and analysis. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model.",
      "year": 2025,
      "venue": "Discover Applied Sciences",
      "authors": [
        "Tao Feng",
        "Huan Gao",
        "Xiaomin Li",
        "Chunyan Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4934c657457df8eae2b5193b1744f87906d37cfa",
      "pdf_url": "https://doi.org/10.1007/s42452-025-06854-0",
      "publication_date": "2025-04-24",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
      "abstract": "To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35\u00d7 (avg. 5.85\u00d7) faster than the state-of-the-art work, without accuracy loss.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tong Sun",
        "Bowen Jiang",
        "Hailong Lin",
        "Borui Li",
        "Yixiao Teng",
        "Yi Gao",
        "Wei Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "title": "Inside the Mind of an Attacker: Review Sistematika Tujuan Pencurian Machine Learning Model",
      "abstract": "Abstrak - Pencurian model (model stealing) menjadi salah satu ancaman serius dalam penerapan machine learning modern, terutama pada layanan berbasis API dan cloud. Artikel ini mengulas secara sistematik berbagai tujuan di balik serangan pencurian model untuk memahami motif penyerang dan implikasinya bagi pengembang sistem. Metode penulisan berupa kajian literatur terkini yang mengklasifikasikan tujuan pencurian ke dalam delapan kategori utama: (1) pencurian properti internal seperti arsitektur, bobot, dan hyperparameter; (2) peniruan perilaku model untuk menghasilkan efektivitas setara dan konsistensi prediksi pada data normal maupun adversarial; (3) transfer pengetahuan untuk distillation dan deployment ringan; (4) serangan privasi berupa membership inference dan model inversion; (5) monetisasi dengan menjual model bajakan atau menyediakan layanan API ilegal; (6) pencurian kemampuan pertahanan adversarial untuk meningkatkan efektivitas serangan; (7) spionase industri untuk reverse engineering model pesaing; serta (8) penghindaran regulasi dengan mencuri model yang sudah tersertifikasi. Review ini menegaskan bahwa ancaman pencurian model tidak hanya merugikan secara teknis, tetapi juga membuka peluang eksploitasi ekonomi ilegal, kebocoran data sensitif, dan persaingan usaha tidak sehat. Pemahaman yang detail atas ragam tujuan ini diharapkan mendorong perancang sistem untuk mengembangkan strategi pertahanan yang lebih cermat dan menyeluruh.Kata kunci: Machine Learning; Model Stealing; API; Adversarial; Transfer Pengetahuan;\u00a0Abstract - Model stealing has become one of the most serious threats in modern machine learning applications, especially in API- and cloud-based services. This article systematically reviews the various objectives behind model stealing attacks to understand the attackers\u2019 motivations and their implications for system developers. The writing method is a current literature review that classifies model stealing objectives into eight main categories: (1) theft of internal properties such as architecture, weights, and hyperparameters; (2) imitation of model behavior to achieve comparable effectiveness and prediction consistency on both normal and adversarial data; (3) knowledge transfer for distillation and lightweight deployment; (4) privacy attacks through membership inference and model inversion; (5) monetization by selling stolen models or offering illegal API services; (6) stealing adversarial robustness to improve attack effectiveness; (7) industrial espionage for reverse engineering competitor models; and (8) regulatory evasion by stealing pre-certified models. This review emphasizes that model stealing threats are not merely technical issues but also open opportunities for illegal economic exploitation, leakage of sensitive data, and unfair business competition. A detailed understanding of these diverse objectives is expected to encourage system designers to develop more careful and comprehensive defense strategies.Keywords: Machine Learning; Model Stealing; API; Adversarial; Knowledge Transfer;",
      "year": 2025,
      "venue": "Jurnal Nasional Komputasi dan Teknologi Informasi (JNKTI)",
      "authors": [
        "Mulkan Fadhli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "pdf_url": "",
      "publication_date": "2025-07-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "title": "DeepAW: A Customized DNN Watermarking Scheme Against Unreliable Participants",
      "abstract": "Training DNNs requires large amounts of labeled data, costly computational resources, and tremendous human effort, resulting in such models being a valuable commodity. In collaborative learning scenarios, unreliable participants are widespread due to data collected from a diverse set of end-users that differ in quality and quantity. It is important to note that failure to take into account the contributions of all participants in the collaborative model training process when sharing the model with them could potentially result in a deterioration in collaborative efforts. In this paper, we propose a customized DNN watermarking scheme to safeguard the model ownership, namely DeepAW, achieving robustness to model stealing attacks and collaborative fairness in the presence of unreliable participants. Specifically, DeepAW leverages the tightly binding between the embedded watermarking and the model performance to defend against the model stealing attacks, resulting in the sharp decline of the model performance encountering any attempt at watermarking modification. DeepAW achieves collaborative fairness by detecting unreliable participants and customizing the model performance according to the participants' contributions. Furthermore, we set up three model stealing attacks and four types of unreliable participants. The experimental results demonstrate the effectiveness, robustness, and collaborative fairness of DeepAW.",
      "year": 2025,
      "venue": "IEEE Transactions on Network Science and Engineering",
      "authors": [
        "Shen Lin",
        "Xiaoyu Zhang",
        "Xu Ma",
        "Xiaofeng Chen",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "title": "MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction",
      "abstract": "As deep learning gains popularity, edge IoT devices have seen proliferating deployment of pre-trained Deep Neural Network (DNN) models. These DNNs represent valuable intellectual property and face significant confidentiality threats from side-channel analysis (SCA), particularly non-invasive Differential Electromagnetic (EM) Analysis (DEMA), which retrieves individual model parameters from EM traces collected during model inference. Traditional SCA mitigation methods, such as masking and shuffling, can still be applied to DNN inference, but will incur significant performance degradation due to the large volume of operations and parameters. Based on the insight that DNN models have high redundancy and are robust to input variation, we introduce MACPruning, a novel lightweight defense against DEMA-based parameter extraction attacks, exploiting specific characteristics of DNN execution. The design principle of MACPruning is to randomly deactivate input pixels and prune the operations (typically multiply-accumulate-MAC) on those pixels. The technique removes certain leakages and overall redistributes weight-dependent EM leakages temporally, and thus effectively mitigates DEMA. To maintain DNN performance, we propose an importance-aware pixel map that preserves critical input pixels, keeping randomness in the defense while minimizing its impact on DNN performance due to operation pruning. We conduct a comprehensive security analysis of MACPruning on various datasets for DNNs on edge devices. Our evaluations demonstrate that MACPruning effectively reduces EM leakages with minimal impact on the model accuracy and negligible computational overhead.",
      "year": 2025,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Ruyi Ding",
        "Gongye Cheng",
        "Davis Ranney",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "112442beb4161e251a44ea532893c8a32eb17ba3",
      "title": "Ownership Infringement Detection for Generative Adversarial Networks Against Model Stealing",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process to verify the ownership of a GAN. In this article, we show that these methods are not robust to emerging model extraction attacks. Then, we propose a new method GAN-Guards which utilizes the common characteristics of a target model and its stolen models for ownership infringement detection. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method achieves superior detection performance, compared with the watermark-based and fingerprint-based methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, and adaptive attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/112442beb4161e251a44ea532893c8a32eb17ba3",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "title": "Stabilizing Data-Free Model Extraction",
      "abstract": "Model extraction is a severe threat to Machine Learning-as-a-Service systems, especially through data-free approaches, where dishonest users can replicate the functionality of a black-box target model without access to realistic data. Despite recent advancements, existing data-free model extraction methods suffer from the oscillating accuracy of the substitute model. This oscillation, which could be attributed to the constant shift in the generated data distribution during the attack, makes the attack impractical since the optimal substitute model cannot be determined without access to the target model's in-distribution data. Hence, we propose MetaDFME, a novel data-free model extraction method that employs meta-learning in the generator training to reduce the distribution shift, aiming to mitigate the substitute model's accuracy oscillation. In detail, we train our generator to iteratively capture the meta-representations of the synthetic data during the attack. These meta-representations can be adapted with a few steps to produce data that facilitates the substitute model to learn from the target model while reducing the effect of distribution shifts. Our experiments on popular baseline image datasets, MNIST, SVHN, CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current state-of-the-art data-free model extraction method while exhibiting a more stable substitute model's accuracy during the attack.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dat-Thinh Nguyen",
        "Kim-Hung Le",
        "Nhien-An Le-Khac"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "pdf_url": "",
      "publication_date": "2025-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "title": "Extracting Proxy Models from Side-Channel Insights to Enhance Adversarial Attacks on Black-Box DNNs",
      "abstract": "Side-channel information leakage can be exploited to reverse engineer critical architectural details of a target DNN model executing on a hardware accelerator. However, using these details to apply a practical adversarial attack remains a significant challenge. In this paper, we first introduce a novel approach to analyze side-channel data and extract detailed architectural information of DNN models, including accurate prediction of layer hyperparameters and inter-layer skip connections. Next, we develop techniques to construct effective proxy models from this information. We then leverage white-box access to these proxies to generate adversarial examples capable of effectively deceiving the target DNN model. We illustrate our techniques using popular DNNs as target models, and demonstrate that the constructed proxy models achieve up to 89.8% similarity in performance compared to the target models. Furthermore, we achieve adversarial transferability rates of up to 72.34% and induce up to 60.4% drop in accuracy in the target models using the crafted adversarial images. Compared to off-the-shelf substitute models, our method improves transferability by as much as 30% in untargeted adversarial attacks. Even when the target model is protected by a state-of-the-art denoiser, our proxy models generate 5.5% more transferable adversarial examples compared to other substitute models in untargeted adversarial attacks.",
      "year": 2025,
      "venue": "Proceedings of the 11th ACM Cyber-Physical System Security Workshop",
      "authors": [
        "Srivatsan Chandrasekar",
        "Likith Anaparty",
        "Siew-Kei Lam",
        "Vivek Chaturvedi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "pdf_url": "",
      "publication_date": "2025-08-25",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "98df987a99601fd41816c0ad0a3a6c65748a2eb5",
      "title": "FewMEA: Few-shot Model Extraction Attack against Sequential Recommenders",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Fu Liu",
        "Hui Zhang",
        "Yuqin Lan",
        "Min Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/98df987a99601fd41816c0ad0a3a6c65748a2eb5",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
      "abstract": "Pre-trained models are valuable intellectual property, capturing both domain-specific and domain-invariant features within their weight spaces. However, model extraction attacks threaten these assets by enabling unauthorized source-domain inference and facilitating cross-domain transfer via the exploitation of domain-invariant features. In this work, we introduce **ProDiF**, a novel framework that leverages targeted weight space manipulation to secure pre-trained models against extraction attacks. **ProDiF** quantifies the transferability of filters and perturbs the weights of critical filters in unsecured memory, while preserving actual critical weights in a Trusted Execution Environment (TEE) for authorized users. A bi-level optimization further ensures resilience against adaptive fine-tuning attacks. Experimental results show that **ProDiF** reduces source-domain accuracy to near-random levels and decreases cross-domain transferability by 74.65\\%, providing robust protection for pre-trained models. This work offers comprehensive protection for pre-trained DNN models and highlights the potential of weight space manipulation as a novel approach to model security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tong Zhou",
        "Shijin Duan",
        "Gaowen Liu",
        "Charles Fleming",
        "R. Kompella",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "pdf_url": "",
      "publication_date": "2025-03-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "17440e21a757f14e630d7443c89f018b79cc1754",
      "title": "StegGuard: Secrets Encoder and Decoder Act as Fingerprint of Self-Supervised Pretrained Model",
      "abstract": "In this work, we propose StegGuard, a novel fingerprinting mechanism to verify the ownership of a suspect pretrained model using steganography, where the pretrained model is obtained via self-supervised learning. A critical perspective in StegGuard is that the unique characteristic of the transformation from an image to an embedding, conducted by the pretrained model, can be equivalently captured by how an encoder embeds secrets into images and how a decoder extracts them from the embeddings with tolerable error. While each independently trained pretrained model has a distinct transformation, a piracy model exhibits a transformation similar to that of the victim. Based on these observations, StegGuard learns a pair of secrets encoder and decoder as the fingerprint of the victim model. Additionally, a Frequency domain channel attention Embedding block is introduced into the encoder to adaptively embed secrets into suitable frequency bands. During verification, if the secrets embedded into the query images can be extracted with an acceptable error from the embeddings of the query images, the suspect model is determined to be piracy; otherwise, it is deemed independent. Extensive experiments demonstrate that with as few as 100 query images, StegGuard achieves high piracy detection accuracy and robustness against model stealing attacks, including model extraction, fine-tuning, pruning, embedding noising and shuffle. Compared to existing methods, StegGuard consistently achieves lower p-values for piracy models (as low as 1e-14) and higher p-values for independent models (up to 0.99), confirming its effectiveness and reliability.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xingdong Ren",
        "Hanzhou Wu",
        "Yinggui Wang",
        "Haojie Liu",
        "Xiaofeng Lu",
        "Guangling Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/17440e21a757f14e630d7443c89f018b79cc1754",
      "pdf_url": "",
      "publication_date": "2025-09-15",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "title": "Explore the vulnerability of black-box models via diffusion models",
      "abstract": "Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Huajie Shao",
        "Ashley Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "title": "Misclassification-driven Fingerprinting for DNNs Using Frequency-aware GANs",
      "abstract": "Deep neural networks (DNNs) have become valuable assets due to their success in various tasks, but their high training costs also make them targets for model theft. Fingerprinting techniques are commonly used to verify model ownership, but existing methods either require training many additional models, leading to increased costs, or rely on GANs to generate fingerprints near decision boundaries, which may compromise image quality. To address these challenges, we propose a GAN-based fingerprint generation method that applies frequency-domain perturbations to normal samples, effectively creating fingerprints. This approach not only resists intellectual property (IP) threats, but also improves fingerprint acquisition efficiency while maintaining high imperceptibility. Extensive experiments demonstrate that our method achieves a state-of-the-art (SOTA) AUC of 0.98 on the Tiny-ImageNet dataset under IP removal attacks, outperforming existing methods by 8%, and consistently achieves the best ABP for three types of IP detection and erasure attacks on the GTSRB dataset. Our source code is available at https://github.com/wason981/Frequency-Fingerprinting.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Weixing Liu",
        "Shenghua Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6a4dab913871b24c71bb3e66db20babe2a68880f",
      "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
      "abstract": "Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Kacem Khaled",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6a4dab913871b24c71bb3e66db20babe2a68880f",
      "pdf_url": "",
      "publication_date": "2025-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2026-01-02"
    },
    {
      "paper_id": "14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "pdf_url": "https://arxiv.org/pdf/2401.15239",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "85f707934e1630695fbfbdf1934a21760917416d",
      "title": "TinyPower: Side-Channel Attacks with Tiny Neural Networks",
      "abstract": "Side-channel attacks leverage correlations between power consumption and intermediate encryption results to infer encryption keys. Recent studies show that deep learning offers promising results in the context of side-channel attacks. However, neural networks utilized in deep-learning side-channel attacks are complex with a substantial number of parameters and consume significant memory. As a result, it is challenging to perform deep-learning side-channel attacks on resource-constrained devices. In this paper, we propose a framework, TinyPower, which leverages pruning to reduce the number of neural network parameters for side-channel attacks. Pruned neural networks obtained from our framework can successfully run side-channel attacks with significantly fewer parameters and less memory. Specifically, we focus on structured pruning over filters of Convolutional Neural Networks (CNNs). We demonstrate the effectiveness of structured pruning over power and EM traces of AES-128 running on microcontrollers (AVR XMEGA and ARM STM32) and FPGAs (Xilinx Artix-7). Our experimental results show that we can achieve a reduction rate of 98.8% (e.g., reducing the number of parameters from 53.1 million to 0.59 million) on a CNN and still recover keys on XMEGA. For STM32 and Artix-7, we achieve a reduction rate of 92.9% and 87.3% on a CNN respectively. We also demonstrate that our pruned CNNs can effectively perform the attack phase of side-channel attacks on a Raspberry Pi 4 with less than 2.5 millisecond inference time per trace and less than 41 MB memory usage per CNN.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Haipeng Li",
        "Mabon Ninan",
        "Boyang Wang",
        "J. M. Emmert"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/85f707934e1630695fbfbdf1934a21760917416d",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "title": "High-Frequency Matters: Attack and Defense for Image-Processing Model Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks with an almost 100% success rate. To counter this attack, we propose an adversarial framework for the watermarking network. The framework incorporates a specially-designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "title": "Design of Time-Delay Convolutional Neural Networks(TDCNN) Model for Feature Extraction for Side-Channel Attacks",
      "abstract": ": This work explores a novel method of SCA profiling to address compatibility problems and strengthen Deep Learning (DL) models. Convolutional Neural Networks are proposed in this research as a countermeasure to misalignment-focused countermeasures. \u201dTime-Delay Convolutional Neural Networks\u201d (TDCNN) is more accurate than \u201dConvolutional Neural Network,\u201d yet it\u2019s still acceptable. It\u2019s true that TDCNNs are neural networks based on convolution learned on single spatial information, just as side-channel tracings. However, given to recent surge in popularity of CNNs, particularly from the year 2012 when CNN framework (\u201dAlexNet\u201d) achieved Image Net Large Scale Visual Recognition Competition which is a notable image detection competition, a novel TDCNN has been termed out in DL literature. Currently, it needs to employ the characteristics related to CNN design, including declaring that one input feature equals 1 for instance, to establish a TDCNN in the most widely used DL libraries.",
      "year": 2024,
      "venue": "International Journal of Computing and Digital Systems",
      "authors": [
        "Amjed Abbas Ahmed",
        "Mohammad Kamrul Hasan",
        "Shahrul Azman Mohd Noah",
        "Azana Hafizah Aman"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/d3b74a3fdb14c606b9daaf61c5409f7d58c3b3aa",
      "pdf_url": "https://journal.uob.edu.bh:443/bitstream/123456789/5437/3/IJCDS160127_1570996366.pdf",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "title": "DeepCache: Revisiting Cache Side-Channel Attacks in Deep Neural Networks Executables",
      "abstract": "Deep neural networks (DNN) are increasingly deployed in heterogeneous hardware, including high-performance devices like GPUs and low-power devices like mobile/IoT CPUs, FPGAs, and accelerators. In order to unlock the full performance potential of various hardware, deep learning (DL) compilers automatically optimize DNN inference computations and compile DNN models into DNN executables for efficient computations across hardware backends. As valuable intellectual properties, DNN architectures are one primary attack target. Since previous works already demonstrate the abuse of cache side channels to steal DNN architectures from DL frameworks (e.g., PyTorch and TensorFlow), we first study using those known side-channel attacks against DNN executables. We find that attacking DNN executables presents unique challenges, and existing works can hardly apply. Particularly, DNN executables exhibit a standalone paradigm that largely reduces cache side channel attack surfaces. Meanwhile, cache side channels capture only limited behaviors of the whole DNN execution while facing daunting technical challenges (e.g., noise and low time resolution). However, we unveil a unique attack vector in DNN executables, such that the cache-aware optimizations, which are extensively employed by contemporary DL compilers to harvest the full potentials of hardware, would result in distinguishable DNN operator cache access patterns, making model architecture recovery possible. We propose DeepCache, an end-to-end side channel attack framework, to infer DNN model architectures from DNN executables. DeepCache \\ leverages cache side channels as the attacking primitives and combines contrastive learning and anomaly detection to enable precise inference. Our evaluation using the standard Prime+Probe shows that DeepCache \\ yields a high accuracy in exploiting complex DNN executables under both the basic L1 cache attack and the more practical but challenging last level cache (LLC) attack settings.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhibo Liu",
        "Yuanyuan Yuan",
        "Yanzuo Chen",
        "Sihang Hu",
        "Tianxiang Li",
        "Shuai Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690241",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "title": "Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation",
      "abstract": "Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Di Mi",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Shengshan Hu",
        "Qi Zhong",
        "Haizhuan Yuan",
        "Shirui Pan"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "pdf_url": "https://research-repository.griffith.edu.au/bitstreams/4224c508-a124-40eb-94f2-46c6b9821946/download",
      "publication_date": "2024-03-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0a54d21c8cb4bde33a93fd8a835bb929fa06bb8e",
      "title": "Side-channel attacks based on attention mechanism and multi-scale convolutional neural network",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & electrical engineering",
      "authors": [
        "Pengfei He",
        "Ying Zhang",
        "Han Gan",
        "Jianfei Ma",
        "Hongxin Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0a54d21c8cb4bde33a93fd8a835bb929fa06bb8e",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be6dc29e9773c5242b0a44877df60af0feae9adb",
      "title": "Sample Correlation for Fingerprinting Deep Face Recognition",
      "abstract": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC.",
      "year": 2024,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "Yanbo Wang",
        "R. He"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/be6dc29e9773c5242b0a44877df60af0feae9adb",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b3744d928fcd84b7e2296d5983ba199a300955e0",
      "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
      "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Younghan Lee",
        "Sohee Jun",
        "Yungi Cho",
        "Woorim Han",
        "Hyungon Moon",
        "Y. Paek"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b3744d928fcd84b7e2296d5983ba199a300955e0",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d678d169552a667531d66e09a75b4e4d13e3c044",
      "title": "Exploring the Efficacy of Learning Techniques in Model Extraction Attacks on Image Classifiers: A Comparative Study",
      "abstract": "In the rapidly evolving landscape of cybersecurity, model extraction attacks pose a significant challenge, undermining the integrity of machine learning models by enabling adversaries to replicate proprietary algorithms without direct access. This paper presents a comprehensive study on model extraction attacks towards image classification models, focusing on the efficacy of various Deep Q-network (DQN) extensions for enhancing the performance of surrogate models. The goal is to identify the most efficient approaches for choosing images that optimize adversarial benefits. Additionally, we explore synthetic data generation techniques, including the Jacobian-based method, Linf-projected Gradient Descent (LinfPGD), and Fast Gradient Sign Method (FGSM) aiming to facilitate the training of adversary models with enhanced performance. Our investigation also extends to the realm of data-free model extraction attacks, examining their feasibility and performance under constrained query budgets. Our investigation extends to the comparison of these methods under constrained query budgets, where the Prioritized Experience Replay (PER) technique emerges as the most effective, outperforming other DQN extensions and synthetic data generation methods. Through rigorous experimentation, including multiple trials to ensure statistical significance, this work provides valuable insights into optimizing model extraction attacks.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Dong Han",
        "Reza Babaei",
        "Shangqing Zhao",
        "Samuel Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d678d169552a667531d66e09a75b4e4d13e3c044",
      "pdf_url": "",
      "publication_date": "2024-04-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "title": "EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning",
      "abstract": "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA\".",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jingtao Li",
        "Xing Chen",
        "Li Yang",
        "A. S. Rakin",
        "Deliang Fan",
        "Chaitali Chakrabarti"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/29258/30374",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models Through Adversarial Examples and Timing Side-Channels",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous AI systems. A common practice in this domain is the use of transfer learning, where a pre-trained model\u2019s architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it is crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present ArchWhisperer, a model fingerprinting attack approach based on the novel observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with model inference times is used to further enhance our attack in terms of attack effectiveness as well as query budget. ArchWhisperer is designed for typical user-level access in remote MLaaS environments and it exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20. This is a marked improvement compared to state-of-the-art works.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Shubhi Shukla",
        "Manaar Alam",
        "Pabitra Mitra",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "pdf_url": "https://arxiv.org/pdf/2402.11953",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation",
      "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "title": "Unveiling Intellectual Property Vulnerabilities of GAN-Based Distributed Machine Learning through Model Extraction Attacks",
      "abstract": "Generative Adversarial Networks (GANs), as a cornerstone of artificial intelligence (AI), are widely recognized as the intellectual property (IP) of their owners, given the sensitivity of the training data and the commercial value tied to the models. Model extraction attacks, which aim to steal well-trained proprietary models, pose a significant threat to model IP. Nevertheless, current research predominately focuses on the context of machine learning as a service (MLaaS), where the emphasis lies in understanding the attack knowledge acquired through black-box API queries. This restricted perspective exposes a critical gap in investigating model extraction attacks within realistic distributed settings for generative tasks. In this work, we present the first investigation into model extraction attacks against GANs in distributed settings. We provide a comprehensive attack taxonomy, considering three different levels of knowledge the adversary can obtain in practice. Based on it, we introduce a novel model extraction attack named MoEx, which focuses on the GAN-based distributed learning scenario, i.e., Multi-Discriminator GANs, a typical asymmetric distributed setting. MoEx uses the objective function simulation, leveraging data exchanged during the learning process, to approximate the GAN generator owned by the server. We define two attack goals for MoEx, fidelity extraction and accuracy extraction. Then we comprehensively evaluate the effectiveness of MoEx's two goals with real-world datasets. Our results demonstrate its robust capabilities in extracting generators with high fidelity and accuracy compared with existing methods.",
      "year": 2024,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Mengyao Ma",
        "Shuofeng Liu",
        "M.A.P. Chamikara",
        "Mohan Baruwal Chhetri",
        "Guangdong Bai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
      "abstract": "We introduce Adversarial Sparse Teacher (AST), a robust defense method against distillation-based model stealing attacks. Our approach trains a teacher model using adversarial examples to produce sparse logit responses and increase the entropy of the output distribution. Typically, a model generates a peak in its output corresponding to its prediction. By leveraging adversarial examples, AST modifies the teacher model\u2019s original response, embedding a few altered logits into the output, while keeping the primary response slightly higher. Concurrently, all remaining logits are elevated to further increase the output distribution\u2019s entropy. All these complex manipulations are performed using an optimization function with our proposed Exponential Predictive Divergence (EPD) loss function. EPD allows us to maintain higher entropy levels compared to traditional KL divergence, effectively confusing attackers. Experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that AST outperforms state-of-the-art methods, providing effective defense against model stealing, while preserving high accuracy. The source codes are publicly available at https://github.com/codeofanon/AdversarialSparseTeacher",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "E. Y\u0131lmaz",
        "H. Keles"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "pdf_url": "",
      "publication_date": "2024-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "15a81a4579949690c706f1bd9bbdda93681c35c3",
      "title": "Poisoning-Free Defense Against Black-Box Model Extraction",
      "abstract": "Recent research has shown that an adversary can use a surrogate model to steal the functionality of a target deep learning model even under the black-box condition and without data curation, while the existing defense mainly relies on API poisoning to disturb the surrogate training. Unfortunately, due to poisoning, the defense is achieved at the price of fidelity loss, sacrificing the interests of honest users. To solve this problem, we propose an Adversarial Fine-Tuning (AdvFT) framework, incorporating the generative adversarial network (GAN) structure that disturbs the feature representations of out-of-distribution (OOD) queries while preserving those of in-distribution (ID) ones, circumventing the need for OOD sample collection and API poisoning. Extensive experiments verify the effectiveness of the proposed framework. Code is available at github.com/Hatins/AdvFT.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Wen Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/15a81a4579949690c706f1bd9bbdda93681c35c3",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "title": "Genetic Improvement for DNN Security",
      "abstract": "Genetic improvement (GI) in Deep Neural Networks (DNNs) has traditionally enhanced neural architecture and trained DNN parameters. Recently, GI has supported large language models by optimizing DNN operator scheduling on accelerator clusters. However, with the rise of adversarial AI, particularly model extraction attacks, there is an unexplored potential for GI in fortifying Machine Learning as a Service (MLaaS) models. We suggest a novel application of GI \u2014 not to improve model performance, but to diversify operator parallelism for the purpose of a moving target defense against model extraction attacks. We discuss an application of GI to create a DNN model defense strategy that uses probabilistic isolation, offering unique benefits not present in current DNN defense systems.",
      "year": 2024,
      "venue": "International Genetic Improvement Workshop",
      "authors": [
        "Hunter Baxter",
        "Yu Huang",
        "Kevin Leach"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "pdf_url": "",
      "publication_date": "2024-04-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "title": "Enhancing Side-Channel Attacks Prediction using Convolutional Neural Networks",
      "abstract": "A common type of cyberattack is the side channel attack (SCA), which affects many devices and equipment connected to a network. These attacks have different types, like power attacks such as DPA, electromagnetic attacks, storage attacks, and others. Researchers and information security experts are concerned about SCA targeting devices, as they can lead to the loss and theft of important information. Using deep learning (DL) techniques in SCA analysis can be an efficient tool for detecting the SCA. Many previous works have tried to carry out the SCA in order to mitigate the impact of these attacks, but they encountered difficulties in detecting the SCA, whether in selecting the suitable dataset or applying the most efficient machine learning or deep learning techniques for achieving high performance. Therefore, we developed in this paper a deep learning-based model to detect SCAs using a dataset related to power attacks (the DPAv4 dataset) and the Convolutional Neural Networks (CNN) algorithm to train and test the DPAv4 dataset. The findings of our experiments showed a significant improvement in the performance of DL-based techniques in the detection of SCA. The proposed CNN-based model achieved an accuracy of 0.814 in detecting SCA and a loss rate of 0.581.",
      "year": 2024,
      "venue": "Automation, Control, and Information Technology",
      "authors": [
        "Khalid Alemerien",
        "Sadeq Al-Suhemat",
        "F. Alsuhimat",
        "Enshirah Altarawneh"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "pdf_url": "",
      "publication_date": "2024-09-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "title": "Quantum Neural Network Extraction Attack via Split Co-Teaching",
      "abstract": "Quantum Neural Networks (QNNs), now offered as QNN-as-a-Service (QNNaaS), have become key targets for model extraction attacks. Existing methods use ensemble learning to train substitute QNNs, but our analysis reveals significant limitations in real-world environments, where noise and cost constraints undermine their effectiveness. In this work, we introduce a novel attack, split co-teaching, which uses label variations to split queried data by noise sensitivity and employs co-teaching schemes to enhance extraction accuracy. The experimental results show that our approach outperforms classical extraction attacks by 6.5%~9.5% and existing QNN extraction methods by 0.1%~3.7% across various tasks.",
      "year": 2024,
      "venue": "2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "pdf_url": "",
      "publication_date": "2024-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
      "abstract": "With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as \\emph{multi-view data}, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. The code is available at: \\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yuxuan Li",
        "Sarthak Kumar Maharana",
        "Yunhui Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "pdf_url": "",
      "publication_date": "2024-03-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "title": "Advanced Side-Channel Profiling Attacks with Deep Neural Networks: A Hill Climbing Approach",
      "abstract": "Deep learning methods have significantly advanced profiling side-channel attacks. Finding the optimal set of hyperparameters for these models remains challenging. Effective hyperparameter optimization is crucial for training accurate neural networks. In this work, we introduce a novel hill climbing optimization algorithm that is specifically designed for deep learning in profiled side-channel analysis. This algorithm iteratively explores hyperparameter space using gradient-based techniques to make precise, localized adjustments. By incorporating performance feedback at each iteration, our approach efficiently converges on optimal hyperparameters, surpassing traditional Random Search methods. Extensive experiments\u2014covering protected implementations, leakage models, and various neural network architectures\u2014demonstrate that our hill climbing method consistently achieves superior performance in over 80% of test cases, predicting the secret key with fewer attack traces and outperforming both Random Search and state-of-the-art techniques.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Faisal Hameed",
        "Hoda Alkhzaimi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "pdf_url": "https://doi.org/10.3390/electronics13173530",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "title": "SparseLeakyNets: Classification Prediction Attack Over Sparsity-Aware Embedded Neural Networks Using Timing Side-Channel Information",
      "abstract": "This letter explores security vulnerabilities in sparsity-aware optimizations for Neural Network (NN) platforms, specifically focusing on timing side-channel attacks introduced by optimizations such as skipping sparse multiplications. We propose a classification prediction attack that utilizes this timing side-channel information to mimic the NN's prediction outcomes. Our techniques were demonstrated for CIFAR-10, MNIST, and biomedical classification tasks using diverse dataflows and processing loads in timing models. The demonstrated results could predict the original classification decision with high accuracy.",
      "year": 2024,
      "venue": "IEEE computer architecture letters",
      "authors": [
        "Saurav Maji",
        "Kyungmi Lee",
        "A. Chandrakasan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
      "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jinyin Chen",
        "Haonan Ma",
        "Haibin Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks",
      "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim's model (95.8% and 96.7% transfer rate).",
      "year": 2024,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Beno\u00eet Coqueret",
        "Mathieu Carbone",
        "Olivier Sentieys",
        "Gabriel Zaid"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "pdf_url": "",
      "publication_date": "2024-11-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "title": "Stealing Brains: From English to Czech Language Model",
      "abstract": ": We present a simple approach for efficiently adapting pre-trained English language models to generate text in lower-resource language, specifically Czech. We propose a vocabulary swap method that leverages parallel corpora to map tokens between languages, allowing the model to retain much of its learned capabilities. Experiments conducted on a Czech translation of the TinyStories dataset demonstrate that our approach significantly outperforms baseline methods, especially when using small amounts of training data. With only 10% of the data, our method achieves a perplexity of 17.89, compared to 34.19 for the next best baseline. We aim to contribute to work in the field of cross-lingual transfer in natural language processing and we propose a simple to implement, computationally efficient method tested in a controlled environment.",
      "year": 2024,
      "venue": "International Joint Conference on Computational Intelligence",
      "authors": [
        "Petr Hyner",
        "Petr Marek",
        "D. Adamczyk",
        "Jan Hula",
        "Jan \u0160ediv\u00fd"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ed9b2ca6d5741a9af8c3f2c368cfdf68eed98a48",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87a82c2177550ed7e10f635eadc8657e8f830c74",
      "title": "Time-Aware Face Anti-Spoofing with Rotation Invariant Local Binary Patterns and Deep Learning",
      "abstract": "Facial recognition systems have become an integral part of the modern world. These methods accomplish the task of human identification in an automatic, fast, and non-interfering way. Past research has uncovered high vulnerability to simple imitation attacks that could lead to erroneous identification and subsequent authentication of attackers. Similar to face recognition, imitation attacks can also be detected with Machine Learning. Attack detection systems use a variety of facial features and advanced machine learning models for uncovering the presence of attacks. In this work, we assess existing work on liveness detection and propose a novel approach that promises high classification accuracy by combining previously unused features with time-aware deep learning strategies.",
      "year": 2024,
      "venue": "IFIP International Information Security Conference",
      "authors": [
        "Moritz Finke",
        "Alexandra Dmitrienko"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/87a82c2177550ed7e10f635eadc8657e8f830c74",
      "pdf_url": "",
      "publication_date": "2024-08-27",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "title": "Camo-DNN: Layer Camouflaging to Protect DNNs against Timing Side-Channel Attacks",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose Camo $D N N$, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, Camo DNN performs various layer-obfuscation operations, e.g., layer branching layer deepening, etc., to alter the run-time traces while maintaining the functionality. Camo-DNN deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the architecture of DNN protected by Camo-DNN accurately. Further, we highlight that the adversarial attack on our obfuscated DNNs are unsuccessful.",
      "year": 2024,
      "venue": "IEEE International Symposium on On-Line Testing and Robust System Design",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "Ozgur Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "title": "Invisible DNN Watermarking Against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) models are widely used in various fields, such as pattern recognition and natural language processing, and provide considerable commercial value to their owners. Embedding a digital watermark in the model allows the legitimate owner to detect unauthorized use of the model. However, the existing DNN watermarking methods are vulnerable to model extraction attacks since the watermark task and the original model task are independent. In this article, a novel collaborative DNN watermarking framework is proposed to defend against model extraction attacks by establishing cooperation between the watermark generation and embedding. Specifically, the trigger samples are not only imperceptible to ensure perceptual stealth security but also infused with target-label information to guide the following feature associations. In the process of watermark embedding, the feature representation of trigger samples is forced to be similar to that of the task distribution samples via feature coupling. Consequently, the trigger samples from our framework can be recognized in the stolen model as task distribution samples, so that the ownership of the model can be successfully verified. Extensive experiments on CIFAR10, CIFAR100, and ImageNet demonstrate the effectiveness and superior performance of the proposed watermarking framework against various model extraction attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Zuping Xi",
        "Zuomin Qu",
        "Wei Lu",
        "Xiangyang Luo",
        "Xiaochun Cao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "pdf_url": "",
      "publication_date": "2024-12-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "title": "Evading VBA Malware Classification using Model Extraction Attacks and Stochastic Search Methods",
      "abstract": "Antivirus (AV) software that relies on learning-based methods is potentially vulnerable to adversarial attacks from threat actors. Threat actors can utilize model-extraction attacks against AV software to create a surrogate model. Malware samples can be tested against the surrogate model to determine how the target AV software will classify a given sample. Using a surrogate model speeds the process of malware development by allowing modifications to first be tested in feature space, which is significantly faster than performing modifications in code space. This work investigates performing evasion attacks against Windows Defender VBA malware classifier in an offline mode. The performance of five machine learning models is compared for their use as surrogate models. The models are reinforced by augmenting their training sets with samples that are generated by modifying existing samples. The results show that model performance is greatly improved with the augmented data and the best surrogate model achieved an accuracy of over 90% in predicting Defender\u2019s classifications. The best surrogate model is then used to test four search methods to find feature values to target when modifying malicious VBA samples to evade detection. The feature values found in feature space are used to guide modification of VBA samples in code space and then tested against Defender. Over 60% of the modified malicious-samples were able to evade detection after the targeted modifications based upon the results of the best search algorithm.",
      "year": 2024,
      "venue": "Computer Assisted Radiology and Surgery - International Congress and Exhibition",
      "authors": [
        "Brian Fehrman",
        "Francis Akowuah",
        "Randy Hoover"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
      "abstract": "Pre-trained encoders available online have been widely adopted to build downstream machine learning (ML) services, but various attacks against these encoders also post security and privacy threats toward such a downstream ML service paradigm. We unveil a new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which can extract sensitive encoder information from a targeted downstream ML service that can then be used to promote other ML attacks against the targeted service. By only providing API accesses to a targeted downstream service and a set of candidate encoders, the PEI attack can successfully infer which encoder is secretly used by the targeted service based on candidate ones. Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat. We empirically verify the effectiveness of the PEI attack on vision encoders. we first conduct PEI attacks against two downstream services (i.e., image classification and multimodal generation), and then show how PEI attacks can facilitate other ML attacks (i.e., model stealing attacks vs. image classification models and adversarial attacks vs. multimodal generative models). Our results call for new security and privacy considerations when deploying encoders in downstream services. The code is available at https://github.com/fshp971/encoder-inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shaopeng Fu",
        "Xuexue Sun",
        "Ke Qing",
        "Tianhang Zheng",
        "Di Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "pdf_url": "",
      "publication_date": "2024-08-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "title": "Late Breaking Results: Extracting QNNs from NISQ Computers via Ensemble Learning",
      "abstract": "The recent success of Quantum Neural Networks (QNNs) prompts model extraction attacks on cloud platforms, even under black-box constraints. These attacks repeatedly query the victim QNN with malicious inputs. However, existing extraction attacks tailored for classical models yield local substitute QNNs with limited performance due to NISQ computer noise. Drawing from bagging-based ensemble learning, which uses independent weak learners to learn from noisy data, we introduce a novel QNN extraction approach. Our experimental results show this quantum ensemble learning approach improves local QNN accuracy by up to 15.09% compared to previous techniques.",
      "year": 2024,
      "venue": "Design Automation Conference",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "title": "Model extraction via active learning by fusing prior and posterior knowledge from unlabeled data",
      "abstract": "As machine learning models become increasingly integrated into practical applications and are made accessible via public APIs, the risk of model extraction attacks has gained prominence. This study presents an innovative and efficient approach to model extraction attacks, aimed at reducing query costs and enhancing attack effectiveness. The method begins by leveraging a pre-trained model to identify high-confidence samples from unlabeled datasets. It then employs unsupervised contrastive learning to thoroughly dissect the structural nuances of these samples, constructing a dataset of high quality that precisely mirrors a variety of features. A mixed information confidence strategy is employed to refine the query set, effectively probing the decision boundaries of the target model. By integrating consistency regularization and pseudo-labeling techniques, reliance on authentic labels is minimized, thus improving the feature extraction capabilities and predictive precision of the surrogate models. Evaluation on four major datasets reveals that the models crafted through this method bear a close functional resemblance to the original models, with a real-world API test success rate of 62.35%, which vouches for the method\u2019s validity.",
      "year": 2024,
      "venue": "Journal of Intelligent &amp; Fuzzy Systems",
      "authors": [
        "Lijun Gao",
        "Kai Liu",
        "Wenjun Liu",
        "Jiehong Wu",
        "Xiao Jin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "pdf_url": "",
      "publication_date": "2024-03-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eb9378e6c7613404e347327caf130bdcdff1d66f",
      "title": "Model Extraction Attacks on Text-to-Image Generative Adversarial Networks",
      "abstract": "Model extraction attack refers to attackers ille-gally obtaining the functionality of a victim model by querying it. Currently, attacks primarily focus on discriminative models in computer vision. However, model extraction attacks on generative models, especially tasks like generating images from text, remain underexplored. The task of generating corresponding images for text is not only captivating but also highly challenging. In this study, we are the first to comprehensively investigate the feasibility of executing model extraction attacks on Text-to-Image Generative Adversarial Networks (T2I-GANs). To provide a more nuanced understanding, we introduce the concepts of fidelity and accuracy in model extraction attacks targeting T2I-GANs. Extensive experimental validation in black-box attack scenarios demonstrates that we achieve high-fidelity and high-accuracy extraction of T2I-GAN models. We employ the CLIP model to filter queried data, resulting in a fidelity of 81 % for the substitute model. Furthermore, through subsampling techniques, we effectively filter high-quality samples that closely resemble the distribution of real datasets, thereby increasing accuracy to 87 %.",
      "year": 2024,
      "venue": "2024 IEEE Cyber Science and Technology Congress (CyberSciTech)",
      "authors": [
        "Ying Chen",
        "Weihao Guo",
        "Pingyuan Ge",
        "Xiaoji Ma",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eb9378e6c7613404e347327caf130bdcdff1d66f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "title": "MirrorNet: A TEE-Friendly Framework for Secure On-Device DNN Inference",
      "abstract": "Deep neural network (DNN) models have become prevalent in edge devices for real-time inference. However, they are vulnerable to model extraction attacks and require protection. Existing defense approaches either fail to fully safeguard model confidentiality or result in significant latency issues. To overcome these challenges, this paper presents MirrorNet, which leverages Trusted Execution Environment (TEE) to enable secure on-device DNN inference. It generates a TEE-friendly implementation for any given DNN model to protect the model confidentiality, while meeting the stringent computation and storage constraints of TEE. The framework consists of two key components: the backbone model (BackboneNet), which is stored in the normal world but achieves lower inference accuracy, and the Companion Partial Monitor (CPM), a lightweight mirrored branch stored in the secure world, preserving model confidentiality. During inference, the CPM monitors the intermediate results from the BackboneNet and rectifies the classification output to achieve higher accuracy. To enhance flexibility, MirrorNet incorporates two modules: the CPM Strategy Generator, which generates various protection strategies, and the Performance Emulator, which estimates the performance of each strategy and selects the most optimal one. Extensive experiments demonstrate the effectiveness of MirrorNet in providing security guarantees while maintaining low computation latency, making MirrorNet a practical and promising solution for secure on-device DNN inference. For the evaluation, MirrorNet can achieve a 18.6% accuracy gap between authenticated and illegal use, while only introducing 0.99% hardware overhead.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Ziyu Liu",
        "Yukui Luo",
        "Shijin Duan",
        "Tong Zhou",
        "Xiaolin Xu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "pdf_url": "https://arxiv.org/pdf/2311.09489",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "title": "SCMA: A Scattering Center Model Attack on CNN-SAR Target Recognition",
      "abstract": "Convolutional neural networks (CNNs) have been widely used in synthetic aperture radar (SAR) target recognition, which can extract feature automatically. However, due to its own structural flaws, CNNs are easy to be fooled by adversarial examples, even if they have excellent performance. In this letter, a novel attack named scattering center model attack (SCMA) is designed, and its generation process does not rely on the prior knowledge of any neural network. Therefore, we can get a stable way which can be applied to any neural network. In addition, an improved scattering center model extraction method, which is the pre-part of SCMA, can filter out the useless noise to optimize the stability of attack. In the experiment, SCMA is compared with advanced attack algorithms. From the experimental results, it is clear to find that SCMA has excellent performance in terms of transfer attack success rate. Furthermore, visualization and interpretability analysis underpin the theoretical feasibility of SCMA.",
      "year": 2023,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Weibo Qin",
        "Bo Long",
        "Feng Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a3280f5d697c09946b371c8db82da514a4fa3d47",
      "title": "Efficient Nonprofiled Side-Channel Attack Using Multi-Output Classification Neural Network",
      "abstract": "Differential deep learning analysis (DDLA) is the first deep-learning-based nonprofiled side-channel attack (SCA) on embedded systems. However, DDLA requires many training processes to distinguish the correct key. In this letter, we introduce a nonprofiled SCA technique using multi-output classification to mitigate the aforementioned issue. Specifically, a multi-output multilayer perceptron and a multi-output convolutional neural network are introduced against various SCA protected schemes, such as masking, noise generation, and trace de-synchronization countermeasures. The experimental results on different power side channel datasets have clarified that our model performs the attack up to 9\u201330 times faster than DDLA in the case of masking and de-synchronization countermeasures, respectively. In addition, regarding combined masking and noise generation countermeasure, our proposed model achieves a higher success rate of at least 20% in the cases of the standard deviation equal to 1.0 and 1.5.",
      "year": 2023,
      "venue": "IEEE Embedded Systems Letters",
      "authors": [
        "Van-Phuc Hoang",
        "Ngoc-Tuan Do",
        "Van-Sang Doan"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/a3280f5d697c09946b371c8db82da514a4fa3d47",
      "pdf_url": "",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8a04f36017f7a8864118ce801029c21972c6fda8",
      "title": "DNN-Alias: Deep Neural Network Protection Against Side-Channel Attacks via Layer Balancing",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose DNN-Alias, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, DNN-Alias performs various layer-obfuscation operations, e.g., layer branching, layer deepening, etc, to alter the run-time traces while maintaining the functionality. DNN-Alias deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget. We demonstrate the effectiveness of our DNN-Alias technique by obfuscating the architecture of 700 randomly generated and obfuscated DNNs running on multiple Nvidia RTX 2080 TI GPU-based machines. Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the original DNN accurately. Moreover, we obfuscate the architecture of various DNNs, such as the VGG-11, VGG-13, ResNet-20, and ResNet-32 networks. Training the DNNs using the standard CIFAR10 dataset, we show that our DNN-Alias maintains the functionality of the original DNNs by preserving the original inference accuracy. Further, the experiments highlight that adversarial attack on obfuscated DNNs is unsuccessful.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "O. Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8a04f36017f7a8864118ce801029c21972c6fda8",
      "pdf_url": "http://arxiv.org/pdf/2303.06746",
      "publication_date": "2023-03-12",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3c103408ff825aad19d715edc01025a8c3fccdb4",
      "title": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles",
      "abstract": "Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to pruned models, and when applied across GPUs. We find that EZClone correctly predicts DNN architectures for the entire set of PyTorch vision architectures with 100% accuracy. No other work has shown this degree of architecture prediction accuracy with the same adversarial constraints or using aggregate side-channel information. Prior work has shown that, once a DNN has been successfully cloned, further attacks such as model evasion or model inversion can be accelerated significantly.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jonah O'Brien Weiss",
        "Tiago A. O. Alves",
        "S. Kundu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3c103408ff825aad19d715edc01025a8c3fccdb4",
      "pdf_url": "http://arxiv.org/pdf/2304.03388",
      "publication_date": "2023-04-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "468a78d431be0d6290bb3007e6920e15761b751e",
      "title": "SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models",
      "abstract": "While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community. and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Zheng Li",
        "Ziqing Yang",
        "Xinlei He",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/468a78d431be0d6290bb3007e6920e15761b751e",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eacb66ac30b489f704dedae7abc6e98429f95c88",
      "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
      "abstract": "As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Guangjing Wang",
        "Ce Zhou",
        "Yuanda Wang",
        "Bocheng Chen",
        "Hanqing Guo",
        "Qiben Yan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eacb66ac30b489f704dedae7abc6e98429f95c88",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bbd454a77f507fb292b607c6bacc941f52474009",
      "title": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps",
      "abstract": "The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the operation of a DL app. To demonstrate the effectiveness of our attack, we carry out an empirical study on 517 real-world DL apps collected from Google Play. Among 320 apps utilizing MLkit, we find that 81.56% of them can be successfully attacked. The results emphasize the importance of DL app developers being aware of and taking actions to secure on-device models from the perspective of data processing.",
      "year": 2023,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "Ye Sang",
        "Yujin Huang",
        "Shuo Huang",
        "Helei Cui"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/bbd454a77f507fb292b607c6bacc941f52474009",
      "pdf_url": "https://arxiv.org/pdf/2305.03963",
      "publication_date": "2023-05-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "title": "FDINet: Protecting Against DNN Model Extraction Using Feature Distortion Index",
      "abstract": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINet, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary\u2019s queries, we reveal that the feature distribution of these queries deviates from that of the model\u2019s problem domain. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINet utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINet against six state-of-the-art extraction attacks on four benchmark datasets and four popular model architectures. Empirical results demonstrate the following findings: 1) FDINet proves to be highly effective in detecting model extraction, achieving a 100% detection accuracy on DFME and DaST. 2) FDINet is highly efficient, using just 50 queries to raise an extraction alarm with an average confidence of 96.08% for GTSRB. 3) FDINet exhibits the capability to identify colluding adversaries with an accuracy exceeding 91%. Additionally, it demonstrates the ability to detect two types of adaptive attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zheng Li",
        "Haiqin Weng",
        "Feng Xue",
        "Kui Ren",
        "Zhan Qin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "pdf_url": "",
      "publication_date": "2023-06-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "34bed407d65517ed2c8b98bab3a33da175677c59",
      "title": "A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots",
      "abstract": "Building advanced machine learning (ML) models requires expert knowledge and many trials to discover the best architecture and hyperparameter settings. Previous work demonstrates that model information can be leveraged to assist other attacks, such as membership inference, generating adversarial examples. Therefore, such information, e.g., hyperparameters, should be kept confidential. It is well known that an adversary can leverage a target ML model's output to steal the model's information. In this paper, we discover a new side channel for model information stealing attacks, i.e., models' scientific plots which are extensively used to demonstrate model performance and are easily accessible. Our attack is simple and straightforward. We leverage the shadow model training techniques to generate training data for the attack model which is essentially an image classifier. Extensive evaluation on three benchmark datasets shows that our proposed attack can effectively infer the architecture/hyperparameters of image classifiers based on convolutional neural network (CNN) given the scientific plot generated from it. We also reveal that the attack's success is mainly caused by the shape of the scientific plots, and further demonstrate that the attacks are robust in various scenarios. Given the simplicity and effectiveness of the attack method, our study indicates scientific plots indeed constitute a valid side channel for model information stealing attacks. To mitigate the attacks, we propose several defense mechanisms that can reduce the original attacks' accuracy while maintaining the plot utility. However, such defenses can still be bypassed by adaptive attacks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Xinlei He",
        "Yun Shen",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/34bed407d65517ed2c8b98bab3a33da175677c59",
      "pdf_url": "http://arxiv.org/pdf/2302.11982",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0005c9691c8c299476d201d0a5a3c86b49593fac",
      "title": "High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks, with an almost 100% success rate. To counter this attack, we devise an adversarial framework for the watermarking network. The framework incorporates a specially designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0005c9691c8c299476d201d0a5a3c86b49593fac",
      "pdf_url": "http://arxiv.org/pdf/2302.08637",
      "publication_date": "2023-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b8c962da13cbca14d510b3359b42533291ad853",
      "title": "APGP: Accuracy-Preserving Generative Perturbation for Defending Against Model Cloning Attacks",
      "abstract": "Well-trained Deep Neural Networks (DNNs) are valuable intellectual properties. Recent studies show that adversaries only with black-box query access can steal the functionality of DNNs by using knowledge distillation (KD) techniques. In this paper, we propose a novel formulation to defend against model cloning attacks. Then we implement our defense as a plug-and-play generative perturbation model, dubbed as Accuracy-Preserving Generative Perturbation (APGP). Our method is the first to effectively defend against KD-based model cloning without damaging model accuracy. Numerous experiments demonstrate the effectiveness of our defense across different datasets and DNN model cloning attacks, and the advances compared to existing methods.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Anda Cheng",
        "Jian Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8b8c962da13cbca14d510b3359b42533291ad853",
      "pdf_url": "https://doi.org/10.1109/icassp49357.2023.10094956",
      "publication_date": "2023-06-04",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "title": "BarraCUDA: Edge GPUs do Leak DNN Weights",
      "abstract": "Over the last decade, applications of neural networks (NNs) have spread to various aspects of our lives. A large number of companies base their businesses on building products that use neural networks for tasks such as face recognition, machine translation, and self-driving cars. Much of the intellectual property underpinning these products is encoded in the exact parameters of the neural networks. Consequently, protecting these is of utmost priority to businesses. At the same time, many of these products need to operate under a strong threat model, in which the adversary has unfettered physical control of the product. In this work, we present BarraCUDA, a novel attack on general purpose Graphic Processing Units (GPUs) that can extract parameters of neural networks running on the popular Nvidia Jetson Nano device. BarraCUDA uses correlation electromagnetic analysis to recover parameters of real-world convolutional neural networks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "P\u00e9ter Horv\u00e1th",
        "Lukasz Chmielewski",
        "L\u00e9o Weissbart",
        "L. Batina",
        "Y. Yarom"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "cdc97244b92d836d759776aced5fba4f4a976e66",
      "title": "Enabling DVFS Side-Channel Attacks for Neural Network Fingerprinting in Edge Inference Services",
      "abstract": "The Inference-as-a-Service (IaaS) delivery model provides users access to pre-trained deep neural networks while safeguarding network code and weights. However, IaaS is not immune to security threats, like side-channel attacks (SCAs), that exploit unintended information leakage from the physical characteristics of the target device. Exposure to such threats grows when IaaS is deployed on distributed computing nodes at the edge. This work identifies a potential vulnerability of low-power CPUs that facilitates stealing the deep neural network architecture without physical access to the hardware or interference with the execution flow. Our approach relies on a Dynamic Voltage and Frequency Scaling (DVFS) side-channel attack, which monitors the CPU frequency state during the inference stages. Specifically, we introduce a dedicated load-testing methodology that imprints distinguishable signatures of the network on the frequency traces. A machine learning classifier is then used to infer the victim architecture. Experimental results on two commercial ARM Cortex-A CPUs, the A72 and A57, demonstrate the attack can identify the target architecture from a pool of 12 convolutional neural networks with an average accuracy of 98.7% and 92.4%",
      "year": 2023,
      "venue": "International Symposium on Low Power Electronics and Design",
      "authors": [
        "Erich Malan",
        "Valentino Peluso",
        "A. Calimera",
        "Enrico Macii"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/cdc97244b92d836d759776aced5fba4f4a976e66",
      "pdf_url": "",
      "publication_date": "2023-08-07",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0d0a3261b710c43bb443598b407694922a475ba2",
      "title": "A two-stage model extraction attack on GANs with a small collected dataset",
      "abstract": null,
      "year": 2023,
      "venue": "Computers & security",
      "authors": [
        "Hui Sun",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0d0a3261b710c43bb443598b407694922a475ba2",
      "pdf_url": "",
      "publication_date": "2023-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d79a93189a8acc8306504ed20a48fec9934897d9",
      "title": "Electromagnetic Field Model of Tubular Permanent Magnet Synchronous Linear Motor Based on Deep Transfer Neural Network",
      "abstract": "During high-speed operation, tubular permanent magnet linear motors with slotted stators often experience large fluctuations in thrust, overheating of the coils and irreversible demagnetization of the permanent magnets. For the first two cases, this paper proposes a deep transfer neural network (DTNN)-based electromagnetic field model for TPMLM, the specific implementation steps of which include: (1) Using different geometric parameters of TPMLM as input and average thrust, thrust fluctuation and coil copper loss as output, finite element analysis (FEA) and analytical method (AM) are used to provide the electromagnetic parameter dataset of the motor respectively, and these two datasets are used as the source and target domains for transfer learning; (2) Based on the features of the sample dataset, the source tasks corresponding to the source domain are pre-trained using DTNN, and the target tasks corresponding to the target domain are fitted by fine-tuning the model parameters. Then, in order to verify the accuracy of the proposed model, this paper compares the output prediction results with some non-parametric models, such as Random Forest (RF), Support Vector Machine (SVM), and Deep Neural Network (DNN), by dividing the target domain dataset with different scales. The results show that the best prediction results are obtained from the DTNN model, which fully combines the accuracy of FEA and the efficiency of AM, and shows better generalization ability in the case of insufficient real training data.",
      "year": 2023,
      "venue": "ACM Cloud and Autonomic Computing Conference",
      "authors": [
        "Kai Zhu",
        "Tao Wu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d79a93189a8acc8306504ed20a48fec9934897d9",
      "pdf_url": "",
      "publication_date": "2023-11-17",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "title": "Interesting Near-boundary Data: Inferring Model Ownership for DNNs",
      "abstract": "Deep neural networks (DNNs) require expensive training, which is why the protection of model intellectual property (IP) is becoming more critical. Recently, model stealing has emerged frequently, and many researchers design model watermarking and fingerprinting for verifying model ownership. However, attacks such as ambiguity statements have been used to break the current defense, which poses a challenge to model ownership verification. Therefore, this paper proposes an interesting near-boundary data as evidence for obtaining model ownership and innovatively proposes to infer model ownership instead of verifying model ownership. In this paper, we propose to generate the initial near-boundary data using an algorithm that adds slight noise to generate adversarial examples. We design a generator to privatize the near-boundary data. Our main observation is that the near-boundary data exhibit results close to the classification boundary in both the source model and its derived stolen model. At the end of this work, we design many experiments to verify the effectiveness of the proposed method. The experimental results demonstrate that model ownership can be inferred with high confidence. Noting that our method does not require the training data to be private, and it is extremely costly for model stealers to reuse our method.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhe Sun",
        "Zongwen Yang",
        "Zhongyu Huang",
        "Yu Zhang",
        "Jianzhong Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "title": "Decepticon: Attacking Secrets of Transformers",
      "abstract": "With the growing burden of training deep learning models with huge datasets, transfer learning has been widely adopted (e.g., Transformers like BERT, GPT). Transfer learning significantly reduces the time and effort of model training. However, the security impact of using shared pre-trained models has not been evaluated. In this paper, we provide in-depth characterizations of the fine-tuning process and reveal the security vulnerabilities of transfer-learned models. Then, we show a novel two-level model extraction attack; 1) identifying the pre-trained model of a victim transfer-learned model through model fingerprint collected from off-the-shelf GPUs and 2) extracting the entire weights of the victim black-box model based on the hints in the pre-trained model. The extracted model shows almost alike prediction accuracy with over 94% matching prediction outputs with the victim model. The two-level model extraction enables large model weight extraction that is considered as challenging if not impossible through significantly reduced extraction effort.",
      "year": 2023,
      "venue": "IEEE International Symposium on Workload Characterization",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Fan Yao",
        "Meng Tang",
        "Hyeran Jeon"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "title": "Ownership Protection of Generative Adversarial Networks",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process, and they are not robust to emerging model extraction attacks. In this paper, we propose a new ownership protection method based on the common characteristics of a target model and its stolen models. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method can achieve the best protection performance, compared to the state-of-the-art methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, different datasets, as well as adaptive attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "pdf_url": "http://arxiv.org/pdf/2306.05233",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "title": "Semantic Awareness Model For Binary Operator Code",
      "abstract": "With the promotion of artificial intelligence in various industries, there have been some organizations and individuals using various means to attack it. Common types of attacks against models include adversarial sample attacks, data poisoning attacks, and model stealing attacks. The above attack methods require a certain understanding of the structure of the model, so it becomes a challenge to restore the category of binary operators from the model.Binary code similarity detection (BCSD) has important applications in code checking, vulnerability detection, and malicious co de analysis. Due to the lack of syntax structure information in binary operator code, it is difficult to determine the type of operator. Recent research has focused on using deep learning models to understand the semantics and structural information of binary code to achieve better results. Recent research has shown that deep learning models, especially natural language processing models, can comprehend the semantics of binary code. In this paper, we propose a method for identifying binary operators which uses a sequence-aware model and a structure-aware model to model binary operators. It combines CNN semantics and Transformer semantics for classification. The evaluation shows that our method can achieve good performance in binary operator classification.",
      "year": 2023,
      "venue": "2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)",
      "authors": [
        "Haichao Gao",
        "Liming Fang",
        "Yang Li",
        "Minghui Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "pdf_url": "",
      "publication_date": "2023-11-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "title": "Safety or Not? A Comparative Study for Deep Learning Apps on Smartphones",
      "abstract": "Recent years have witnessed an astonishing explosion in the evolution of mobile applications powered by deep learning (DL) technologies. Considering that inference of DL models in the cloud requires transferring user data to server, which is prone to the risk of user privacy leakage, many developers choose to deploy the models on local devices for executing the inference process. However, this also raises a number of other security issues, such as adversarial attacks, model stealing attacks, etc. To explore the security issues that exist in deep learning applications (DL apps), we conducted the first comprehensive comparative study of the top 200 apps in each category on Google Play. We built DLApplnspector, a vulnerability detection tool that combines dynamic and static analysis methods for dissecting apps, which helped us automate our empirical study on real-world mobile DL apps. First, we identify DL apps and extract their models by using DL Checker. Subsequently, Static Scoper is provided to detect encryption and reusability of DL models. Finally, within Dynamic Scoper, we use reverse engineering techniques on the network traffic to parse out the packets and collect side-channel information during application runtime. Our research shows that the majority of developers prefer to use open-source models, with almost 92% of models successfully parsed. This suggests that most models are unprotected. DL apps are more likely to upload user behaviour and collect private data than Non-DL apps. We provide security recommendations for developers and users to address the issues discovered.",
      "year": 2023,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Jin Au-Yeung",
        "Shanshan Wang",
        "Yuchen Liu",
        "Zhenxiang Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "44c48936a86b61d584b38661e3849d5345c1520f",
      "title": "StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor",
      "abstract": "In this work, we propose StegGuard, a novel fingerprinting mechanism to verify the ownership of the suspect pre-trained encoder using steganography. A critical perspective in StegGuard is that the unique characteristic of the transformation from an image to an embedding, conducted by the pre-trained encoder, can be equivalently exposed how an embeder embeds secrets into images and how an extractor extracts the secrets from encoder's embeddings with a tolerable error after the secrets are subjected to the encoder's transformation. While each independent encoder has a distinct transformation, the piracy encoder has a similar transformation to the victim. Based on these, we learn a pair of secrets embeder and extractor as the fingerprint for the victim encoder. We introduce a frequency-domain channel attention embedding block into the embeder to adaptively embed secrets into suitable frequency bands. During verification, if the secrets embedded into the query images can be extracted with an acceptable error from the suspect encoder's embeddings, the suspect encoder is determined as piracy, otherwise independent. Extensive experiments demonstrate that depending on a very limited number of query images, StegGuard can reliably identify across varied independent encoders, and is robust against model stealing related attacks including model extraction, fine-tuning, pruning, embedding noising and shuffle.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xingdong Ren",
        "Tianxing Zhang",
        "Hanzhou Wu",
        "Xinpeng Zhang",
        "Yinggui Wang",
        "Guangling Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/44c48936a86b61d584b38661e3849d5345c1520f",
      "pdf_url": "https://arxiv.org/pdf/2310.03380",
      "publication_date": "2023-10-05",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4fb6895f62f27f200b9f9d3556257473e411ba86",
      "title": "Cloning Object Detectors with Limited Access to In-Distribution Samples",
      "abstract": "An object detector identifies and locates objects in images. Object detectors are widely deployed in practice, for example in driver assistance systems. Recently it has been shown that state-of-the-art object detectors based on neural networks can be cloned successfully if the adversary has oracle access to the detector, and if the adversary has access to either: (i) sufficiently many images drawn from the same distribution as the images of the detector's train set, also referred to as in-distribution samples, or (ii) a publicly available generative AI network capable of generating images that are close to in-distribution samples. This paper presents a new cloning attack that uses images from a publicly and freely available dataset, referred to as out-of-distribution samples, and a limited number of in-distribution samples. The new attack includes a strategy for combining in-and out-of-distribution samples during training and a calibration step to better mimic the functionality of the oracle detector. Our experiments show that CenterNet and RetinaNet object detectors trained with the Oxford-IIIT Pet, the WIDER FACE, or the Tsinghua-Tencent 100K dataset can be cloned successfully using images from the ImageNet-1K dataset supplemented with a limited number of in-distribution samples.",
      "year": 2023,
      "venue": "2023 IEEE 13th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)",
      "authors": [
        "Arne Aarts",
        "Wil Michiels",
        "Peter Roelse"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4fb6895f62f27f200b9f9d3556257473e411ba86",
      "pdf_url": "",
      "publication_date": "2023-09-03",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
      "abstract": "Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.11See our code in https://github.com/zeyangsha/Cont-Steal.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zeyang Sha",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "pdf_url": "https://arxiv.org/pdf/2201.07513",
      "publication_date": "2022-01-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "title": "Efficient Query-based Black-box Attack against Cross-modal Hashing Retrieval",
      "abstract": "Deep cross-modal hashing retrieval models inherit the vulnerability of deep neural networks. They are vulnerable to adversarial attacks, especially for the form of subtle perturbations to the inputs. Although many adversarial attack methods have been proposed to handle the robustness of hashing retrieval models, they still suffer from two problems: (1) Most of them are based on the white-box settings, which is usually unrealistic in practical application. (2) Iterative optimization for the generation of adversarial examples in them results in heavy computation. To address these problems, we propose an Efficient Query-based Black-Box Attack (EQB2A) against deep cross-modal hashing retrieval, which can efficiently generate adversarial examples for the black-box attack. Specifically, by sending a few query requests to the attacked retrieval system, the cross-modal retrieval model stealing is performed based on the neighbor relationship between the retrieved results and the query, thus obtaining the knockoffs to substitute the attacked system. A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation. While the entire network training converges, EQB2A can efficiently generate adversarial examples by forward-propagation with only given benign images. Experiments show that EQB2A achieves superior attacking performance under the black-box setting.",
      "year": 2022,
      "venue": "ACM Trans. Inf. Syst.",
      "authors": [
        "Lei Zhu",
        "Tianshi Wang",
        "Jingjing Li",
        "Zheng Zhang",
        "Jialie Shen",
        "Xinhua Wang"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "pdf_url": "",
      "publication_date": "2022-09-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a4c748225360d89a6b03e1f277daddb64f429bcc",
      "title": "Imitated Detectors: Stealing Knowledge of Black-box Object Detectors",
      "abstract": "Deep neural networks have shown great potential in many practical applications, yet their knowledge is at the risk of being stolen via exposed services (\\eg APIs). In contrast to the commonly-studied classification model extraction, there exist no studies on the more challenging object detection task due to the sufficiency and efficiency of problem domain data collection. In this paper, we for the first time reveal that black-box victim object detectors can be easily replicated without knowing the model structure and training data. In particular, we treat it as black-box knowledge distillation and propose a teacher-student framework named Imitated Detector to transfer the knowledge of the victim model to the imitated model. To accelerate the problem domain data construction, we extend the problem domain dataset by generating synthetic images, where we apply the text-image generation process and provide short text inputs consisting of object categories and natural scenes; to promote the feedback information, we aim to fully mine the latent knowledge of the victim model by introducing an iterative adversarial attack strategy, where we feed victim models with transferable adversarial examples making victim provide diversified predictions with more information. Extensive experiments on multiple datasets in different settings demonstrate that our approach achieves the highest model extraction accuracy and outperforms other model stealing methods by large margins in the problem domain dataset. Our codes can be found at \\urlhttps://github.com/LiangSiyuan21/Imitated-Detectors.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Siyuan Liang",
        "Aishan Liu",
        "Jiawei Liang",
        "Longkang Li",
        "Yang Bai",
        "Xiaochun Cao"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/a4c748225360d89a6b03e1f277daddb64f429bcc",
      "pdf_url": "",
      "publication_date": "2022-10-10",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "title": "Clairvoyance: Exploiting Far-field EM Emanations of GPU to \"See\" Your DNN Models through Obstacles at a Distance",
      "abstract": "Deep neural networks (DNNs) are becoming increasingly popular in real-world applications, and they are considered valuable assets of enterprises. In recent years, a number of model extraction attacks have been formulated that can be mounted to successfully steal proprietary DNN models. Nevertheless, previous model extraction attacks require either logical access to the target models or physical access to the victim machines, and thus are not suitable for performing model stealing in scenarios where an outside attacker is in the proximity but at a distance.In this paper, we propose a new model extraction attack named Clairvoyance that exploits certain far-field electromagnetic signals emanated from a GPU to steal DNN models at a distance of several meters away from the victim machine even with some obstacles in-between. Using Clairvoyance, an attacker can effectively deduce DNN architectures (e.g., the number of layers and their types) and layer configurations (e.g., the number of kernels, sizes of layers, and sizes of strides). We use several case studies (e.g., VGG and ResNet) to demonstrate its effectiveness.",
      "year": 2022,
      "venue": "2022 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Sisheng Liang",
        "Zihao Zhan",
        "Fan Yao",
        "Long Cheng",
        "Zhenkai Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "pdf_url": "",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "title": "Stealthy Inference Attack on DNN via Cache-based Side-Channel Attacks",
      "abstract": "The advancement of deep neural networks (DNNs) motivates the deployment in various domains, including image classification, disease diagnoses, voice recognition, etc. Since some tasks that DNN undertakes are very sensitive, the label information is confidential and contains a commercial value or critical privacy. This paper demonstrates that DNNs also bring a new security threat, leading to the leakage of label information of input instances for the DNN models. In particular, we leverage the cache-based side-channel attack (SCA), i.e., Flush-Reload on the DNN (victim) models, to observe the execution of computation graphs, and create a database of them for building a classifier that the attacker can use to decide the label information of (unknown) input instances for victim models. Then we deploy the cache-based SCA on the same host machine with victim models and deduce the labels with the attacker's classification model to compromise the privacy and confidentiality of victim models. We explore different settings and classification techniques to achieve a high attack success rate of stealing label information from the victim models. Additionally, we consider two attacking scenarios: binary attacking identifies specific sensitive labels and others while multi-class attacking targets recognize all classes victim DNNs provide. Last, we implement the attack on both static DNN models with identical architectures for all inputs and dynamic DNN models with an adaptation of architectures for different inputs to demonstrate the vast existence of the proposed attack, including DenseNet 121, DenseNet 169, VGG 16, VGG 19, MobileNet v1, and MobileNet v2. Our experiment exhibits that MobileNet v1 is the most vulnerable one with 99% and 75.6% attacking success rates for binary and multi-class attacking scenarios, respectively.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Han Wang",
        "Syed Mahbub Hafiz",
        "Kartik Patwari",
        "Chen-Nee Chuah",
        "Zubair Shafiq",
        "H. Homayoun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c57688fde64650fe767a2ba57341520595b5b13",
      "title": "A Practical Introduction to Side-Channel Extraction of Deep Neural Network Parameters",
      "abstract": "Model extraction is a major threat for embedded deep neural network models that leverages an extended attack surface. Indeed, by physically accessing a device, an adversary may exploit side-channel leakages to extract critical information of a model (i.e., its architecture or internal parameters). Different adversarial objectives are possible including a fidelity-based scenario where the architecture and parameters are precisely extracted (model cloning). We focus this work on software implementation of deep neural networks embedded in a high-end 32-bit microcontroller (Cortex-M7) and expose several challenges related to fidelity-based parameters extraction through side-channel analysis, from the basic multiplication operation to the feed-forward connection through the layers. To precisely extract the value of parameters represented in the single-precision floating point IEEE-754 standard, we propose an iterative process that is evaluated with both simulations and traces from a Cortex-M7 target. To our knowledge, this work is the first to target such an high-end 32-bit platform. Importantly, we raise and discuss the remaining challenges for the complete extraction of a deep neural network model, more particularly the critical case of biases.",
      "year": 2022,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/4c57688fde64650fe767a2ba57341520595b5b13",
      "pdf_url": "https://arxiv.org/pdf/2211.05590",
      "publication_date": "2022-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dba696ce4856d7fad6a51847a8997145050eb9c3",
      "title": "GAME: Generative-Based Adaptive Model Extraction Attack",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Yi Xie",
        "Mengdie Huang",
        "Xiaoyu Zhang",
        "Changyu Dong",
        "W. Susilo",
        "Xiaofeng Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/dba696ce4856d7fad6a51847a8997145050eb9c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "title": "Transformer-based Extraction of Deep Image Models",
      "abstract": "Model extraction attacks pose a threat to the security of ML models and to the privacy of the data used for training. Previous research has shown that such attacks can be either monetarily motivated to gain an edge over competitors or maliciously in order to mount subsequent attacks on the extracted model. In this paper, recent advances in the field of transformers are exploited to propose an attack tailored to the task of image classification that allows stealing complex convolutional neural network models without any knowledge of their architecture. The attack was performed on a range of datasets and target architectures to evaluate the robustness of the proposed attack. With only 100k queries, we were able to recover up to 99.2% of the black-box target network's accuracy on the test set. We conclude that it is possible to effectively steal complex neural networks with relatively little expertise and conventional means \u2013 even without knowledge of the target's architecture. Recently proposed defences have also been examined for their effectiveness in preventing the attack proposed in this paper.",
      "year": 2022,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Verena Battis",
        "A. Penner"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c31794c04c50ec4386110cc9efa206dce344919b",
      "title": "DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks",
      "abstract": "Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim\u2019s architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim\u2019s data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model\u2019s architecture deviates from the victim\u2019s. To accomplish this, DeepTaster generates adversarial images with perturbations, transforms them into the Fourier frequency domain, and uses these transformed images to identify the dataset used in a suspect model. The underlying premise is that adversarial images can capture the unique characteristics of DNNs built with a specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated the effectiveness of DeepTaster by assessing its detection accuracy on three datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures (ResNet18, VGG16, and DenseNet161). We conducted experiments under various attack scenarios, including transfer learning, pruning, fine-tuning, and data augmentation. Specifically, in the Multi-Architecture Attack scenario, DeepTaster was able to identify all the stolen cases across all datasets, while DeepJudge failed to detect any of the cases.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Seonhye Park",
        "A. Abuadbba",
        "Shuo Wang",
        "Kristen Moore",
        "Yansong Gao",
        "Hyoungshick Kim",
        "Surya Nepal"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c31794c04c50ec4386110cc9efa206dce344919b",
      "pdf_url": "",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "title": "Recovering the Weights of Convolutional Neural Network via Chosen Pixel Horizontal Power Analysis",
      "abstract": null,
      "year": 2022,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Sihan He",
        "Weibin Wu",
        "Yanbin Li",
        "Lu Zhou",
        "Liming Fang",
        "Zhe Liu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/32d8e4dd6b800ba50d9224ae007707b345c7e711",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b18ab575ec113d117bb2482243c412cceb544756",
      "title": "Data-free Defense of Black Box Models Against Adversarial Attacks",
      "abstract": "Several companies often safeguard their trained deep models (i.e. details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as \u2018black boxes' through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via a generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose \u2018wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our \u2018wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a \u2018regenerator' network with an objective to retrieve the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% against the state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender.",
      "year": 2022,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Gaurav Kumar Nayak",
        "Inder Khatri",
        "Shubham Randive",
        "Ruchit Rawal",
        "Anirban Chakraborty"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b18ab575ec113d117bb2482243c412cceb544756",
      "pdf_url": "https://arxiv.org/pdf/2211.01579",
      "publication_date": "2022-11-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87333028a8169e957d1d74b4054ceac9b3d2116a",
      "title": "An Adversarial Learning-based Tor Malware Traffic Detection Model",
      "abstract": "Attackers often use Tor to launch cyberattacks and conduct illegal transactions, threatening cyberspace's security and people's daily lives. Existing methods for malware traffic detection on Tor can be classified as rule-based and network-based, both of which apply machine learning extensively. Tor malware traffic detection systems are often deployed in open network environments. Their machine learning systems are the first to be attacked by adversarial samples. To ensure that Tor is not abused, this paper proposes an Adversarial Learning-based Tor Malware Traffic Detection model, AL-TMTD. We generate realistic attack samples that can evade detection and use these samples to produce an augmented training set for producing hardened detectors. In such a way, we obtain a more resilient Tor malware traffic detection model that achieves adversarial robustness. We validate our proposal through an extensive experimental campaign that considers multiple machine learning algorithms and shadow models. We simulate the adversary to construct functionally approximate shadow models through black-box model extraction and generate adversarial samples to validate the adversarial robustness of our proposed AL-TMTD model. Our experimental results demonstrate that the average accuracy of AL-TMTD after the adversarial retraining is as high as 0.995 in detecting adversarial samples, which is 0.314 without the adversarial retraining, a significant improvement.",
      "year": 2022,
      "venue": "Global Communications Conference",
      "authors": [
        "Xiaoyan Hu",
        "Yishu Gao",
        "Guang Cheng",
        "Hua Wu",
        "Ruidong Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/87333028a8169e957d1d74b4054ceac9b3d2116a",
      "pdf_url": "",
      "publication_date": "2022-12-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "title": "Adversarial Training of Anti-Distilled Neural Network with Semantic Regulation of Class Confidence",
      "abstract": "Knowledge distillation (KD) has been identified as an effective knowledge transfer approach. By learning from the outputs of a pre-trained, over-parameterized teacher network, a compact student network can be trained efficiently to achieve superior performance. Although KD has gained substantial successes, exposure to pre-trained models usually causes potential risks of intellectual property leaks. From a model stealing attacker\u2019s perspective, one can easily mimic the model functionality via KD, resulting in huge financial loss. In this paper, we propose a novel adversarial training framework called semantic nasty teacher, which prevents the teacher model from being copied by the attacker. In specific, we disentangle the semantic relationship in the output logits when training the teacher model, which is the key to success in KD. Experiment results show that neural networks trained with our approach only sacrifices little performance while canceling out the probability of KD-based model stealing.",
      "year": 2022,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Z. Wang",
        "Chengcheng Li",
        "Husheng Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "pdf_url": "",
      "publication_date": "2022-10-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "title": "Generative Extraction of Audio Classifiers for Speaker Identification",
      "abstract": "It is perhaps no longer surprising that machine learning models, especially deep neural networks, are particularly vulnerable to attacks. One such vulnerability that has been well studied is model extraction: a phenomenon in which the attacker attempts to steal a victim's model by training a surrogate model to mimic the decision boundaries of the victim model. Previous works have demonstrated the effectiveness of such an attack and its devastating consequences, but much of this work has been done primarily for image and text processing tasks. Our work is the first attempt to perform model extraction on {\\em audio classification models}. We are motivated by an attacker whose goal is to mimic the behavior of the victim's model trained to identify a speaker. This is particularly problematic in security-sensitive domains such as biometric authentication. We find that prior model extraction techniques, where the attacker \\textit{naively} uses a proxy dataset to attack a potential victim's model, fail. We therefore propose the use of a generative model to create a sufficiently large and diverse pool of synthetic attack queries. We find that our approach is able to extract a victim's model trained on \\texttt{LibriSpeech} using queries synthesized with a proxy dataset based off of \\texttt{VoxCeleb}; we achieve a test accuracy of 84.41\\% with a budget of 3 million queries.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tejumade Afonja",
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Sageev Oore",
        "Nicolas Papernot"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "pdf_url": "http://arxiv.org/pdf/2207.12816",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "title": "Revealing Secrets From Pre-trained Models",
      "abstract": "\u2014With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Trans- former models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers \ufb01ne-tune the model with their own datasets. Transfer-learning signi\ufb01cantly reduces the time and effort of training models. However, it comes at the cost of security concerns. In this paper, we show a new observation that pre-trained models and \ufb01ne-tuned models have signi\ufb01cantly high similarities in weight values. Also, we demonstrate that there exist vendor-speci\ufb01c computing patterns even for the same models. With these new \ufb01ndings, we propose a new model extraction attack that reveals the model architecture and the pre-trained model used by the black-box victim model with vendor-speci\ufb01c computing patterns and then estimates the entire model weights based on the weight value similarities between the \ufb01ne-tuned model and pre-trained model. We also show that the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning. ,",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Hyeran Jeon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "pdf_url": "http://arxiv.org/pdf/2207.09539",
      "publication_date": "2022-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "title": "Adversarial Attacks Against Network Intrusion Detection in IoT Systems",
      "abstract": "Deep learning (DL) has gained popularity in network intrusion detection, due to its strong capability of recognizing subtle differences between normal and malicious network activities. Although a variety of methods have been designed to leverage DL models for security protection, whether these systems are vulnerable to adversarial examples (AEs) is unknown. In this article, we design a novel adversarial attack against DL-based network intrusion detection systems (NIDSs) in the Internet-of-Things environment, with only black-box accesses to the DL model in such NIDS. We introduce two techniques: 1) model extraction is adopted to replicate the black-box model with a small amount of training data and 2) a saliency map is then used to disclose the impact of each packet attribute on the detection results, and the most critical features. This enables us to efficiently generate AEs using conventional methods. With these tehniques, we successfully compromise one state-of-the-art NIDS, Kitsune: the adversary only needs to modify less than 0.005% of bytes in the malicious packets to achieve an average 94.31% attack success rate.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Han Qiu",
        "Tian Dong",
        "Tianwei Zhang",
        "Jialiang Lu",
        "G. Memmi",
        "Meikang Qiu"
      ],
      "citation_count": 213,
      "url": "https://www.semanticscholar.org/paper/4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5beafcd6c0222a2f45863058280873c1ef088ec4",
      "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks",
      "abstract": "Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called \"Simulator\", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online 1.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Chen Ma",
        "Li Chen",
        "Junhai Yong"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/5beafcd6c0222a2f45863058280873c1ef088ec4",
      "pdf_url": "https://arxiv.org/pdf/2009.00960",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e32ab49974b032faeca3804685ba739f5fb09790",
      "title": "Trustworthy and Intelligent COVID-19 Diagnostic IoMT Through XR and Deep-Learning-Based Clinic Data Access",
      "abstract": "This article presents a novel extended reality (XR) and deep-learning-based Internet-of-Medical-Things (IoMT) solution for the COVID-19 telemedicine diagnostic, which systematically combines virtual reality/augmented reality (AR) remote surgical plan/rehearse hardware, customized 5G cloud computing and deep learning algorithms to provide real-time COVID-19 treatment scheme clues. Compared to existing perception therapy techniques, our new technique can significantly improve performance and security. The system collected 25 clinic data from the 347 positive and 2270 negative COVID-19 patients in the Red Zone by 5G transmission. After that, a novel auxiliary classifier generative adversarial network-based intelligent prediction algorithm is conducted to train the new COVID-19 prediction model. Furthermore, The Copycat network is employed for the model stealing and attack for the IoMT to improve the security performance. To simplify the user interface and achieve an excellent user experience, we combined the Red Zone\u2019s guiding images with the Green Zone\u2019s view through the AR navigate clue by using 5G. The XR surgical plan/rehearse framework is designed, including all COVID-19 surgical requisite details that were developed with a real-time response guaranteed. The accuracy, recall, F1-score, and area under the ROC curve (AUC) area of our new IoMT were 0.92, 0.98, 0.95, and 0.98, respectively, which outperforms the existing perception techniques with significantly higher accuracy performance. The model stealing also has excellent performance, with the AUC area of 0.90 in Copycat slightly lower than the original model. This study suggests a new framework in the COVID-19 diagnostic integration and opens the new research about the integration of XR and deep learning for IoMT implementation.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Yonghang Tai",
        "Bixuan Gao",
        "Qiong Li",
        "Zhengtao Yu",
        "Chunsheng Zhu",
        "Victor I. Chang"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/e32ab49974b032faeca3804685ba739f5fb09790",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6488907/9585129/09343340.pdf",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "title": "QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval",
      "abstract": "We study the query-based attack against image retrieval to evaluate its robustness against adversarial examples under the black-box setting, where the adversary only has query access to the top-k ranked unlabeled images from the database. Compared with query attacks in image classification, which produce adversaries according to the returned labels or confidence score, the challenge becomes even more prominent due to the difficulty in quantifying the attack effectiveness on the partial retrieved list. In this paper, we make the first attempt in Query-based Attack against Image Retrieval (QAIR), to completely subvert the top-k retrieval results. Specifically, a new relevance-based loss is designed to quantify the attack effects by measuring the set similarity on the top-k retrieval results before and after attacks and guide the gradient optimization. To further boost the attack efficiency, a recursive model stealing method is proposed to acquire transferable priors on the target model and generate the prior-guided gradients. Comprehensive experiments show that the proposed attack achieves a high attack success rate with few queries against the image retrieval systems under the black-box setting. The attack evaluations on the real-world visual search engine show that it successfully deceives a commercial system such as Bing Visual Search with 98% attack success rate by only 33 queries on average.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiaodan Li",
        "Jinfeng Li",
        "Yuefeng Chen",
        "Shaokai Ye",
        "Yuan He",
        "Shuhui Wang",
        "Hang Su",
        "Hui Xue"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "pdf_url": "https://arxiv.org/pdf/2103.02927",
      "publication_date": "2021-03-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "title": "Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define fidelity and accuracy on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of fidelity extraction and accuracy extraction, according to the adversary\u2019s goals and background knowledge. We further conduct a case study where the adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3485832.3485838",
      "publication_date": "2021-12-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "title": "Secure Automatic Speaker Verification (SASV) System Through sm-ALTP Features and Asymmetric Bagging",
      "abstract": "The growing number of voice-enabled devices and applications consider automatic speaker verification (ASV) a fundamental component. However, maximum outreach for ASV in critical domains e.g., financial services and health care, is not possible unless we overcome security breaches caused by voice cloning algorithms and replayed audios. Therefore, to overcome these vulnerabilities, a secure ASV (SASV) system based on the novel sign modified acoustic local ternary pattern (sm-ALTP) features and asymmetric bagging-based classifier-ensemble with enhanced attack vector is presented. The proposed audio representation approach clusters the high and low frequency components in audio frames by normally distributing frequency components against a convex function. Then, the neighborhood statistics are applied to capture the user specific vocal tract information. The proposed SASV system simultaneously verifies the bonafide speakers and detects the voice cloning attack, cloning algorithm used to synthesize cloned audio (in the defined settings), and voice-replay attacks over the ASVspoof 2019 dataset. In addition, the proposed method detects the voice replay and cloned voice replay attacks over the VSDC dataset. Both the voice cloning algorithm detection and cloned-replay attack detection are novel concepts introduced in this paper. The voice cloning algorithm detection module determines the voice cloning algorithm used to generate the fake audios. Whereas, the cloned voice replay attack detection is performed to determine the SASV behavior when audio samples are simultaneously contemplated with cloning and replay artifacts.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Muteb Aljasem",
        "Aun Irtaza",
        "Hafiz Malik",
        "Noushin Saba",
        "A. Javed",
        "K. Malik",
        "Mohammad Meharmohammadi"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "pdf_url": "https://doi.org/10.1109/tifs.2021.3082303",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f99776ebfbac6da0689e8e43d36f61172c95a92",
      "title": "Back to the Basics: Seamless Integration of Side-Channel Pre-Processing in Deep Neural Networks",
      "abstract": "Deep learning approaches have become popular for Side-Channel Analysis (SCA) in the recent years. Especially Convolutional Neural Networks (CNN) due to their natural ability to overcome jitter-based as well as masking countermeasures. Most of the recent works have been focusing on optimising the performance on given dataset, for example finding optimal architecture and using ensemble, and bypass the need for trace pre-processing. However, trace pre-processing is a long studied topic and several proven techniques exist in the literature. There is no straightforward manner to integrate those techniques into deep learning based SCA. In this paper, we propose a generic framework which allows seamless integration of multiple, user defined pre-processing techniques into the neural network architecture. The framework is based on Multi-scale Convolutional Neural Networks ( $\\mathsf {MCNN}$ ) that were originally proposed for time series analysis. $\\mathsf {MCNN}$ are composed of multiple branches that can apply independent transformation to input data in each branch to extract the relevant features and allowing a better generalization of the model. In terms of SCA, these transformations can be used for integration of pre-processing techniques, such as phase-only correlation, principal component analysis, alignment methods, etc. We present successful results on generic network which generalizes to different publicly available datasets. Our findings show that it is possible to design a network that can be used in a more general way to analyze side-channel leakage traces and perform well across datasets.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yoo-Seung Won",
        "Xiaolu Hou",
        "Dirmanto Jap",
        "J. Breier",
        "S. Bhasin"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/1f99776ebfbac6da0689e8e43d36f61172c95a92",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b408a5e1a60c3afd5954613516126b6c512a7804",
      "title": "Model Extraction and Defenses on Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define accuracy and fidelity on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of accuracy extraction and fidelity extraction, according to the adversary's goals and background knowledge. We further conduct a case study where an adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b408a5e1a60c3afd5954613516126b6c512a7804",
      "pdf_url": "",
      "publication_date": "2021-01-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "title": "Ownership Verification of DNN Architectures via Hardware Cache Side Channels",
      "abstract": "Deep Neural Networks (DNN) are gaining higher commercial values in computer vision applications, e.g., image classification, video analytics, etc. This calls for urgent demands of the intellectual property (IP) protection of DNN models. In this paper, we present a novel watermarking scheme to achieve the ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we claim the model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such watermarks can be extracted via side-channel-based model extraction techniques with high fidelity. We conduct comprehensive experiments on watermarked CNN models for image classification tasks and the experimental results show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations and adaptive attacks.",
      "year": 2021,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Xiaoxuan Lou",
        "Shangwei Guo",
        "Jiwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/159773/2/main.pdf",
      "publication_date": "2021-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c09eaf8e58fabd2371294b26a37376be960546a2",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "N. Asokan"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/c09eaf8e58fabd2371294b26a37376be960546a2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "title": "SCANet: Securing the Weights With Superparamagnetic-MTJ Crossbar Array Networks",
      "abstract": "Deep neural networks (DNNs) form a critical infrastructure supporting various systems, spanning from the iPhone neural engine to imaging satellites and drones. The design of these neural cores is often proprietary or a military secret. Nevertheless, they remain vulnerable to model replication attacks that seek to reverse engineer the network\u2019s synaptic weights. In this article, we propose SCANet (Superparamagnetic-MTJ Crossbar Array Networks), a novel defense mechanism against such model stealing attacks by utilizing the innate stochasticity in superparamagnets. When used as the synapse in DNNs, superparamagnetic magnetic tunnel junctions (s-MTJs) are shown to be significantly more secure than prior memristor-based solutions. The thermally induced telegraphic switching in the s-MTJs is robust and uncontrollable, thus thwarting the attackers from obtaining sensitive data from the network. Using a mixture of both superparamagnetic and conventional MTJs in the neural network (NN), the designer can optimize the time period between the weight updation and the power consumed by the system. Furthermore, we propose a modified NN architecture that can prevent replication attacks while minimizing power consumption. We investigate the effect of the number of layers in the deep network and the number of neurons in each layer on the sharpness of accuracy degradation when the network is under attack. We also explore the efficacy of SCANet in real-time scenarios, using a case study on object detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Dinesh Rajasekharan",
        "N. Rangarajan",
        "Satwik Patnaik",
        "O. Sinanoglu",
        "Y. Chauhan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "pdf_url": "",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "541d59eb64b3ce26c7846f5de0b8f39f4e16de2a",
      "title": "Study on intelligent anti\u2013electricity stealing early-warning technology based on convolutional neural networks",
      "abstract": "In recent years, electricity stealing has been repeatedly prohibited, and as the methods of stealing electricity have become more intelligent and concealed, it is growing increasingly difficult to extract high-dimensional data features of power consumption. In order to solve this problem, a correlation model of power-consumption data based on convolutional neural networks (CNN) is established. First, the original user signal is preprocessed to remove the noise. The user signal with a fixed signal length is then intercepted and the parallel class labelled. The segmented user signals and corresponding labels are input into the convolutional neural network for training, and the trained convolutional neural network is then used to detect and classify the test user signals. Finally, the actual steal leak dataset is used to verify the effectiveness of this algorithm, which proves that the algorithm can effectively carry out anti\u2013-electricity stealing by warning of abnormal power consumption behavior. There are lots of line traces on the surface of the broken ends which left in the cable cutting case crime scene along the high-speed railway in China. The line traces usually present nonlinear morphological features and has strong randomness. It is not very effective when using existing image-processing and three-dimensional scanning methods to do the trace comparison, therefore, a fast algorithm based on wavelet domain feature aiming at the nonlinear line traces is put forward to make fast trace analysis and infer the criminal tools. The proposed algorithm first applies wavelet decomposition to the 1-D signals which picked up by single point laser displacement sensor to partially reduce noises. After that, the dynamic time warping is employed to do trace feature similarity matching. Finally, using linear regression machine learning algorithm based on gradient descent method to do constant iteration. The experiment results of cutting line traces sample data comparison demonstrate the accuracy and reliability of the proposed algorithm.",
      "year": 2021,
      "venue": "Journal of Intelligent & Fuzzy Systems",
      "authors": [
        "Nan Pan",
        "Xin Shen",
        "Xiaojue Guo",
        "M. Cao",
        "Dilin Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/541d59eb64b3ce26c7846f5de0b8f39f4e16de2a",
      "pdf_url": "",
      "publication_date": "2021-01-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3ddb72980630648e61f2223ffc6ef809a98ddd1d",
      "title": "Filter Model Extraction with Convolutional Neural Network Based on Magnitude Information",
      "abstract": "In this paper, a new model extraction method for band-pass filters with convolutional neural network (CNN) based on magnitude information only is introduced. A data augmentation approach and a data decentralization process to improve the CNN model performance by statistical analysis are proposed. A correction process utilizing real test data is introduced to improve the model performance. Compared to the existing model extraction method, this CNN model approach needs no phase information that is highly dependent on the loading circuit of a filter. Merely using the magnitude information, a good fitness for the response of high-order filters with complicated topology is achieved with this method. CNN model demonstrates a faster convergence compared to the traditional fully connected neural model.",
      "year": 2021,
      "venue": "2021 IEEE MTT-S International Microwave Filter Workshop (IMFW)",
      "authors": [
        "Junyi Liu",
        "Ke Wu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3ddb72980630648e61f2223ffc6ef809a98ddd1d",
      "pdf_url": "",
      "publication_date": "2021-11-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models",
      "abstract": "Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "Nirmal Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "pdf_url": "",
      "publication_date": "2021-04-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "89e225f6d755599d14d25580315d60725977ca62",
      "title": "Fast Prediction for Electromagnetic Shielding Effectiveness of Ground-Via Distribution of SiP by Convolutional Neural Network",
      "abstract": "Ground vias are often used as an essential shielding structure in System-in-Package (SiP) to suppress electromagnetic leakage. In this paper, a new method based on Convolutional Neural Network (CNN) is adopted to predict the shielding effectiveness of ground-via distributions. In particular, the suitability of the pooling layer is studied separately. Furthermore, the appropriateness of using the convolutional layer is proved by a comparative experiment with a Deep Neural Network (DNN) model. The method proposed in this paper shows good accuracy in electromagnetic leakage prediction and has universal value in similar prediction tasks. Compared with traditional analysis methods, the proposed CNN model has a significant time advantage, making it possible to use other optimization algorithms to replace the artificial shielding structure design.",
      "year": 2021,
      "venue": "2021 13th Global Symposium on Millimeter-Waves & Terahertz (GSMM)",
      "authors": [
        "Zheming Gu",
        "Tuomin Tao",
        "Erping Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/89e225f6d755599d14d25580315d60725977ca62",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46582b5600668089180542b725a2868692bcce87",
      "title": "An extraction attack on image recognition model using VAE-kdtree model",
      "abstract": "This paper proposes a black box extraction attack model on pre-trained image classifiers to rebuild a functionally equivalent model with high similarity. Common model extraction attacks use a large number of training samples to feed the target classifier which is time-consuming with redundancy. The attack results have a high dependency on the selected training samples and the target model. The extracted model may only get part of crucial features because of inappropriate sample selection. To eliminate these uncertainties, we proposed the VAE-kdtree attack model which eliminates the high dependency between selected training samples and the target model. It can not only save redundant computation, but also extract critical boundaries more accurately in image classification. This VAE-kdtree model has shown to achieve around 90% similarity on MNIST and around 80% similarity on MNIST-Fashion with a target Convolutional Network Model and a target Support Vector Machine Model. The performance of this VAE-kdtree model could be further improved by adopting higher dimension space of the kdtree.",
      "year": 2021,
      "venue": "Other Conferences",
      "authors": [
        "Tianqi Wen",
        "Haibo Hu",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/46582b5600668089180542b725a2868692bcce87",
      "pdf_url": "",
      "publication_date": "2021-03-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "56e39821bebdfa91f9056dc5e1249d927ca8bf23",
      "title": "Telepathic Headache: Mitigating Cache Side-Channel Attacks on Convolutional Neural Networks",
      "abstract": null,
      "year": 2021,
      "venue": "International Conference on Applied Cryptography and Network Security",
      "authors": [
        "H. Chabanne",
        "J. Danger",
        "Linda Guiga",
        "U. K\u00fchne"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/56e39821bebdfa91f9056dc5e1249d927ca8bf23",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "abstract": "Cloud-based Machine Learning as a Service (MLaaS) is gradually gaining acceptance as a reliable solution to various real-life scenarios. These services typically utilize Deep Neural Networks (DNNs) to perform classification and detection tasks and are accessed through Application Programming Interfaces (APIs). Unfortunately, it is possible for an adversary to steal models from cloud-based platforms, even with black-box constraints, by repeatedly querying the public prediction API with malicious inputs. In this paper, we introduce an effective and efficient black-box attack methodology that extracts largescale DNN models from cloud-based platforms with near-perfect performance. In comparison to existing attack methods, we significantly reduce the number of queries required to steal the target model by incorporating several novel algorithms, including active learning, transfer learning, and adversarial attacks. During our experimental evaluations, we validate our proposed model for conducting theft attacks on various commercialized MLaaS platforms hosted by Microsoft, Face++, IBM, Google and Clarifai. Our results demonstrate that the proposed method can easily reveal/steal large-scale DNN models from these cloud platforms. The proposed attack method can also be used to accurately evaluates the robustness of DNN based MLaaS classifiers against theft attacks.",
      "year": 2020,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "citation_count": 183,
      "url": "https://www.semanticscholar.org/paper/4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24178",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "title": "ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data",
      "abstract": "Machine learning models are increasingly being deployed in practice. Machine Learning as a Service (MLaaS) providers expose such models to queries by third-party developers through application programming interfaces (APIs). Prior work has developed model extraction attacks, in which an attacker extracts an approximation of an MLaaS model by making black-box queries to it. We design ActiveThief \u2013 a model extraction framework for deep neural networks that makes use of active learning techniques and unannotated public datasets to perform model extraction. It does not expect strong domain knowledge or access to annotated data on the part of the attacker. We demonstrate that (1) it is possible to use ActiveThief to extract deep classifiers trained on a variety of datasets from image and text domains, while querying the model with as few as 10-30% of samples from public datasets, (2) the resulting model exhibits a higher transferability success rate of adversarial examples than prior work, and (3) the attack evades detection by the state-of-the-art model extraction detection method, PRADA.",
      "year": 2020,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 147,
      "url": "https://www.semanticscholar.org/paper/d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5432/5288",
      "publication_date": "2020-04-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "title": "Now You See Me (CME): Concept-based Model Extraction",
      "abstract": "Deep Neural Networks (DNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering DNN-based approaches is improving their explainability. In this work we present CME: a concept-based model extraction framework, used for analysing DNN models via concept-based extracted models. Using two case studies (dSprites, and Caltech UCSD Birds), we demonstrate how CME can be used to (i) analyse the concept information learned by a DNN model (ii) analyse how a DNN uses this concept information when predicting output labels (iii) identify key concept information that can further improve DNN predictive performance (for one of the case studies, we showed how model accuracy can be improved by over 14%, using only 30% of the available concepts).",
      "year": 2020,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'",
        "Adrian Weller"
      ],
      "citation_count": 81,
      "url": "https://www.semanticscholar.org/paper/ab9f91fe93b59bf44811e6fc1fcdf1b61cf9a5c9",
      "pdf_url": "",
      "publication_date": "2020-10-25",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "title": "GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel",
      "abstract": "In recent years, deep neural networks (DNN) have become an important type of intellectual property due to their high performance on various classification tasks. As a result, DNN stealing attacks have emerged. Many attack surfaces have been exploited, among which cache timing side-channel attacks are hugely problematic because they do not need physical probing or direct interaction with the victim to estimate the DNN model. However, existing cache-side-channel-based DNN reverse engineering attacks rely on analyzing the binary code of the DNN library that must be shared between the attacker and the victim in the main memory. In reality, the DNN library code is often inaccessible because 1) the code is proprietary, or 2) memory sharing has been disabled by the operating system. In our work, we propose GANRED, an attack approach based on the generative adversarial nets (GAN) framework which utilizes cache timing side-channel information to accurately recover the structure of DNNs without memory sharing or code access. The benefit of GANRED is four-fold. 1) There is no need for DNN library code analysis. 2) No shared main memory segment between the victim and the attacker is needed. 3) Our attack locates the exact structure of the victim model, unlike existing attacks which only narrow down the structure search space. 4) Our attack efficiently scales to deeper DNNs, exhibiting only linear growth in the number of layers in the victim DNN.",
      "year": 2020,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23a47ede9466d119448de4226aebe3193b7ce13d",
      "title": "An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its Visualization",
      "abstract": "In recent years, the convolutional neural networks (CNNs) have received a lot of interest in the side-channel community. The previous work has shown that CNNs have the potential of breaking the cryptographic algorithm protected with masking or desynchronization. Before, several CNN models have been exploited, reaching the same or even better level of performance compared to the traditional side-channel attack (SCA). In this paper, we investigate the architecture of Residual Network and build a new CNN model called attention network. To enhance the power of the attention network, we introduce an attention mechanism - Convolutional Block Attention Module (CBAM) and incorporate CBAM into the CNN architecture. CBAM points out the informative points of the input traces and makes the attention network focus on the relevant leakages of the measurements. It is able to improve the performance of the CNNs. Because the irrelevant points will introduce the extra noises and cause a worse performance of attacks. We compare our attention network with the one designed for the masking AES implementation called ASCAD network in this paper. We show that the attention network has a better performance than the ASCAD network. Finally, a new visualization method, named Class Gradient Visualization (CGV) is proposed to recognize which points of the input traces have a positive influence on the predicted result of the neural networks. In another aspect, it can explain why the attention network is superior to the ASCAD network. We validate the attention network through extensive experiments on four public datasets and demonstrate that the attention network is efficient in different AES implementations.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Minhui Jin",
        "Mengce Zheng",
        "Honggang Hu",
        "Nenghai Yu"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/23a47ede9466d119448de4226aebe3193b7ce13d",
      "pdf_url": "",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "title": "FaDec: A Fast Decision-based Attack for Adversarial Machine Learning",
      "abstract": "Due to the excessive use of cloud-based machine learning (ML) services, the smart cyber-physical systems (CPS) are increasingly becoming vulnerable to black-box attacks on their ML modules. Traditionally, the black-box attacks are either transfer attacks requiring model stealing, or score/decision-based gradient estimation attacks requiring a large number of queries. In practical scenarios, especially for cloud-based ML services and timing-constrained CPS use-cases, every query incurs a huge cost, thereby rendering state-of-the-art decision-based attacks ineffective in such settings. Towards this, we propose a novel methodology for automatically generating an extremely fast and imperceptible decision-based attack called FaDec. It follows two main steps: (1) fast estimation of the classification boundary by combining the half-interval search-based algorithm with gradient sign estimation to reduce the number of queries; and (2) adversarial noise optimization to ensure the imperceptibility. For illustration, we evaluate FaDec on the image recognition and traffic sign detection using multiple state-of-the-art DNNs trained on CIFAR-10 and the German Traffic Sign Recognition Benchmarks (GTSRB) datasets. The experimental analysis shows that the proposed FaDec attack is 16x faster compared to the state-of-the-art decision-based attacks, and generates an attack image with better imperceptibility for a much lesser number of iterations, thereby making our attack more powerful in practical scenarios. We open-sourced the complete code and results of our methodology at https://github.com/fklodhi/FaDec.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Faiq Khalid",
        "Hassan Ali",
        "M. Hanif",
        "Semeen Rehman",
        "Rehan Ahmed",
        "M. Shafique"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "85bab667c93b5983c641c03a81e8812669ba6ab8",
      "title": "Improved Hybrid Approach for Side-Channel Analysis Using Efficient Convolutional Neural Network and Dimensionality Reduction",
      "abstract": "Deep learning-based side channel attacks are burgeoning due to their better efficiency and performance, suppressing the traditional side-channel analysis. To launch the successful attack on a particular public key cryptographic (PKC) algorithm, a large number of samples per trace might need to be acquired to capture all the minor useful details from the leakage information, which increases the number of features per instance. The decreased instance-feature ratio increases the computational complexity of the deep learning-based attacks, limiting the attack efficiency. Moreover, data class imbalance can be a hindrance in accurate model training, leading to an accuracy paradox. We propose an efficient Convolutional Neural Network (CNN) based approach in which the dimensionality of the large leakage dataset is reduced, and then the data is processed using the proposed CNN based model. In the proposed model, the optimal number of convolutional blocks is used to build powerful features extractors within the cost limit. We have also analyzed and presented the impact of using the Synthetic Minority Over-sampling Technique (SMOTE) on the proposed model performance. We propose that a data-balancing step should be mandatory for analysis in the side channel attack scenario. We have also provided a performance-based comparative analysis between proposed and existing deep learning models for unprotected and protected Elliptic curve (ECC) Montgomery Power ladder implementations. The reduced network complexity, together with an improved attack efficiency, promote the proposed approach to be effectively used for side-channel attacks.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Naila Mukhtar",
        "A. Fournaris",
        "T. Khan",
        "Charis Dimopoulos",
        "Yinan Kong"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/85bab667c93b5983c641c03a81e8812669ba6ab8",
      "pdf_url": "https://doi.org/10.1109/access.2020.3029206",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "39bf1cd08237224cb0a5b372aa0201ba98f16e4f",
      "title": "Linear-Model-Inspired Neural Network for Electromagnetic Inverse Scattering",
      "abstract": "Electromagnetic inverse scattering problems (ISPs) aim to retrieve permittivities of dielectric scatterers from the scattering measurement. It is often highly nonlinear, causing the problem to be very difficult to solve. To alleviate the issue, this letter exploits a linear-model-based network (LMN) learning strategy, which benefits from both model complexity and data learning. By introducing a linear model for ISPs, a new model with network-driven regularizer is proposed. For attaining efficient end-to-end learning, the network architecture and hyper-parameter estimation are presented. Experimental results validate its superiority to some state of the art.",
      "year": 2020,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Huilin Zhou",
        "Ouyang Tao",
        "Yadan Li",
        "Jian Liu",
        "Qiegen Liu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/39bf1cd08237224cb0a5b372aa0201ba98f16e4f",
      "pdf_url": "https://arxiv.org/pdf/2003.01465",
      "publication_date": "2020-03-03",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "title": "Simple Electromagnetic Analysis Against Activation Functions of Deep Neural Networks",
      "abstract": null,
      "year": 2020,
      "venue": "ACNS Workshops",
      "authors": [
        "Go Takatoi",
        "T. Sugawara",
        "K. Sakiyama",
        "Y. Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/c9c7498580d105ebac92ed80ec6698bb6dbeb76f",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "electromagnetic analysis (title)",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9187199b87187d8abc769d6d8e61a45de8f7870",
      "title": "Low-Dose CT Image Denoising Using Parallel-Clone Networks",
      "abstract": "Deep neural networks have a great potential to improve image denoising in low-dose computed tomography (LDCT). Popular ways to increase the network capacity include adding more layers or repeating a modularized clone model in a sequence. In such sequential architectures, the noisy input image and end output image are commonly used only once in the training model, which however limits the overall learning performance. In this paper, we propose a parallel-clone neural network method that utilizes a modularized network model and exploits the benefit of parallel input, parallel-output loss, and clone-toclone feature transfer. The proposed model keeps a similar or less number of unknown network weights as compared to conventional models but can accelerate the learning process significantly. The method was evaluated using the Mayo LDCT dataset and compared with existing deep learning models. The results show that the use of parallel input, parallel-output loss, and clone-to-clone feature transfer all can contribute to an accelerated convergence of deep learning and lead to improved image quality in testing. The parallel-clone network has been demonstrated promising for LDCT image denoising.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Siqi Li",
        "Guobao Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/b9187199b87187d8abc769d6d8e61a45de8f7870",
      "pdf_url": "",
      "publication_date": "2020-05-14",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "93c0fa4c3b700b1d62affb1e3f1cde5707e29e55",
      "title": "Cell-Phone Classification: A Convolutional Neural Network Approach Exploiting Electromagnetic Emanations",
      "abstract": "In this paper, we propose a methodology to identify both the brand of a cell-phone, and the status of its camera by exploiting electromagnetic (EM) emanations. The method is composed of two parts: Feature extraction and Convolutional Neural Network (CNN). We first extract features by averaging magnitudes of short-time Fourier transform (STFT) of the measured EM signal, which helps to reduce input dimension of the neural network, and to filter spurious emissions. The extracted features are fed into the proposed CNN, which contains two convolutional layers (followed by max-pooling layers), and four fully-connected layers. Finally, we provide experimental results which exhibit more than 99% classification accuracy for the test signals.",
      "year": 2020,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "B. Yilmaz",
        "E. Ugurlu",
        "A. Zaji\u0107",
        "Milos Prvulovi\u0107"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/93c0fa4c3b700b1d62affb1e3f1cde5707e29e55",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57099cf852807c652fd55274a3ce4a8c9777ef56",
      "title": "Neural Model Stealing Attack to Smart Mobile Device on Intelligent Medical Platform",
      "abstract": "To date, the Medical Internet of Things (MIoT) technology has been recognized and widely applied due to its convenience and practicality. The MIoT enables the application of machine learning to predict diseases of various kinds automatically and accurately, assisting and facilitating effective and efficient medical treatment. However, the MIoT are vulnerable to cyberattacks which have been constantly advancing. In this paper, we establish a MIoT platform and demonstrate a scenario where a trained Convolutional Neural Network (CNN) model for predicting lung cancer complicated with pulmonary embolism can be attacked. First, we use CNN to build a model to predict lung cancer complicated with pulmonary embolism and obtain high detection accuracy. Then, we build a copycat model using only a small amount of data labeled by the target network, aiming to steal the established prediction model. Experimental results prove that the stolen model can also achieve a relatively high prediction outcome, revealing that the copycat network could successfully copy the prediction performance from the target network to a large extent. This also shows that such a prediction model deployed on MIoT devices can be stolen by attackers, and effective prevention strategies are open questions for researchers.",
      "year": 2020,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Liqiang Zhang",
        "Guanjun Lin",
        "Bixuan Gao",
        "Zhibao Qin",
        "Yonghang Tai",
        "Jun Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/57099cf852807c652fd55274a3ce4a8c9777ef56",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2020/8859489.pdf",
      "publication_date": "2020-11-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "copycat model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "title": "Learning From A Big Brother - Mimicking Neural Networks in Profiled Side-channel Analysis",
      "abstract": "Recently, deep learning has emerged as a powerful technique for side-channel attacks, capable of even breaking common countermeasures. Still, trained models are generally large, and thus, performing evaluation becomes resource-intensive. The resource requirements increase in realistic settings where traces can be noisy, and countermeasures are active. In this work, we exploit mimicking to compress the learned models. We demonstrate up to 300 times compression of a state-of-the-art CNN. The mimic shallow network can also achieve much better accuracy as compared to when trained on original data and even reach the performance of a deeper network.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Daan van der Valk",
        "Marina Kr\u010dek",
        "S. Picek",
        "S. Bhasin"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "title": "Adversarial Imitation Attack",
      "abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Mingyi Zhou",
        "Jing Wu",
        "Yipeng Liu",
        "Xiaolin Huang",
        "Shuaicheng Liu",
        "Xiang Zhang",
        "Ce Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "pdf_url": "",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
      "abstract": "In Machine Learning as a Service, a provider trains a deep neural network and provides many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a \\emph{surrogate model} from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call \\emph{conferrable} adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the unremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches an AUC of 1.0 in verifying surrogates, compared to an AUC of 0.63 by previous fingerprints.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Nils Lukas",
        "Yuxuan Zhang",
        "F. Kerschbaum"
      ],
      "citation_count": 169,
      "url": "https://www.semanticscholar.org/paper/5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "pdf_url": "",
      "publication_date": "2019-12-02",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "title": "Performance Analysis and Dynamic Evolution of Deep Convolutional Neural Network for Electromagnetic Inverse Scattering",
      "abstract": "The solution of electromagnetic (EM) inverse scattering problems is hindered by challenges, such as ill-posedness, nonlinearity, and high computational costs. Recently, deep learning was shown to be a promising tool in addressing these challenges. In particular, it is possible to establish a connection between a deep convolutional neural network (CNN) and iterative solution methods of EM inverse scattering. This led to the development of an efficient CNN-based solution to EM inverse problems, termed DeepNIS. It has been shown that DeepNIS can outperform conventional inverse scattering solution methods in terms of both the image quality and computational time. In this letter, we evaluate the DeepNIS performance as a function of the number of layers using structure similarity index measure and mean square error metrics. In addition, we probe the dynamic evolution behavior of DeepNIS by examining its near-isometry property and show that, after a proper training stage, the proposed CNN has near-optimal stability properties.",
      "year": 2019,
      "venue": "IEEE Antennas and Wireless Propagation Letters",
      "authors": [
        "Lianlin Li",
        "Longgang Wang",
        "F. Teixeira"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/ce90e2faf697bff599ea8cac9dd47bda9911abf4",
      "pdf_url": "https://arxiv.org/pdf/1901.02610",
      "publication_date": "2019-01-09",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a82250312de7809d139d0244f4b8bf3f183472d",
      "title": "Unleashing the Power of Convolutional Neural Networks for Profiled Side-channel Analysis",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Jaehun Kim",
        "S. Picek",
        "Annelie Heuser",
        "S. Bhasin",
        "A. Hanjalic"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/0a82250312de7809d139d0244f4b8bf3f183472d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3624e4ef54b4646ebe0ab040000d685664dbc937",
      "title": "Neural Network Model Assessment for Side-Channel Analysis",
      "abstract": null,
      "year": 2019,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Guilherme Perin",
        "Baris Ege",
        "L. Chmielewski"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3624e4ef54b4646ebe0ab040000d685664dbc937",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cc7f4441bad698aa020898c849e3c3a8d17eda71",
      "title": "FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis Of VoIP Stream Via Multi-Head Attention",
      "abstract": "Extracting correlation features between codes-words with high computational efficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this paper, we utilized attention mechanisms, which have recently attracted enormous interests due to their highly parallelizable computation and flexibility in modeling correlation in sequence, to tackle steganalysis problem of Quantization Index Modulation (QIM) based steganography in compressed VoIP stream. We design a light-weight neural network named Fast Correlation Extract Model (FCEM) only based on a variant of attention called multi-head attention to extract correlation features from VoIP frames. Despite its simple form, FCEM outperforms complicated Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) models on both prediction accuracy and time efficiency. It significantly improves the best result in detecting both low embedded rates and short samples recently. Besides, the proposed model accelerates the detection speed as twice as before when the sample length is as short as 0.1s, making it a excellent method for online services.",
      "year": 2019,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hao Yang",
        "Zhongliang Yang",
        "YongJian Bao",
        "Sheng Liu",
        "Yongfeng Huang"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/cc7f4441bad698aa020898c849e3c3a8d17eda71",
      "pdf_url": "https://arxiv.org/pdf/1911.00682",
      "publication_date": "2019-11-02",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "title": "The Feasibility of Deep Learning Use for Adversarial Model Extraction in the Cybersecurity Domain",
      "abstract": null,
      "year": 2019,
      "venue": "Ideal",
      "authors": [
        "M. Chora\u015b",
        "M. Pawlicki",
        "R. Kozik"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/ec2de6ca1857b6ec36f8aba1b940281d6f0a6246",
      "pdf_url": "",
      "publication_date": "2019-11-14",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "title": "VAWS: Vulnerability Analysis of Neural Networks using Weight Sensitivity",
      "abstract": "The advancement in deep learning has taken the technology world by storm in the last decade. Although, there is enormous progress made in terms of algorithm performance, the security aspect of these algorithms has not received a lot of attention from the research community. As more industries start to adopt these algorithms the issue of security is becoming even more relevant. Security vulnerabilities in machine learning (ML), especially in deep neural networks (DNN), is becoming a concern. Various techniques have been proposed, including data manipulations and model stealing. However, most of them are focused on ML algorithms and target threat models that require access to training dataset. In this paper, we present a methodology that analyzes the DNN weight parameters under the threat model that assumes the attacker has the access to the weight memory only. This analysis is then used to develop an attack that manipulates weight parameters with respect to their sensitivity. To evaluate this attack, we implemented our methodology on a MLP trained on IRIS dataset and LeNet (DNN architecture) trained on MNIST dataset. Our experimental results demonstrate that alteration of model parameters results in subtle accuracy drop of the model. Depending on the applications such subtle changes can cause significant system malfunction or disruption, for example in vision-based industrial applications. Our results show that using our methodology a subtle accuracy drop can be achieved in a reasonable amount of time with very few parameter changes.",
      "year": 2019,
      "venue": "Midwest Symposium on Circuits and Systems",
      "authors": [
        "Muluken Hailesellasie",
        "Jacob Nelson",
        "Faiq Khalid",
        "S. R. Hasan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "pdf_url": "",
      "publication_date": "2019-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1a593b834e01da0e1bf3c15cf8ee4420285227fd",
      "title": "Convolutional Neural Network Based Side-Channel Attacks with Customized Filters",
      "abstract": null,
      "year": 2019,
      "venue": "International Conference on Information, Communications and Signal Processing",
      "authors": [
        "Man Wei",
        "Danping Shi",
        "Siwei Sun",
        "Peng Wang",
        "Lei Hu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a593b834e01da0e1bf3c15cf8ee4420285227fd",
      "pdf_url": "",
      "publication_date": "2019-12-15",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "title": "Stealing Neural Networks via Timing Side Channels",
      "abstract": "Deep learning is gaining importance in many applications. However, Neural Networks face several security and privacy threats. This is particularly significant in the scenario where Cloud infrastructures deploy a service with Neural Network model at the back end. Here, an adversary can extract the Neural Network parameters, infer the regularization hyperparameter, identify if a data point was part of the training data, and generate effective transferable adversarial examples to evade classifiers. This paper shows how a Neural Network model is susceptible to timing side channel attack. In this paper, a black box Neural Network extraction attack is proposed by exploiting the timing side channels to infer the depth of the network. Although, constructing an equivalent architecture is a complex search problem, it is shown how Reinforcement Learning with knowledge distillation can effectively reduce the search space to infer a target model. The proposed approach has been tested with VGG architectures on CIFAR10 data set. It is observed that it is possible to reconstruct substitute models with test accuracy close to the target models and the proposed approach is scalable and independent of type of Neural Network architectures.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "D. Samanta",
        "D. V. Rao",
        "V. Balas"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "pdf_url": "",
      "publication_date": "2018-12-31",
      "keywords_matched": [
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "title": "Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks",
      "abstract": "Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim's deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim's entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations. Our empirical security analysis represents a step toward understanding the DNNs' vulnerability to cache side-channel attacks.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Sanghyun Hong",
        "Michael Davinroy",
        "Yigitcan Kaya",
        "S. Locke",
        "Ian Rackow",
        "Kevin Kulda",
        "Dana Dachman-Soled",
        "Tudor Dumitras"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "pdf_url": "",
      "publication_date": "2018-09-27",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dc32688d395c57cff2dbecf65b9c5b6a52a5c10c",
      "title": "Convolutional Neural Network Based Side-Channel Attacks in Time-Frequency Representations",
      "abstract": null,
      "year": 2018,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Guang Yang",
        "Huizhong Li",
        "Jingdian Ming",
        "Yongbin Zhou"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/dc32688d395c57cff2dbecf65b9c5b6a52a5c10c",
      "pdf_url": "",
      "publication_date": "2018-11-12",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}