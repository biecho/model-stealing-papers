{
  "owasp_id": "ML05",
  "owasp_name": "Data Extraction & Leakage",
  "total": 12,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "https://openalex.org/W3112689365",
      "title": "Extracting Training Data from Large Language Models",
      "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss",
        "Katherine Lee",
        "Adam P. Roberts",
        "T. B. Brown",
        "Dawn Song",
        "\u00dalfar Erlingsson",
        "Alina Oprea",
        "Colin Raffel"
      ],
      "url": "https://openalex.org/W3112689365",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3027379683",
      "title": "Analyzing Information Leakage of Updates to Natural Language Models",
      "abstract": "To continuously improve quality and reflect changes in data, machine learning\\napplications have to regularly retrain and update their core models. We show\\nthat a differential analysis of language model snapshots before and after an\\nupdate can reveal a surprising amount of detailed information about changes in\\nthe training data. We propose two new metrics---\\\\emph{differential score} and\\n\\\\emph{differential rank}---for analyzing the leakage due to updates of natural\\nlanguage models. We perform leakage analysis using these metrics across models\\ntrained on several different datasets using different methods and\\nconfigurations. We discuss the privacy implications of our findings, propose\\nmitigation strategies and evaluate their effect.\\n",
      "year": 2020,
      "venue": null,
      "authors": [
        "Santiago Zanella-B\u00e9guelin",
        "Lukas Wutschitz",
        "Shruti Tople",
        "Victor R\u00fchle",
        "Andrew Paverd",
        "Olga Ohrimenko",
        "Boris K\u00f6pf",
        "Marc Brockschmidt"
      ],
      "url": "https://openalex.org/W3027379683",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2302.00539",
      "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
      "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Nils Lukas",
        "Ahmed Salem",
        "Robert Sim",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B\u00e9guelin"
      ],
      "url": "https://arxiv.org/abs/2302.00539",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2311.07389",
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "abstract": "Deep neural networks are normally executed in the forward direction. However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks. Adversaries can exploit this capability to hide rogue models within seemingly legitimate models. In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets. Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models. We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models. Moreover, to mitigate this threat we propose a novel approach for detecting infected models.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Guy Amit",
        "Mosh Levy",
        "Yisroel Mirsky"
      ],
      "url": "https://arxiv.org/abs/2311.07389",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2406.14114",
      "title": "Dye4AI: Assuring Data Boundary on Generative AI Services",
      "abstract": "Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Shu Wang",
        "Kun Sun",
        "Yan Zhai"
      ],
      "url": "https://arxiv.org/abs/2406.14114",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3171497996",
      "title": "Leakage of Dataset Properties in Multi-Party Machine Learning",
      "abstract": "Secure multi-party machine learning allows several parties to build a model on their pooled data to increase utility while not explicitly sharing data with each other. We show that such multi-party computation can cause leakage of global dataset properties between the parties even when parties obtain only black-box access to the final model. In particular, a ``curious'' party can infer the distribution of sensitive attributes in other parties' data with high accuracy. This raises concerns regarding the confidentiality of properties pertaining to the whole dataset as opposed to individual data records. We show that our attack can leak population-level properties in datasets of different types, including tabular, text, and graph data. To understand and measure the source of leakage, we consider several models of correlation between a sensitive attribute and the rest of the data. Using multiple machine learning models, we show that leakage occurs even if the sensitive attribute is not included in the training data and has a low correlation with other attributes or the target variable.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wanrong Zhang",
        "Shruti Tople",
        "Olga Ohrimenko"
      ],
      "url": "https://openalex.org/W3171497996",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "1912.03817",
      "title": "Machine Unlearning",
      "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Christopher A. Choquette-Choo",
        "Hengrui Jia",
        "Adelin Travers",
        "Baiwu Zhang",
        "David Lie",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/1912.03817",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2311.16136",
      "title": "ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach",
      "abstract": "Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the \"right to be forgotten (RTBF)\" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the ERASER framework for machinE unleaRning in MLaAS via an inferencE seRving-aware approach. ERASER strategically choose appropriate unlearning execution timing to address the inference service obsolescence issue. A novel inference consistency certification mechanism is proposed to avoid the violation of RTBF principle caused by postponed unlearning executions, thereby mitigating the undesirable exposure vulnerability. ERASER offers three groups of design choices to allow for tailor-made variants that best suit the specific environments and preferences of various MLaaS systems. Extensive empirical evaluations across various settings confirm ERASER's effectiveness, e.g., it can effectively save up to 99% of inference latency and 31% of computation overhead over the inference-oblivion baseline.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Yuke Hu",
        "Jian Lou",
        "Jiaqi Liu",
        "Wangze Ni",
        "Feng Lin",
        "Zhan Qin",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2311.16136",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2309.03081",
      "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
      "abstract": "Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset. To this end, we propose ORL-AUDITOR, which is the first trajectory-level dataset auditing mechanism for offline RL scenarios. Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than 2.88%. We also provide valuable insights into the practical implementation of ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind, highlighting its effectiveness in auditing published datasets. ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Shouling Ji",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "url": "https://arxiv.org/abs/2309.03081",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2410.16618",
      "title": "SoK: Dataset Copyright Auditing in Machine Learning Systems",
      "abstract": "As the implementation of machine learning (ML) systems becomes more widespread, especially with the introduction of larger ML models, we perceive a spring demand for massive data. However, it inevitably causes infringement and misuse problems with the data, such as using unauthorized online artworks or face images to train ML models. To address this problem, many efforts have been made to audit the copyright of the model training dataset. However, existing solutions vary in auditing assumptions and capabilities, making it difficult to compare their strengths and weaknesses. In addition, robustness evaluations usually consider only part of the ML pipeline and hardly reflect the performance of algorithms in real-world ML applications. Thus, it is essential to take a practical deployment perspective on the current dataset copyright auditing tools, examining their effectiveness and limitations. Concretely, we categorize dataset copyright auditing research into two prominent strands: intrusive methods and non-intrusive methods, depending on whether they require modifications to the original dataset. Then, we break down the intrusive methods into different watermark injection options and examine the non-intrusive methods using various fingerprints. To summarize our results, we offer detailed reference tables, highlight key points, and pinpoint unresolved issues in the current literature. By combining the pipeline in ML systems and analyzing previous studies, we highlight several future directions to make auditing tools more suitable for real-world copyright protection requirements.",
      "year": 2025,
      "venue": "IEEE S&P",
      "authors": [
        "Linkang Du",
        "Xuanru Zhou",
        "Min Chen",
        "Chusong Zhang",
        "Zhou Su",
        "Peng Cheng",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "url": "https://arxiv.org/abs/2410.16618",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4402427674",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "abstract": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yingzhen Liu",
        "Y. Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://openalex.org/W4402427674",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391725245",
      "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
      "abstract": "Deep neural networks are normally executed in the forward direction.However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks.Adversaries can exploit this capability to hide rogue models within seemingly legitimate models.In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets.Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models.We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models.Moreover, to mitigate this threat we propose a novel approach for detecting infected models.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Guy Amit",
        "Moshe Levy",
        "Yisroel Mirsky"
      ],
      "url": "https://openalex.org/W4391725245",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    }
  ]
}