{
  "updated": "2026-01-06",
  "total": 296,
  "owasp_id": "ML05",
  "owasp_name": "Model Theft",
  "description": "Attacks that steal, copy, or clone machine learning models themselves.\n        This includes model extraction attacks, model stealing attacks, knockoff\n        networks, functionality stealing, query-based model cloning, surrogate\n        model training, and techniques to replicate or steal model weights,\n        architecture, or behavior through API queries. The goal is stealing the\n        MODEL, not the training data.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "f8068a3d7d06db00d2d650c92ff1804a168b0a99",
      "title": "A Server-Side Model Intellectual Property Protection Method for Federated Learning against Model Theft",
      "abstract": null,
      "year": 2026,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Wenxiong Chen",
        "Xuantao Tang",
        "Dan Wang",
        "Ju Ren"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8068a3d7d06db00d2d650c92ff1804a168b0a99",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2026-01-04"
    },
    {
      "paper_id": "253a59d979d560456c2984742466b796a983da0e",
      "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging k-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments. Our source code is available at https://github.com/LabRAI/ATOM.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhan Cheng",
        "Bolin Shen",
        "Tianming Sha",
        "Yuan Gao",
        "Shibo Li",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/253a59d979d560456c2984742466b796a983da0e",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "title": "SIGFinger: A Subtle and Interactive GNN Fingerprinting Scheme Via Spatial Structure Inference Perturbation",
      "abstract": "There have been significant improvements in intellectual property (IP) protection for deep learning models trained on euclidean data. However, the complex and irregular graph-structured data in non-euclidean space poses a huge challenge to the IP protection of graph neural networks (GNNs). To address this issue, we propose a subtle and interactive GNN fingerprinting scheme through spatial structure inference perturbation, which captures the stable coordination patterns of fingerprint to guarantee the reliability of copyright verification. Specifically, the data augmentation based on adaptive graph diffusion is first exploited to generate more samples, which enables the exploration of fingerprint information from coarse to fine. Subsequently, the graph-structured data are manipulated by multi-constrained spectral clustering to analyze intrinsic and extrinsic structure correlations in a causal inference manner. Ultimately, the cycle-consistent statistical optimization is performed to determine the copyright of GNN models from both intra-graph and inter-graph perspectives. Extensive experiments show that our proposed scheme can effectively verify the IP of GNN models on various challenging graph-structured datasets. Furthermore, we reveal that the space causality inference can facilitate the acquisition of inherent structural information, which improves the quality and robustness of the fingerprint under model modification operations and other model stealing attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Ju Jia",
        "Renjie Li",
        "Cong Wu",
        "Siqi Ma",
        "Lina Wang",
        "Rebert H. Deng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "95b993a3b281c920589fb5b158ff07009ff628b9",
      "title": "CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable utility across diverse applications, and their growing complexity has made Machine Learning as a Service (MLaaS) a viable platform for scalable deployment. However, this accessibility also exposes GNN to serious security threats, most notably model extraction attacks (MEAs), in which adversaries strategically query a deployed model to construct a high-fidelity replica. In this work, we evaluate the vulnerability of GNNs to MEAs and explore their potential for cost-effective model acquisition in non-adversarial research settings. Importantly, adaptive node querying strategies can also serve a critical role in research, particularly when labeling data is expensive or time-consuming. By selectively sampling informative nodes, researchers can train high-performing GNNs with minimal supervision, which is particularly valuable in domains such as biomedicine, where annotations often require expert input. To address this, we propose a node querying strategy tailored to a highly practical yet underexplored scenario, where bulk queries are prohibited, and only a limited set of initial nodes is available. Our approach iteratively refines the node selection mechanism over multiple learning cycles, leveraging historical feedback to improve extraction efficiency. Extensive experiments on benchmark graph datasets demonstrate our superiority over comparable baselines on accuracy, fidelity, and F1 score under strict query-size constraints. These results highlight both the susceptibility of deployed GNNs to extraction attacks and the promise of ethical, efficient GNN acquisition methods to support low-resource research environments.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zebin Wang",
        "Menghan Lin",
        "Bolin Shen",
        "Ken Anderson",
        "Molei Liu",
        "Tianxi Cai",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/95b993a3b281c920589fb5b158ff07009ff628b9",
      "pdf_url": "",
      "publication_date": "2025-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "633f03f1eea4387f02e826e4768841ef10190446",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nianfeng Tzeng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/633f03f1eea4387f02e826e4768841ef10190446",
      "pdf_url": "",
      "publication_date": "2025-01-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
      "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models. To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shiqian Zhao",
        "Chong Wang",
        "Yiming Li",
        "Yihao Huang",
        "Wenjie Qu",
        "Siew-Kei Lam",
        "Yi Xie",
        "Kangjie Chen",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "pdf_url": "",
      "publication_date": "2025-08-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
      "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \\texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \\texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lin Lu",
        "Zhigang Zuo",
        "Ziji Sheng",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "268ce1e9492455e825a3894b0f2713d14e376d36",
      "title": "ADAGE: Active Defenses Against GNN Extraction",
      "abstract": "Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the observation that stealing a model's full functionality requires highly diverse queries to leak its behavior across the input space. Our defense monitors this query diversity and progressively perturbs outputs as the accumulated leakage grows. In contrast to prior work, ADAGE can prevent stealing across all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst preserving predictive performance on downstream tasks. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jing Xu",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/268ce1e9492455e825a3894b0f2713d14e376d36",
      "pdf_url": "",
      "publication_date": "2025-02-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6678e884da10e0d6cd763811069a813700b685a6",
      "title": "Exploring Query Efficient Data Generation Towards Data-Free Model Stealing in Hard Label Setting",
      "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. Instead, the adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (QEDG). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real MLaaS scenario and five datasets.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Gaozheng Pei",
        "Shaojie Lyu",
        "Ke Ma",
        "Pinci Yang",
        "Qianqian Xu",
        "Yingfei Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6678e884da10e0d6cd763811069a813700b685a6",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "title": "Examining the Threat Landscape: Foundation Models and Model Stealing",
      "abstract": "Foundation models (FMs) for computer vision learn rich and robust representations, enabling their adaptation to task/domain-specific deployments with little to no fine-tuning. However, we posit that the very same strength can make applications based on FMs vulnerable to model stealing attacks. Through empirical analysis, we reveal that models fine-tuned from FMs harbor heightened susceptibility to model stealing, compared to conventional vision architectures like ResNets. We hypothesize that this behavior is due to the comprehensive encoding of visual patterns and features learned by FMs during pre-training, which are accessible to both the attacker and the victim. We report that an attacker is able to obtain 94.28% agreement (matched predictions with victim) for a Vision Transformer based victim model (ViT-L/16) trained on CIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim, when using ViT-L/16 as the thief model. We arguably show, for the first time, that utilizing FMs for downstream tasks may not be the best choice for deployment in commercial APIs due to their susceptibility to model theft. We thereby alert model owners towards the associated security risks, and highlight the need for robust security measures to safeguard such models against theft. Code is available at https://github.com/rajankita/foundation_model_stealing.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ankita Raj",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "abstract": "On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yujin Huang",
        "Zhi Zhang",
        "Qingchuan Zhao",
        "Xingliang Yuan",
        "Chunyang Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a40984e29d01c54f7a2b5039378ae1f9bc740629",
      "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking",
      "abstract": null,
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Toluwani Aremu",
        "Noor Hussein",
        "Munachiso Nwadike",
        "Samuele Poppi",
        "Jie Zhang",
        "Karthik Nandakumar",
        "Neil Gong",
        "Nils Lukas"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a40984e29d01c54f7a2b5039378ae1f9bc740629",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models",
      "abstract": "Model extraction attacks aim to replicate the functionality of a black-box model through query access, threatening the intellectual property (IP) of machine-learning-as-a-service (MLaaS) providers. Defending against such attacks is challenging, as it must balance efficiency, robustness, and utility preservation in the real-world scenario. Despite the recent advances, most existing defenses presume that attacker queries have out-of-distribution (OOD) samples, enabling them to detect and disrupt suspicious inputs. However, this assumption is increasingly unreliable, as modern models are trained on diverse datasets and attackers often operate under limited query budgets. As a result, the effectiveness of these defenses is significantly compromised in realistic deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel optimization problem that simultaneously preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. Our framework combines data augmentation to simulate attacker queries with an ensemble of heterogeneous distilled models to improve robustness and diversity. We further provide a tractable approximation algorithm and derive theoretical error bounds to characterize defense effectiveness. Extensive experiments across various settings validate the utility-preserving and extraction-resistant properties of our proposed defense strategy. Our code is available at https://github.com/LabRAI/MISLEADER.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xueqi Cheng",
        "Minxing Zheng",
        "Shixiang Zhu",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "title": "Unlocking High-Fidelity Learning: Towards Neuron-Grained Model Extraction",
      "abstract": "Model extraction (ME) attacks replicate valuable closed-box machine learning (ML) models via malicious query interactions. Cutting-edge attacks focus on actively designing query samples to enhance model fidelity and imprudently adhere to the standard ML training approach. This causes a deviation from the true objective of learning a model over a task. In this article, we innovatively shift our focus from query selection to training process optimization, aiming to boost the similarity of the copy model with the victim model from neuron to model level. We leverage neuron matching theory to attain this objective and develop a general training booster framework, MEBooster, to fully exploit this theory. MEBooster comprises an initial bootstrapping phase that furnishes initial parameters and an optimal model architecture, followed by a post-processing phase that employs fine-tuning for enhanced neuron matching. Notably, MEBooster can seamlessly integrate with all existing model extraction attacks, enhancing their overall performance. Performance evaluation shows up to 58.10% fidelity gain in image classification. From a defender's perspective, we introduce a novel defensive strategy called Stochastic Norm Enlargement (SNE) to mitigate the risk of such attacks by enlarging the model parameters\u2019 norm property in training. Performance evaluation shows up to 58.81% extractability (i.e., fidelity) reduction.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Yaxin Xiao",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Tang",
        "Zi Liang",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "LLM stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "title": "Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks",
      "abstract": "Machine learning (ML) models were shown to be vulnerable to model stealing attacks, which lead to intellectual property infringement. Among other attack methods, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting, conclusions. In this work, we exhaustively examine the ambivalent influence of different factors resulting from varying the attacker's capabilities and knowledge on a substitute training attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between attack conditions and success rate. In particular, we demonstrate that better-performing target models enable higher-fidelity attacks and explain the intuition behind this phenomenon. Further, we propose to shift the focus from the complexity of target models toward the complexity of their learning tasks. Therefore, for the substitute model, rather than aiming for a higher architecture complexity, we suggest focusing on getting data of higher complexity and an appropriate architecture. Finally, we demonstrate that even in the most limited data-free scenario, there is no need to overcompensate weak knowledge with unrealistic capabilities in the form of millions of queries. Our results often exceed or match the performance of previous attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now.",
      "year": 2025,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "pdf_url": "",
      "publication_date": "2025-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b066eaa019bf6e94540dc54e735defb19d910bf8",
      "title": "Stealix: Model Stealing via Prompt Evolution",
      "abstract": "Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model's data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Hui-Po Wang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b066eaa019bf6e94540dc54e735defb19d910bf8",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5ad7f669afa84855e00c743422026c8c5e91e411",
      "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
      "abstract": "Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Kathrin Grosse",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5ad7f669afa84855e00c743422026c8c5e91e411",
      "pdf_url": "",
      "publication_date": "2025-08-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "title": "Model-Guardian: Protecting against Data-Free Model Stealing Using Gradient Representations and Deceptive Predictions",
      "abstract": "Model stealing attack is increasingly threatening the confidentiality of machine learning models deployed in the cloud. Recent studies reveal that adversaries can exploit data synthesis techniques to steal machine learning models even in scenarios devoid of real data, leading to data-free model stealing attacks. Existing defenses against such attacks suffer from limitations, including poor effectiveness, insufficient generalization ability, and low comprehensiveness. In response, this paper introduces a novel defense framework named Model-Guardian. Comprising two components, Data-Free Model Stealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds), Model-Guardian is designed to address the shortcomings of current defenses with the help of the artifact properties of synthetic samples and gradient representations of samples. Extensive experiments on seven prevalent data-free model stealing attacks showcase the effectiveness and superior generalization ability of Model-Guardian, outperforming eleven defense methods and establishing a new state-of-the-art performance. Notably, this work pioneers the utilization of various GANs and diffusion models for generating highly realistic query samples in attacks, with Model-Guardian demonstrating accurate detection capabilities.",
      "year": 2025,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "pdf_url": "",
      "publication_date": "2025-03-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "title": "Defenses against model stealing attacks in MLaaS: literature review and challenges",
      "abstract": null,
      "year": 2025,
      "venue": "Cluster Computing",
      "authors": [
        "Aouatef Mahani",
        "O. Kazar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/509a77d3e7a4a48ff7b191fe1802db4d597de8c8",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16cfca0d74d329b2b3f2d927047468f09300e33e",
      "title": "CERBEROS: Compression-Based Efficient and Robust Optimized Security for Model Stealing Defense",
      "abstract": "Model stealing attacks pose an increasing threat to the confidentiality and intellectual property of artificial intelligence (AI) models. Existing defenses\u2013such as query monitoring, output perturbation, multi-model output variation, and post hoc verification\u2013fall short in on-device applications where models must run under strict memory and computation budgets. These approaches typically incur high memory or latency overhead due to their reliance on auxiliary models or additional inference-time processing. To address these limitations, we propose CERBEROS, a defense framework designed to achieve security against model stealing with deployability in resource-constrained environments. At its core, CERBEROS introduces a novel neural architecture with multiple classification heads trained jointly for output diversification, while sharing a single feature extraction backbone to minimize unnecessary memory usage. At inference, CERBEROS reveals the prediction of a randomly selected head, thereby misleading adversaries while preserving test accuracy for legitimate users, without requiring separate models or costly output modification. In addition, we integrate structured pruning into training to compress the backbone while retaining the classification heads. This ensures that functional diversity across heads remains achievable even under tight resource constraints. Our experiments show that CERBEROS effectively mitigates model replication attacks while consistently maintaining task performance across widely used convolutional neural networks and benchmark datasets. Furthermore, it achieves significant reductions in memory consumption and inference latency compared to prior defenses, offering a practical and efficient solution for securing on-device AI models.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Sohyun Keum",
        "Jeonghyun Lee",
        "Sangkyun Lee"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16cfca0d74d329b2b3f2d927047468f09300e33e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16743a0a8daad27538a0bc734ed42abca3a14289",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "abstract": "Malicious users attempt to replicate commercial models functionally at low cost by training a clone model with query responses. It is challenging to timely prevent such model-stealing attacks to achieve strong protection and maintain utility. In this paper, we propose a novel non-parametric detector called Account-aware Distribution Discrepancy (ADD) to recognize queries from malicious users by leveraging account-wise local dependency. We formulate each class as a Multivariate Normal distribution (MVN) in the feature space and measure the malicious score as the sum of weighted class-wise distribution discrepancy. The ADD detector is combined with random-based prediction poisoning to yield a plug-and-play defense module named D-ADD for image classification models. Results of extensive experimental studies show that D-ADD achieves strong defense against different types of attacks with little interference in serving benign users for both soft and hard-label settings.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16743a0a8daad27538a0bc734ed42abca3a14289",
      "pdf_url": "",
      "publication_date": "2025-03-16",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "title": "Self-enhancing defense for protecting against model stealing attacks on deep learning systems",
      "abstract": null,
      "year": 2025,
      "venue": "Expert systems with applications",
      "authors": [
        "Chenlong Zhang",
        "Senlin Luo",
        "Jiawei Li",
        "Limin Pan",
        "Chuan Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8f0626c98083f77d2f5c776cc3a4b91fa273b73",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "title": "Model Rake: A Defense Against Stealing Attacks in Split Learning",
      "abstract": "Split learning is a prominent framework for vertical federated learning, where multiple clients collaborate with a central server for model training by exchanging intermediate embeddings. Recently, it is shown that an adversarial server can exploit the intermediate embeddings to train surrogate models to replace the bottom models on the clients (i.e., model stealing). The surrogate models can also be used to reconstruct private training data of the clients (i.e., data stealing).\n\nTo defend against these stealing attacks, we propose Model Rake (i.e., Rake), which runs two bottom models on each client and differentiates their output spaces to make the two models distinct. Rake hinders the stealing attacks because it is difficult for a surrogate model to approximate two distinct bottom models. We prove that, under some assumptions, the surrogate model converges to the average of the two bottom models and thus will be inaccurate. Extensive experiments show that Rake is much more effective than existing methods in defending against both model and data stealing attacks, and the accuracy of normal model training is not affected.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yanfeng Zhao",
        "Fangcheng Fu",
        "Quanqing Xu",
        "Yukai Ding",
        "Xiaokai Zhou",
        "Chuang Hu",
        "Jiawei Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "title": "On Stealing Graph Neural Network Models",
      "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "Agnieszka Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
      "abstract": "Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Haiqin Weng",
        "Yan Liu",
        "Tianwei Zhang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "056036a3ce3a21daf80167dee622b6b515f9490e",
      "title": "Black-box model functionality stealing for Vietnamese sentiment analysis",
      "abstract": "Black-box deep learning models often keep critical components such as model architecture, hyperparameters, and training data confidential, allowing users to observe only the inputs and outputs without understanding their internal workings. Consequently, there is growing interested in developing \"knockoff\" models that replicate the behavior of these black-box models without direct access to internal details. We have conducted extensive studies on function extraction attacks targeting English text sentiment analysis models. By employing random or adaptive sampling methods, we have successfully reconstructed knockoff models that achieve functionality equivalent to the original models with high similarity. In this study, we extend our investigation to sentiment analysis datasets in Vietnamese. Experimental results demonstrate that for black-box models in Vietnamese text sentiment analysis, our method remains effective, successfully constructing models with equivalent functionality.",
      "year": 2025,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Viet-Binh Do",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/056036a3ce3a21daf80167dee622b6b515f9490e",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "title": "Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks",
      "abstract": "The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model's functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model's training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at https://github.com/rajankita/QueryWise.",
      "year": 2025,
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "authors": [
        "Ankita Raj",
        "Harsh Swaika",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "title": "Middleware Architecture for the Management and Mitigation of OWASP ML05: Model Theft in IoT Machine Learning Networks",
      "abstract": "The increasing integration of machine learning (ML) models into Internet of Things (IoT) applications has led to notable advancements in automation and decision-making. However, these models are vulnerable to modern attack vectors recognized by the OWASP Top 10 for Large Language Model Applications, specifically ML05: Model Theft, where adversaries gain unauthorized access to model parameters and training data, compromising intellectual property and sensitive information. Such threats are particularly concerning in IoT environments due to their distributed nature and resource limitations. This paper proposes a middleware architecture for the management and mitigation of model theft risks by incorporating encryption, access control, obfuscation, watermarking, continuous monitoring, and service assurance programmability. By strengthening the security management framework of ML models deployed in IoT, the proposed architecture aims to protect against theft, ensure data confidentiality, and maintain network resilience. The approach includes detailed mathematical models and an evaluation of existing security measures, demonstrating the architecture's effectiveness in diverse IoT deployments, such as telemedicine and smart cities.",
      "year": 2025,
      "venue": "Global",
      "authors": [
        "Yair Enrique Rivera Julio",
        "\u00c1ngel Pinto",
        "Nelson A. P\u00e9rez-Garc\u00eda",
        "M\u00f3nica-Karel Huerta",
        "C\u00e9sar Viloria-N\u00fa\u00f1ez",
        "Marvin Luis P\u00e9rez Cabrera",
        "Frank Ibarra Hern\u00e1ndez",
        "Juan Manuel Torres Tovio",
        "Erwin J. Sacoto-Cabrera"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5420739477bc061905a058bdcae98f42ccb870a2",
      "title": "Unveiling Deep Learning Models: Leveraging Active Learning and High-Confidence Feature Extraction in Model Stealing",
      "abstract": "Recent research reveals that weaknesses in black box models can be exposed by stealing their functional characteristics. However, the current stealing process faces challenges such as low accuracy, insufficient hard label guidance and limited query quantity. To overcome these limitations, we have developed a method that facilitates the stealing of hard-label black-box models, called ALHC (Active Learning and High- confidence Feature Extraction). It incorporates a high-confidence stealing module to generate high-quality datasets. And it also introduces an efficient active learning selection strategy for training substitute models that exhibit higher similarity within the same query budget. In multiple datasets and real-world APIs, the ALHC has demonstrated remarkable effectiveness, surpassed traditional approaches and improved consistency and accuracy.",
      "year": 2025,
      "venue": "2025 8th International Conference on Advanced Algorithms and Control Engineering (ICAACE)",
      "authors": [
        "Lei Liu",
        "Lei Wang",
        "Ce Gu",
        "Hongwei Ma",
        "Jieyu Wang",
        "Wenjun Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5420739477bc061905a058bdcae98f42ccb870a2",
      "pdf_url": "",
      "publication_date": "2025-03-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "title": "Knockoff Branch: Model Stealing Attack via Adding Neurons in the Pre-Trained Model",
      "abstract": "We introduce Knockoff Branch: adding few neurons as a knockoff container for learning stolen features. Model stealing attacks extract the functionality from the victim model by querying APIs. Prior work substantially enhanced transferability and improved query efficiency between the adversary model and the victim model. However, there is still a limited understanding of the knockoff itself. For knockoff, the model is either compared to the same type but with different structures or different types and capacities. For this reason, we propose a framework to analyze the knockoff quality for a single model, specifically reinvestigating transformer-based extraction. We observed that 1) when the adversary can access the public pretrained model, full fine-tuning is not necessary. This allows a knockoff to require only about 0.5% of trainable parameters and 20 epochs. 2) Although querying by out-of-distribution datasets leads to a sub-optimal knockoff, this issue can be mitigated by scaling branch features, even without using complicated sampling strategies. Our proposed method is lightweight and achieves high accuracy, at most similar to white-box knowledge distillation (higher performance than the victim model). https://github.com/onlyin-hung/knockoff-branch.",
      "year": 2025,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Li-Ying Hung",
        "Cooper Cheng-Yuan Ku"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "pdf_url": "",
      "publication_date": "2025-02-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "title": "Too Clever by Half: Detecting Sampling-based Model Stealing Attacks by Their Own Cleverness",
      "abstract": "Machine learning as a service (MLaaS) has gained significant popularity and market traction in recent years, driven by advancements in Artificial Intelligence particularly Generative AI (GAI). However, MLaaS faces severe challenges from sampling-based model stealing attacks (MSAs), where attackers strategically query the targeted ML models provided by MLaaS providers to minimize the query burden while closely replicating the model\u2019s functionality. Such MSAs pose severe consequences, including intellectual property (IP) theft and potential leakage of private training data. Unfortunately, existing defenses either sacrifice model utility or fail to generalize across diverse MSAs.In this paper, we propose DIARY, an innovative detection method specifically tailored to sampling-based MSAs by exploiting their inherent sophistication. Our key insight is that \u2018clever\u2019 malicious queries tend to extract more information from the targeted (victim) model than typical benign queries, as these attacks iteratively refine their queries by examining and analyzing prior queries and the corresponding responses. Hence we design DIARY to extract timing dependence within a query sequence and incorporate contrastive learning for properly characterizing such dependency that holds for different sampling-based MSAs. Comprehensive evaluations using five different sampling-based MSAs and two state-of-the-art defense baselines across four popular datasets consistently validate DIARY\u2019s superior performance.",
      "year": 2025,
      "venue": "IEEE International Conference on Distributed Computing Systems",
      "authors": [
        "Xin Yao",
        "Chenyang Wang",
        "Yimin Chen",
        "Kecheng Huang",
        "Jiawei Guo",
        "Ming Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "title": "DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks",
      "abstract": "Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao",
        "Xin Zhao",
        "He Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "pdf_url": "",
      "publication_date": "2025-11-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "title": "A Method for Stealing Traffic Detection Models Based on Equivalent Feature Sets",
      "abstract": "Machine learning models have shown excellent performance in the field of traffic detection. However, they also face new security challenges. This study focuses on model stealing attacks in the traffic detection field. Existing model stealing methods are difficult to implement in real - world network environments. They either cannot be applied to the traffic detection field or require prior knowledge of the target model's feature set. To achieve the stealing of traffic detection models in a truly black - box scenario and overcome the strong assumption of relying on the known target model feature set in existing methods, this study proposes a method for stealing traffic detection models based on feature inference. This study introduces the concept of \u201cequivalent feature set\u201d, analyzes the prediction logic of the target model and the characteristics of traffic data through two feature inference algorithms, constructs an equivalent feature set, and uses it as the feature set for training a substitute model. The results of stealing experiments on multiple target models show that in most experimental settings, the stealing rate exceeds 85%, with a maximum of 96.64%, demonstrating the high efficiency and stability of the method. At the same time, experimental verification shows that the equivalent feature set can accurately capture key features, significantly improving the stealing effect of the substitute model, with an improvement amplitude of more than 40.48%.",
      "year": 2025,
      "venue": "International Conference Civil Engineering and Architecture",
      "authors": [
        "Long Meng",
        "Bin Lu",
        "Xu Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a744498909536ee4a0752f3fe2430c13268a2b82",
      "title": "Siamese: Stealing Fine-Tuned Visual Foundation Models via Diversified Prompting",
      "abstract": "Visual foundation models, characterized by their robust generalization and adaptability, serve as the basis for a wide array of downstream tasks. When fine-tuned for specific tasks, these models encapsulate confidential and valuable task-specific knowledge, making them prime targets for model stealing (MS) attacks. While recent efforts have exposed MS threats in practical scenarios such as data-free and hard-label contexts, these attacks predominantly target traditional victim models trained from scratch. Fine-tuned visual foundation models, pre-trained on vast and diverse datasets and then fine-tuned on downstream tasks, present significant challenges for traditional MS attacks to extract task-specific knowledge. In this paper, we introduce an innovative MS attack, named SIAMESE, to steal fine-tuned visual foundation models under black-box, data-free, and hard-label settings. The core approach of SIAMESE involves constructing a stolen model using a foundation model that is efficiently and concurrently fine-tuned with multiple diversified soft prompts. To integrate the knowledge derived from these prompts, we propose a novel and tractable loss function that analyzes the output distributions while enforcing orthogonality among the prompts to minimize interference. Additionally, a unique alignment module enhances SIAMESE by synchronizing interpretations between the victim and stolen models. Extensive experiments validate that SIAMESE outperforms state-of-the-art baseline attacks over 10% in accuracy, exposing the heightened vulnerability of fine-tuned visual foundation models to MS threats.",
      "year": 2025,
      "venue": "Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing",
      "authors": [
        "Madhureeta Das",
        "Gaurav Bagwe",
        "Miao Pan",
        "Kaichen Yang",
        "X. Yuan",
        "Lan Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a744498909536ee4a0752f3fe2430c13268a2b82",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "title": "DAV: An Adaptive Defense Framework for Model Extraction Attacks",
      "abstract": "Machine learning platforms offer paid APIs to enable personalized inference services. However, model extraction attacks greatly threaten their intellectual property rights. Malicious users can create query samples using proxy datasets or generative models to train a clone model. Existing defense approaches usually focus on models that return soft-labels, and cannot effectively handle extracting attacks against hard-label models. In this paper, we propose an adaptive defense framework named DAV, which consists of a malicious query detector and an adaptive perturbation mechanism. Two perturbation strategies can be selected based on the detection results and the malicious query rate within the buffer queue, including accuracy-preserving perturbation and maximum-minimum probability inverse perturbation. Comprehensive experimental results show that DAV can significantly reduce the accuracy of the clone model with little impact on the performance of the victim model and benign queries, no matter whether the returned probabilities are for soft-label or hard-label.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Peng Sui",
        "Jiapeng Zhou",
        "Yu Chen",
        "Youhuizi Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ae1157c1fba9603a98566209bc266f6b7a534629",
      "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
      "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful machine learning models through cloud-based APIs, offering scalability and ease of deployment. However, these services are vulnerable to model extraction attacks, where adversaries repeatedly query the application programming interface (API) to reconstruct a functionally similar model, compromising intellectual property and security. Despite various defense strategies being proposed, many suffer from high computational costs, limited adaptability to evolving attack techniques, and a reduction in performance for legitimate users. In this paper, we introduce a Resilient Adaptive Defense Framework for Model Extraction Attack Protection (RADEP), a multifaceted defense framework designed to counteract model extraction attacks through a multi-layered security approach. RADEP employs progressive adversarial training to enhance model resilience against extraction attempts. Malicious query detection is achieved through a combination of uncertainty quantification and behavioral pattern analysis, effectively identifying adversarial queries. Furthermore, we develop an adaptive response mechanism that dynamically modifies query outputs based on their suspicion scores, reducing the utility of stolen models. Finally, ownership verification is enforced through embedded watermarking and backdoor triggers, enabling reliable identification of unauthorized model use. Experimental evaluations demonstrate that RADEP significantly reduces extraction success rates while maintaining high detection accuracy with minimal impact on legitimate queries. Extensive experiments show that RADEP effectively defends against model extraction attacks and remains resilient even against adaptive adversaries, making it a reliable security framework for MLaaS models.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Amit Chakraborty",
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae1157c1fba9603a98566209bc266f6b7a534629",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "title": "Augmenting Model Extraction Attacks Against Disruption-Based Defenses",
      "abstract": "Existing research has demonstrated that deep neural networks are susceptible to model extraction attacks, where an attacker can construct a substitute model with similar functionality to the victim model by querying the black-box victim model. To counter such attacks, various disruption-based defenses have been proposed. These defenses disrupt the output results of queries before returning them to potential attackers. In this paper, we propose the first defense-penetrating model extraction attack framework, aimed at breaking disruption-based defense methods. Our proposed attack framework comprises two key modules: disruption detection and disruption recovery, which can be integrated into generic model extraction attacks. Specifically, the disruption detection module uses a novel meta-learning-based algorithm to infer the defense strategy employed by the defender, by learning the key differences between the distributions of disrupted and undisrupted query results. Once the defense method is inferred, the disruption recovery module is designed to restore clean query results from the disrupted query results, using a carefully-designed generative model. We conducted extensive experiments on 5 commonly-used datasets to evaluate the effectiveness of our proposed framework. The results demonstrate that the substitute model accuracy of current model extraction attacks can be significantly improved by up to 82.42%, even when faced with four state-of-the-art model extraction defenses. Moreover, our attack approach shows promising results in penetrating unknown defenses in real-world cloud service APIs hosted by Microsoft Azure and Face++.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xueluan Gong",
        "Shuaike Li",
        "Yanjiao Chen",
        "Mingzhe Li",
        "Rubin Wei",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "15cb47e68968064996190efae5a5f875af516ac9",
      "title": "POSTER: Disappearing Ink: How Partial Model Extraction Erases Watermarks",
      "abstract": "Deep neural networks have become invaluable intellectual property in machine learning. To deter model theft, Watermarking has emerged as a prominent defense by embedding hidden \u201ctrigger sets\u201d that aid in ownership verification. However, current watermarking solutions primarily address scenarios where adversaries steal the entire model. In this paper, we reveal a critical gap: partial model extraction, where only a subset of classes is stolen, substantially degrading the watermark\u2019s reliability. We introduce two attacks, Partial Model Extraction and Partial Knowledge Distillation, which reduce watermark accuracy by up to 80% while retaining strong performance on the stolen classes. Through extensive experiments on CIFAR10 and CIFAR100 against two state-of-the-art watermarking schemes, we demonstrate the need for more robust watermarking strategies that resist partial-class theft.",
      "year": 2025,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Venkata Sai Pranav Bachina",
        "Ankit Gangwal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/15cb47e68968064996190efae5a5f875af516ac9",
      "pdf_url": "",
      "publication_date": "2025-08-24",
      "keywords_matched": [
        "model extraction",
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "title": "Model Extraction Attack and Its Countermeasure for Denoising Diffusion Implicit Models",
      "abstract": "Recently, the threat of cyber attacks against machine learning models has been increasing. Typical examples include Model Extraction Attack (MEA), which steals the functionality of a victim model by creating its clone model that has almost the same functionality. Thus, the literature has studied MEA and its defense methods, mainly focusing on image recognition models. However, no existing studies evaluate the risk of MEA on diffusion-based image generation models, despite the recent advances and widespread use of image generation AI services powered by diffusion models. In this paper, we first demonstrate the feasibility of MEA on DDIM, one of the most common diffusion-based image generation models. Then, as a countermeasure, we propose a defense method that detects clone models of DDIM. In the proposed method, we add a small number of out-of-distribution images, referred to as \u201cmarking images\u201d, to the training dataset of a victim DDIM model. This technique provides the property of occasionally generating marking images for the victim model. This property works as a watermark and is inherited by the clone models, being used as a clue for detecting them. In the results of our experiments conducted on face, fruit, and church image datasets, the proposed defense method can correctly detect all clone models without seriously degrading the usability of victim DDIM models.",
      "year": 2025,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Hayato Shoji",
        "Kazuaki Nakamura"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "63fb4344b3b77b8b23b89c732d31274520c8f026",
      "title": "Exploring A Model Type Detection Attack against Machine Learning as A Service",
      "abstract": "Recently, Machine-Learning-as-a-Service (MLaaS) systems are reported to be vulnerable to varying novel attacks, e.g., model extraction attacks and adversarial examples. However, in our investigation, we notice that the majority of MLaas attacks are not as threatening as expected due to model-type-sensitive problem. Literally speaking, many MLaaS attacks are designed for only a specific type of models. Without model type info as default prior knowledge, these attacks suffer from great performance degradation, or even become infeasible! In this paper, we demonstrate a novel attack method, named SNOOPER, to resolve the model-type-sensitive problem of MLaaS attacks. Specifically, SNOOPER is integrated with multiple self-designed model-type-detection modules. Each module can judge whether a given black-box model belongs to a specific type of models by analyzing its query-response pattern. Then, after proceeding with all modules, the attacker can know the type of its target model in the querying stage, and accordingly choose the optimal attack method. Also, to save budget, the queries can be re-used in the latter attack stage. We call such a kind of attack as model-type-detection attack. Finally, we experiment with SNOOPER on some popular model classes, including decision trees, linear models, non-linear models and neural networks. The results show that SNOOPER is capable of detecting the model type with more than 90% accuracy.",
      "year": 2025,
      "venue": "Journal of Intelligent Computing and Networking",
      "authors": [
        "Yilong Yang",
        "Xinjing Liu",
        "Ruidong Han",
        "Yang Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/63fb4344b3b77b8b23b89c732d31274520c8f026",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Shufan Yang",
        "Zifeng Cheng",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Cong Wang",
        "Shiping Ge",
        "Yuchen Fu",
        "Qing Gu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "pdf_url": "",
      "publication_date": "2025-11-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ff22d38603caf509358f50ef67244b039c63a5b1",
      "title": "Budget and Frequency Controlled Cost-Aware Model Extraction Attack on Sequential Recommenders",
      "abstract": "Sequential recommenders are integral to many applications yet remain vulnerable to model extraction attacks, in which adversaries can recover information about the deployed model by issuing queries to a black-box without internal access. From the attacker's perspective, existing studies impose a fixed and limited query budget but overlook optimal allocation, resulting in redundant or low-value requests. Furthermore, the scarce data obtained through these costly queries is typically handled by crude random sampling, resulting in low diversity and information coverage with actual data. In this paper, we propose a novel approach, named Budget and Frequency Controlled Cost-Aware Model Extraction Attack (BECOME), for extracting black-box sequential recommenders, which extends the standard extraction framework with two cost-aware innovations: Feedback-Driven Dynamic Budgeting periodically evaluates the victim model to refine query allocation and steer sequence generation adaptively. Rank-Aware Frequency Controlling integrates frequency constraints with ranking guidance in the next-item sampler to select high-value items and broaden information coverage. Experiments on public datasets and representative sequential recommender architectures demonstrate that our method achieves superior extraction performance. Our code is released at https://github.com/Loche2/BECOME.",
      "year": 2025,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Lei Zhou",
        "Min Gao",
        "Zongwei Wang",
        "Yibing Bai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ff22d38603caf509358f50ef67244b039c63a5b1",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "title": "BarkBeetle: Stealing Decision Tree Models with Fault Injection",
      "abstract": "Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qifan Wang",
        "Jonas Sander",
        "Minmin Jiang",
        "Thomas Eisenbarth",
        "David Oswald"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "title": "Hybrid Fingerprinting for Effective Detection of Cloned Neural Networks",
      "abstract": "As artificial intelligence plays an increasingly important role in decision-making within critical infrastructure, ensuring the authenticity and integrity of neural networks is crucial. This paper addresses the problem of detecting cloned neural networks. We present a method for identifying clones that employs a combination of metrics from both the information and physical domains: output predictions, probability score vectors, and power traces measured from the device running the neural network during inference. We compare the effectiveness of each metric individually, as well as in combination. Our results show that the effectiveness of both the information and the physical domain metrics is excellent for a clone that is a near replica of the target neural network. Furthermore, both the physical domain metric individually and the hybrid approach outperform the information domain metrics at detecting clones whose weights were extracted with low accuracy. The presented method offers a practical solution for verifying neural network authenticity and integrity. It is particularly useful in scenarios where neural networks are at risk of model extraction attacks, such as in cloud-based machine learning services.",
      "year": 2025,
      "venue": "IEEE International Symposium on Multiple-Valued Logic",
      "authors": [
        "Can Aknesil",
        "Elena Dubrova",
        "Niklas Lindskog",
        "Jakob Sternby",
        "H\u00e5kan Englund"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e86113fbdd6ac033b1dbe7e985646fc407ef8332",
      "title": "How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment",
      "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing graph-structured data in domains such as drug discovery and financial analysis, leading to growing demands for model transparency. Recent advances in explainable GNNs have addressed this need by revealing important subgraphs that influence predictions, but these explanation mechanisms may inadvertently expose models to security risks. This paper investigates how such explanations potentially leak critical decision logic that can be exploited for model stealing. We propose {\\method}, a novel stealing framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries, enabling effective replication of both the predictive behavior and underlying reasoning patterns of target models. Experiments on molecular graph datasets demonstrate that our approach shows advantages over conventional methods in model stealing. This work highlights important security considerations for the deployment of explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks. Our code is available at https://github.com/beanmah/EGSteal.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Bin Ma",
        "Yuyuan Feng",
        "Minhua Lin",
        "Enyan Dai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e86113fbdd6ac033b1dbe7e985646fc407ef8332",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6c1f2957365c03221827f076190da814769a79b2",
      "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
      "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the\"oracle''queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Awa Khouna",
        "Julien Ferry",
        "Thibaut Vidal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6c1f2957365c03221827f076190da814769a79b2",
      "pdf_url": "",
      "publication_date": "2025-02-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
      "abstract": "Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks. In this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from 193,252 to 1,857 with zero training cost. The code is available at https://github.com/NeurHT/NeurHT.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixiao Xu",
        "Binxing Fang",
        "Rui Wang",
        "Yinghai Zhou",
        "Shouling Ji",
        "Yuan Liu",
        "Mohan Li",
        "Zhihong Tian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "pdf_url": "",
      "publication_date": "2025-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "54ca14c7326e174dfe3627ced320a122e11d6d29",
      "title": "Model Extraction for Image Denoising Networks",
      "abstract": "Model Extraction (ME) replicates the performance of another entity\u2019s pretrained model without authorization. While extensively studied in image classification, object detection, and other tasks, ME for image restoration has been scarcely studied despite its broad applications. This paper presents a novel ME framework for image denoising networks, a fundamental one in image restoration. The framework tackles unique challenges like the black-box nature of the victim model, limiting access to its parameters, gradients, and outputs, and the difficulty of acquiring data that matches the original noise distribution while having adequate diversity. Our solution involves simulating the victim\u2019s noise conditions to transform clean images into noisy ones and introducing loss functions to optimize the generator and substitute model. Experiments show that our method closely approximates the victim model\u2019s performance and improves generalization in some scenarios. To the best of our knowledge, this work is the first to address ME in the field of image restoration, paving the way for future research in this area.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huan Teng",
        "Yuhui Quan",
        "Yong Xu",
        "Jun Huang",
        "Hui Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/54ca14c7326e174dfe3627ced320a122e11d6d29",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are increasingly deployed in real-world applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both previous methods enable ownership verification but do not convey additional information about the copy model. If the owner has multiple models, each model requires a distinct trigger graph. To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string in GNN models without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes in a graph. By assigning unique LDDE values to the edges and employing the LDDE sequence as the watermark, WGLE supports multi-bit capacity without relying on backdoor mechanisms. We evaluate WGLE on six public datasets across six mainstream GNN architectures, and compare WGLE with state-of-the-art GNN watermarking and fingerprinting methods. WGLE achieves 100% ownership verification accuracy, with an average fidelity degradation of only 1.41%. Additionally, WGLE exhibits robust resilience against potential attacks. The code is available in the repository.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tingzhi Li",
        "Xuefeng Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5e91b9b928b33ce05273507cf8dea1ade2450598",
      "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
      "abstract": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Xuebin Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5e91b9b928b33ce05273507cf8dea1ade2450598",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "title": "Watermark Stealing in Large Language Models",
      "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nikola Jovanovi'c",
        "Robin Staab",
        "Martin T. Vechev"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "836c652834b0f6ffe10e53ede1c0b9433cfad9ea",
      "title": "Copyright Protection in Generative AI: A Technical Perspective",
      "abstract": "Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jie Ren",
        "Han Xu",
        "Pengfei He",
        "Yingqian Cui",
        "Shenglai Zeng",
        "Jiankun Zhang",
        "Hongzhi Wen",
        "Jiayuan Ding",
        "Hui Liu",
        "Yi Chang",
        "Jiliang Tang"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/836c652834b0f6ffe10e53ede1c0b9433cfad9ea",
      "pdf_url": "",
      "publication_date": "2024-02-04",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9487d29364645c2f086387ff817ad5fd14b33c41",
      "title": "A Comprehensive Defense Framework Against Model Extraction Attacks",
      "abstract": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9487d29364645c2f086387ff817ad5fd14b33c41",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "title": "FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices",
      "abstract": "Sub-model extraction based federated learning has emerged as a popular strategy for training models on resource-constrained devices. However, existing methods treat all clients equally and extract sub-models using predetermined rules, which disregard the statistical heterogeneity across clients and may lead to fierce competition among them. Specifically, this paper identifies that when making predictions, different clients tend to activate different neurons of the entire model related to their respective distributions. If highly activated neurons from some clients with one distribution are incorporated into the sub-model allocated to other clients with different distributions, they will be forced to fit the new distributions, which can hinder their activation over the previous clients and result in a performance reduction. Motivated by this finding, we propose a novel method called FedDSE, which can reduce the conflicts among clients by extracting sub-models based on the data distribution of each client. The core idea of FedDSE is to empower each client to adaptively extract neurons from the entire model based on their activation over the local dataset. We theoretically show that FedDSE can achieve an improved classification score and convergence over general neural networks with the ReLU activation function. Experimental results on various datasets and models show that FedDSE outperforms all state-of-the-art baselines.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Haozhao Wang",
        "Yabo Jia",
        "Meng Zhang",
        "Qi Hu",
        "Hao Ren",
        "Peng Sun",
        "Yonggang Wen",
        "Tianwei Zhang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/3c3479215bcc775f8e37d7d5055a16b698904c9a",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f39d1a38b1b9513d6d50ceb8c4663a97deafd632",
      "title": "A Fast, Performant, Secure Distributed Training Framework For LLM",
      "abstract": "The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream task. Numerous experiments have shown that our method guarantees accuracy while maintaining security.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Wei Huang",
        "Yinggui Wang",
        "Anda Cheng",
        "Aihui Zhou",
        "Chaofan Yu",
        "Lei Wang"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/f39d1a38b1b9513d6d50ceb8c4663a97deafd632",
      "pdf_url": "",
      "publication_date": "2024-01-18",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "650f6db095276ca03d03f4951587d7383ca3b39d",
      "title": "ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minxue Tang",
        "Anna Dai",
        "Louis DiValentin",
        "Aolin Ding",
        "Amin Hass",
        "Neil Zhenqiang Gong",
        "Yiran Chen",
        "Helen Li"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/650f6db095276ca03d03f4951587d7383ca3b39d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "title": "SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tushar Nayan",
        "Qiming Guo",
        "Mohammed A. Duniawi",
        "Marcus Botacin",
        "Selcuk Uluagac",
        "Ruimin Sun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6d556b1dbb4a351b12a759c6581ed903e728c09e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "abstract": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "title": "DeMistify: Identifying On-Device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps",
      "abstract": "Mobile apps have become popular for providing artificial intelligence (AI) services via on-device machine learning (ML) techniques. Unlike accomplishing these AI services on remote servers traditionally, these on-device techniques process sensitive information required by AI services locally, which can mitigate the severe con-cerns of the sensitive data collection on the remote side. However, these on-device techniques have to push the core of ML expertise (e.g., models) to smartphones locally, which are still subject to similar vulnerabilities on the remote clouds and servers, especially when facing the model stealing attack. To defend against these attacks, developers have taken various protective measures. Unfor-tunately, we have found that these protections are still insufficient, and on-device ML models in mobile apps could be extracted and reused without limitation. To better demonstrate its inadequate protection and the feasibility of this attack, this paper presents DeMistify, which statically locates ML models within an app, slices relevant execution components, and finally generates scripts auto-matically to instrument mobile apps to successfully steal and reuse target ML models freely. To evaluate DeMistify and demonstrate its applicability, we apply it on 1,511 top mobile apps using on-device ML expertise for several ML services based on their install numbers from Google Play and DeMistify can successfully execute 1250 of them (82.73%). In addition, an in-depth study is conducted to understand the on-device ML ecosystem in the mobile application.",
      "year": 2024,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Pengcheng Ren",
        "Chaoshun Zuo",
        "Xiaofeng Liu",
        "Wenrui Diao",
        "Qingchuan Zhao",
        "Shanqing Guo"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "title": "QUEEN: Query Unlearning Against Model Extraction",
      "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://github.com/MaraPapMann/QUEEN.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Bo Liu",
        "Derui Wang",
        "Wanlei Zhou",
        "Minhui Xue"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "pdf_url": "http://arxiv.org/pdf/2407.01251",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
      "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. Our findings revealed that model stealing attacks can produce clone models achieving up to 0.9 \u00d7 and 0.99 \u00d7 clone test accuracy when trained using Top-1 and Top-k labels, respectively (k: num_classes). To defend against these attacks, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Despite limited success with our defense techniques, it has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.",
      "year": 2024,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Satwik Kundu",
        "Debarshi Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "pdf_url": "https://arxiv.org/pdf/2402.11687",
      "publication_date": "2024-02-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines",
      "abstract": "Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99%~7.35% across diverse datasets and VQC architectures.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Min Yang",
        "Cheng Chu",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "pdf_url": "http://arxiv.org/pdf/2403.10790",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "model stealing",
        "stealing language model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3434f582cc9c2818785e2920241d43d932625539",
      "title": "ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack",
      "abstract": "Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai",
        "Minghu Jiang",
        "Yongfeng Huang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3434f582cc9c2818785e2920241d43d932625539",
      "pdf_url": "",
      "publication_date": "2024-05-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "title": "Probabilistically Robust Watermarking of Neural Networks",
      "abstract": "As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Mikhail Aleksandrovich Pautov",
        "Nikita Bogdanov",
        "Stanislav Pyatkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "pdf_url": "https://arxiv.org/pdf/2401.08261",
      "publication_date": "2024-01-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "title": "TPUXtract: An Exhaustive Hyperparameter Extraction Framework",
      "abstract": "Model stealing attacks on AI/ML devices undermine intellectual property rights, compromise the competitive advantage of the original model developers, and potentially expose sensitive data embedded in the model\u2019s behavior to unauthorized parties. While previous research works have demonstrated successful side-channelbased model recovery in embedded microcontrollers and FPGA-based accelerators, the exploration of attacks on commercial ML accelerators remains largely unexplored. Moreover, prior side-channel attacks fail when they encounter previously unknown models. This paper demonstrates the first successful model extraction attack on the Google Edge Tensor Processing Unit (TPU), an off-the-shelf ML accelerator. Specifically, we show a hyperparameter stealing attack that can extract all layer configurations including the layer type, number of nodes, kernel/filter sizes, number of filters, strides, padding, and activation function. Most notably, our attack is the first comprehensive attack that can extract previously unseen models. This is achieved through an online template-building approach instead of a pre-trained ML-based approach used in prior works. Our results on a black-box Google Edge TPU evaluation show that, through obtained electromagnetic traces, our proposed framework can achieve 99.91% accuracy, making it the most accurate one to date. Our findings indicate that attackers can successfully extract various types of models on a black-box commercial TPU with utmost detail and call for countermeasures.",
      "year": 2024,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Ashley Kurian",
        "Anuj Dubey",
        "Ferhat Yaman",
        "Aydin Aysu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "pdf_url": "https://doi.org/10.46586/tches.v2025.i1.78-103",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9fef6937c39de73fdc3531790e938274caa84de0",
      "title": "AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks",
      "abstract": "With the proliferation of machine learning models in diverse applications, the issue of model security has increasingly become a focal point. Model steal attacks can cause significant financial losses to model owners and potentially threaten the security of their application scenarios. Traditional model steal attacks are primarily directed at soft-label black boxes, but their effectiveness significantly diminishes or even fails in hard-label scenarios. To address this, for hard-label black boxes, this study proposes an active learning-based Fusion Augmentation Model Stealing Framework (AugSteal). This framework initially utilizes large-scale irrelevant public datasets for deep filtering and feature extraction to generate reliable, diverse, and representative high-quality data subsets as the stealing dataset. Subsequently, we developed an adaptive active learning selection strategy that selects data samples with significant information gain for different black-box models, enhancing the attack\u2019s specificity and effectiveness. Finally, to further address the trade-off between query budget and steal precision, this paper designed a Fusion Augmentation training method constituted of two different loss functions, enabling the substitute model to closely approximate the decision distribution of the target black box.The comprehensive experimental results indicate that, compared to the current state-of-the-art attack methods, our approach achieved a maximum performance gain of 8.21% in functional similarity for the substitute models in simulated black-box scenarios CIFAR10, SVHN, CALTECH256, and the real-world application Tencent Cloud API.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Lijun Gao",
        "Wenjun Liu",
        "Kai Liu",
        "Jiehong Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9fef6937c39de73fdc3531790e938274caa84de0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks",
      "abstract": "Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Qiongkai Xu",
        "Jey Han Lau"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "pdf_url": "",
      "publication_date": "2024-08-29",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0207b4706683924d5b0e1399bd08b49fd19c394f",
      "title": "Stealing Training Graphs from Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.",
      "year": 2024,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Min Lin",
        "Enyan Dai",
        "Junjie Xu",
        "Jinyuan Jia",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0207b4706683924d5b0e1399bd08b49fd19c394f",
      "pdf_url": "",
      "publication_date": "2024-11-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0219db84513f4c2604ed15fce52c8f810b463ba5",
      "title": "Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models",
      "abstract": "Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being\"hidden\"behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to\"steal\"these models for retrieval by training thief models on text-embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "M. Tamber",
        "Jasper Xian",
        "Jimmy Lin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0219db84513f4c2604ed15fce52c8f810b463ba5",
      "pdf_url": "",
      "publication_date": "2024-06-13",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "94621729b6ca9e811d8a052dc3457e98df457676",
      "title": "Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing",
      "abstract": "Model stealing (MS) involves querying and observing the output of a machine learning model to steal its capabilities. The quality of queried data is crucial, yet obtaining a large amount of real data for MS is often challenging. Recent works have reduced reliance on real data by using generative models. However, when high-dimensional query data is required, these methods are impractical due to the high costs of querying and the risk of model collapse. In this work, we propose using sample gradients (SG) to enhance the utility of each real sample, as SG provides crucial guidance on the decision boundaries of the victim model. However, utilizing SG in the model stealing scenario faces two challenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. The estimation of sample gradients has a significant variance. This paper proposes Superpixel Sample Gradient stealing (SPSG) for model stealing under the constraint of limited real samples. With the basic idea of imitating the victim model's low-variance patch-level gradients instead ofpixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wise perturbations on query images to estimate the average gradient in different regions of the image. Then, we filter the gradients through a threshold strategy to reduce variance. Exhaustive experiments demonstrate that, with the same number of real samples, SPSG achieves accuracy, agreements, and adversarial success rate significantly surpassing the current state-of-the-art MS methods. Codes are available at https://github.com/zyI123456aBISPSG_attack.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yunlong Zhao",
        "Xiaoheng Deng",
        "Yijing Liu",
        "Xin-jun Pei",
        "Jiazhi Xia",
        "Wei Chen"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/94621729b6ca9e811d8a052dc3457e98df457676",
      "pdf_url": "https://arxiv.org/pdf/2406.18540",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "389f1fad962f58799327a0112526452d2da5157d",
      "title": "TEXTKNOCKOFF: KNOCKOFF NETS FOR STEALING FUNCTIONALITY OF TEXT SENTIMENT MODELS",
      "abstract": "Most commercial machine learning models today are designed to require significant amounts of time, money, and human effort. Therefore, intrinsic information about the model (such as architecture, hyperparameters, and training data) needs to be kept confidential. These models are referred to as black boxes, and there is an increasing amount of research focused on both attacking and protecting them. Recent publications have often concentrated on the field of computer vision; in contrast, there is still relatively little research on methods for attacking black box models with textual data. This article introduces a research method for extracting the functionality of a black box model in the task of text sentiment analysis. The method has been effectively tested based on random sampling techniques to reconstruct a new model with equivalent functionality to the original model, achieving high accuracy (94.46% compared to 94.92%) and high similarity (96.82%).",
      "year": 2024,
      "venue": "Journal of Science and Technique",
      "authors": [
        "X. Pham"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/389f1fad962f58799327a0112526452d2da5157d",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f63f7d623e1738fbc314143a1ad1812045caffff",
      "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which allow for the processing of graph-structured data without relying on predefined graph structures, are becoming increasingly important in a wide range of applications. As such these networks become attractive targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. Significant efforts have been devoted to developing model-stealing attacks that extract models trained on images and texts. However, little attention has been given to stealing GNNs trained on graph data. This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model. The new type of attack is thoroughly evaluated on six datasets and the results show that our approach outperforms the current state-of-the-art by Shen et al. (2021). In particular, our attack surpasses the baseline across all benchmarks, attaining superior fidelity and downstream accuracy of the stolen model while necessitating fewer queries directed toward the target model.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f63f7d623e1738fbc314143a1ad1812045caffff",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a5b7633d836b0430225774cdcf63e6a18385c251",
      "title": "PRSA: Prompt Reverse Stealing Attacks against Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yong Yang",
        "Xuhong Zhang",
        "Yi Jiang",
        "Xi Chen",
        "Haoyu Wang",
        "Shouling Ji",
        "Zonghui Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a5b7633d836b0430225774cdcf63e6a18385c251",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b28136746a29ff698f57106c292d0d6e0181629",
      "title": "Efficient Data-Free Model Stealing with Label Diversity",
      "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the proliferation of machine learning based applications, while on the other hand, it introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yiyong Liu",
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2b28136746a29ff698f57106c292d0d6e0181629",
      "pdf_url": "",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "title": "Efficient Model Stealing Defense with Noise Transition Matrix",
      "abstract": null,
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Dong-Dong Wu",
        "Chilin Fu",
        "Weichang Wu",
        "Wenwen Xia",
        "Xiaolu Zhang",
        "Jun Zhou",
        "Min-Ling Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4595c8a7e1571c9974b2393ed127f05fabe72164",
      "title": "Model Stealing Detection for IoT Services Based on Multidimensional Features",
      "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a tradeoff between false alarm rates and detection accuracy. To address this challenge, we design a multidimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multidimensional features between attack and benign samples and 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature-based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xinjing Liu",
        "Taifeng Liu",
        "Hao Yang",
        "Jiakang Dong",
        "Zuobin Ying",
        "Zhuo Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4595c8a7e1571c9974b2393ed127f05fabe72164",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a6854f91dbad0b39a0289872ac838545a9c18225",
      "title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225",
      "pdf_url": "",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
      "abstract": "Deep reinforcement learning policies, which are integral to modern control systems, represent valuable intellectual property. The development of these policies demands considerable resources, such as domain expertise, simulation fidelity, and real-world validation. These policies are potentially vulnerable to model stealing attacks, which aim to replicate their functionality using only black-box access. In this paper, we propose Stealthy Imitation, the first attack designed to steal policies without access to the environment or knowledge of the input range. This setup has not been considered by previous model stealing methods. Lacking access to the victim's input states distribution, Stealthy Imitation fits a reward model that allows to approximate it. We show that the victim policy is harder to imitate when the distribution of the attack queries matches that of the victim. We evaluate our approach across diverse, high-dimensional control tasks and consistently outperform prior data-free approaches adapted for policy stealing. Lastly, we propose a countermeasure that significantly diminishes the effectiveness of the attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "pdf_url": "",
      "publication_date": "2024-05-11",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "title": "Efficient and Effective Model Extraction",
      "abstract": "Model extraction aims to steal a functionally similar copy from a machine learning as a service (MLaaS) API with minimal overhead, typically for illicit profit or as a precursor to further attacks, posing a significant threat to the MLaaS ecosystem. However, recent studies have shown that model extraction is highly inefficient, particularly when the target task distribution is unavailable. In such cases, even substantially increasing the attack budget fails to produce a sufficiently similar replica, reducing the adversary\u2019s motivation to pursue extraction attacks. In this paper, we revisit the elementary design choices throughout the extraction lifecycle. We propose an embarrassingly simple yet dramatically effective algorithm, Efficient and Effective Model Extraction (E3), focusing on both query preparation and training routine. E3 achieves superior generalization compared to state-of-the-art methods while minimizing computational costs. For instance, with only 0.005\u00d7 the query budget and less than 0.2\u00d7 the runtime, E3 outperforms classical generative model based data-free model extraction by an absolute accuracy improvement of over 50% on CIFAR-10. Our findings underscore the persistent threat posed by model extraction and suggest that it could serve as a valuable benchmarking algorithm for future security evaluations.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hongyu Zhu",
        "Wentao Hu",
        "Sichu Liang",
        "Fangqi Li",
        "Wenwen Wang",
        "Shilin Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "pdf_url": "",
      "publication_date": "2024-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
      "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential. In this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.",
      "year": 2024,
      "venue": "ARES",
      "authors": [
        "Dirmanto Jap",
        "J. Breier",
        "Zdenko Lehock'y",
        "S. Bhasin",
        "Xiaolu Hou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "pdf_url": "",
      "publication_date": "2024-07-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "99712e009858ba394bd119092bc2ebcc502a8892",
      "title": "DualCOS: Query-Efficient Data-Free Model Stealing with Dual Clone Networks and Optimal Samples",
      "abstract": "Although data-free model stealing attacks are free from reliance on real data, they suffer from limitations, including low accuracy and high query budgets, which restrict their practical feasibility. In this paper, we propose a novel data-free model stealing framework called DualCOS. As a whole, DualCOS is divided into two stages: interactive training and semi-supervised boosting. To optimize the usage of query budgets, we use a dual clone model architecture to address the challenge of querying victim model during generator training. We also introduce active learning-based sampling strategy and sample reuse mechanism to achieve an efficient query process. Furthermore, once query budget is exhausted, the semi-supervised boosting is employed to continue improving the final clone accuracy. Through extensive evaluations, we demonstrate the superiority of our proposed method in terms of accuracy and query efficiency, particularly in scenarios involving hard labels and multiple classes.",
      "year": 2024,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/99712e009858ba394bd119092bc2ebcc502a8892",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21c62f2ef68323099480c80318e48baae8b9098f",
      "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
      "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Liaoyaqi Wang",
        "Minhao Cheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "title": "SwiftThief: Enhancing Query Efficiency of Model Stealing by Contrastive Learning",
      "abstract": "Model-stealing attacks are emerging as a severe threat to AI-based services because an adversary can create models that duplicate the functionality of the black-box AI models inside the services with regular query-based access. To avoid detection or query costs, the model-stealing adversary must consider minimizing the number of queries to obtain an accurate clone model. To achieve this goal, we propose SwiftThief, a novel model-stealing framework that utilizes both queried and unqueried data to reduce query complexity. In particular, SwiftThief uses contrastive learning, a recent technique for representation learning. We formulate a new objective function for model stealing consisting of self-supervised (for abundant unqueried inputs from public datasets) and soft-supervised (for queried inputs) contrastive losses, jointly optimized with an output matching loss (for queried inputs). In addition, we suggest a new sampling strategy to prioritize rarely queried classes to improve attack performance. Our experiments proved that SwiftThief could significantly enhance the efficiency of model-stealing attacks compared to the existing methods, achieving similar attack performance using only half of the query budgets of the competing approaches. Also, SwiftThief showed high competence even when a defense was activated for the victims.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "title": "I Can Retrieve More than Images: Contrastive Stealing Attack against Deep Hashing Models",
      "abstract": "Deep hashing models have revolutionized traditional hashing methods by delivering superior performance, and have been applied in real-world applications such as Pinterest and Amazon, which are known as deep hashing-based retrieval systems. Behind their revolutionary representation capability, the requirements for training a deep hashing model expose it to the risks of potential model stealing attacks - a cheap way to mimic the well-trained hashing performance while circumventing the demanding requirements. Since the attacker is able to obtain the outputs of deep hashing models by querying the retrieval systems, the conventional stealing attacks relying on matching exact outputs can not be applied in this problem. In this paper, we propose a contrastive-based and GAN-enhanced stealing framework to leverage the informative knowledge of retrieved data. Our empirical results demonstrate that our stealing framework can train a substitute hashing model with a retrieval accuracy ranging from 80% to 110% of the target hashing model while utilizing significantly fewer training resources. Furthermore, we conduct attacks on the target hashing model using adversarial examples generated by the stolen model, resulting in an attack success rate that can be 3 times higher compared to attacks conducted without the substitute model. Finally, we leverage existing defense strategies to mitigate our attack, resulting in a stealing effectiveness decrease of no more than 4%.",
      "year": 2024,
      "venue": "2024 IEEE International Conference on Web Services (ICWS)",
      "authors": [
        "X. You",
        "Mi Zhang",
        "Jianwei Xu",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "pdf_url": "",
      "publication_date": "2024-07-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "title": "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality",
      "abstract": "Machine learning, particularly deep learning, is extensively applied in various real-life scenarios. However, recent research has highlighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. Most of the existing model stealing attacks rely on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, black-box, hard-label outputs, and there is a distribution shift during the testing phase. STMS achieves comparable or better stealing accuracy and generalization performance than prior works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can also be migrated to other areas of machine learning.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Zhendong Zhao",
        "Yu Xuan",
        "Bisheng Tang",
        "Xiaoying Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "title": "MCD: Defense Against Query-Based Black-Box Surrogate Attacks",
      "abstract": "Deep neural networks (DNNs) is susceptible to surrogate attacks, where adversaries use surrogate data and corresponding outputs from the target model to build their own stolen model. Model stealing attacks jeopardize model privacy and model owners' commercial benefits. To address this issue, this paper proposes a hybrid protection approach-Maximize the confidence differences between benign samples and adversarial samples (MCD), to protect models from theft. Firstly, the LogitNorm approach is used to overcome the overconfidence problem in adversary query classification. Then, samples are divided into four groups according to ES and RS. Different groups are poisoned by different degrees. In addition to enhancing defensive performance and accounting for model integrity, the MCD uses a trigger to confirm the cloned model's owner. Experimental results show that the MCD defends against a variety of original models and attack techniques well. Against KnockoffNets and DFME attacks, the MCD yields an average defense performance of 54.58 % on five datasets, which is a great improvement over other defenses. Compared to other poisoning techniques, the Strong Poisoning (SP) module reduces the adversary's accuracy by 48.23 % on average. Additionally, the MCD overcomes the issue of OOD overconfidence while safeguarding the model accuracy in OOD detection and reduces the misclassification rate of ID samples for multiple OOD datasets.",
      "year": 2024,
      "venue": "IEEE International Conference on Systems, Man and Cybernetics",
      "authors": [
        "Yiwen Zou",
        "Wing W. Y. Ng",
        "Xueli Zhang",
        "Brick Loo",
        "Xingfu Yan",
        "Ran Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "pdf_url": "",
      "publication_date": "2024-10-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "title": "TrustZoneTunnel: A Cross-World Pattern History Table-Based Microarchitectural Side-Channel Attack",
      "abstract": "ARM's TrustZone is a hardware-based trusted execution environment (TEE), prevalent in mobile devices, IoT edge systems, and autonomous systems. Within TrustZone, security-sensitive applications reside in a hardware-isolated secure world, protected from the normal-world's applications, OS, debugger, peripherals, and memory. However, microarchitectural side-channel vulnerabilities have been discovered on shared on-chip resources, such as caches and branch prediction unit (BPU). In this paper, we propose TrustZoneTunnel, the first Pattern History Table (PHT)-based side-channel attack on TrustZone, which is able to reveal the complete control flow of a trusted application in the secure world. We reverse-engineer the PHT indexing for ARM processors and develop key primitives for cross-world attacks, including well-controlled world-switching, PHT collision construction between two worlds, and precise PHT state-setting and checking functions. Furthermore, we introduce a novel model extraction attack against TrustZone based deep neural network, which can recover model parameters using only the side-channel leakage of vital branch instructions, obviating the need for model output or logits while prior research work requires such knowledge for model extraction.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Tianhong Xu",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8c34055674248b2b3c2de461eab414e0704f9782",
      "title": "Model theft attack against a tinyML application running on an Ultra-Low-Power Open-Source SoC",
      "abstract": "With the advent of tinyML, IoT devices have expanded their range of operations from simple data gathering and transmission to full-fledged inference. This expansion has been further enabled by the rise in popularity of open-source hardware, with the RISC-V architecture being the most prominent example. TinyML's decentralization can solve the current privacy and security issues of IoT infrastructures. However, it also shifts the burden of security on already resource-constrained devices. Ultra-low-power devices, in particular, often sacrifice security features for energy and area efficiency. This work aims at showing that, in the context of edge computing based on open-source hardware, neglecting hardware security features for the sake of efficiency is not an acceptable trade-off with respect to AI security.",
      "year": 2024,
      "venue": "ACM International Conference on Computing Frontiers",
      "authors": [
        "A. Porsia",
        "A. Ruospo",
        "Ernesto S\u00e1nchez"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8c34055674248b2b3c2de461eab414e0704f9782",
      "pdf_url": "",
      "publication_date": "2024-05-07",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "66431dd64545057c084c9f83a87ad0eb7c609f92",
      "title": "Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors",
      "abstract": "Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.",
      "year": 2024,
      "venue": "ECML/PKDD",
      "authors": [
        "Satoru Koda",
        "I. Morikawa"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66431dd64545057c084c9f83a87ad0eb7c609f92",
      "pdf_url": "",
      "publication_date": "2024-11-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "title": "MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction",
      "abstract": "The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes\"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Gurve",
        "S. Behera",
        "Satyadev Ahlawat",
        "Yamuna Prasad"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "model extraction",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3082e09e126c2c851e96cda93a9cb0010085d8b9",
      "title": "Making models more secure: An efficient model stealing detection method",
      "abstract": null,
      "year": 2024,
      "venue": "Computers & electrical engineering",
      "authors": [
        "Chenlong Zhang",
        "Senlin Luo",
        "Limin Pan",
        "Chuan Lu",
        "Zhao Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3082e09e126c2c851e96cda93a9cb0010085d8b9",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "title": "Proteus: Preserving Model Confidentiality during Graph Optimizations",
      "abstract": "Deep learning (DL) models have revolutionized numerous domains, yet optimizing them for computational efficiency remains a challenging endeavor. Development of new DL models typically involves two parties: the model developers and performance optimizers. The collaboration between the parties often necessitates the model developers exposing the model architecture and computational graph to the optimizers. However, this exposure is undesirable since the model architecture is an important intellectual property, and its innovations require significant investments and expertise. During the exchange, the model is also vulnerable to adversarial attacks via model stealing. This paper presents Proteus, a novel mechanism that enables model optimization by an independent party while preserving the confidentiality of the model architecture. Proteus obfuscates the protected model by partitioning its computational graph into subgraphs and concealing each subgraph within a large pool of generated realistic subgraphs that cannot be easily distinguished from the original. We evaluate Proteus on a range of DNNs, demonstrating its efficacy in preserving confidentiality without compromising performance optimization opportunities. Proteus effectively hides the model as one alternative among up to $10^{32}$ possible model architectures, and is resilient against attacks with a learning-based adversary. We also demonstrate that heuristic based and manual approaches are ineffective in identifying the protected model. To our knowledge, Proteus is the first work that tackles the challenge of model confidentiality during performance optimization. Proteus will be open-sourced for direct use and experimentation, with easy integration with compilers such as ONNXRuntime.",
      "year": 2024,
      "venue": "Conference on Machine Learning and Systems",
      "authors": [
        "Yubo Gao",
        "Maryam Haghifam",
        "Christina Giannoula",
        "Renbo Tu",
        "Gennady Pekhimenko",
        "Nandita Vijaykumar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "pdf_url": "",
      "publication_date": "2024-04-18",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "title": "Exploring the Validity of Knockoff Nets Model Stealing Attack on Vgg16 Based on Different Models",
      "abstract": "Model stealing attacks represented by the Knockoff Nets method steal the intellectual property of AI models by black-box querying. Model stealing attacks on a wide range of deep learning models have attracted widespread attention in recent years. However, there has not been any research on stealing attacks based on common models such as VGG16, ResNet18, AlexNet, etc., especially since the research on the validity of the attack on the VGG16 model is still insufficient. Therefore, in this paper, three types of models, VGG16, ResNet18, and AlexNet, are used as the models for stealing, and the Knockoff Nets method is used to carry out stealing attacks on the pre-trained model of VGG16, which is capable of cat and dog image recognition. This paper analyzes the stealing similarity, stealing model accuracy and stealing training time so as to reflect the validity of stealing. The paper shows that Knockoff Nets based on three types of models, VGG16, ResNet18, and AlexNet, are all effective against the VGG16 model stealing attack, and the more similar the architectures of the stealing model and the victim's model are, the better the stealing effect is. In addition, to a certain extent, the stealing training time and the stealing model accuracy are affected by the architecture of model used to steal. This paper reveals the validity of the Knockoff Nets model stealing attack against VGG16 based on three types of models, namely VGG16, ResNet18, and AlexNet, to provide a reference for model security protection.",
      "year": 2024,
      "venue": "Applied and Computational Engineering",
      "authors": [
        "Yunxi Hei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "pdf_url": "https://www.ewadirect.com/proceedings/ace/article/view/17285/pdf",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "model stealing",
        "stealing model",
        "model stealing attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "title": "Enhancing Data-Free Model Stealing Attack on Robust Models",
      "abstract": "Machine Learning Model Deployment as a Service (MLaaS) has surged in popularity, offering substantial business value. However, the significant resources and costs required to train models have raised concerns about Model Stealing Attacks (MSAs), where attackers create a clone model to replicate the knowledge of a victim model without access to its parameters. In data-free MSA, attackers also lack access to the training data for the victim model. In this setting, existing MSA methods rely on Generative Adversarial Networks (GANs) to generate images to query the victim model. However, GANs are known to suffer from model collapse, resulting in limited diversity in generated images. The lack of diversity in generated images will significantly impact the accuracy of the clone model, especially in stealing robust models trained with adversarial training. Recent studies have demonstrated that Denoising Diffusion Probabilistic Models (DDPMs) outperform GANs in generating images with greater diversity. In our data-free MSA framework, using DDPM as the generator to steal robust models significantly increases the effectiveness, improving the accuracy of the clone model from 21.34% to 60.23% compared to the GANs-based approach DFME, and requires fewer queries. We further use denoise diffusion GANs to address the problem of low sampling speed of DDPM, while retaining the advantage of its high sample diversity and obtaining better results.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jianping He",
        "Haichang Gao",
        "Yunyi Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "title": "On the Security of Privacy-Preserving Machine Learning Against Model Stealing Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Cryptology and Network Security",
      "authors": [
        "Bhuvnesh Chaturvedi",
        "Anirban Chakraborty",
        "Ayantika Chatterjee",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d9dcd3bf3aa0262eea6e17b5bb35705cc5f0301e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9bedd67004ef344b801365ae28c09dce28410517",
      "title": "Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing",
      "abstract": "In learning-based functionality stealing, the attacker is trying to build a local model based on the victim's outputs. The attacker has to make choices regarding the local model's architecture, optimization method and, specifically for NLP models, subword vocabulary, such as BPE. On the machine translation task, we explore (1) whether the choice of the vocabulary plays a role in model stealing scenarios and (2) if it is possible to extract the victim's vocabulary. We find that the vocabulary itself does not have a large effect on the local model's performance. Given gray-box model access, it is possible to collect the victim's vocabulary by collecting the outputs (detokenized subwords on the output). The results of the minimum effect of vocabulary choice are important more broadly for black-box knowledge distillation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Vil\u00e9m Zouhar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9bedd67004ef344b801365ae28c09dce28410517",
      "pdf_url": "",
      "publication_date": "2024-01-29",
      "keywords_matched": [
        "model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "66025882080646af642638429d89a473c9569718",
      "title": "LSSMSD: defending against black-box DNN model stealing based on localized stochastic sensitivity",
      "abstract": null,
      "year": 2024,
      "venue": "International Journal of Machine Learning and Cybernetics",
      "authors": [
        "Xueli Zhang",
        "Jiale Chen",
        "Qihua Li",
        "Jianjun Zhang",
        "Wing W. Y. Ng",
        "Ting Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/66025882080646af642638429d89a473c9569718",
      "pdf_url": "",
      "publication_date": "2024-09-18",
      "keywords_matched": [
        "model stealing",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08e1f102344bc341e8109a5c23f78093a4c53323",
      "title": "Protecting Object Detection Models from Model Extraction Attack via Feature Space Coverage",
      "abstract": "The model extraction attack is an attack pattern aimed at stealing well-trained machine learning models' functionality or privacy information. With the gradual popularization of AI-related technologies in daily life, various well-trained models are being deployed. As a result, these models are considered valuable assets and attractive to model extraction attackers. Currently, the academic community primarily focuses on defense for model extraction attacks in the context of classification, with little attention to the more commonly used task scenario of object detection. Therefore, we propose a detection framework targeting model extraction attacks against object detection models in this paper. The framework first locates suspicious users based on feature coverage in query traffic and uses an active verification module to confirm whether the identified suspicious users are attackers. Through experiments conducted in multiple task scenarios, we validate the effectiveness and detection efficiency of the proposed method.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Zeyu Li",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/08e1f102344bc341e8109a5c23f78093a4c53323",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "title": "Preserving Accuracy While Stealing Watermarked Deep Neural Networks",
      "abstract": "The deployment of Deep Neural Networks (DNNs) as cloud services has accelerated significantly over the years. Training an application-specific DNN for cloud deployment requires substantial computational resources and costs associated with hyper-parameter tuning and model selection. To preserve Intellectual Property (IP) rights, model owners embed watermarks into publicly deployed DNNs. These trigger inputs and labels are uniquely selected and embedded into the watermarked DNN by the model owner, remaining undisclosed during deployment. If a watermarked DNN (target classifier) is stolen via white-box access and re-deployed by an adversary (pirated classifier) without securing the IP rights from the model owner, the model owner can identify their IP by sending trigger inputs to retrieve trigger labels. Typically, adversaries tamper with the model weights of the target classifier prior to deployment, which in turn reduces the utility of the well-trained DNN. The authors proposes re-deploying the target classifier without altering the model weights to preserve model utility, and using a small sample of non-identical in-distribution inputs (used for training the target classifier) to train a Siamese neural network to evade detection, at inference stage. Experimental evaluations on standard benchmark datasets- MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100- using ResNet architectures with varying triggers demonstrate that the proposed method achieves zero false positive rate (fraction of clean testing input incorrectly labelled as trigger inputs) and false negative rate (fraction of trigger inputs incorrectly labelled as clean in-distribution inputs) in nearly all cases, proving its efficacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Aritra Ray",
        "F. Firouzi",
        "Kyle Lafata",
        "Krishnendu Chakrabarty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/264566b429bd774b7827cc1b5e4c4bd0efb84f1e",
      "pdf_url": "",
      "publication_date": "2024-12-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "96745ffac8b170dcb9dbc60e704d390d92d059d2",
      "title": "Beyond Labeling Oracles - What does it mean to steal ML models?",
      "abstract": null,
      "year": 2024,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Avital Shafran",
        "Ilia Shumailov",
        "Murat A. Erdogdu",
        "Nicolas Papernot"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96745ffac8b170dcb9dbc60e704d390d92d059d2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "title": "CLUES: Collusive Theft of Conditional Generative Adversarial Networks",
      "abstract": "Conditional Generative Adversarial Networks (cGANs) are increasingly popular web-based synthesis services accessed through a query API, e.g., cGANs generate a cat image based on a \u201ccat\u201d query. However, cGAN-based synthesizers can be stolen via adversaries' queries, i.e., model thieves. The prevailing adversarial assumption is that thieves act independently: they query the deployed cGAN (i.e., the victim), and train a stolen cGAN using the images obtained from the victim. A popular anti-theft defense consists in throttling down the number of queries from any given user. We consider a more realistic adversarial scenario: model thieves collude to query the victim, and then train the stolen cGAN. Clues is a new collusive model stealing framework, enabling thieves to bypass throttle-based defenses and steal cGANs more efficiently than through individual efforts. Thieves collect queried images and train a stolen cGAN in a federated manner. We evaluate Clues on three image datasets, e.g., MNIST, FashionMNIST and CelebA. We experimentally show the scalability of the proposed attack strategies against the number of thieves and the queried images, the impact of a classical noise-based defense, a passive watermarking defense and a JPEG-based countermeasure. Our evaluation shows that such a collusive stealing strategy gets close to 4 units of Frechet Inception Distance from a victim model. Our code is readily available to the research community: https://zenodo.org/records/10224340.",
      "year": 2024,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "Simon Queyrut",
        "V. Schiavoni",
        "Lydia Chen",
        "Pascal Felber",
        "Robert Birke"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "pdf_url": "",
      "publication_date": "2024-09-30",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f98f66ade820ce31d5aa5099f60e856f52db700",
      "title": "AuthNet: Neural Network with Integrated Authentication Logic",
      "abstract": "Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuling Cai",
        "Fan Xiang",
        "Guozhu Meng",
        "Yinzhi Cao",
        "Kai Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f98f66ade820ce31d5aa5099f60e856f52db700",
      "pdf_url": "",
      "publication_date": "2024-05-24",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "title": "Attack Data is Not Solely Paramount: A Universal Model Extraction Enhancement Method",
      "abstract": "Model extraction (ME) attacks, aiming to steal the functionality or parameters of the victim model, have become a widespread research topic. Most functional ME attack methodologies follow a uniform framework, which we summarize in three steps: initially choosing appropriate attack data, then querying the victim model with this data, and finally, training an incipient clone model based on the victim model\u2019s outputs. Despite much focus on data selection, the latter two steps have been somewhat neglected. Noticing this, we explore a method for the information of attack data labels to enhance the accuracy of the clone model. Specifically, we utilized the incipient clone model to identify similarities between the leaked private data and the attack data, subsequently appending the labels from the leaked data to those of the attack data. Then, we employed these modified attack data labels to fine-tune the incipient clone model, obtaining an enhanced clone model with higher accuracy. The enhancement was applied to three representative ME attack methodologies that primarily focus on the first step. Results show that the enhanced model reveals a higher accuracy than the three basic attacks. In summary, our approach suggests that future research should extend beyond data selection.",
      "year": 2024,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Chuang Liang",
        "Jie Huang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f5a16eb238e96accbe067c68886fc29d1202817f",
      "title": "Efficient Model Extraction via Boundary Sampling",
      "abstract": "This paper introduces a novel data-free model extraction attack that significantly advances the current state-of-the-art in terms of efficiency, accuracy, and effectiveness. Traditional black-box methods rely on using the victim's model as an oracle to label a vast number of samples within high-confidence areas. This approach not only requires an extensive number of queries but also results in a less accurate and less transferable model. In contrast, our method innovates by focusing on sampling low-confidence areas (along the decision boundaries) and employing an evolutionary algorithm to optimize the sampling process. These novel contributions allow for a dramatic reduction in the number of queries needed by the attacker by a factor of 10x to 600x while simultaneously improving the accuracy of the stolen model. Moreover, our approach improves boundary alignment, resulting in better transferability of adversarial examples from the stolen model to the victim's model (increasing the attack success rate from 60% to 82% on average). Finally, we accomplish all of this with a strict black-box assumption on the victim, with no knowledge of the target's architecture or dataset. We demonstrate our attack on three datasets with increasingly larger resolutions and compare our performance to four state-of-the-art model extraction attacks.",
      "year": 2024,
      "venue": "AISec@CCS",
      "authors": [
        "Maor Biton Dor",
        "Yisroel Mirsky"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f5a16eb238e96accbe067c68886fc29d1202817f",
      "pdf_url": "http://arxiv.org/pdf/2410.15429",
      "publication_date": "2024-10-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "title": "CaBaGe: Data-Free Model Extraction using ClAss BAlanced Generator Ensemble",
      "abstract": "Machine Learning as a Service (MLaaS) is often provided as a pay-per-query, black-box system to clients. Such a black-box approach not only hinders open replication, validation, and interpretation of model results, but also makes it harder for white-hat researchers to identify vulnerabilities in the MLaaS systems. Model extraction is a promising technique to address these challenges by reverse-engineering black-box models. Since training data is typically unavailable for MLaaS models, this paper focuses on the realistic version of it: data-free model extraction. We propose a data-free model extraction approach, CaBaGe, to achieve higher model extraction accuracy with a small number of queries. Our innovations include (1) a novel experience replay for focusing on difficult training samples; (2) an ensemble of generators for steadily producing diverse synthetic data; and (3) a selective filtering process for querying the victim model with harder, more balanced samples. In addition, we create a more realistic setting, for the first time, where the attacker has no knowledge of the number of classes in the victim training data, and create a solution to learn the number of classes on the fly. Our evaluation shows that CaBaGe outperforms existing techniques on seven datasets -- MNIST, FMNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet-subset, and Tiny ImageNet -- with an accuracy improvement of the extracted models by up to 43.13%. Furthermore, the number of queries required to extract a clone model matching the final accuracy of prior work is reduced by up to 75.7%.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jonathan Rosenthal",
        "Shanchao Liang",
        "Kevin Zhang",
        "Lin Tan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "pdf_url": "",
      "publication_date": "2024-09-16",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "title": "Enhancing Data-Free Robustness Stealing Attack via Boundary Data Generation",
      "abstract": "With the continuous development of Machine Learning as a Service (MLaaS), model stealing has become an emerging problem in machine learning security in recent years. In model stealing, one typically obtains the soft labels of model queries and a proxy dataset as prior knowledge, but this scenario is highly idealised. How to steal models without data and hard labels is a pressing problem that needs to be solved. The current mainstream of model stealing attack methods mainly focus on stealing the accuracy of the model and overlook the robustness of the model. However, robustness is essential in security applications such as facial recognition and secure payment scenarios. Moreover, building robust models usually requires costly adversarial training and fine-tuning, making these models the primary targets for theft. To address these issues, in this paper, we propose a new data-free robustness stealing method under data-free conditions from the perspective of data generation, thereby better shaping the classification boundary data to optimise the accuracy and robustness of the models. Through testing, our method achieved clean accuracy and robust accuracy of 53.69% and 21.0%, respectively, under the more complex CIFAR-100 dataset classification. These results are only 3.06% and 3.94% different from the target model, respectively, showing a significant improvement over recent research.",
      "year": 2024,
      "venue": "2024 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics",
      "authors": [
        "Xiaoji Ma",
        "Weihao Guo",
        "Pingyuan Ge",
        "Ying Chen",
        "Qiuling Yue",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "pdf_url": "",
      "publication_date": "2024-08-19",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "title": "FP-OCS: A Fingerprint Based Ownership Detection System for Insulator Fault Detection Model",
      "abstract": "In smart grids, the robustness and reliability of the transmission system depend on the operational integrity of the insulators. The success of deep learning has facilitated the development of advanced fault detection algorithms for classifying and identifying insulator states. However, these machine learning based detection systems rely on high-quality datasets, making them potential targets for intellectual property theft, and model extraction attacks pose risks of privacy breaches and unauthorized exploitation. To address the challenge of protecting neural network ownership in this situation, we introduce a fingerprint based ownership detection system for insulator fault detection model FP-OCS. FP-OCS uses model extraction attack to generate a series of piracy models, and uses white-box access victim models to generate similarity models and universal adversarial perturbation. The system\u2019s fingerprint generation module augments the original dataset to craft distinctive model fingerprints. Subsequently, FP-OCS\u2019s encoder training module extends the fingerprint dataset using K-means methods and uses contrast learning to train the encoder network and the projection network. Upon finalization of training, FP-OCS evaluates a model\u2019s authenticity by matching its derived fingerprint against the victim model. We evaluated the effectiveness of the system using data-enhanced InsPLAD datasets. Our findings prove that FP-OCS can achieve 100% accuracy in Ownership Detection tasks with 50% similarity dividing line.",
      "year": 2024,
      "venue": "International Conference on Innovative Computing and Cloud Computing",
      "authors": [
        "Wenqian Xu",
        "Fazhong Liu",
        "Ximing Zhang",
        "Yixin Jiang",
        "Tian Dong",
        "Zhihong Liang",
        "Yiwei Yang",
        "Yan Meng",
        "Haojin Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "48861a4166786313ff97a3c946c08574716aabce",
      "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
      "abstract": "With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs, employing low-frequency words as triggers and embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks, robustness against model extraction, pruning, fusion-based backdoor removal attacks, and high efficiency with 10-second embedding. Our code is available at https://github.com/edu-yinzhaoxia/Med-PLMW.",
      "year": 2024,
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "authors": [
        "Cong Kong",
        "Rui Xu",
        "Jiawei Chen",
        "Zhaoxia Yin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/48861a4166786313ff97a3c946c08574716aabce",
      "pdf_url": "",
      "publication_date": "2024-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c25d2a27f1abe169d7b68078071b6698f0980469",
      "title": "Protecting Language Generation Models via Invisible Watermarking",
      "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as\"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Xuandong Zhao",
        "Yu-Xiang Wang",
        "Lei Li"
      ],
      "citation_count": 107,
      "url": "https://www.semanticscholar.org/paper/c25d2a27f1abe169d7b68078071b6698f0980469",
      "pdf_url": "https://arxiv.org/pdf/2302.03162",
      "publication_date": "2023-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
      "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \u2018METHOD\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.Our code is available at https://github.com/yjw1029/EmbMarker.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Zhu",
        "L. Lyu",
        "Binxing Jiao",
        "Tongye Xu",
        "Guangzhong Sun",
        "Xing Xie"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "pdf_url": "http://arxiv.org/pdf/2305.10036",
      "publication_date": "2023-05-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d032a269b465df9116f080ff9c56049bc581acb4",
      "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
      "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Shuai Wang",
        "Cuiyun Gao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d032a269b465df9116f080ff9c56049bc581acb4",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "68b560859078171978f2c040b1522f4e7668c38e",
      "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
      "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \u201ccode synthesis\u201d and \u201ccode translation:\u2019 We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Pingchuan Ma",
        "Chaowei Liu",
        "Shuai Wang",
        "Daoyuan Wu",
        "Cuiyun Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/68b560859078171978f2c040b1522f4e7668c38e",
      "pdf_url": "https://arxiv.org/pdf/2303.03012",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "40b40e942db469663609ad6c1911cca235079434",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40b40e942db469663609ad6c1911cca235079434",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "title": "Stealing the Decoding Algorithms of Language Models",
      "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., 0.8, 1, 4, and 40 for the four versions of GPT-3.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "A. Naseh",
        "Kalpesh Krishna",
        "Mohit Iyyer",
        "Amir Houmansadr"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "pdf_url": "https://arxiv.org/pdf/2303.04729",
      "publication_date": "2023-03-08",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "title": "Dual Student Networks for Data-Free Model Stealing",
      "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "James Beetham",
        "Navid Kardan",
        "A. Mian",
        "M. Shah"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "pdf_url": "https://arxiv.org/pdf/2309.10058",
      "publication_date": "2023-09-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "title": "DeepTheft: Stealing DNN Model Architectures through Power Side Channel",
      "abstract": "Deep Neural Network (DNN) models are often deployed in resource-sharing clouds as Machine Learning as a Service (MLaaS) to provide inference services. To steal model architectures that are of valuable intellectual properties, a class of attacks has been proposed via different side-channel leakage, posing a serious security challenge to MLaaS.Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to accurately recover complex DNN model architectures on general processors via the RAPL (Running Average Power Limit)-based power side channel. While unprivileged access to the RAPL has been disabled in bare-metal OSes, we observe that the RAPL is still legitimately accessible in a platform as a service, e.g., the latest docker environment of version 20.10.18 used in this work. However, an attacker can acquire only a low sampling rate (1 KHz) of the time-series energy traces from the RAPL interface, rendering existing techniques ineffective in stealing large and deep DNN models. To this end, we design a novel and generic learning-based framework consisting of a set of meta-models, based on which DeepTheft is demonstrated to have high accuracy in recovering a large number (thousands) of models architectures from different model families including the deepest ResNet152. Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75% in recovering network structures, and a weighted average F1 score of 99.60% in recovering diverse layer-wise hyperparameters. Besides, our proposed learning framework is general to other time-series side-channel signals. To validate its generalization, another existing side channel is exploited, i.e., CPU frequency. Different from RAPL, CPU frequency is accessible to unprivileged users in bare-metal OSes. By using our generic learning framework trained against CPU frequency traces, DeepTheft has shown similarly high attack performance in stealing model architectures.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yansong Gao",
        "Huming Qiu",
        "Zhi Zhang",
        "Binghui Wang",
        "Hua Ma",
        "A. Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Surya Nepal"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "pdf_url": "https://arxiv.org/pdf/2309.11894",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b4149005980a11919731e6b4c1833d8b0af59424",
      "title": "Deep Neural Network Watermarking against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) watermarking is an emerging technique to protect the intellectual property of deep learning models. At present, many DNN watermarking algorithms have been proposed to achieve provenance verification by embedding identify information into the internals or prediction behaviors of the host model. However, most methods are vulnerable to model extraction attacks, where attackers collect output labels from the model to train a surrogate or a replica. To address this issue, we present a novel DNN watermarking approach, named SSW, which constructs an adaptive trigger set progressively by optimizing over a pair of symmetric shadow models to enhance the robustness to model extraction. Precisely, we train a positive shadow model supervised by the prediction of the host model to mimic the behaviors of potential surrogate models. Additionally, a negative shadow model is normally trained to imitate irrelevant independent models. Using this pair of shadow models as a reference, we design a strategy to update the trigger samples appropriately such that they tend to persist in the host model and its stolen copies. Moreover, our method could well support two specific embedding schemes: embedding the watermark via fine-tuning or from scratch. Our extensive experimental results on popular datasets demonstrate that our SSW approach outperforms state-of-the-art methods against various model extraction attacks in whether trigger set classification accuracy based or hypothesis test based verification. The results also show that our method is robust to common model modification schemes including fine-tuning and model compression.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jingxuan Tan",
        "Nan Zhong",
        "Zhenxing Qian",
        "Xinpeng Zhang",
        "Sheng Li"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/b4149005980a11919731e6b4c1833d8b0af59424",
      "pdf_url": "",
      "publication_date": "2023-10-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "title": "QUDA: Query-Limited Data-Free Model Extraction",
      "abstract": "Model extraction attack typically refers to extracting non-public information from a black-box machine learning model. Its unauthorized nature poses significant threat to intellectual property rights of the model owners. By using the well-designed queries and the predictions returned from the victim model, the adversary is able to train a clone model from scratch to obtain similar functionality as victim model. Recently, some methods have been proposed to perform model extraction attacks without using any in-distribution data (Data-free setting). Although these methods have been shown to achieve high clone accuracy, their query budgets are typically around 10 million or even exceed 20 million in some datasets, which lead to a high cost of model stealing and can be easily defended by limiting the number of queries. To illustrate the severe threats induced by model extraction attacks with limited query budget in realistic scenarios, we propose QUDA \u2013 a novel QUey-limited DAta-free model extraction attack that incorporates GAN pre-trained by public unrelated dataset to provide weak image prior and the technique of deep reinforcement learning to make query generation strategy more efficient. Compared with the state-of-the-art data-free model extraction method, QUDA achieves better results under query-limited condition (0.1M query budget) in FMNIST and CIFAR-10 datasets, and even outperforms the baseline method in most cases when QUDA uses only 10% query budget of its. QUDA issued a warning that solely relying on the limited numbers of queries or the confidentiality of training data is not reliable to protect model\u2019s security and privacy. Potential countermeasures, such as detection-based defense approach, are also provided.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Zijun Lin",
        "Ke Xu",
        "Chengfang Fang",
        "Huadi Zheng",
        "Aneez Ahmed Jaheezuddin",
        "Jie Shi"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
      "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query.\nWe create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jonathan Rosenthal",
        "Eric Enouen",
        "H. Pham",
        "Lin Tan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26150/25922",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "model extraction",
        "stealing model",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "460e48454b209e957b4942303507bf756c4a6a31",
      "title": "On the Feasibility of Specialized Ability Stealing for Large Language Code Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Zongjie Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/460e48454b209e957b4942303507bf756c4a6a31",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "title": "False Claims against Model Ownership Resolution",
      "abstract": "Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model. In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our core idea is that a malicious accuser can deviate (without detection) from the specified MOR process by finding (transferable) adversarial examples that successfully serve as evidence against independent suspect models. To this end, we first generalize the procedures of common MOR schemes and show that, under this generalization, defending against false claims is as challenging as preventing (transferable) adversarial examples. Via systematic empirical evaluation, we show that our false claim attacks always succeed in the MOR schemes that follow our generalization, including in a real-world model: Amazon's Rekognition API.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jian Liu",
        "Rui Zhang",
        "Sebastian Szyller",
        "Kui Ren",
        "Nirmal Asokan"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "pdf_url": "http://arxiv.org/pdf/2304.06607",
      "publication_date": "2023-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "title": "GrOVe: Ownership Verification of Graph Neural Networks using Embeddings",
      "abstract": "Graph neural networks (GNNs) have emerged as a state-of-the-art approach to model and draw inferences from large scale graph-structured data in various application settings such as social networking. The primary goal of a GNN is to learn an embedding for each graph node in a dataset that encodes both the node features and the local graph structure around the node.Prior work has shown that GNNs are prone to model extraction attacks. Model extraction attacks and defenses have been explored extensively in other non-graph settings. While detecting or preventing model extraction appears to be difficult, deterring them via effective ownership verification techniques offer a potential defense. In non-graph settings, fingerprinting models, or the data used to build them, have shown to be a promising approach toward ownership verification.We present GrOVe, a state-of-the-art GNN model fingerprinting scheme that, given a target model and a suspect model, can reliably determine if the suspect model was trained independently of the target model or if it is a surrogate of the target model obtained via model extraction. We show that GrOVe can distinguish between surrogate and independent models even when the independent model uses the same training dataset and architecture as the original target model.Using six benchmark datasets and three model architectures, we show that GrOVe consistently achieves low falsepositive and false-negative rates. We demonstrate that GrOVe is robust against known fingerprint evasion techniques while remaining computationally efficient.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Asim Waheed",
        "Vasisht Duddu",
        "N. Asokan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "pdf_url": "https://arxiv.org/pdf/2304.08566",
      "publication_date": "2023-04-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
      "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4\u00d7 faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jun Guo",
        "Xingyu Zheng",
        "Aishan Liu",
        "Siyuan Liang",
        "Yisong Xiao",
        "Yichao Wu",
        "Xianglong Liu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "pdf_url": "http://arxiv.org/pdf/2308.00958",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "title": "Practical and Efficient Model Extraction of Sentiment Analysis APIs",
      "abstract": "Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Weibin Wu",
        "Jianping Zhang",
        "Victor Junqiu Wei",
        "Xixian Chen",
        "Zibin Zheng",
        "Irwin King",
        "M. Lyu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "title": "Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders",
      "abstract": "Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jan Dubi'nski",
        "S. Pawlak",
        "Franziska Boenisch",
        "Tomasz Trzci'nski",
        "Adam Dziedzic"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "pdf_url": "https://arxiv.org/pdf/2310.08571",
      "publication_date": "2023-10-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b885e291f660df34d9f22777dc0678bdf8e0860d",
      "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data",
      "abstract": "We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\\sim 60-95\\%$ of true model's accuracy and uses $\\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\\sim 2-4\\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to $84-96\\%$ accuracy under membership inference attacks. Experimental results validate that Marich is query-efficient, and capable of performing task-accurate, high-fidelity, and informative model extraction.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pratik Karmakar",
        "D. Basu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b885e291f660df34d9f22777dc0678bdf8e0860d",
      "pdf_url": "http://arxiv.org/pdf/2302.08466",
      "publication_date": "2023-02-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "title": "LibSteal: Model Extraction Attack Towards Deep Learning Compilers by Reversing DNN Binary Library",
      "abstract": ": The need for Deep Learning (DL) based services has rapidly increased in the past years. As part of the trend, the privatization of Deep Neural Network (DNN) models has become increasingly popular. The authors give customers or service providers direct access to their created models and let them deploy models on devices or infrastructure out of the control of the authors. Meanwhile, the emergence of DL Compilers makes it possible to compile a DNN model into a lightweight binary for faster inference, which is attractive to many stakeholders. However, distilling the essence of a model into a binary that is free to be examined by untrusted parties creates a chance to leak essential information. With only DNN binary library, it is possible to extract neural network architecture using reverse engineering. In this paper, we present LibSteal . This framework can leak DNN architecture information by reversing the binary library generated from the DL Compiler, which is similar to or even equivalent to the original. The evaluation shows that LibSteal can efficiently steal the architecture information of victim DNN models. After training the extracted models with the same hyper-parameter, we can achieve accuracy comparable to that of the original models.",
      "year": 2023,
      "venue": "International Conference on Evaluation of Novel Approaches to Software Engineering",
      "authors": [
        "Jinquan Zhang",
        "Pei Wang",
        "Dinghao Wu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/aa4824feda684fd8d1cedb5361574cc4cff35d75",
      "pdf_url": "https://doi.org/10.5220/0011754900003464",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7b6db013d28e72374f301f758f432545a92b22fb",
      "title": "DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer",
      "abstract": "Recently, model stealing attacks are widely studied but most of them are focused on stealing a single non-discrete model, e.g., neural networks. For ensemble models, these attacks are either non-executable or suffer from intolerant performance degradation due to the complex model structure (multiple sub-models) and the discreteness possessed by the sub-model (e.g., decision trees). To overcome the bottleneck, this paper proposes a divide-and-conquer strategy called DivTheft to formulate the model stealing attack to common ensemble models by combining active learning (AL). Specifically, based on the boosting learning concept, we divide a hard ensemble model stealing task into multiple simpler ones about single sub-model stealing. Then, we adopt AL to conquer the data-free sub-model stealing task. During the process, the current AL algorithm easily causes the stolen model to be biased because of ignoring the past useful memories. Thus, DivTheft involves a newly designed uncertainty sampling scheme to filter reusable samples from the previously used ones. Experiments show that compared with the prior work, DivTheft can save almost 50% queries while ensuring a competitive agreement rate to the victim model.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhuo Ma",
        "Xinjing Liu",
        "Yang Liu",
        "Ximeng Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7b6db013d28e72374f301f758f432545a92b22fb",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
      "abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Chengwei Dai",
        "Minxuan Lv",
        "Kun Li",
        "Wei Zhou"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "title": "Data-Free Hard-Label Robustness Stealing Attack",
      "abstract": "The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaojian \\ Yuan",
        "Kejiang Chen",
        "Wen Huang",
        "Jie Zhang",
        "Weiming Zhang",
        "Neng H. Yu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "pdf_url": "",
      "publication_date": "2023-12-10",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "title": "Extracting Cloud-based Model with Prior Knowledge",
      "abstract": "Machine Learning-as-a-Service, a pay-as-you-go business pattern, is widely accepted by third-party users and developers. However, the open inference APIs may be utilized by malicious customers to conduct model extraction attacks, i.e., attackers can replicate a cloud-based black-box model merely via querying malicious examples. Existing model extraction attacks mainly depend on the posterior knowledge (i.e., predictions of query samples) from Oracle. Thus, they either require high query overhead to simulate the decision boundary, or suffer from generalization errors and overfitting problems due to query budget limitations. To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity with merely 1.8K queries (cost 2.16$) on the NSFW Recognition API. Also, the adversarial examples generated with our substitute model have better transferability than others, which reveals that our scheme is more conducive to downstream attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "S. Zhao",
        "Kangjie Chen",
        "Meng Hao",
        "Jian Zhang",
        "Guowen Xu",
        "Hongwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "pdf_url": "http://arxiv.org/pdf/2306.04192",
      "publication_date": "2023-06-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "title": "No Forking Way: Detecting Cloning Attacks on Intel SGX Applications",
      "abstract": "Forking attacks against TEEs like Intel SGX can be carried out either by rolling back the application to a previous state, or by cloning the application and by partitioning its inputs across the cloned instances. Current solutions to forking attacks require Trusted Third Parties (TTP) that are hard to find in real-world deployments. In the absence of a TTP, many TEE applications rely on monotonic counters to mitigate forking attacks based on rollbacks; however, they have no protection mechanism against forking attack based on cloning. In this paper, we analyze 72 SGX applications and show that approximately 20% of those are vulnerable to forking attacks based on cloning\u2014including those that rely on monotonic counters. To address this problem, we present CloneBuster, the first practical clone-detection mechanism for Intel SGX that does not rely on a TTP and, as such, can be used directly to protect existing applications. CloneBuster allows enclaves to (self-) detect whether another enclave with the same binary is running on the same platform. To do so, CloneBuster relies on a cache-based covert channel for enclaves to signal their presence to (and detect the presence of) clones on the same machine. We show that CloneBuster is robust despite a malicious OS, only incurs a marginal impact on the application performance, and adds approximately 800 LoC to the TCB. When used in conjunction with monotonic counters, CloneBuster allows applications to benefit from a comprehensive protection against forking attacks.",
      "year": 2023,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Samira Briongos",
        "Ghassan O. Karame",
        "Claudio Soriente",
        "Annika Wilde"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3627106.3627187",
      "publication_date": "2023-10-04",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a273c40cc1f1d7096efe40d62fe28befa524245c",
      "title": "Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks",
      "abstract": "The increasing prevalence of Deep Neural Networks (DNNs) in cloud-based services has led to their widespread use through various APIs. However, recent studies reveal the susceptibility of these public APIs to model extraction attacks, where adversaries attempt to create a local duplicate of the private model using data and API-generated predictions. Existing defense methods often involve perturbing prediction distributions to hinder an attacker's training goals, inadvertently affecting API utility. In this study, we extend the concept of digital watermarking to protect DNNs' APIs. We suggest embedding a watermark into the safeguarded APIs; thus, any model attempting to copy will inherently carry the watermark, allowing the defender to verify any suspicious models. We propose a simple yet effective framework to increase watermark transferability. By requiring the model to memorize the preset watermarks in the final decision layers, we significantly enhance the transferability of watermarks. Comprehensive experiments show that our proposed framework not only successfully watermarks APIs but also maintains their utility.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Ruixiang Tang",
        "Hongye Jin",
        "Mengnan Du",
        "Curtis Wigington",
        "R. Jain",
        "Xia Hu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a273c40cc1f1d7096efe40d62fe28befa524245c",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "title": "ShrewdAttack: Low Cost High Accuracy Model Extraction",
      "abstract": "Machine learning as a service (MLaaS) plays an essential role in the current ecosystem. Enterprises do not need to train models by themselves separately. Instead, they can use well-trained models provided by MLaaS to support business activities. However, such an ecosystem could be threatened by model extraction attacks\u2014an attacker steals the functionality of a trained model provided by MLaaS and builds a substitute model locally. In this paper, we proposed a model extraction method with low query costs and high accuracy. In particular, we use pre-trained models and task-relevant data to decrease the size of query data. We use instance selection to reduce query samples. In addition, we divided query data into two categories, namely low-confidence data and high-confidence data, to reduce the budget and improve accuracy. We then conducted attacks on two models provided by Microsoft Azure as our experiments. The results show that our scheme achieves high accuracy at low cost, with the substitution models achieving 96.10% and 95.24% substitution while querying only 7.32% and 5.30% of their training data on the two models, respectively. This new attack approach creates additional security challenges for models deployed on cloud platforms. It raises the need for novel mitigation strategies to secure the models. In future work, generative adversarial networks and model inversion attacks can be used to generate more diverse data to be applied to the attacks.",
      "year": 2023,
      "venue": "Entropy",
      "authors": [
        "Yang Liu",
        "Ji Luo",
        "Yi Yang",
        "Xuan Wang",
        "M. Gheisari",
        "Feng Luo"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "pdf_url": "https://www.mdpi.com/1099-4300/25/2/282/pdf?version=1675337976",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "title": "A Taxonomic Survey of Model Extraction Attacks",
      "abstract": "A model extraction attack aims to clone a machine learning target model deployed in the cloud solely by querying the target in a black-box manner. Once a clone is obtained it is possible to launch further attacks with the aid of the local model. In this survey, we analyze existing approaches and present a taxonomic overview of this field based on several important aspects that affect attack efficiency and performance. We present both early works and recently explored directions. We conclude with an analysis of future directions based on recent developments in machine learning methodology.",
      "year": 2023,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Didem Gen\u00e7",
        "Mustafa \u00d6zuysal",
        "E. Tomur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
      "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model\u2019s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd\u2019s wisdom. Based on the ensemble\u2019s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Akshit Jindal",
        "Vikram Goyal",
        "Saket Anand",
        "Chetan Arora"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "pdf_url": "https://arxiv.org/pdf/2311.04588",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "title": "Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis",
      "abstract": "Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e7b4e85992bc24fafe6baedb901a8d687cad2fa",
      "title": "DNN Model Theft Through Trojan Side-Channel on Edge FPGA Accelerator",
      "abstract": null,
      "year": 2023,
      "venue": "International Workshop on Applied Reconfigurable Computing",
      "authors": [
        "Srivatsan Chandrasekar",
        "Siew-Kei Lam",
        "S. Thambipillai"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/8e7b4e85992bc24fafe6baedb901a8d687cad2fa",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "252cc0c47b76f804a2e839773a90e7a389289695",
      "title": "SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors",
      "abstract": "The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent sensors for feature extraction has garnered substantial interest. The Neural Network (NN) architecture implemented of MLA are intellectual property for the vendors. Along with improved power-efficiency and reduced bandwidth, the hardware based ML models embedded in the sensor also provides additional security against cyber-attacks on the ML. In this paper, we introduce an attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims to steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU) IP by Xilinx). We use electromagnetic side channel leakage from a clone device to create a profiler and then attack the victim's device to steal the NN architecture. Stealing the ML model undermines the intellectual property rights of the vendors of a sensor. Further, it also allows an adversary to mount critical Denial of Service and misuse attack.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Sudarshan Sharma",
        "U. Kamal",
        "Jianming Tong",
        "Tushar Krishna",
        "S. Mukhopadhyay"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/252cc0c47b76f804a2e839773a90e7a389289695",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "title": "A Model Stealing Attack Against Multi-Exit Networks",
      "abstract": "Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Pan Li",
        "Peizhuo Lv",
        "Kai Chen",
        "Yuling Cai",
        "Fan Xiang",
        "Shengzhi Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "pdf_url": "",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c2a10426d91b95197f8489699026326bddb0eabc",
      "title": "MEAOD: Model Extraction Attack against Object Detectors",
      "abstract": "The widespread use of deep learning technology across various industries has made deep neural network models highly valuable and, as a result, attractive targets for potential attackers. Model extraction attacks, particularly query-based model extraction attacks, allow attackers to replicate a substitute model with comparable functionality to the victim model and present a significant threat to the confidentiality and security of MLaaS platforms. While many studies have explored threats of model extraction attacks against classification models in recent years, object detection models, which are more frequently used in real-world scenarios, have received less attention. In this paper, we investigate the challenges and feasibility of query-based model extraction attacks against object detection models and propose an effective attack method called MEAOD. It selects samples from the attacker-possessed dataset to construct an efficient query dataset using active learning and enhances the categories with insufficient objects. We additionally improve the extraction effectiveness by updating the annotations of the query dataset. According to our gray-box and black-box scenarios experiments, we achieve an extraction performance of over 70% under the given condition of a 10k query budget.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zeyu Li",
        "Chenghui Shi",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c2a10426d91b95197f8489699026326bddb0eabc",
      "pdf_url": "",
      "publication_date": "2023-12-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "title": "Rethink before Releasing your Model: ML Model Extraction Attack in EDA",
      "abstract": "Machine learning (ML)-based techniques for electronic design automation (EDA) have boosted the performance of modern integrated circuits (ICs). Such achievement makes ML model to be of importance for the EDA industry. In addition, ML models for EDA are widely considered having high development cost because of the time-consuming and complicated training data generation process. Thus, confidentiality protection for EDA models is a critical issue. However, an adversary could apply model extraction attacks to steal the model in the sense of achieving the comparable performance to the victim's model. As model extraction attacks have posed great threats to other application domains, e.g., computer vision and natural language process, in this paper, we study model extraction attacks for EDA models under two real-world scenarios. It is the first work that (1) introduces model extraction attacks on EDA models and (2) proposes two attack methods against the unlimited and limited query budget scenarios. Our results show that our approach can achieve competitive performance with the well-trained victim model without any performance degradation. Based on the results, we demonstrate that model extraction attacks truly threaten the EDA model privacy and hope to raise concerns about ML security issues in EDA.",
      "year": 2023,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Chen-Chia Chang",
        "Jingyu Pan",
        "Zhiyao Xie",
        "Jiangkun Hu",
        "Yiran Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "pdf_url": "https://doi.org/10.1145/3566097.3567896",
      "publication_date": "2023-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model theft",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "title": "Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience",
      "abstract": "Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Janvi Thakkar",
        "Giulio Zizzo",
        "S. Maffeis"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "pdf_url": "",
      "publication_date": "2023-12-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d7139b54040b34c6a64187622f8b8038775ff45",
      "title": "On the Limitations of Model Stealing with Uncertainty Quantification Models",
      "abstract": "Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted to leverage using uncertainty quantification models is not (high) enough for improvements on the model stealing task.",
      "year": 2023,
      "venue": "The European Symposium on Artificial Neural Networks",
      "authors": [
        "David Pape",
        "Sina D\u00e4ubener",
        "Thorsten Eisenhofer",
        "A. E. Cin\u00e0",
        "Lea Sch\u00f6nherr"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d7139b54040b34c6a64187622f8b8038775ff45",
      "pdf_url": "https://arxiv.org/pdf/2305.05293",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "title": "Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation",
      "abstract": "Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing.",
      "year": 2023,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Vlad Hondru",
        "R. Ionescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "title": "FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems",
      "abstract": "Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Kaisheng Fan",
        "Weizhe Zhang",
        "Guangrui Liu",
        "Hui He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00171-y",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "title": "Efficient Model Extraction by Data Set Stealing, Balancing, and Filtering",
      "abstract": "Model extraction replicates the functionality of machine learning models deployed as a service. Recently, generative adversarial networks (GANs)-based methods have achieved remarkable performance in data-free model extraction. However, previous methods generate random data in every training batch, resulting in slow convergence and redundant queries. We propose to tackle the task with a much simpler paradigm. Specifically, we steal a data set with GAN before training the clone model rather than during every training batch. Benefiting from full use of the generated data, the proposed paradigm needs less training time and query cost. To improve the class distribution of data, a balancing strategy is applied. Furthermore, the balanced data set is filtered based on adversarial robustness for better quality. Combining the above strategies, we propose an efficient model extraction by data set stealing, balancing, and filtering (DSBF). Experiments on three widely used data sets show that DSBF outperforms previous methods while converging faster and costing fewer queries.",
      "year": 2023,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Panpan Yang",
        "Qinglong Wu",
        "Xinming Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2cd2ba46e7b6195521d02606aac5e2cda751b271",
      "pdf_url": "",
      "publication_date": "2023-12-15",
      "keywords_matched": [
        "model extraction",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "title": "Exploring and Exploiting Data-Free Model Stealing",
      "abstract": null,
      "year": 2023,
      "venue": "ECML/PKDD",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1cd0dec5ecb256c132c72475856536e9d3012220",
      "title": "Research on Anti-stealing Algorithm of Distributed Photovoltaic Power System Based on Power Big Data",
      "abstract": "DPV (Distributed Photovoltaic Power System) is an important development direction in the field of renewable energy, which helps to reduce greenhouse gas emissions, improve the sustainability of energy supply and provide consumers with more energy choices. Power BD (Big Data) technology, as a BD technology oriented by the power industry under the background of the new era, is widely used in anti-theft investigation and work, which has good accuracy, convenience and scientificity, and is of great significance to anti-theft investigation. In this paper, a DPV anti-stealing algorithm is established by BD mining. Based on RF (Random forest), these features are used to identify the behavior of stealing electricity and find the features related to stealing electricity. An anti-stealing model is built by LR (Logical regression) algorithm, and the suspected stealing degree of users is calculated. The research results show that RF algorithm performs better on data sets, with the highest accuracy and recall of 0.84 and 0.89 respectively, and F1 also reaches 0.81. The results show that the anti-stealing prediction method based on electric power BD proposed in this paper is efficient and feasible in identifying suspected stealing users.",
      "year": 2023,
      "venue": "2023 International Conference on Internet of Things, Robotics and Distributed Computing (ICIRDC)",
      "authors": [
        "Junjie Zheng"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1cd0dec5ecb256c132c72475856536e9d3012220",
      "pdf_url": "",
      "publication_date": "2023-12-29",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b7a846254b166447bfbf28852bfde7495332e14d",
      "title": "Making Watermark Survive Model Extraction Attacks in Graph Neural Networks",
      "abstract": "Collecting graph data is costly and well-trained graph neural networks (GNNs) are viewed as intellectual property. To make better use of GNNs, they are used to provide cloud-based services. However, models on cloud-based services may be leaked under model extraction attacks. Adversaries can extract an imitation model by simply querying the GNNs on the cloud-based services. To protect GNNs, watermarks are embedded in the models. However, the watermarks can be removed by the model extraction attacks. To address this issue, we propose adding a watermark that cannot be ignored by queries from the model extraction attacks. Concretely, we add the soft nearest neighbor loss to the loss function of the watermark embedding process to merge the distributions for the normal tasks and watermarks. We also observe that the watermark brings a performance loss to GNNs and propose an optimization method to maintain the model performance. We evaluate our method on multiple real-world datasets to demonstrate the superiority of the method.",
      "year": 2023,
      "venue": "ICC 2023 - IEEE International Conference on Communications",
      "authors": [
        "Haiming Wang",
        "Zhikun Zhang",
        "Min Chen",
        "Shibo He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7a846254b166447bfbf28852bfde7495332e14d",
      "pdf_url": "",
      "publication_date": "2023-05-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12129493fcc85044c80a3d46f5ef73d3bb4ea6f9",
      "title": "Query-efficient model extraction for text classification model in a hard label setting",
      "abstract": null,
      "year": 2023,
      "venue": "Journal of King Saud University: Computer and Information Sciences",
      "authors": [
        "Hao Peng",
        "Shixin Guo",
        "Dandan Zhao",
        "Yiming Wu",
        "Jianxiong Han",
        "Zhe Wang",
        "S. Ji",
        "Ming-Hong Zhong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/12129493fcc85044c80a3d46f5ef73d3bb4ea6f9",
      "pdf_url": "https://doi.org/10.1016/j.jksuci.2023.02.019",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "title": "Data-Free Model Stealing Attack Based on Denoising Diffusion Probabilistic Model",
      "abstract": "Data-free model stealing (MS) attacks use synthetic samples to query a target model and train a substitute model to fit the target model\u2019s predictions, avoiding strong dependence on real datasets used by model developers. However, the existing data-free MS attack methods still have a big gap in generating high-quality query samples for high-precision MS attacks. In this paper, we construct the DDPM-optimized generator to generate data, in which a residual network-like structure is designed to fuse data to synthesize query samples. Our method further improves the quantity and quality of synthetic query samples, and effectively reduces the number of queries to the target model. The results show that the proposed method achieves superior performance compared to state-of-the-art methods.",
      "year": 2023,
      "venue": "2023 IEEE Smart World Congress (SWC)",
      "authors": [
        "Guofeng Gao",
        "Xiaodong Wang",
        "Zhiqiang Wei",
        "Jinghai Ai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "pdf_url": "",
      "publication_date": "2023-08-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "title": "Sniffer: A Novel Model Type Detection System against Machine-Learning-as-a-Service Platforms",
      "abstract": "\n Recent works explore several attacks against Machine-Learning-as-a-Service (MLaaS) platforms (e.g., the model stealing attack), allegedly posing potential real-world threats beyond viability in laboratories. However, hampered by\n model-type-sensitive\n , most of the attacks can hardly break mainstream real-world MLaaS platforms. That is, many MLaaS attacks are designed against only one certain type of model, such as tree models or neural networks. As the black-box MLaaS interface hides model type info, the attacker cannot choose a proper attack method with confidence, limiting the attack performance. In this paper, we demonstrate a system, named Sniffer, that is capable of making model-type-sensitive attacks \"great again\" in real-world applications. Specifically, Sniffer consists of four components: Generator, Querier, Probe, and Arsenal. The first two components work for preparing attack samples. Probe, as the most characteristic component in Sniffer, implements a series of self-designed algorithms to determine the type of models hidden behind the black-box MLaaS interfaces. With model type info unraveled, an optimum method can be selected from Arsenal (containing multiple attack methods) to accomplish its attack. Our demonstration shows how the audience can interact with Sniffer in a web-based interface against five mainstream MLaaS platforms.\n",
      "year": 2023,
      "venue": "Proceedings of the VLDB Endowment",
      "authors": [
        "Zhuo Ma",
        "Yilong Yang",
        "Bin Xiao",
        "Yang Liu",
        "Xinjing Liu",
        "Tong Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "pdf_url": "",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "title": "SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack",
      "abstract": "Previous studies have revealed that artificial intelligence (AI) systems are vulnerable to adversarial attacks. Among them, model extraction attacks fool the target model by generating adversarial examples on a substitute model. The core of such an attack is training a substitute model as similar to the target model as possible, where the simulation process can be categorized in a data-dependent and data-free manner. Compared with the data-dependent method, the data-free one has been proven to be more practical in the real world since it trains the substitute model with synthesized data. However, the distribution of these fake data lacks diversity and cannot detect the decision boundary of the target model well, resulting in the dissatisfactory simulation effect. Besides, these data-free techniques need a vast number of queries to train the substitute model, increasing the time and computing consumption and the risk of exposure. To solve the aforementioned problems, in this paper, we propose a novel data-free model extraction method named SCME (Self-Contrastive Model Extraction), which considers both the inter- and intra-class diversity in synthesizing fake data. In addition, SCME introduces the Mixup operation to augment the fake data, which can explore the target model's decision boundary effectively and improve the simulating capacity. Extensive experiments show that the proposed method can yield diversified fake data. Moreover, our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks, the SCME outperforms SOTA methods by 11.43\\% on average for five baseline datasets.",
      "year": 2023,
      "venue": "International Conference on Neural Information Processing",
      "authors": [
        "Renyang Liu",
        "Jinhong Zhang",
        "Kwok-Yan Lam",
        "Jun Zhao",
        "Wei Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "pdf_url": "",
      "publication_date": "2023-10-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7a89a2882cacc41d9c30d108d03db2f1bc3821ad",
      "title": "Remote Identification of Neural Network FPGA Accelerators by Power Fingerprints",
      "abstract": "Machine learning acceleration has become increasingly popular in recent years, with machine learning-as-a-service (MLaaS) scenarios offering convenient and efficient ways to access pre-trained neural network models on devices such as cloud FPGAs. However, the ease of access and use also raises concerns over model theft or misuse through model manipulation. To address these concerns, this paper proposes a method for identifying neural network models in MLaaS scenarios by their unique power consumption. Current fingerprinting methods for neural networks rely on input/output pairs or characteristic of the decision boundary, which might not always be accessible in more complex systems. Our proposed method utilizes unique power characteristics of the black-box neural network accelerator to extract a fingerprint by measuring the voltage fluctuations of the device when querying specially crafted inputs. We take advantage of the fact that the power consumption of the accelerator varies depending on the input being processed. For evaluation of our method we conduct 200 fingerprint extraction and matching experiments and the results confirm that the proposed method can distinguish between correct and incorrect models in 100% of the cases. Furthermore, we show that the fingerprint is robust to environmental and chip-to-chip variations.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Logic and Applications",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7a89a2882cacc41d9c30d108d03db2f1bc3821ad",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "349062d82b56fb516b0c7061940470f8d9c256a6",
      "title": "Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems",
      "abstract": "Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\\%$ gain in accuracy and up to $7.7\\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\\%$ and $4.8\\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\\%$). We implement the proposed defense in a real system with plans to open source.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Debopam Sanyal",
        "Jui-Tse Hung",
        "Manavi Agrawal",
        "Prahlad Jasti",
        "Shahab Nikkhoo",
        "S. Jha",
        "Tianhao Wang",
        "Sibin Mohan",
        "Alexey Tumanov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/349062d82b56fb516b0c7061940470f8d9c256a6",
      "pdf_url": "https://arxiv.org/pdf/2307.01292",
      "publication_date": "2023-07-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "title": "Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity",
      "abstract": "Recent research demonstrates that GNNs are vulnerable to the model stealing attack, a nefarious endeavor geared towards duplicating the target model via query permissions. However, they mainly focus on node classification tasks, neglecting the potential threats entailed within the domain of graph classification tasks. Furthermore, their practicality is questionable due to unreasonable assumptions, specifically concerning the large data requirements and extensive model knowledge. To this end, we advocate following strict settings with limited real data and hard-label awareness to generate synthetic data, thereby facilitating the stealing of the target model. Specifically, following important data generation principles, we introduce three model stealing attacks to adapt to different actual scenarios: MSA-AU is inspired by active learning and emphasizes the uncertainty to enhance query value of generated samples; MSA-AD introduces diversity based on Mixup augmentation strategy to alleviate the query inefficiency issue caused by over-similar samples generated by MSA-AU; MSA-AUD combines the above two strategies to seamlessly integrate the authenticity, uncertainty, and diversity of the generated samples. Finally, extensive experiments consistently demonstrate the superiority of the proposed methods in terms of concealment, query efficiency, and stealing performance.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "78c6861c798e06b5651f45594eb6c8da43708e08",
      "title": "GNMS: A novel method for model stealing based on GAN",
      "abstract": "Many well-performing models are currently deployed on the cloud to provide machine Learning as a service (MLaaS). However, these models are susceptible to Model Stealing Attacks, where attackers can access the model\u2019s functionality, parameters, and internal structure in a black-box. As a result, data-free model stealing methods have gained popularity due to their higher accuracy and not requiring real data. Previous data-free model stealing methods have mainly focused on single scenarios and limited model and dataset variations. In this paper, we introduce a novel generalized network model Stealing method (GNMS), which is suitable for both benchmark and transfer models, achieving high model stealing accuracy across various scenarios. We pre-train generative adversarial network (GAN) using publicly available datasets and efficiently steal model functionality by training a student model with the pre-trained generator and the discriminator. Adversarial samples and the generated image dataset are also used to explore the model\u2019s decision boundaries. During the training of the clone model, we train two clone models to minimize the differences with the target model further. We employ a contrastive learning approach to encourage the models to learn meaningful feature representations by distinguishing between similar and dissimilar data points, thereby enhancing the model\u2019s accuracy. We achieve a model stealing accuracy of 73.02% and 72.93% on more complex datasets CIFAR100 and Caltech101. Surpass the latest DisGUIDE by 3.55% and 2.61%.",
      "year": 2023,
      "venue": "International Conference on Advanced Cloud and Big Data",
      "authors": [
        "Moxuan Zeng",
        "Yangzhong Wang",
        "Yangming Zhang",
        "Jun Niu",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/78c6861c798e06b5651f45594eb6c8da43708e08",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "title": "An attack framework for stealing black-box based on active semi-supervised learning",
      "abstract": "Neural network models are commonly used as black-box services, but they are vulnerable to model stealing attacks, where an attacker can train a substitute model with similar performance to the original model by exploiting limited information related to the target model. This can cause significant losses to the owner of the target model and pose a serious security risk. To advance our understanding of neural networks and promote the evolution of model protection mechanisms, we conducted in-depth research on neural network model stealing attacks. In this paper, we propose a black-box stealing attack framework that combines active and semi-supervised learning, even if the target black-box only provides hard-label output, an effective attack can be achieved, generating a substitute model with the same functionality as the black-box. The framework involves selectively querying the most informative samples for black-box labeling using active learning, which significantly reduces the workload of querying the black-box and enables to achieve better performance with fewer training samples. We also apply semi-supervised learning to leverage the abundance of unlabeled data and further improve model performance. We evaluated our method on various data sets and proved that the stealing ability of our method was significantly higher than 3.86%~26.64% other methods when faced with hardlabel black-box with the same number of queries, which can achieve effective black-box function stealing.",
      "year": 2023,
      "venue": "6th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2023)",
      "authors": [
        "Lijun Gao",
        "Yuting Wang",
        "Wenjun Liu",
        "Kai Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "pdf_url": "",
      "publication_date": "2023-08-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model Extraction",
      "abstract": "Most cross-device federated learning (FL) studies focus on the model-homogeneous setting where the global server model and local client models are identical. However, such constraint not only excludes low-end clients who would otherwise make unique contributions to model training but also restrains clients from training large models due to on-device resource bottlenecks. In this work, we propose FedRolex, a partial training (PT)-based approach that enables model-heterogeneous FL and can train a global server model larger than the largest client model. At its core, FedRolex employs a rolling sub-model extraction scheme that allows different parts of the global server model to be evenly trained, which mitigates the client drift induced by the inconsistency between individual client models and server model architectures. We show that FedRolex outperforms state-of-the-art PT-based model-heterogeneous FL methods (e.g. Federated Dropout) and reduces the gap between model-heterogeneous and model-homogeneous FL, especially under the large-model large-dataset regime. In addition, we provide theoretical statistical analysis on its advantage over Federated Dropout and evaluate FedRolex on an emulated real-world device distribution to show that FedRolex can enhance the inclusiveness of FL and boost the performance of low-end devices that would otherwise not benefit from FL. Our code is available at: https://github.com/AIoT-MLSys-Lab/FedRolex",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Samiul Alam",
        "Luyang Liu",
        "Ming Yan",
        "Mi Zhang"
      ],
      "citation_count": 197,
      "url": "https://www.semanticscholar.org/paper/4fc5f31a38a19e3acc9501aeb3006810eec0e767",
      "pdf_url": "http://arxiv.org/pdf/2212.01548",
      "publication_date": "2022-12-03",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "Machine-Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex Machine Learning models available for clients via, e.g., a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property such as sensitive training data, optimised hyperparameters, or learned model parameters. In some cases, adversaries can create a copy of the model with (almost) identical behaviour using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies that address isolated threats have been proposed. To arrive at a comprehensive understanding why these attacks are successful and how they could be holistically defended against, a thorough systematisation of the field of model stealing is necessary. We address this by categorising and comparing model stealing attacks, assessing their performance, and exploring corresponding defence techniques in different settings. We propose a taxonomy for attack and defence approaches and provide guidelines on how to select the right attack or defence strategy based on the goal and available resources. Finally, we analyse which defences are rendered less effective by current attack strategies.",
      "year": 2022,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "A. Rauber"
      ],
      "citation_count": 143,
      "url": "https://www.semanticscholar.org/paper/d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3595292",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "title": "Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations",
      "abstract": "In this paper, we propose a novel and practical mechanism to enable the service provider to verify whether a suspect model is stolen from the victim model via model extraction attacks. Our key insight is that the profile of a DNN model's decision boundary can be uniquely characterized by its Universal Adversarial Perturbations (UAPs). UAPs belong to a low-dimensional subspace and piracy models' subspaces are more consistent with victim model's subspace compared with non-piracy model. Based on this, we propose a UAP fingerprinting method for DNN models and train an encoder via contrastive learning that takes fingerprints as inputs, outputs a similarity score. Extensive studies show that our framework can detect model Intellectual Property (IP) breaches with confidence > 99.99 % within only 20 fingerprints of the suspect model. It also has good generalizability across different model architectures and is robust against post-modifications on stolen models.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zirui Peng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Cheng Zhang",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "pdf_url": "https://arxiv.org/pdf/2202.08602",
      "publication_date": "2022-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks",
      "abstract": "Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, a recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, \\cater can effectively identify the IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Fangzhao Wu",
        "Jiwei Li",
        "R. Jia"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "pdf_url": "http://arxiv.org/pdf/2209.08773",
      "publication_date": "2022-09-19",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning (ML) paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the ML model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "pdf_url": "",
      "publication_date": "2022-01-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0a58e101142efdd9dd653b41d504152e940096be",
      "title": "Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks",
      "abstract": "An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "R. He"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/0a58e101142efdd9dd653b41d504152e940096be",
      "pdf_url": "https://arxiv.org/pdf/2210.15427",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "title": "Dataset Inference for Self-Supervised Models",
      "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Adam Dziedzic",
        "Haonan Duan",
        "Muhammad Ahmad Kaleem",
        "Nikita Dhawan",
        "Jonas Guan",
        "Yannis Cattan",
        "Franziska Boenisch",
        "Nicolas Papernot"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "pdf_url": "http://arxiv.org/pdf/2209.09024",
      "publication_date": "2022-09-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3886b3c22675da54b0d55a5bc16754e3399c979",
      "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations",
      "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
      "year": 2022,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Yongjie Wang",
        "Hangwei Qian",
        "C. Miao"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/f3886b3c22675da54b0d55a5bc16754e3399c979",
      "pdf_url": "https://arxiv.org/pdf/2205.06504",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
      "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
      "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Adam Dziedzic",
        "Muhammad Ahmad Kaleem",
        "Y. Lu",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/2ee4127a2a6aab51a03305d8a564693181bc6424",
      "pdf_url": "",
      "publication_date": "2022-01-23",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "af72f901d7a0f2edca55ee008c8893ae93b09971",
      "title": "How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection",
      "abstract": "Model stealing attacks present a dilemma for public machine learning APIs. To protect financial investments, companies may be forced to withhold important information about their models that could facilitate theft, including uncertainty estimates and prediction explanations. This compromise is harmful not only to users but also to external transparency. Model stealing defenses seek to resolve this dilemma by making models harder to steal while preserving utility for benign users. However, existing defenses have poor performance in practice, either requiring enormous computational overheads or severe utility trade-offs. To meet these challenges, we present a new approach to model stealing defenses called gradient redirection. At the core of our approach is a provably optimal, efficient algorithm for steering an adversary's training updates in a targeted manner. Combined with improvements to surrogate networks and a novel coordinated defense strategy, our gradient redirection defense, called GRAD${}^2$, achieves small utility trade-offs and low computational overhead, outperforming the best prior defenses. Moreover, we demonstrate how gradient redirection enables reprogramming the adversary with arbitrary behavior, which we hope will foster work on new avenues of defense.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mantas Mazeika",
        "B. Li",
        "David A. Forsyth"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/af72f901d7a0f2edca55ee008c8893ae93b09971",
      "pdf_url": "https://arxiv.org/pdf/2206.14157",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "title": "On the Difficulty of Defending Self-Supervised Learning against Model Extraction",
      "abstract": "Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Adam Dziedzic",
        "Nikita Dhawan",
        "Muhammad Ahmad Kaleem",
        "Jonas Guan",
        "Nicolas Papernot"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "pdf_url": "http://arxiv.org/pdf/2205.07890",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "51b8619bcac38d6f1880a313a4af212aac92f71c",
      "title": "MOVE: Effective and Harmless Ownership Verification via Embedded External Features",
      "abstract": "Currently, deep neural networks (DNNs) are widely adopted in different applications. Despite its commercial values, training a well-performing DNN is resource-consuming. Accordingly, the well-trained model is valuable intellectual property for its owner. However, recent studies revealed the threats of model stealing, where the adversaries can obtain a function-similar copy of the victim model, even when they can only query the model. In this paper, we propose an effective and harmless model ownership verification (MOVE) to defend against different types of model stealing simultaneously, without introducing new security risks. In general, we conduct the ownership verification by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by modifying a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. In particular, we develop our MOVE method under both glass-boxand closed-box settings and analyze its theoretical foundation to provide comprehensive model protection. Extensive experiments on benchmark datasets verify the effectiveness of our method and its resistance to potential adaptive attacks.",
      "year": 2022,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Yiming Li",
        "Linghui Zhu",
        "Xiaojun Jia",
        "Yang Bai",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/51b8619bcac38d6f1880a313a4af212aac92f71c",
      "pdf_url": "",
      "publication_date": "2022-08-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "title": "Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets",
      "abstract": "Model stealing techniques allow adversaries to create attack models that mimic the functionality of black-box machine learning models, querying only class membership or probability outcomes. Recently, interpretable AI is getting increasing attention, to enhance our understanding of AI models, provide additional information for diagnoses, or satisfy legal requirements. However, it has been recently reported that providing such additional information can make AI models more vulnerable to model stealing attacks. In this paper, we propose DeepDefense, the first defense mechanism that protects an AI model against model stealing attackers exploiting both class probabilities and interpretations. DeepDefense uses a misdirection model to hide the critical information of the original model against model stealing attacks, with minimal degradation on both the class probability and the interpretability of prediction output. DeepDefense is highly applicable for any model stealing scenario since it makes minimal assumptions about the model stealing adversary. In our experiments, DeepDefense shows significantly higher defense performance than the existing state-of-the-art defenses on various datasets and interpreters.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "pdf_url": "https://www.ijcai.org/proceedings/2022/0100.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8119ab9eb8974693705bde0fa074439da40fda96",
      "title": "HDLock: exploiting privileged encoding to protect hyperdimensional computing models against IP stealing",
      "abstract": "Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC --- an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.",
      "year": 2022,
      "venue": "Design Automation Conference",
      "authors": [
        "Shijin Duan",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/8119ab9eb8974693705bde0fa074439da40fda96",
      "pdf_url": "",
      "publication_date": "2022-03-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db83dc37159c92e689bc246000e51b54c666eb04",
      "title": "A novel defense mechanism to protect users from profile cloning attack on Online Social Networks (OSNs)",
      "abstract": null,
      "year": 2022,
      "venue": "Peer-to-Peer Networking and Applications",
      "authors": [
        "Gordhan Jethava",
        "U. P. Rao"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/db83dc37159c92e689bc246000e51b54c666eb04",
      "pdf_url": "",
      "publication_date": "2022-07-05",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "title": "DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking",
      "abstract": "The functionality of a deep learning (DL) model can be stolen via model extraction where an attacker obtains a surrogate model by utilizing the responses from a prediction API of the original model. In this work, we propose a novel watermarking technique called DynaMarks to protect the intellectual property (IP) of DL models against such model extraction attacks in a black-box setting. Unlike existing approaches, DynaMarks does not alter the training process of the original model but rather embeds watermark into a surrogate model by dynamically changing the output responses from the original model prediction API based on certain secret parameters at inference runtime. The experimental outcomes on Fashion MNIST, CIFAR-10, and ImageNet datasets demonstrate the efficacy of DynaMarks scheme to watermark surrogate models while preserving the accuracies of the original models deployed in edge devices. In addition, we also perform experiments to evaluate the robustness of DynaMarks against various watermark removal strategies, thus allowing a DL model owner to reliably prove model ownership.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Chakraborty",
        "Daniel Xing",
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "pdf_url": "http://arxiv.org/pdf/2207.13321",
      "publication_date": "2022-07-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "89571d0a03b694c9f5470892a7fc5d980d793518",
      "title": "SeInspect: Defending Model Stealing via Heterogeneous Semantic Inspection",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Xinjing Liu",
        "Zhuo Ma",
        "Yang Liu",
        "Zhan Qin",
        "Junwei Zhang",
        "Zhuzhu Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/89571d0a03b694c9f5470892a7fc5d980d793518",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "title": "Security and Privacy Challenges for Intelligent Internet of Things Devices 2022 TADW: Traceable and Antidetection Dynamic Watermarking of Deep Neural Networks",
      "abstract": "Deep neural networks (DNN) with incomparably advanced performance have been extensively applied in diverse fields (e.g., image recognition, natural language processing, and speech recognition). Training a high-performance DNN model requires a lot of training data and intellectual and computing resources, which bring a high cost to the model owners. Therefore, illegal model abuse (model theft, derivation, resale or redistribution, etc.) seriously infringes model owners\u2019 legitimate rights and interests. Watermarking is considered the main topic of DNN ownership protection. However, almost all existing watermarking works apply solely to image data. They do not trace the unique infringing model, and the adversary easily detects these ownership verification samples (trigger set) simultaneously. This paper introduces TADW, a dynamic watermarking scheme with tracking and antidetection abilities in the deep learning (DL) textual domain. Specifically, we propose a new approach to construct trigger set samples for antidetection and innovatively design a mapping algorithm that assigns a unique serial number (SN) to every watermarked model. Furthermore, we implement and detailedly evaluate TADW on 2 benchmark datasets and 3 popular DNNs. Experiment results show that TADW can successfully verify the ownership of the target model at a less than 0.5% accuracy cost and identify unique infringing models. In addition, TADW is excellently robust against different model modifications and can serve numerous users.",
      "year": 2022,
      "venue": "Security and Communication Networks",
      "authors": [
        "Jinwei Dong",
        "He Wang",
        "Zhipeng He",
        "Jun Niu",
        "Xiaoyan Zhu",
        "Gaofei Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9efd7f888e7ca30c04eaf3fb53ab258782989fe3",
      "pdf_url": "https://downloads.hindawi.com/journals/scn/2022/9505808.pdf",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "title": "PUFs Physical Learning: Accelerating the Enrollment via Delay-Based Model Extraction",
      "abstract": "The introduction of Physical Unclonable Functions (PUFs) has been originally motivated by their ability to resist physical attacks, particularly in anti-counterfeiting scenarios. In these one-way functions, machine learning, cryptanalysis, and side-channel attacks are common attack vectors threatening the promised PUF's property of unclonability. These attacks often emulate a PUF by employing a large number of Challenge-Response Pairs (CRPs). Some solutions to defeat such attacks are based on a protocol, where a model of the underlying PUF primitives should be extracted during the enrollment phase. In this article, we introduce a novel physical cloning approach applicable to FPGA-based implementations, which allows extracting the PUF's unique physical characteristics with a few number of Challenge-Response Pairs (CRPs), that increases only linearly for a higher number of PUF components. Indeed, our proposed approach significantly accelerates the enrollment phase and makes complex enrollment protocols feasible. Our core idea relies on an on-chip delay sensor, which can be realized by ordinary FPGA components, measuring the unique characteristic of the PUF elements. We demonstrate the feasibility of our introduced technique by practical experiments on different FPGA platforms, cloning a couple of (complex) PUF constructions, i.e., XOR APUF, iPUF, composed of delay-based Arbiter PUFs.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Anita Aghaie",
        "Maik Ender",
        "A. Moradi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "title": "Enhance Model Stealing Attack via Label Refining",
      "abstract": "With machine learning models being increasingly deployed, model stealing attacks have raised an increasing interest. Extracting decision-based models is a more challenging task with the information of class similarity missing. In this paper, we propose a novel and effective model stealing method as Label Refining via Feature Distance (LRFD), to re-dig the class similarity. Specifically, since the information of class similarity can be represented by the distance between samples from different classes in the feature space, we design a soft label construction module inspired by the prototype learning, and transfer the knowledge in the soft label to the substitution model. Extensive experiments conducted on four widely-used datasets consistently demonstrate that our method yields a model with significantly greater functional similarity to the victim model.",
      "year": 2022,
      "venue": "International Conference on the Software Process",
      "authors": [
        "Yixu Wang",
        "Xianming Lin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "pdf_url": "",
      "publication_date": "2022-04-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
      "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "L. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "title": "Model Extraction Attack against Self-supervised Speech Models",
      "abstract": "Self-supervised learning (SSL) speech models generate meaningful representations of given clips and achieve incredible performance across various downstream tasks. Model extraction attack (MEA) often refers to an adversary stealing the functionality of the victim model with only query access. In this work, we study the MEA problem against SSL speech model with a small number of queries. We propose a two-stage framework to extract the model. In the first stage, SSL is conducted on the large-scale unlabeled corpus to pre-train a small speech model. Secondly, we actively sample a small portion of clips from the unlabeled corpus and query the target model with these clips to acquire their representations as labels for the small model's second-stage training. Experiment results show that our sampling methods can effectively extract the target model without knowing any information about its model architecture.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tsung-Yuan Hsu",
        "Chen-An Li",
        "Tung-Yu Wu",
        "Hung-yi Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "pdf_url": "https://arxiv.org/pdf/2211.16044",
      "publication_date": "2022-11-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6a82176e62930e8e5bf8ba320e605404be7bf204",
      "title": "Seeds Don't Lie: An Adaptive Watermarking Framework for Computer Vision Models",
      "abstract": "In recent years, various watermarking methods were suggested to detect computer vision models obtained illegitimately from their owners, however they fail to demonstrate satisfactory robustness against model extraction attacks. In this paper, we present an adaptive framework to watermark a protected model, leveraging the unique behavior present in the model due to a unique random seed initialized during the model training. This watermark is used to detect extracted models, which have the same unique behavior, indicating an unauthorized usage of the protected model's intellectual property (IP). First, we show how an initial seed for random number generation as part of model training produces distinct characteristics in the model's decision boundaries, which are inherited by extracted models and present in their decision boundaries, but aren't present in non-extracted models trained on the same data-set with a different seed. Based on our findings, we suggest the Robust Adaptive Watermarking (RAW) Framework, which utilizes the unique behavior present in the protected and extracted models to generate a watermark key-set and verification model. We show that the framework is robust to (1) unseen model extraction attacks, and (2) extracted models which undergo a blurring method (e.g., weight pruning). We evaluate the framework's robustness against a naive attacker (unaware that the model is watermarked), and an informed attacker (who employs blurring strategies to remove watermarked behavior from an extracted model), and achieve outstanding (i.e.,>0.9) AUC values. Finally, we show that the framework is robust to model extraction attacks with different structure and/or architecture than the protected model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Jacob Shams",
        "Ben Nassi",
        "I. Morikawa",
        "Toshiya Shimizu",
        "A. Shabtai",
        "Y. Elovici"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a82176e62930e8e5bf8ba320e605404be7bf204",
      "pdf_url": "https://arxiv.org/pdf/2211.13644",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2841458166d23bd76abd8f2a03b8d57c99522d44",
      "title": "Model Stealing Attack based on Sampling and Weighting",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "\u8bba\u6587 \u57fa\u4e8e\u91c7\u6837\u548c\u52a0\u6743\u635f\u5931\u51fd\u6570\u7684\u6a21\u578b\u7a83\u53d6\u653b\u51fb\u65b9\u6cd5 \u738b\u71a0\u65ed"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2841458166d23bd76abd8f2a03b8d57c99522d44",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f7b70da96899fdac65153e172e426c5b39d2bde5",
      "title": "Detecting Data-Free Model Stealing",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Ashley Borum",
        "James Beetham",
        "Dr. Niels Da",
        "Vitoria Lobo",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f7b70da96899fdac65153e172e426c5b39d2bde5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "title": "Verify Deep Learning Models Ownership via Preset Embedding",
      "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Wenxuan Yin",
        "Hai-feng Qian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c3531dc827334763c8d41ccd6287a2820a840352",
      "title": "Model functionality stealing attacks based on real data awareness",
      "abstract": null,
      "year": 2022,
      "venue": "Journal of Image and Graphics",
      "authors": [
        "Yanming Li",
        "Changsheng Li",
        "Jiaqi Yu",
        "Ye Yuan",
        "Guoren Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c3531dc827334763c8d41ccd6287a2820a840352",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "title": "The Limits of Provable Security Against Model Extraction",
      "abstract": null,
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "title": "Dataset Inference: Ownership Resolution in Machine Learning",
      "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $dataset$ $inference$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Pratyush Maini"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "pdf_url": "",
      "publication_date": "2021-04-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2569a7309142e40815cf556b6417059df9abbda8",
      "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
      "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "L. Lyu",
        "Fangzhao Wu",
        "Chenguang Wang"
      ],
      "citation_count": 115,
      "url": "https://www.semanticscholar.org/paper/2569a7309142e40815cf556b6417059df9abbda8",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/21321/21070",
      "publication_date": "2021-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7026ec6c12f325aec884ff802812c3c80319f668",
      "title": "Model Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary\u2019s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yun Shen",
        "Xinlei He",
        "Yufei Han",
        "Yang Zhang"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/7026ec6c12f325aec884ff802812c3c80319f668",
      "pdf_url": "https://arxiv.org/pdf/2112.08331",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c3111e374ad14357172ef63e7063e0182f8030d4",
      "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
      "abstract": "Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jialuo Chen",
        "Jingyi Wang",
        "Tinglan Peng",
        "Youcheng Sun",
        "Peng Cheng",
        "S. Ji",
        "Xingjun Ma",
        "Bo Li",
        "D. Song"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/c3111e374ad14357172ef63e7063e0182f8030d4",
      "pdf_url": "https://arxiv.org/pdf/2112.05588",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
      "title": "Defending against Model Stealing via Verifying Embedded External Features",
      "abstract": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Xiaojun Jia",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/bfb6e56602b658fdabaaa66987ebf685a8139892",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20036/19795",
      "publication_date": "2021-12-07",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9fd00a4044aa2eab1cc5f8bfafee59604fb01989",
      "title": "ModelDiff: testing-based DNN similarity comparison for model reuse detection",
      "abstract": "The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.",
      "year": 2021,
      "venue": "International Symposium on Software Testing and Analysis",
      "authors": [
        "Yuanchun Li",
        "Ziqi Zhang",
        "Bingyan Liu",
        "Ziyue Yang",
        "Yunxin Liu"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/9fd00a4044aa2eab1cc5f8bfafee59604fb01989",
      "pdf_url": "https://arxiv.org/pdf/2106.08890",
      "publication_date": "2021-06-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c3d0749f519962331d323dd3c4ec1137544d6d03",
      "title": "Fingerprinting Deep Neural Networks - a DeepFool Approach",
      "abstract": "A well-trained deep learning classifier is an expensive intellectual property of the model owner. However, recently proposed model extraction attacks and reverse engineering techniques make model theft possible and similar quality deep learning solution reproducible at a low cost. To protect the interest and revenue of the model owner, watermarking on Deep Neural Network (DNN) has been proposed. However, the extra components and computations due to the embedded watermark tend to interfere with the model training process and result in inevitable degradation in classification accuracy. In this paper, we utilize the geometry characteristics inherited in the DeepFool algorithm to extract data points near the classification boundary of the target model for ownership verification. As the fingerprint is extracted after the training process has been completed, the original achievable classification accuracy will not be compromised. This countermeasure is founded on the hypothesis that different models possess different classification boundaries determined solely by the hyperparameters of the DNN and the training it has undergone. Therefore, given a set of fingerprint data points, a pirated model or its post-processed version will produce similar prediction but another originally designed and trained DNN for the same task will produce very different prediction even if they have similar or better classification accuracy. The effectiveness of the proposed Intellectual Property (IP) protection method is validated on the CIFAR-10, CIFAR-100 and ImageNet datasets. The results show a detection rate of 100% and a false positive rate of 0% for each dataset. More importantly, the fingerprint extraction and its run time are both dataset independent. It is on average ~130\u00d7 faster than two state-of-the-art fingerprinting methods.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Si Wang",
        "Chip-Hong Chang"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/c3d0749f519962331d323dd3c4ec1137544d6d03",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/147023/2/2021021379.pdf",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack",
      "abstract": "Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \\emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixu Wang",
        "Jie Li",
        "Hong Liu",
        "Yongjian Wu",
        "Rongrong Ji"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "pdf_url": "http://arxiv.org/pdf/2105.00623",
      "publication_date": "2021-05-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "title": "SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries",
      "abstract": "Given black-box access to the prediction API, model extraction attacks can steal the functionality of models deployed in the cloud. In this paper, we introduce the SEAT detector, which detects black-box model extraction attacks so that the defender can terminate malicious accounts. SEAT has a similarity encoder trained by adversarial training. Using the similarity encoder, SEAT detects accounts that make queries that indicate a model extraction attack in progress and cancels these accounts. We evaluate our defense against existing model extraction attacks and against new adaptive attacks introduced in this paper. Our results show that even against adaptive attackers, SEAT increases the cost of model extraction attacks by 3.8 times to 16 times.",
      "year": 2021,
      "venue": "AISec@CCS",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3474369.3486863",
      "publication_date": "2021-11-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "36654420b9a635bb860c2fc571a3fcc0a5398013",
      "title": "Nowhere to Hide: Efficiently Identifying Probabilistic Cloning Attacks in Large-Scale RFID Systems",
      "abstract": "Radio-Frequency Identification (RFID) is an emerging technology which has been widely applied in various scenarios, such as tracking, object monitoring, and social networks, etc. Cloning attacks can severely disturb the RFID systems, such as missed detection for the missing tags. Although there are some techniques with physical architecture design or complicated encryption and cryptography proposed to prevent the tags from being cloned, it is difficult to definitely avoid the cloning attack. Therefore, cloning attack detection and identification are critical for the RFID systems. Prior works rely on that each clone tag will reply to the reader when its corresponding genuine tag is queried. In this article, we consider a more general attack model, in which each clone tag replies to the reader\u2019s query with a predefined probability, i.e., attack probability. We concentrate on identifying the tags being attacked with the probability no less than a threshold $P_{t}$ with the required identification reliability $\\alpha $ . We first propose a basic protocol to Identify the Probabilistic Cloning Attacks with required identification reliability for the large-scale RFID systems called IPCA. Then we propose two enhanced protocols called MS-IPCA and S-IPCA respectively to improve the identification efficiency. We theoretically analyze the parameters of the proposed IPCA, MS-IPCA and S-IPCA protocols to maximize the identification efficiency. Finally we conduct extensive simulations to validate the effectiveness of the proposed protocols.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xin Ai",
        "Honglong Chen",
        "Kai Lin",
        "Zhibo Wang",
        "Jiguo Yu"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/36654420b9a635bb860c2fc571a3fcc0a5398013",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "title": "Preventing DNN Model IP Theft via Hardware Obfuscation",
      "abstract": "Training accurate deep learning (DL) models require large amounts of training data, significant work in labeling the data, considerable computing resources, and substantial domain expertise. In short, they are expensive to develop. Hence, protecting these models, which are valuable storehouses of intellectual properties (IP), against model stealing/cloning attacks is of paramount importance. Today\u2019s mobile processors feature Neural Processing Units (NPUs) to accelerate the execution of DL models. DL models executing on NPUs are vulnerable to hyperparameter extraction via side-channel attacks and model parameter theft via bus monitoring attacks. This paper presents a novel solution to defend against DL IP theft in NPUs during model distribution and deployment/execution via lightweight, keyed model obfuscation scheme. Unauthorized use of such models results in inaccurate classification. In addition, we present an ideal end-to-end deep learning trusted system composed of: 1) model distribution via hardware root-of-trust and public-key cryptography infrastructure (PKI) and 2) model execution via low-latency memory encryption. We demonstrate that our proposed obfuscation solution achieves IP protection objectives without requiring specialized training or sacrificing the model\u2019s accuracy. In addition, the proposed obfuscation mechanism preserves the output class distribution while degrading the model\u2019s accuracy for unauthorized parties, covering any evidence of a hacked model.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Brunno F. Goldstein",
        "Vinay C. Patil",
        "V. C. Ferreira",
        "A. S. Nery",
        "F. Fran\u00e7a",
        "S. Kundu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "pdf_url": "",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "645e890034126dfc08484d3509b3493e85fbd603",
      "title": "Stateful Detection of Model Extraction Attacks",
      "abstract": "Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Kanade",
        "S. Shevade"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/645e890034126dfc08484d3509b3493e85fbd603",
      "pdf_url": "",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "title": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
      "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
      "year": 2021,
      "venue": "International Conference on Computational Linguistics",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "pdf_url": "",
      "publication_date": "2021-08-29",
      "keywords_matched": [
        "model extraction",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "title": "Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack",
      "abstract": "Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinyi Zhang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "pdf_url": "",
      "publication_date": "2021-04-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b4d93484790a68c575c4acbeac0cd99a58f46ebe",
      "title": "Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from Black-box Models?",
      "abstract": null,
      "year": 2021,
      "venue": "Pattern Recognition",
      "authors": [
        "Jacson Rodrigues Correia-Silva",
        "Rodrigo Berriel",
        "C. Badue",
        "A. D. Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/b4d93484790a68c575c4acbeac0cd99a58f46ebe",
      "pdf_url": "http://arxiv.org/pdf/2101.08717",
      "publication_date": "2021-01-21",
      "keywords_matched": [
        "copycat CNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "title": "Stealing Neural Network Models through the Scan Chain: A New Threat for ML Hardware",
      "abstract": "Stealing trained machine learning (ML) models is a new and growing concern due to the model's development cost. Existing work on ML model extraction either applies a mathematical attack or exploits hardware vulnerabilities such as side-channel leakage. This paper shows a new style of attack, for the first time, on ML models running on embedded devices by abusing the scan-chain infrastructure. We illustrate that having course-grained scan-chain access to non-linear layer outputs is sufficient to steal ML models. To that end, we propose a novel small-signal analysis inspired attack that applies small perturbations into the input signals, identifies the quiescent operating points and, selectively activates certain neurons. We then couple this with a Linear Constraint Satisfaction based approach to efficiently extract model parameters such as weights and biases. We conduct our attack on neural network inference topologies defined in earlier works, and we automate our attack. The results show that our attack outperforms mathematical model extraction proposed in CRYPTO 2020, USENIX 2020, and ICML 2020 by an increase in accuracy of $2^{20.7}\\times, 2^{50.7}\\times$, and $2^{33.9}\\times$, respectively, and a reduction in queries by $2^{6.5}\\times, 2^{4.6}\\times$, and $2^{14.2}\\times$, respectively.",
      "year": 2021,
      "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "S. Potluri",
        "Aydin Aysu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "title": "Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "imitation attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3813c369accb4b87b5882eec943511fe2e8b4767",
      "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Lingjuan Lyu",
        "Xuanli He",
        "Fangzhao Wu",
        "Lichao Sun"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/3813c369accb4b87b5882eec943511fe2e8b4767",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "25800c4599b2e834899a180d77e093fb9282d97e",
      "title": "HODA: Hardness-Oriented Detection of Model Extraction Attacks",
      "abstract": "Model extraction attacks exploit the target model\u2019s prediction API to create a surrogate model, allowing the adversary to steal or reconnoiter the functionality of the target model in the black-box setting. Several recent studies have shown that a data-limited adversaries with no or limited access to the samples from the target model\u2019s training data distribution, can employ synthesized or semantically similar samples to conduct model extraction attacks. In this paper, we introduce the concept of hardness degree to characterize sample difficulty based on the concept of learning speed. The hardness degree of a sample depends on the epoch number at which the predicted label for that sample converges. We investigate the hardness degree of samples and demonstrate that the hardness degree histogram of a data-limited adversary\u2019s sample sequence is differs significantly from that of benign users\u2019 sample sequences. We propose Hardness-Oriented Detection Approach (HODA) to detect the sample sequences of model extraction attacks. Our results indicate that HODA can effectively detect model extraction attack sequences with a high success rate, using only 100 monitored samples. It outperforms all previously proposed methods for model extraction detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "A. M. Sadeghzadeh",
        "Amir Mohammad Sobhanian",
        "F. Dehghan",
        "R. Jalili"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/25800c4599b2e834899a180d77e093fb9282d97e",
      "pdf_url": "https://arxiv.org/pdf/2106.11424",
      "publication_date": "2021-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44cc3c0d5a80fecd1b6285c73f79986852135162",
      "title": "ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access",
      "abstract": "Machine Learning (ML) models are progressively deployed in many real-world applications to perform a wide range of tasks, but are exposed to the security and privacy threats which aim to infer the details and even steal the functionality of the ML models. Despite extensive attacking efforts which rely on white-box or gray-box access, how to perform attacks with black-box access continues to be elusive. Aspiring to fill this gap, we move one step further and present ML-Stealer that can steal the functionality of any type of ML models with mere black-box access. With two algorithm designs, namely, synthetic data generation and replica model construction, ML-Stealer can construct a deep neural network (DNN)-based replica model which has the similar prediction functionality to the victim ML model. ML-Stealer does not require any knowledge about the victim model, nor does it enforce the access to statistical information or samples of the victim's training data. Experiment results demonstrate that ML-Stealer can achieve the consistent prediction results with the victim model of an averaged testing accuracy of 85.6%, and up to 93.6% at best.",
      "year": 2021,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Gaoyang Liu",
        "Shijie Wang",
        "Borui Wan",
        "Zekun Wang",
        "Chen Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/44cc3c0d5a80fecd1b6285c73f79986852135162",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "title": "Timing shift-based bi-residual network model for the detection of electricity stealing",
      "abstract": "With the increasing number of electricity stealing users, the interests of countries are jeopardized and it brings economic burden to the government. However, due to the small-scale stealing and its random time coherence, it is difficult to find electricity stealing users. To solve this issue, we first generate the hybrid dataset composed of real electricity data and specific electricity stealing data. Then, we put forward the timing shift-based bi-residual network (TS-BiResNet) model. It learns the features of electricity consumption data on two aspects, i.e., shallow features and deep features, and meanwhile takes time factor into consideration. The simulation results show that TS-BiResNet model can detect electricity stealing behaviors that are small scaled and randomly coherent with time. Besides, its detection accuracy is superior to the benchmark schemes, i.e., long short-term memory (LSTM), gated recurrent unit (GRU), combined convolutional neural network and LSTM (CNN-LSTM) and Bi-ResNet.",
      "year": 2021,
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "authors": [
        "Jie Lu",
        "Jingfu Li",
        "Wenjiang Feng",
        "Yongqi Zou",
        "Juntao Zhang",
        "Yuan Li"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "pdf_url": "https://asp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13634-022-00865-4",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "title": "JAXED: Reverse Engineering DNN Architectures Leveraging JIT GEMM Libraries",
      "abstract": "General matrix multiplication (GEMM) libraries on x86 architectures have recently adopted Just-in-time (JIT) based optimizations to dramatically reduce the execution time of small and medium-sized matrix multiplication. The exploitation of the latest CPU architectural extensions, such as the AVX2 and AVX-512 extensions, are the target for these optimizations. Although JIT compilers can provide impressive speedups to GEMM libraries, they expose a new attack surface through the built-in JIT code caches. These software-based caches allow an adversary to extract sensitive information through carefully designed timing attacks. The attack surface of such libraries has become more prominent due to their widespread integration into popular Machine Learning (ML) frameworks such as PyTorch and Tensorflow.In our paper, we present a novel attack strategy for JIT-compiled GEMM libraries called JAXED. We demonstrate how an adversary can exploit the GEMM library\u2019s vulnerable state management to extract confidential CNN model hyperparameters. We show that using JAXED, one can successfully extract the hyperparameters of models with fully-connected layers with an average accuracy of 92%. Further, we demonstrate our attack against the final fully connected layer of 10 popular DNN models. Finally, we perform an end-to-end attack on MobileNetV2, on both the convolution and FC layers, successfully extracting model hyperparameters.",
      "year": 2021,
      "venue": "Seed",
      "authors": [
        "Malith Jayaweera",
        "Kaustubh Shivdikar",
        "Yanzhi Wang",
        "D. Kaeli"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "reverse engineering DNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57c33299e83db01a15807ed5e0f1f1abb2c92578",
      "title": "Towards Characterizing Model Extraction Queries and How to Detect Them",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/57c33299e83db01a15807ed5e0f1f1abb2c92578",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "title": "Digital Watermarking System for Hard Cover Objects Against Cloning Attacks",
      "abstract": "We consider an application of digital watermark system technique for hard carriers (paper or plastic cover objects in addition). Algorithms of watermark embedding and extraction are proposed and the corresponding error probabilities of the extracted bits both for informed and blind decoders are presented. The spread spectrum signals used for embedding of watermark are optimized on their parameters. Similar experimental results are presented for data matrices carriers depending on paper sizes of data matrix copies. A protection against so-called \u201ccloning\u201d attack is elaborated where certificates are copied by attack scanner or photo camera and next printed and fixed as forges. The formulas for a missing the cloning attack and false alarm probabilities are proved. A full-scale experiment with a real scanner and printer confirms that the reliability of cloning attack detection can be provided under appropriate selection of watermark system parameters.",
      "year": 2021,
      "venue": "2021 30th Conference of Open Innovations Association FRUCT",
      "authors": [
        "V. Korzhik",
        "V. Starostin",
        "V. Yakovlev",
        "D. Flaksman",
        "Ivan Bukshin",
        "B. Izotov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "pdf_url": "",
      "publication_date": "2021-10-27",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "244d8f4fc47b94efb976016acf51f3ea5852034f",
      "title": "Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract)",
      "abstract": "Graph data has been widely used to represent data from various domain, e.g., social networks, recommendation system. With great power, the GNN models, usually as valuable properties of their owners, also become attractive targets of the adversary who covets to steal them. While existing works show that simple deep neural networks can be reproduced by so-called Model Extraction Attacks, how to extract a GNN model has not been explored. In this paper, we exploit the threat of model extraction attacks against GNN models. Unlike ordinary attacks which obtain model information via only the input-output query pairs, we utilize both the node queries and the graph structure to extract the GNNs. Furthermore, we consider the stealthiness of the attack and propose to generate legitimate queries so the extraction can be applied discreetly. We implement our attack by leveraging the responses of these queries, as well as other accessible knowledge, e.g., neighbor connectives of the queried nodes. By evaluating over three real-world datasets, our attack is shown to effectively produce a surrogate model with more than 80% equivalent predictions as the target model.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bang Wu",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/244d8f4fc47b94efb976016acf51f3ea5852034f",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17959/17764",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "959f9fa181f57caaa61b7ec422c141cbfa09dedf",
      "title": "Towards Stealing Deep Neural Networks on Mobile Devices",
      "abstract": null,
      "year": 2021,
      "venue": "Security and Privacy in Communication Networks",
      "authors": [
        "S. Danda",
        "Xiaoyong Yuan",
        "Bo Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/959f9fa181f57caaa61b7ec422c141cbfa09dedf",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "citation_count": 206,
      "url": "https://www.semanticscholar.org/paper/e4b1d7553020258d7e537e2cfa53865359389eac",
      "pdf_url": "",
      "publication_date": "2020-05-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d5ffa58133940646d1339c2610cb35f27442e0d3",
      "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
      "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "A. Prakash",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 171,
      "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
      "pdf_url": "https://arxiv.org/pdf/2005.03161",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "title": "DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints",
      "abstract": "As deep neural networks (DNNs) continue their reach into a wide range of application domains, the neural network architecture of DNN models becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. Previous studies explore to leverage architecture-level events disposed in hardware platforms to extract the model architecture information. They pose the following limitations: requiring a priori knowledge of victim models, lacking in robustness and generality, or obtaining incomplete information of the victim model architecture. Our paper proposes DeepSniffer, a learning-based model extraction framework to obtain the complete model architecture information without any prior knowledge of the victim model. It is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. The basic idea of DeepSniffer is to learn the relation between extracted architectural hints (e.g., volumes of memory reads/writes obtained by side-channel or bus snooping attacks) and model internal architectures. Taking GPU platforms as a show case, DeepSniffer conducts model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models. The extracted models are directly helpful to the attempting of crafting adversarial inputs. Our experimental results show that DeepSniffer achieves a high accuracy of model extraction and thus improves the adversarial attack success rate from 14.6%$\\sim$25.5% (without network architecture knowledge) to 75.9% (with extracted network architecture). The DeepSniffer project has been released in Github.",
      "year": 2020,
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Shuangchen Li",
        "Lei Deng",
        "Pengfei Zuo",
        "Yu Ji",
        "Xinfeng Xie",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3373376.3378460",
      "publication_date": "2020-03-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d76407a539addcb39bddef065c4061381591e78c",
      "title": "WAFFLE: Watermarking in Federated Learning",
      "abstract": "Federated learning is a distributed learning technique where machine learning models are trained on client devices in which the local training data resides. The training is coordinated via a central server which is, typically, controlled by the intended owner of the resulting model. By avoiding the need to transport the training data to the central server, federated learning improves privacy and efficiency. But it raises the risk of model theft by clients because the resulting model is available on every client device. Even if the application software used for local training may attempt to prevent direct access to the model, a malicious client may bypass any such restrictions by reverse engineering the application software. Watermarking is a well-known deterrence method against model theft by providing the means for model owners to demonstrate ownership of their models. Several recent deep neural network (DNN) watermarking techniques use backdooring: training the models with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several client devices. In this paper, we present WAFFLE, the first approach to watermark DNN models trained using federated learning. It introduces a retraining step at the server after each aggregation of local models into the global model. We show that WAFFLE efficiently embeds a resilient watermark into models incurring only negligible degradation in test accuracy (-0.17%), and does not require access to training data. We also introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+3.2%) overhead11The research report version of this paper is also available in https://arxiv.org/abs/2008.07298, and the code for reproducing our work can be found at https://github.com/ssg-research/WAFFLE.",
      "year": 2020,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "B. Atli",
        "Yuxi Xia",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 86,
      "url": "https://www.semanticscholar.org/paper/d76407a539addcb39bddef065c4061381591e78c",
      "pdf_url": "http://arxiv.org/pdf/2008.07298",
      "publication_date": "2020-08-17",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7ea18b512074acc10d6a4025cb479955ba295f2d",
      "title": "Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation",
      "abstract": "Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Bang Wu",
        "Xiangwen Yang",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/7ea18b512074acc10d6a4025cb479955ba295f2d",
      "pdf_url": "",
      "publication_date": "2020-10-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d4a679c685de885ab98b7c1f2378e26a255de490",
      "title": "Model Extraction Attacks and Defenses on Cloud-Based Machine Learning Models",
      "abstract": "Machine learning models have achieved state-of-the-art performance in various fields, from image classification to speech recognition. However, such models are trained with a large amount of sensitive training data, and are typically computationally expensive to build. As a result, many cloud providers (e.g., Google) have launched machine-learning-as-a-service, which helps clients benefit from the sophisticated cloud-based machine learning models via accessing public APIs. Such a business paradigm significantly expedites and simplifies the development circles. Unfortunately, the commercial value of such cloud-based machine learning models motivates attackers to conduct model extraction attacks for free use or as a springboard to conduct other attacks (e.g., craft adversarial examples in black-box settings). In this article, we conduct a thorough investigation of existing approaches to model extraction attacks and defenses on cloud-based models. We classify the state-of-the-art attack schemes into two categories based on whether the attacker aims to steal the property (i.e., parameters, hyperparameters, and architecture) or the functionality of the model. We also categorize defending schemes into two groups based on whether the scheme relies on output disturbance or query observation. We not only present a detailed survey of each method, but also demonstrate the comparison of both attack and defense approaches via experiments. We highlight several future directions in both model extraction attacks and its defenses, which shed light on possible avenues for further studies.",
      "year": 2020,
      "venue": "IEEE Communications Magazine",
      "authors": [
        "Xueluan Gong",
        "Qian Wang",
        "Yanjiao Chen",
        "Wang Yang",
        "Xinye Jiang"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/d4a679c685de885ab98b7c1f2378e26a255de490",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e5f8dab798783e4909452920249aca6dce1fa1b0",
      "title": "\u201cIdentity Bracelets\u201d for Deep Neural Networks",
      "abstract": "The power of deep learning and the enormous effort and money required to build a deep learning model makes stealing them a hugely worthwhile and highly lucrative endeavor. Worse still, model theft requires little more than a high-school understanding of computer functions, which ensures a healthy and vibrant black market full of choice for any would-be pirate. As such, estimating how many neural network models are likely to be illegally reproduced and distributed in future is almost impossible. Therefore, we propose an embedded \u2018identity bracelet\u2019 for deep neural networks that acts as proof of a model\u2019s owner. Our solution is an extension to the existing trigger-set watermarking techniques that embeds a post-cryptographic-style serial number into the base deep neural network (DNN). Called a DNN-SN, this identifier works like an identity bracelet that proves a network\u2019s rightful owner. Further, a novel training method based on non-related multitask learning ensures that embedding the DNN-SN does not compromise model performance. Experimental evaluations of the framework confirm that a DNN-SN can be embedded into a model when training from scratch or in the student network component of Net2Net.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Xiangrui Xu",
        "Yaqin Li",
        "Cao Yuan"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e5f8dab798783e4909452920249aca6dce1fa1b0",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09104681.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "title": "Noise Reduction Power Stealing Detection Model Based on Self-Balanced Data Set",
      "abstract": "In recent years, various types of power theft incidents have occurred frequently, and the training of the power-stealing detection model is susceptible to the influence of the imbalanced data set and the data noise, which leads to errors in power-stealing detection. Therefore, a power-stealing detection model is proposed, which is based on Improved Conditional Generation Adversarial Network (CWGAN), Stacked Convolution Noise Reduction Autoencoder (SCDAE) and Lightweight Gradient Boosting Decision Machine (LightGBM). The model performs Generation- Adversarial operations on the original unbalanced power consumption data to achieve the balance of electricity data, and avoids the interference of the imbalanced data set on classifier training. In addition, the convolution method is used to stack the noise reduction auto-encoder to achieve dimension reduction of power consumption data, extract data features and reduce the impact of random noise. Finally, LightGBM is used for power theft detection. The experiments show that CWGAN can effectively balance the distribution of power consumption data. Comparing the detection indicators of the power-stealing model with various advanced power-stealing models on the same data set, it is finally proved that the proposed model is superior to other models in the detection of power stealing.",
      "year": 2020,
      "venue": "Energies",
      "authors": [
        "Haiqing Liu",
        "Zhi-qiang Li",
        "Yuancheng Li"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "pdf_url": "https://www.mdpi.com/1996-1073/13/7/1763/pdf",
      "publication_date": "2020-04-07",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "title": "ACD: An Adaptable Approach for RFID Cloning Attack Detection",
      "abstract": "With the rapid development of the internet of things, radio frequency identification (RFID) technology plays an important role in various fields. However, RFID systems are vulnerable to cloning attacks. This is the fabrication of one or more replicas of a genuine tag, which behave exactly as a genuine tag and fool the reader to gain legal authorization, leading to potential financial loss or reputation damage. Many advanced solutions have been proposed to combat cloning attacks, but they require extra hardware resources, or they cannot detect a clone tag in time. In this article, we make a fresh attempt to counterattack tag cloning based on spatiotemporal collisions. We propose adaptable clone detection (ACD), which can intuitively and accurately display the positions of abnormal tags in real time. It uses commercial off-the-shelf (COTS) RFID devices without extra hardware resources. We evaluate its performance in practice, and the results confirm its success at detecting cloning attacks. The average accuracy can reach 98.7%, and the recall rate can reach 96%. Extensive experiments show that it can adapt to a variety of RFID application scenarios.",
      "year": 2020,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Weiqing Huang",
        "Yanfang Zhang",
        "Yue Feng"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "pdf_url": "https://www.mdpi.com/1424-8220/20/8/2378/pdf",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9356a216e5ae48c0007b66687a3da47fc2bea834",
      "title": "Perturbing Inputs to Prevent Model Stealing",
      "abstract": "We show how perturbing inputs to machine learning services (ML-service) deployed in the cloud can protect against model stealing attacks. In our formulation, there is an ML-service that receives inputs from users and returns the output of the model. There is an attacker that is interested in learning the parameters of the ML-service. We use the linear and logistic regression models to illustrate how strategically adding noise to the inputs fundamentally alters the attacker\u2019s estimation problem. We show that even with infinite samples, the attacker would not be able to recover the true model parameters. We focus on characterizing the trade-off between the error in the attacker\u2019s estimate of the parameters with the error in the ML-service\u2019s output.",
      "year": 2020,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "J. Grana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9356a216e5ae48c0007b66687a3da47fc2bea834",
      "pdf_url": "https://arxiv.org/pdf/2005.05823",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "title": "A Low-Cost Image Encryption Method to Prevent Model Stealing of Deep Neural Network",
      "abstract": "Model stealing attack may happen by stealing useful data transmitted from embedded end to server end for an artificial intelligent systems. In this paper, we are interested in preventing model stealing of neural network for resource-constrained systems. We propose an Image Encryption based on Class Activation Map (IECAM) to encrypt information before transmitting in embedded end. According to class activation map, IECAM chooses certain key areas of the image to be encrypted with the purpose of reducing the model stealing risk of neural network. With partly encrypted information, IECAM can greatly reduce the time overheads of encryption/decryption in both embedded and server ends, especially for big size images. The experimental results demonstrate that our method can significantly reduce time overheads of encryption/decryption and the risk of model stealing compared with traditional methods.",
      "year": 2020,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Wei Jiang",
        "Zicheng Gong",
        "Jinyu Zhan",
        "Zhiyuan He",
        "Weijia Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "pdf_url": "",
      "publication_date": "2020-05-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "title": "Special-purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries",
      "abstract": "Model extraction (ME) attacks have been shown to cause financial losses for Machine-Learning-as-a-Service (MLaaS) providers. Attackers steal ML models on MLaaS platforms by building substitute models using queries to and responses from MLaaS platforms. The ML models targeted by attackers are called targeted models. In previous studies, researchers have assumed that attackers build substitute models that classify the same number of classes as targeted ones, which classify thousands or millions of classes to meet users' diverse expectations. We call such models general-purpose models. In fact, attackers can monetize stolen models if they accurately distinguish some classes from others. We call such models special-purpose models. For instance, a model that detects vehicles is useful for collision avoidance systems, and a model that detects wild animals is useful to drive them away from agricultural land. In this work, we investigate a threat of special-purpose ME attacks that steal special-purpose models. Our experimental results show that attackers can build an accurate special-purpose model, which achieves an 80% f-measure, with as few as 100 queries in the worst case. We discuss the difficulty in preventing the attacks with previously proposed defense methods and point out the necessity of a new defense method.",
      "year": 2020,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Rina Okada",
        "Zen Ishikura",
        "Toshiki Shibahara",
        "Satoshi Hasegawa"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08a5204b3697ce4f6de7e5cf1d0dfd64721299d7",
      "title": "Enhanced Pre-processing and Parameterization Process of Generic Code Clone Detection Model for Clones in Java Applications",
      "abstract": "Code clones are repeated source code in a program. There are four types of code clone which are: Type 1, Type 2, Type 3 and Type 4. Various code clone detection models have been used to detect code clone. Generic Code Clone model is a model that consists of a combination of five processes in detecting code clone from Type-1 until Type-4 in Java Applications. The five processes are Pre-processing, Transformation, Parameterization, Categorization and Match Detection process. This work aims to improve code clone detection by enhancing the Generic Code Clone Detection (GCCD) model. Therefore, the Preprocessing and Parameterization process is enhanced to achieve this aim. The enhancement is to determine the best constant and weightage that can be used to improve the code clone detection result. The code clone detection result from the proposed enhancement shows that private with its weightage is the best constant and weightage for the Generic Code Clone Detection Model.",
      "year": 2020,
      "venue": "",
      "authors": [
        "Nur Nadzirah Mokhtar",
        "Al-Fahim Mubarak-Ali",
        "M. Azwan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/08a5204b3697ce4f6de7e5cf1d0dfd64721299d7",
      "pdf_url": "http://thesai.org/Downloads/Volume11No6/Paper_69-Enhanced_Pre_Processing_and_Parameterization_Process.pdf",
      "publication_date": null,
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "title": "Model Extraction",
      "abstract": null,
      "year": 2020,
      "venue": "S-Parameters for Signal Integrity",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30d6ef1b7d590094b62f38e97603122f65aa0d09",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b27fec3e197ef802999cb3fbedf832945cfef6a1",
      "title": "Detection of Cloned Recognizers: A Defending Method against Recognizer Cloning Attack",
      "abstract": null,
      "year": 2020,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Y. Mori",
        "Kazuaki Nakamura",
        "Naoko Nitta",
        "N. Babaguchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b27fec3e197ef802999cb3fbedf832945cfef6a1",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c40441bd48c08ab7db9443afdb3084387b06e61f",
      "title": "DAWN: Dynamic Adversarial Watermarking of Neural Networks",
      "abstract": "Training machine learning (ML) models is expensive in terms of computational power, amounts of labeled data and human expertise. Thus, ML models constitute business value for their owners. Embedding digital watermarks during model training allows a model owner to later identify their models in case of theft or misuse. However, model functionality can also be stolen via model extraction, where an adversary trains a surrogate model using results returned from a prediction API of the original model. Recent work has shown that model extraction is a realistic threat. Existing watermarking schemes are ineffective against model extraction since it is the adversary who trains the surrogate model. In this paper, we introduce DAWN (Dynamic Adversarial Watermarking of Neural Networks), the first approach to use watermarking to deter model extraction theft. Unlike prior watermarking schemes, DAWN does not impose changes to the training process but operates at the prediction API of the protected model, by dynamically changing the responses for a small subset of queries (e.g., 0.5%) from API clients. This set is a watermark that will be embedded in case a client uses its queries to train a surrogate model. We show that DAWN is resilient against two state-of-the-art model extraction attacks, effectively watermarking all extracted surrogate models, allowing model owners to reliably demonstrate ownership (with confidence greater than 1-2-64), incurring negligible loss of prediction accuracy (0.03-0.5%).",
      "year": 2019,
      "venue": "ACM Multimedia",
      "authors": [
        "Sebastian Szyller",
        "B. Atli",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 203,
      "url": "https://www.semanticscholar.org/paper/c40441bd48c08ab7db9443afdb3084387b06e61f",
      "pdf_url": "https://aaltodoc.aalto.fi/bitstreams/11ec7679-d118-4245-aad9-c8ff13a384e7/download",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
      "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
      "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/da10f79f983fd4fbd589ed7ffa68d33964841443",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "title": "Defending Against Model Stealing Attacks With Adaptive Misinformation",
      "abstract": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose \"Adaptive Misinformation\" to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker's clone model (by up to 40%), while minimally impacting the accuracy (<0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead.",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "pdf_url": "https://arxiv.org/pdf/1911.07100",
      "publication_date": "2019-11-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "title": "Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning architectures are readily available, but obtaining the high quality labeled data for training is costly. Pre-trained models available as cloud services can be used to generate this costly labeled data, and would allow an attacker to replicate trained models, effectively stealing them. Limiting the information provided by cloud based models by omitting class probabilities has been proposed as a means of protection but significantly impacts the utility of the models. In this work, we illustrate how cloud based models can still provide useful class probability information for users, while significantly limiting the ability of an adversary to steal the model. Our defense perturbs the model's final activation layer, slightly altering the output probabilities. This forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate our defense under diverse scenarios and defense aware attacks. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or increase the number of queries required by an adversary 64 fold, all with a negligible decrease in the protected model accuracy.",
      "year": 2019,
      "venue": "2019 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 109,
      "url": "https://www.semanticscholar.org/paper/555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8834415/8844588/08844598.pdf",
      "publication_date": "2019-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "703f6008899db58f8c2bf454f6e146efb1df38e3",
      "title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?",
      "abstract": "Recently, machine learning (ML) has introduced advanced solutions to many domains. Since ML models provide business advantage to model owners, protecting intellectual property of ML models has emerged as an important consideration. Confidentiality of ML models can be protected by exposing them to clients only via prediction APIs. However, model extraction attacks can steal the functionality of ML models using the information leaked to clients through the results returned via the API. In this work, we question whether model extraction is a serious threat to complex, real-life ML models. We evaluate the current state-of-the-art model extraction attack (Knockoff nets) against complex models. We reproduce and confirm the results in the original paper. But we also show that the performance of this attack can be limited by several factors, including ML model architecture and the granularity of API response. Furthermore, we introduce a defense based on distinguishing queries used for Knockoff nets from benign queries. Despite the limitations of the Knockoff nets, we show that a more realistic adversary can effectively steal complex ML models and evade known defenses.",
      "year": 2019,
      "venue": "Communications in Computer and Information Science",
      "authors": [
        "B. Atli",
        "Sebastian Szyller",
        "Mika Juuti",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/703f6008899db58f8c2bf454f6e146efb1df38e3",
      "pdf_url": "",
      "publication_date": "2019-10-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a63fcc399d69068fc2ce6cd38a7dddfe1e95c2e2",
      "title": "Evasion Attacks Against Watermarking Techniques found in MLaaS Systems",
      "abstract": "Deep neural networks have had enormous impact on various domains of computer science applications, considerably outperforming previous state-of-the-art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increase their costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. This threat is very relevant to Machine Learning as a Service (MLaaS) systems, where a provider supplies APIs to clients, allowing them to interact with their trained proprietary deep learning models. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information (watermark) in a given model. Through the use of this watermark, the legitimate owners, remotely interacting with a model through input queries, are able to detect a copyright infringement, and prove the ownership of their models that were stolen/copied illegally. In this paper, we focus on assessing the robustness and reliability of state-of-the-art deep neural network watermarking schemes. In particular we show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification of copyright infringements from the legitimate owners, thus avoiding the detection of the model theft.",
      "year": 2019,
      "venue": "Swiss Conference on Data Science",
      "authors": [
        "Dorjan Hitaj",
        "B. Hitaj",
        "L. Mancini"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/a63fcc399d69068fc2ce6cd38a7dddfe1e95c2e2",
      "pdf_url": "",
      "publication_date": "2019-06-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "title": "Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker",
      "abstract": "This paper explores the scenarios under which an attacker can claim that 'Noise and access to the softmax layer of the model is all you need' to steal the weights of a convolutional neural network whose architecture is already known. We were able to achieve 96% test accuracy using the stolen MNIST model and 82% accuracy using the stolen KMNIST model learned using only i.i.d. Bernoulli noise inputs. We posit that this theft-susceptibility of the weights is indicative of the complexity of the dataset and propose a new metric that captures the same. The goal of this dissemination is to not just showcase how far knowing the architecture can take you in terms of model stealing, but to also draw attention to this rather idiosyncratic weight learnability aspects of CNNs spurred by i.i.d. noise input. We also disseminate some initial results obtained with using the Ising probability distribution in lieu of the i.i.d. Bernoulli distribution.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Nicholas Roberts",
        "Vinay Uday Prabhu",
        "Matthew McAteer"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "pdf_url": "",
      "publication_date": "2019-12-19",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "title": "Prediction Poisoning: Utility-Constrained Defenses Against Model Stealing Attacks",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/e8a58e29056a1db21dd8cf5bcb17eaf4db648aaf",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data",
      "abstract": "As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.",
      "year": 2019,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Itay Mosafi",
        "Eli David",
        "N. Netanyahu"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "pdf_url": "https://arxiv.org/pdf/1912.03959",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0135f83ab34d66668306b2ad8207d028da2539e4",
      "title": "MimosaNet: An Unrobust Neural Network Preventing Model Stealing",
      "abstract": "Deep Neural Networks are robust to minor perturbations of the learned network parameters and their minor modifications do not change the overall network response significantly. This allows space for model stealing, where a malevolent attacker can steal an already trained network, modify the weights and claim the new network his own intellectual property. In certain cases this can prevent the free distribution and application of networks in the embedded domain. In this paper, we propose a method for creating an equivalent version of an already trained fully connected deep neural network that can prevent network stealing: namely, it produces the same responses and classification accuracy, but it is extremely sensitive to weight changes.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "K\u00e1lm\u00e1n Szentannai",
        "Jalal Al-Afandi",
        "A. Horv\u00e1th"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/0135f83ab34d66668306b2ad8207d028da2539e4",
      "pdf_url": "",
      "publication_date": "2019-07-02",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-29"
    },
    {
      "paper_id": "089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
      "abstract": "Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ``victim'' models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we study complex victim blackbox models, and an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ``knockoff'' with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as show that a reasonable knockoff of an image analysis API could be created for as little as $30.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 596,
      "url": "https://www.semanticscholar.org/paper/089c6224cfbcf5c18b63564eb65001c7c42a7acf",
      "pdf_url": "http://arxiv.org/pdf/1812.02766",
      "publication_date": "2018-12-06",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4582e2350e4822834dcf266522690722dd4430d4",
      "title": "PRADA: Protecting Against DNN Model Stealing Attacks",
      "abstract": "Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
      "year": 2018,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Mika Juuti",
        "Sebastian Szyller",
        "A. Dmitrenko",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 481,
      "url": "https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8790377/8806708/08806737.pdf",
      "publication_date": "2018-05-07",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
      "abstract": "In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copy them. Recent studies revealed that stateof-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.Given these facts, in this paper, we investigate if a target blackbox CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.This hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both nonproblem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.",
      "year": 2018,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jacson Rodrigues Correia-Silva",
        "Rodrigo Berriel",
        "C. Badue",
        "A. D. Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "pdf_url": "https://arxiv.org/pdf/1806.05476",
      "publication_date": "2018-06-14",
      "keywords_matched": [
        "copycat CNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
      "abstract": "Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user\u2019s input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user\u2019s client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.",
      "year": 2018,
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "L. Hanzlik",
        "Yang Zhang",
        "Kathrin Grosse",
        "A. Salem",
        "Maximilian Augustin",
        "M. Backes",
        "Mario Fritz"
      ],
      "citation_count": 113,
      "url": "https://www.semanticscholar.org/paper/8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "pdf_url": "https://arxiv.org/pdf/1808.00590",
      "publication_date": "2018-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c5edd98d2a94dd02cdf92d52343140321e51b068",
      "title": "Have You Stolen My Model? Evasion Attacks Against Deep Neural Network Watermarking Techniques",
      "abstract": "Deep neural networks have had enormous impact on various domains of computer science, considerably outperforming previous state of the art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increases their construction costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information(watermark) in a given model. The watermark allows the legitimate owner to detect copyright infringements of his model. This paper focuses on verifying the robustness and reliability of state-of- the-art deep neural network watermarking schemes. We show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification by the legitimate owners, thus avoiding the detection of model theft.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Dorjan Hitaj",
        "L. Mancini"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/c5edd98d2a94dd02cdf92d52343140321e51b068",
      "pdf_url": "",
      "publication_date": "2018-09-03",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "title": "Defending Against Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/42155d5c8c6e46ef26a0d7212123128a364b07a0",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e181270b4456a5ac428ca0a8029718734f74bd2",
      "title": "Defending Against Machine Learning Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning models are vulnerable to simple model stealing attacks if the adversary can obtain output labels for chosen inputs. To protect against these attacks, it has been proposed to limit the information provided to the adversary by omitting probability scores, significantly impacting the utility of the provided service. In this work, we illustrate how a service provider can still provide useful, albeit misleading, class probability information, while significantly limiting the success of the attack. Our defense forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate several attack strategies, model architectures, and hyperparameters under varying adversarial models, and evaluate the efficacy of our defense against the strongest adversary. Finally, we quantify the amount of noise injected into the class probabilities to mesure the loss in utility, e.g., adding 1.26 nats per query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or require up to 64 times more queries while keeping the accuracy of the protected model almost intact.",
      "year": 2018,
      "venue": "",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/1e181270b4456a5ac428ca0a8029718734f74bd2",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "title": "Model Extraction and Active Learning",
      "abstract": null,
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/2cb2f7c824a089422ac210b3f77d2e078b017f66",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7e7157625088e62582f45409cb1704844ddb5535",
      "title": "Poisoning Machine Learning Based Wireless IDSs via Stealing Learning Model",
      "abstract": null,
      "year": 2018,
      "venue": "Wireless Algorithms, Systems, and Applications",
      "authors": [
        "Pan Li",
        "Wentao Zhao",
        "Qiang Liu",
        "Xiao Liu",
        "Linyuan Yu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/7e7157625088e62582f45409cb1704844ddb5535",
      "pdf_url": "",
      "publication_date": "2018-06-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2822dce260dc4930ccd276a9ae6392960d6b460d",
      "title": "Combating Tag Cloning with COTS RFID Devices",
      "abstract": "In RFID systems, a cloning attack is to fabricate one or more replicas of a genuine tag, so that these replicas behave exactly the same as the genuine tag and fool the reader for getting legal authorization, leading to potential financial loss or reputation damage for the corporations. These replicas are called clone tags. Although many advanced solutions have been proposed to combat cloning attack, they need to either modify the MAC- layer protocols or increase extra hardware resources, which cannot be deployed on commercial off-the-shelf (COTS) RFID devices for practical use. In this paper, we take a fresh attempt to counterattack tag cloning based on COTS RFID devices and the universal C1G2 standard, without any software redesign or hardware augment needed. The basic idea is to use the RF signal profile to characterize each tag. Since these physical-layer data are measured by the reader and susceptible to various environmental factors, they are hard to be estimated by the attackers; let alone be cloned. Even so, we assert that it is challenging to identify clone tags as the signal data from a genuine tag and its clones are all mixed together. Besides, the tag moving has a great impact on the measured RF signals. To overcome these challenges, we propose a clustering-based scheme that detects the cloning attack in the still scene and a chain- based scheme for clone detection in the dynamic scene, respectively. Extensive experiments on COTS RFID devices demonstrate that the detection accuracy of our approaches reaches 99.8% in a still case and 99.3% in a dynamic scene.",
      "year": 2018,
      "venue": "Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks",
      "authors": [
        "Xingyu Chen",
        "Jia Liu",
        "Xia Wang",
        "Xiaocong Zhang",
        "Yanyan Wang",
        "Lijun Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/2822dce260dc4930ccd276a9ae6392960d6b460d",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "52551687c11c0e1668ad50bced2bb7a18204aefb",
      "title": "Analysis of IoT devices via API Exploitation and Model Extraction",
      "abstract": null,
      "year": 2018,
      "venue": "",
      "authors": [
        "Alexandre Eraclides",
        "Alexandre gpehotmail.com"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/52551687c11c0e1668ad50bced2bb7a18204aefb",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "181fe8787937dbf7abf886042855be1bc6149f80",
      "title": "Model Extraction Warning in MLaaS Paradigm",
      "abstract": "Machine learning models deployed on the cloud are susceptible to several security threats including extraction attacks. Adversaries may abuse a model's prediction API to steal the model thus compromising model confidentiality, privacy of training data, and revenue from future query payments. This work introduces a model extraction monitor that quantifies the extraction status of models by continually observing the API query and response streams of users. We present two novel strategies that measure either the information gain or the coverage of the feature space spanned by user queries to estimate the learning rate of individual and colluding adversaries. Both approaches have low computational overhead and can easily be offered as services to model owners to warn them against state of the art extraction attacks. We demonstrate empirical performance results of these approaches for decision tree and neural network models using open source datasets and BigML MLaaS platform.",
      "year": 2017,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "M. Kesarwani",
        "B. Mukhoty",
        "V. Arya",
        "S. Mehta"
      ],
      "citation_count": 154,
      "url": "https://www.semanticscholar.org/paper/181fe8787937dbf7abf886042855be1bc6149f80",
      "pdf_url": "https://arxiv.org/pdf/1711.07221",
      "publication_date": "2017-11-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "title": "Symbolic Model Extraction for Web Application Verification",
      "abstract": null,
      "year": 2017,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Ivan Bocic",
        "T. Bultan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/090b9f33bf878f7630e7715c1cd2897df6d89fbd",
      "pdf_url": "",
      "publication_date": "2017-05-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "title": "Assessing the Performance of Automated Model Extraction Rules",
      "abstract": null,
      "year": 2017,
      "venue": "Integrated Spatial Databases",
      "authors": [
        "Jorge Echeverr\u00eda",
        "Francisca P\u00e9rez",
        "\u00d3. Pastor",
        "Carlos Cetina"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8d1192d8f79827f00fa3efe392e5b4a165f5ef8d",
      "pdf_url": "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1150&context=isd2014",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \nThe tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",
      "year": 2016,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Florian Tram\u00e8r",
        "Fan Zhang",
        "A. Juels",
        "M. Reiter",
        "Thomas Ristenpart"
      ],
      "citation_count": 1967,
      "url": "https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "pdf_url": "",
      "publication_date": "2016-08-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "title": "A Reference Architecture for Online Performance Model Extraction in Virtualized Environments",
      "abstract": null,
      "year": 2016,
      "venue": "ICPE Companion",
      "authors": [
        "Simon Spinner",
        "J. Walter",
        "Samuel Kounev"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/4bcd67d7dd2f5fd18986c8c28f254e9a36af6fe7",
      "pdf_url": "",
      "publication_date": "2016-03-12",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "20a199be55d274fc986f32f043ae606d5ce09767",
      "title": "A model of food stealing with asymmetric information",
      "abstract": null,
      "year": 2016,
      "venue": "",
      "authors": [
        "M. Broom",
        "J. Rycht\u00e1\u0159"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/20a199be55d274fc986f32f043ae606d5ce09767",
      "pdf_url": "https://openaccess.city.ac.uk/id/eprint/12831/7/CC%20BY-NC-ND%204.0.pdf",
      "publication_date": "2016-06-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "073e060587a32e9eac3c37105dd1454ecf5a0d80",
      "title": "Deterministic Detection of Cloning Attacks for Anonymous RFID Systems",
      "abstract": null,
      "year": 2015,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Kai Bu",
        "Mingjie Xu",
        "Xuan Liu",
        "Jiaqing Luo",
        "Shigeng Zhang",
        "Minyu Weng"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/073e060587a32e9eac3c37105dd1454ecf5a0d80",
      "pdf_url": "",
      "publication_date": "2015-09-29",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "532d52b130c67736e1ea275196644a1fc02c3e6e",
      "title": "The Mathematical Model and the Problem of Optimal Partitioning of Shared Memory for Work-Stealing Deques",
      "abstract": null,
      "year": 2015,
      "venue": "International Conference on Parallel Architectures and Compilation Techniques",
      "authors": [
        "A. Sokolov",
        "E. Barkovsky"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/532d52b130c67736e1ea275196644a1fc02c3e6e",
      "pdf_url": "",
      "publication_date": "2015-08-31",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "342d508244501218f9be07ee1ab67b7eb3d34086",
      "title": "Detecting Cloning Attack in Low-Cost Passive RFID Tags An Analytic Comparison between KILL Passwords and Synchronized Secrets Obinna",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "Stanley Okpara"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/342d508244501218f9be07ee1ab67b7eb3d34086",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "666f3571b6313d2aa0f26d1f21a37a750e2ad8ad",
      "title": "Detecting cloning attack in Social Networks using classification and clustering techniques",
      "abstract": null,
      "year": 2014,
      "venue": "International Conference on Recent Trends in Information Technology",
      "authors": [
        "S. Kiruthiga",
        "P. Kola Sujatha",
        "A. Kannan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/666f3571b6313d2aa0f26d1f21a37a750e2ad8ad",
      "pdf_url": "",
      "publication_date": "2014-04-10",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ba2b1eaddff998d95a5a3f6657b182ab58cf5526",
      "title": "New Model Introductions, Cannibalization and Market Stealing: Evidence from Shopbot Data",
      "abstract": null,
      "year": 2014,
      "venue": "",
      "authors": [
        "M. Haynes",
        "S. Thompson",
        "P. Wright"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ba2b1eaddff998d95a5a3f6657b182ab58cf5526",
      "pdf_url": "http://eprints.whiterose.ac.uk/95237/1/PW1.pdf",
      "publication_date": "2014-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4be848dfc77b2535fdf5716ebb7635c5f99e87e9",
      "title": "Unreconciled Collisions Uncover Cloning Attacks in Anonymous RFID Systems",
      "abstract": null,
      "year": 2013,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kai Bu",
        "Xuan Liu",
        "Jiaqing Luo",
        "Bin Xiao",
        "Guiyi Wei"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4be848dfc77b2535fdf5716ebb7635c5f99e87e9",
      "pdf_url": "http://hdl.handle.net/10397/14328",
      "publication_date": "2013-03-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1034f4a32a33501c67fad62440bde0e811887dbc",
      "title": "Enhancing and identifying cloning attacks in online social networks",
      "abstract": null,
      "year": 2013,
      "venue": "International Conference on Ubiquitous Information Management and Communication",
      "authors": [
        "Zifei Shan",
        "Haowen Cao",
        "Jason Lv",
        "Cong Yan",
        "Annie Liu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/1034f4a32a33501c67fad62440bde0e811887dbc",
      "pdf_url": "",
      "publication_date": "2013-01-17",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "78fb962e1f3e7d0945ebe72193d3c15c2c1c0207",
      "title": "The limits of clone model standardization",
      "abstract": null,
      "year": 2013,
      "venue": "International Workshop on Software Clones",
      "authors": [
        "Jan Harder"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/78fb962e1f3e7d0945ebe72193d3c15c2c1c0207",
      "pdf_url": "",
      "publication_date": "2013-05-19",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e22539cf9a6ed945aba33e4b8f9ee8553892c446",
      "title": "Game-theoretic analysis of node capture and cloning attack with multiple attackers in wireless sensor networks",
      "abstract": null,
      "year": 2012,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Quanyan Zhu",
        "L. Bushnell",
        "T. Ba\u015far"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/e22539cf9a6ed945aba33e4b8f9ee8553892c446",
      "pdf_url": "",
      "publication_date": "2012-12-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6544e0772d9d2d9e23303179ceb4d3f30aba6ae0",
      "title": "TVMCM: A trusted VM clone model in cloud computing",
      "abstract": null,
      "year": 2012,
      "venue": "International Conference on New Trends in Information Science, Service Science and Data Mining",
      "authors": [
        "Wei-Ming Ma",
        "Xiaoyong Li",
        "Yong Shi",
        "Yu Guo"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/6544e0772d9d2d9e23303179ceb4d3f30aba6ae0",
      "pdf_url": "",
      "publication_date": "2012-10-01",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac9769a7792ab7fda358af7fdd7464583132b840",
      "title": "Cloning Attack Authenticator in Wireless Sensor Networks",
      "abstract": null,
      "year": 2012,
      "venue": "",
      "authors": [
        "A. Vanathi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/ac9769a7792ab7fda358af7fdd7464583132b840",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "title": "Symmetries in Quantum Key Distribution and the Connection between Optimal Attacks and Optimal Cloning",
      "abstract": "We investigate the connection between the optimal collective eavesdropping attack and the optimal cloning attack where the eavesdropper employs an optimal cloner to attack the quantum key distribution (QKD) protocol. The analysis is done in the context of the security proof in Refs. [1, 2] for discrete variable protocols in d-dimensional Hilbert spaces. We consider a scenario in which the protocols and cloners are equipped with symmetries. These symmetries are used to dene a quantum cloning scenario. We nd that, in general, it does not hold that the optimal attack is an optimal cloner. However, there are classes of protocols, where we can identify an optimal attack by an optimal cloner. We analyze protocols with 2, d and d + 1 mutually unbiased bases where d is a prime, and show that for the protocols with 2 and d + 1 MUBs the optimal attack is an optimal cloner, but for the protocols with d MUBs, it is not 1 . Finally, we give criteria to identify protocols which have dierent signal states, but the same optimal attack. Using these criteria, we present qubit protocols which have the same optimal attack as the BB84 protocol or the 6-state protocol.",
      "year": 2011,
      "venue": "",
      "authors": [
        "A. Ferenczi",
        "N. Lutkenhaus"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "pdf_url": "http://arxiv.org/pdf/1112.3396",
      "publication_date": "2011-12-15",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3102c84eebc9e4b3e9b7f34747c872c53c036866",
      "title": "A contract-extended push-pull-clone model",
      "abstract": null,
      "year": 2011,
      "venue": "International Conference on Collaborative Computing",
      "authors": [
        "H. Truong",
        "C. Ignat",
        "Pascal Molli"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3102c84eebc9e4b3e9b7f34747c872c53c036866",
      "pdf_url": "https://hal.inria.fr/hal-00761038/file/c-ppc.pdf",
      "publication_date": "2011-10-15",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "32478a09717677f06958cdbbdc3ff1ef15b39ef5",
      "title": "A mean field model of work stealing in large-scale systems",
      "abstract": null,
      "year": 2010,
      "venue": "Measurement and Modeling of Computer Systems",
      "authors": [
        "Nicolas Gast",
        "B. Gaujal"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/32478a09717677f06958cdbbdc3ff1ef15b39ef5",
      "pdf_url": "https://infoscience.epfl.ch/record/169412/files/workstealing_gast.pdf",
      "publication_date": "2010-06-12",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a3474a051f454516d9fd2cb7fc1c12cb6edebb85",
      "title": "Efficient Detecting of RFID Tag Cloning Attacks using Chaos Theory",
      "abstract": null,
      "year": 2010,
      "venue": "",
      "authors": [
        "M. Babaie",
        "H. Rahimov"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a3474a051f454516d9fd2cb7fc1c12cb6edebb85",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-22"
    },
    {
      "paper_id": "d0e6254805e30303659dbd37b61ab4ab06189359",
      "title": "Enhancing RFID Tag Resistance against Cloning Attack",
      "abstract": null,
      "year": 2009,
      "venue": "2009 Third International Conference on Network and System Security",
      "authors": [
        "J. Abawajy"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/d0e6254805e30303659dbd37b61ab4ab06189359",
      "pdf_url": "",
      "publication_date": "2009-10-19",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4cf22a03e71fa83089e7e653aa91e83b1ba8e019",
      "title": "Model Extraction for Sockets-based Distributed Programs",
      "abstract": null,
      "year": 2009,
      "venue": "",
      "authors": [
        "Alexander Heu\u00dfner"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4cf22a03e71fa83089e7e653aa91e83b1ba8e019",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "title": "Muscle contraction-induced steal model in the anesthetized cat.",
      "abstract": null,
      "year": 1998,
      "venue": "Journal of pharmacological and toxicological methods",
      "authors": [
        "S. Poucher"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "pdf_url": "",
      "publication_date": "1998-11-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5d74cc67a112ed43d0c3b13c5e40c73d9572bc55",
      "title": "Stealing a Defended Model without Data",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Brett Reynolds",
        "James Beetham",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5d74cc67a112ed43d0c3b13c5e40c73d9572bc55",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}