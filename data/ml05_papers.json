{
  "updated": "2026-01-06",
  "total": 113,
  "owasp_id": "ML05",
  "owasp_name": "Model Theft",
  "description": "Attacks that steal, copy, or clone machine learning models themselves.\n        This includes model extraction attacks, model stealing attacks, knockoff\n        networks, functionality stealing, query-based model cloning, surrogate\n        model training, and techniques to replicate or steal model weights,\n        architecture, or behavior through API queries. The goal is stealing the\n        MODEL, not the training data.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal model",
        "model stealing",
        "input perturbation",
        "stealing model",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
      "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
      "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g.,\"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. $\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations) $\\textit{during pretraining}$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides $\\textbf{several key recommendations for LLM pretraining in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li"
      ],
      "citation_count": 228,
      "url": "https://www.semanticscholar.org/paper/f29f8b8aa2b7e608199b65d3cf751969d4024132",
      "pdf_url": "https://arxiv.org/pdf/2309.14316",
      "publication_date": "2023-09-25",
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "79b2c9715586161bad6194c6bf27a845cf027f7e",
      "title": "Unified-Copycat: Unifying Heterogeneous Classifiers Using Non Problem Domain Data",
      "abstract": "With the success of Convolutional Neural Networks, companies are increasingly offering trained models as paid services through APIs, where users are charged to query different models, without any access to their architecture or training datasets. Additionally, privacy regulations and ethical concerns have also led to the use of black-box models. These constraints make datasets and detailed information about the classifiers entirely unavailable to users. The challenge is further exacerbated when unifying models designed for tasks with different class sets, as class disparities significantly complicate the unification process. Given this scenario, unifying such specialized models into a single classifier provides clear advantages, but existing solutions often assume access to domain data or internal model structures. To address this, we propose a novel approach based on the Copycat framework that unifies heterogeneous models into one unified model capable of inference across all class domains. Unlike prior methods, our approach leverages random natural images from outside the domains of the heterogeneous models, reducing the assumptions required for unification. Comprehensive evaluations show that our method achieves competitive performance with significantly fewer assumptions, marking a step forward in unifying heterogeneous classifiers.",
      "year": 2025,
      "venue": "SIBGRAPI Conference on Graphics, Patterns and Images",
      "authors": [
        "Gabriel Braga Ladislau",
        "Jhonatan Machado Le\u00e3o",
        "Marlon Moratti do Amaral",
        "C. Badue",
        "Alberto F. De Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/79b2c9715586161bad6194c6bf27a845cf027f7e",
      "pdf_url": "",
      "publication_date": "2025-09-30",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b4b345e37c27dd17c3372858b10a3c9b93563eb4",
      "title": "High-accuracy model recognition method of mobile device based on weighted feature similarity",
      "abstract": "Accurately model recognition of mobile device is of great significance for identifying copycat device and protecting intellectual property rights. Although existing methods have realized high-accuracy recognition about device\u2019s category and brand, the accuracy of model recognition still needs to be improved. For that, we propose Recognizer, a high-accuracy model recognition method of mobile device based on weighted feature similarity. We extract 20 features from the network traffic and physical attributes of device, and design feature similarity metric rules, and calculate inter-device similarity further. In addition, we propose feature importance evaluation strategies to assess the role of feature in recognition and determine the weight of each feature. Finally, based on all or part of 20 features, the similarity between the target device and known devices is calculated to recognize the brand and model of target device. Based on 587 models of mobile devices of 17 widely used brands such as Apple and Samsung, we carry out device recognition experiments. The results show that Recognizer can identify the device\u2019s brand and model than existing methods more effectively. In average, the model recognition accuracy of Recognizer is 99.08% (+\u20099.25%\u2191) when using 20 features and 92.08% (+\u200929.26%\u2191) when using 13 features.",
      "year": 2022,
      "venue": "Scientific Reports",
      "authors": [
        "Ruixiang Li",
        "Xiuting Wang",
        "X. Luo"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b4b345e37c27dd17c3372858b10a3c9b93563eb4",
      "pdf_url": "https://www.nature.com/articles/s41598-022-26518-y.pdf",
      "publication_date": "2022-12-18",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
      "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. Our findings revealed that model stealing attacks can produce clone models achieving up to 0.9 \u00d7 and 0.99 \u00d7 clone test accuracy when trained using Top-1 and Top-k labels, respectively (k: num_classes). To defend against these attacks, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Despite limited success with our defense techniques, it has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.",
      "year": 2024,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Satwik Kundu",
        "Debarshi Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "pdf_url": "https://arxiv.org/pdf/2402.11687",
      "publication_date": "2024-02-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "title": "Model Stealing for Any Low-Rank Language Model",
      "abstract": "Model stealing, where a learner tries to recover an unknown model via carefully chosen queries, is a critical problem in machine learning, as it threatens the security of proprietary models and the privacy of data they are trained on. In recent years, there has been particular interest in stealing large language models (LLMs). In this paper, we aim to build a theoretical understanding of stealing language models by studying a simple and mathematically tractable setting. We study model stealing for Hidden Markov Models (HMMs), and more generally low-rank language models. We assume that the learner works in the conditional query model, introduced by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient algorithm in the conditional query model, for learning any low-rank distribution. In other words, our algorithm succeeds at stealing any language model whose output distribution is low-rank. This improves upon the previous result which also requires the unknown distribution to have high \u201cfidelity\u201d \u2013 a property that holds only in restricted cases. There are two key insights behind our algorithm: First, we represent the conditional distributions at each timestep by constructing barycentric spanners among a collection of vectors of exponentially large dimension. Second, for sampling from our representation, we iteratively solve a sequence of convex optimization problems that involve projection in relative entropy to prevent compounding of errors over the length of the sequence. This is an interesting example where, at least theoretically, allowing a machine learning model to solve more complex problems at inference time can lead to drastic improvements in its performance.",
      "year": 2024,
      "venue": "Symposium on the Theory of Computing",
      "authors": [
        "Allen Liu",
        "Ankur Moitra"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2b89dd26035172807159d1f5bc972ed04d7c4bb2",
      "pdf_url": "",
      "publication_date": "2024-11-12",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "title": "Dual Student Networks for Data-Free Model Stealing",
      "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "James Beetham",
        "Navid Kardan",
        "A. Mian",
        "M. Shah"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "pdf_url": "https://arxiv.org/pdf/2309.10058",
      "publication_date": "2023-09-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f63f7d623e1738fbc314143a1ad1812045caffff",
      "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which allow for the processing of graph-structured data without relying on predefined graph structures, are becoming increasingly important in a wide range of applications. As such these networks become attractive targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. Significant efforts have been devoted to developing model-stealing attacks that extract models trained on images and texts. However, little attention has been given to stealing GNNs trained on graph data. This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model. The new type of attack is thoroughly evaluated on six datasets and the results show that our approach outperforms the current state-of-the-art by Shen et al. (2021). In particular, our attack surpasses the baseline across all benchmarks, attaining superior fidelity and downstream accuracy of the stolen model while necessitating fewer queries directed toward the target model.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f63f7d623e1738fbc314143a1ad1812045caffff",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "94621729b6ca9e811d8a052dc3457e98df457676",
      "title": "Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing",
      "abstract": "Model stealing (MS) involves querying and observing the output of a machine learning model to steal its capabilities. The quality of queried data is crucial, yet obtaining a large amount of real data for MS is often challenging. Recent works have reduced reliance on real data by using generative models. However, when high-dimensional query data is required, these methods are impractical due to the high costs of querying and the risk of model collapse. In this work, we propose using sample gradients (SG) to enhance the utility of each real sample, as SG provides crucial guidance on the decision boundaries of the victim model. However, utilizing SG in the model stealing scenario faces two challenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. The estimation of sample gradients has a significant variance. This paper proposes Superpixel Sample Gradient stealing (SPSG) for model stealing under the constraint of limited real samples. With the basic idea of imitating the victim model's low-variance patch-level gradients instead ofpixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wise perturbations on query images to estimate the average gradient in different regions of the image. Then, we filter the gradients through a threshold strategy to reduce variance. Exhaustive experiments demonstrate that, with the same number of real samples, SPSG achieves accuracy, agreements, and adversarial success rate significantly surpassing the current state-of-the-art MS methods. Codes are available at https://github.com/zyI123456aBISPSG_attack.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yunlong Zhao",
        "Xiaoheng Deng",
        "Yijing Liu",
        "Xin-jun Pei",
        "Jiazhi Xia",
        "Wei Chen"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/94621729b6ca9e811d8a052dc3457e98df457676",
      "pdf_url": "https://arxiv.org/pdf/2406.18540",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b28136746a29ff698f57106c292d0d6e0181629",
      "title": "Efficient Data-Free Model Stealing with Label Diversity",
      "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the proliferation of machine learning based applications, while on the other hand, it introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yiyong Liu",
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2b28136746a29ff698f57106c292d0d6e0181629",
      "pdf_url": "",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4595c8a7e1571c9974b2393ed127f05fabe72164",
      "title": "Model Stealing Detection for IoT Services Based on Multidimensional Features",
      "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a tradeoff between false alarm rates and detection accuracy. To address this challenge, we design a multidimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multidimensional features between attack and benign samples and 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature-based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xinjing Liu",
        "Taifeng Liu",
        "Hao Yang",
        "Jiakang Dong",
        "Zuobin Ying",
        "Zhuo Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4595c8a7e1571c9974b2393ed127f05fabe72164",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "title": "Efficient Model Stealing Defense with Noise Transition Matrix",
      "abstract": null,
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Dong-Dong Wu",
        "Chilin Fu",
        "Weichang Wu",
        "Wenwen Xia",
        "Xiaolu Zhang",
        "Jun Zhou",
        "Min-Ling Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a916a716b23490f87844b3ff44cb2d18284b2ac6",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "99712e009858ba394bd119092bc2ebcc502a8892",
      "title": "DualCOS: Query-Efficient Data-Free Model Stealing with Dual Clone Networks and Optimal Samples",
      "abstract": "Although data-free model stealing attacks are free from reliance on real data, they suffer from limitations, including low accuracy and high query budgets, which restrict their practical feasibility. In this paper, we propose a novel data-free model stealing framework called DualCOS. As a whole, DualCOS is divided into two stages: interactive training and semi-supervised boosting. To optimize the usage of query budgets, we use a dual clone model architecture to address the challenge of querying victim model during generator training. We also introduce active learning-based sampling strategy and sample reuse mechanism to achieve an efficient query process. Furthermore, once query budget is exhausted, the semi-supervised boosting is employed to continue improving the final clone accuracy. Through extensive evaluations, we demonstrate the superiority of our proposed method in terms of accuracy and query efficiency, particularly in scenarios involving hard labels and multiple classes.",
      "year": 2024,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/99712e009858ba394bd119092bc2ebcc502a8892",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "title": "SwiftThief: Enhancing Query Efficiency of Model Stealing by Contrastive Learning",
      "abstract": "Model-stealing attacks are emerging as a severe threat to AI-based services because an adversary can create models that duplicate the functionality of the black-box AI models inside the services with regular query-based access. To avoid detection or query costs, the model-stealing adversary must consider minimizing the number of queries to obtain an accurate clone model. To achieve this goal, we propose SwiftThief, a novel model-stealing framework that utilizes both queried and unqueried data to reduce query complexity. In particular, SwiftThief uses contrastive learning, a recent technique for representation learning. We formulate a new objective function for model stealing consisting of self-supervised (for abundant unqueried inputs from public datasets) and soft-supervised (for queried inputs) contrastive losses, jointly optimized with an output matching loss (for queried inputs). In addition, we suggest a new sampling strategy to prioritize rarely queried classes to improve attack performance. Our experiments proved that SwiftThief could significantly enhance the efficiency of model-stealing attacks compared to the existing methods, achieving similar attack performance using only half of the query budgets of the competing approaches. Also, SwiftThief showed high competence even when a defense was activated for the victims.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3b2ec64ab223bcf70700eb3cd7ef54ae630b8b15",
      "title": "Exploring Query Efficient Data Generation towards Data-free Model Stealing in Hard Label Setting",
      "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. The adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (\\textbf{QEDG}). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real \\textbf{MLaaS} scenario and five datasets.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Gaozheng Pei",
        "Shaojie Lyu",
        "Ke Ma",
        "Pinci Yang",
        "Qianqian Xu",
        "Yingfei Sun"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/3b2ec64ab223bcf70700eb3cd7ef54ae630b8b15",
      "pdf_url": "",
      "publication_date": "2024-12-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "title": "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality",
      "abstract": "Machine learning, particularly deep learning, is extensively applied in various real-life scenarios. However, recent research has highlighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. Most of the existing model stealing attacks rely on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, black-box, hard-label outputs, and there is a distribution shift during the testing phase. STMS achieves comparable or better stealing accuracy and generalization performance than prior works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can also be migrated to other areas of machine learning.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Zhendong Zhao",
        "Yu Xuan",
        "Bisheng Tang",
        "Xiaoying Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4582e2350e4822834dcf266522690722dd4430d4",
      "title": "PRADA: Protecting Against DNN Model Stealing Attacks",
      "abstract": "Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
      "year": 2018,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Mika Juuti",
        "Sebastian Szyller",
        "A. Dmitrenko",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 481,
      "url": "https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8790377/8806708/08806737.pdf",
      "publication_date": "2018-05-07",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d5ffa58133940646d1339c2610cb35f27442e0d3",
      "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
      "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "A. Prakash",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 171,
      "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
      "pdf_url": "https://arxiv.org/pdf/2005.03161",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "stealing model",
        "model zoo attack",
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
      "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4\u00d7 faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jun Guo",
        "Xingyu Zheng",
        "Aishan Liu",
        "Siyuan Liang",
        "Yisong Xiao",
        "Yichao Wu",
        "Xianglong Liu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "pdf_url": "http://arxiv.org/pdf/2308.00958",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7b6db013d28e72374f301f758f432545a92b22fb",
      "title": "DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer",
      "abstract": "Recently, model stealing attacks are widely studied but most of them are focused on stealing a single non-discrete model, e.g., neural networks. For ensemble models, these attacks are either non-executable or suffer from intolerant performance degradation due to the complex model structure (multiple sub-models) and the discreteness possessed by the sub-model (e.g., decision trees). To overcome the bottleneck, this paper proposes a divide-and-conquer strategy called DivTheft to formulate the model stealing attack to common ensemble models by combining active learning (AL). Specifically, based on the boosting learning concept, we divide a hard ensemble model stealing task into multiple simpler ones about single sub-model stealing. Then, we adopt AL to conquer the data-free sub-model stealing task. During the process, the current AL algorithm easily causes the stolen model to be biased because of ignoring the past useful memories. Thus, DivTheft involves a newly designed uncertainty sampling scheme to filter reusable samples from the previously used ones. Experiments show that compared with the prior work, DivTheft can save almost 50% queries while ensuring a competitive agreement rate to the victim model.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhuo Ma",
        "Xinjing Liu",
        "Yang Liu",
        "Ximeng Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7b6db013d28e72374f301f758f432545a92b22fb",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "title": "Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation",
      "abstract": "Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing.",
      "year": 2023,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Vlad Hondru",
        "R. Ionescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "title": "FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems",
      "abstract": "Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Kaisheng Fan",
        "Weizhe Zhang",
        "Guangrui Liu",
        "Hui He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00171-y",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0d7139b54040b34c6a64187622f8b8038775ff45",
      "title": "On the Limitations of Model Stealing with Uncertainty Quantification Models",
      "abstract": "Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted to leverage using uncertainty quantification models is not (high) enough for improvements on the model stealing task.",
      "year": 2023,
      "venue": "The European Symposium on Artificial Neural Networks",
      "authors": [
        "David Pape",
        "Sina D\u00e4ubener",
        "Thorsten Eisenhofer",
        "A. E. Cin\u00e0",
        "Lea Sch\u00f6nherr"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d7139b54040b34c6a64187622f8b8038775ff45",
      "pdf_url": "https://arxiv.org/pdf/2305.05293",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "title": "A Model Stealing Attack Against Multi-Exit Networks",
      "abstract": "Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Pan Li",
        "Peizhuo Lv",
        "Kai Chen",
        "Yuling Cai",
        "Fan Xiang",
        "Shengzhi Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "pdf_url": "",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "title": "Data-Free Model Stealing Attack Based on Denoising Diffusion Probabilistic Model",
      "abstract": "Data-free model stealing (MS) attacks use synthetic samples to query a target model and train a substitute model to fit the target model\u2019s predictions, avoiding strong dependence on real datasets used by model developers. However, the existing data-free MS attack methods still have a big gap in generating high-quality query samples for high-precision MS attacks. In this paper, we construct the DDPM-optimized generator to generate data, in which a residual network-like structure is designed to fuse data to synthesize query samples. Our method further improves the quantity and quality of synthetic query samples, and effectively reduces the number of queries to the target model. The results show that the proposed method achieves superior performance compared to state-of-the-art methods.",
      "year": 2023,
      "venue": "2023 IEEE Smart World Congress (SWC)",
      "authors": [
        "Guofeng Gao",
        "Xiaodong Wang",
        "Zhiqiang Wei",
        "Jinghai Ai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "pdf_url": "",
      "publication_date": "2023-08-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "title": "Exploring and Exploiting Data-Free Model Stealing",
      "abstract": null,
      "year": 2023,
      "venue": "ECML/PKDD",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/42a2e32a77ffa1ea1671c1543ee0a71164375305",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af72f901d7a0f2edca55ee008c8893ae93b09971",
      "title": "How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection",
      "abstract": "Model stealing attacks present a dilemma for public machine learning APIs. To protect financial investments, companies may be forced to withhold important information about their models that could facilitate theft, including uncertainty estimates and prediction explanations. This compromise is harmful not only to users but also to external transparency. Model stealing defenses seek to resolve this dilemma by making models harder to steal while preserving utility for benign users. However, existing defenses have poor performance in practice, either requiring enormous computational overheads or severe utility trade-offs. To meet these challenges, we present a new approach to model stealing defenses called gradient redirection. At the core of our approach is a provably optimal, efficient algorithm for steering an adversary's training updates in a targeted manner. Combined with improvements to surrogate networks and a novel coordinated defense strategy, our gradient redirection defense, called GRAD${}^2$, achieves small utility trade-offs and low computational overhead, outperforming the best prior defenses. Moreover, we demonstrate how gradient redirection enables reprogramming the adversary with arbitrary behavior, which we hope will foster work on new avenues of defense.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mantas Mazeika",
        "B. Li",
        "David A. Forsyth"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/af72f901d7a0f2edca55ee008c8893ae93b09971",
      "pdf_url": "https://arxiv.org/pdf/2206.14157",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7026ec6c12f325aec884ff802812c3c80319f668",
      "title": "Model Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary\u2019s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yun Shen",
        "Xinlei He",
        "Yufei Han",
        "Yang Zhang"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/7026ec6c12f325aec884ff802812c3c80319f668",
      "pdf_url": "https://arxiv.org/pdf/2112.08331",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bfb6e56602b658fdabaaa66987ebf685a8139892",
      "title": "Defending against Model Stealing via Verifying Embedded External Features",
      "abstract": "Obtaining a well-trained model involves expensive data collection and training procedures, therefore the model is a valuable intellectual property. Recent studies revealed that adversaries can `steal' deployed models even when they have no training samples and can not get access to the model parameters or structures. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense from another angle by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by tempering a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. We examine our method on both CIFAR-10 and ImageNet datasets. Experimental results demonstrate that our method is effective in detecting different types of model stealing simultaneously, even if the stolen model is obtained via a multi-stage stealing process. The codes for reproducing main results are available at Github (https://github.com/zlh-thu/StealingVerification).",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Xiaojun Jia",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/bfb6e56602b658fdabaaa66987ebf685a8139892",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20036/19795",
      "publication_date": "2021-12-07",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
      "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
      "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/da10f79f983fd4fbd589ed7ffa68d33964841443",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "title": "Defending Against Model Stealing Attacks With Adaptive Misinformation",
      "abstract": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose \"Adaptive Misinformation\" to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker's clone model (by up to 40%), while minimally impacting the accuracy (<0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead.",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "pdf_url": "https://arxiv.org/pdf/1911.07100",
      "publication_date": "2019-11-16",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "title": "Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets",
      "abstract": "Model stealing techniques allow adversaries to create attack models that mimic the functionality of black-box machine learning models, querying only class membership or probability outcomes. Recently, interpretable AI is getting increasing attention, to enhance our understanding of AI models, provide additional information for diagnoses, or satisfy legal requirements. However, it has been recently reported that providing such additional information can make AI models more vulnerable to model stealing attacks. In this paper, we propose DeepDefense, the first defense mechanism that protects an AI model against model stealing attackers exploiting both class probabilities and interpretations. DeepDefense uses a misdirection model to hide the critical information of the original model against model stealing attacks, with minimal degradation on both the class probability and the interpretability of prediction output. DeepDefense is highly applicable for any model stealing scenario since it makes minimal assumptions about the model stealing adversary. In our experiments, DeepDefense shows significantly higher defense performance than the existing state-of-the-art defenses on various datasets and interpreters.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "pdf_url": "https://www.ijcai.org/proceedings/2022/0100.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack",
      "abstract": "Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \\emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixu Wang",
        "Jie Li",
        "Hong Liu",
        "Yongjian Wu",
        "Rongrong Ji"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "pdf_url": "http://arxiv.org/pdf/2105.00623",
      "publication_date": "2021-05-03",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "title": "Enhance Model Stealing Attack via Label Refining",
      "abstract": "With machine learning models being increasingly deployed, model stealing attacks have raised an increasing interest. Extracting decision-based models is a more challenging task with the information of class similarity missing. In this paper, we propose a novel and effective model stealing method as Label Refining via Feature Distance (LRFD), to re-dig the class similarity. Specifically, since the information of class similarity can be represented by the distance between samples from different classes in the feature space, we design a soft label construction module inspired by the prototype learning, and transfer the knowledge in the soft label to the substitution model. Extensive experiments conducted on four widely-used datasets consistently demonstrate that our method yields a model with significantly greater functional similarity to the victim model.",
      "year": 2022,
      "venue": "International Conference on the Software Process",
      "authors": [
        "Yixu Wang",
        "Xianming Lin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "pdf_url": "",
      "publication_date": "2022-04-15",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
      "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "L. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "title": "Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning architectures are readily available, but obtaining the high quality labeled data for training is costly. Pre-trained models available as cloud services can be used to generate this costly labeled data, and would allow an attacker to replicate trained models, effectively stealing them. Limiting the information provided by cloud based models by omitting class probabilities has been proposed as a means of protection but significantly impacts the utility of the models. In this work, we illustrate how cloud based models can still provide useful class probability information for users, while significantly limiting the ability of an adversary to steal the model. Our defense perturbs the model's final activation layer, slightly altering the output probabilities. This forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate our defense under diverse scenarios and defense aware attacks. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or increase the number of queries required by an adversary 64 fold, all with a negligible decrease in the protected model accuracy.",
      "year": 2019,
      "venue": "2019 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 109,
      "url": "https://www.semanticscholar.org/paper/555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8834415/8844588/08844598.pdf",
      "publication_date": "2019-05-19",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f7b70da96899fdac65153e172e426c5b39d2bde5",
      "title": "Detecting Data-Free Model Stealing",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Ashley Borum",
        "James Beetham",
        "Dr. Niels Da",
        "Vitoria Lobo",
        "Dr. Mubarak Shah"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f7b70da96899fdac65153e172e426c5b39d2bde5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "89571d0a03b694c9f5470892a7fc5d980d793518",
      "title": "SeInspect: Defending Model Stealing via Heterogeneous Semantic Inspection",
      "abstract": null,
      "year": 2022,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Xinjing Liu",
        "Zhuo Ma",
        "Yang Liu",
        "Zhan Qin",
        "Junwei Zhang",
        "Zhuzhu Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/89571d0a03b694c9f5470892a7fc5d980d793518",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model",
      "abstract": "Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Wenhong Zhu",
        "Zhiwei He",
        "Xiaofeng Wang",
        "Pengfei Liu",
        "Rui Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e536b831cf1053d2c523d2e16656ad27c7369f2",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9fef6937c39de73fdc3531790e938274caa84de0",
      "title": "AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks",
      "abstract": "With the proliferation of machine learning models in diverse applications, the issue of model security has increasingly become a focal point. Model steal attacks can cause significant financial losses to model owners and potentially threaten the security of their application scenarios. Traditional model steal attacks are primarily directed at soft-label black boxes, but their effectiveness significantly diminishes or even fails in hard-label scenarios. To address this, for hard-label black boxes, this study proposes an active learning-based Fusion Augmentation Model Stealing Framework (AugSteal). This framework initially utilizes large-scale irrelevant public datasets for deep filtering and feature extraction to generate reliable, diverse, and representative high-quality data subsets as the stealing dataset. Subsequently, we developed an adaptive active learning selection strategy that selects data samples with significant information gain for different black-box models, enhancing the attack\u2019s specificity and effectiveness. Finally, to further address the trade-off between query budget and steal precision, this paper designed a Fusion Augmentation training method constituted of two different loss functions, enabling the substitute model to closely approximate the decision distribution of the target black box.The comprehensive experimental results indicate that, compared to the current state-of-the-art attack methods, our approach achieved a maximum performance gain of 8.21% in functional similarity for the substitute models in simulated black-box scenarios CIFAR10, SVHN, CALTECH256, and the real-world application Tencent Cloud API.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Lijun Gao",
        "Wenjun Liu",
        "Kai Liu",
        "Jiehong Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9fef6937c39de73fdc3531790e938274caa84de0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "19375a3fdd33e4849936f1d0e7dc488d75462160",
      "title": "Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!",
      "abstract": "Large language models (LLMs) face significant copyright and intellectual property challenges as the cost of training increases and model reuse becomes prevalent. While watermarking techniques have been proposed to protect model ownership, they may not be robust to continue training and development, posing serious threats to model attribution and copyright protection. This work introduces a simple yet effective approach for robust LLM fingerprinting based on intrinsic model characteristics. We discover that the standard deviation distributions of attention parameter matrices across different layers exhibit distinctive patterns that remain stable even after extensive continued training. These parameter distribution signatures serve as robust fingerprints that can reliably identify model lineage and detect potential copyright infringement. Our experimental validation across multiple model families demonstrates the effectiveness of our method for model authentication. Notably, our investigation uncovers evidence that a recently Pangu Pro MoE model released by Huawei is derived from Qwen-2.5 14B model through upcycling techniques rather than training from scratch, highlighting potential cases of model plagiarism, copyright violation, and information fabrication. These findings underscore the critical importance of developing robust fingerprinting methods for protecting intellectual property in large-scale model development and emphasize that deliberate continued training alone is insufficient to completely obscure model origins.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Do-hyeon Yoon",
        "Minsoo Chun",
        "Thomas Allen",
        "Hans M\u00fcller",
        "Min Wang",
        "Rajesh Sharma"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/19375a3fdd33e4849936f1d0e7dc488d75462160",
      "pdf_url": "",
      "publication_date": "2025-07-02",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "title": "Muscle contraction-induced steal model in the anesthetized cat.",
      "abstract": null,
      "year": 1998,
      "venue": "Journal of pharmacological and toxicological methods",
      "authors": [
        "S. Poucher"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7e636ba98486dfa19ab8ebc986a8f0c4f03be36a",
      "pdf_url": "",
      "publication_date": "1998-11-01",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cc104c1c3daa07e298220c42eaf01dc24ff0c15a",
      "title": "Beyond Labeling Oracles: What does it mean to steal ML models?",
      "abstract": "Model extraction attacks are designed to steal trained models with only query access, as is often provided through APIs that ML-as-a-Service providers offer. Machine Learning (ML) models are expensive to train, in part because data is hard to obtain, and a primary incentive for model extraction is to acquire a model while incurring less cost than training from scratch. Literature on model extraction commonly claims or presumes that the attacker is able to save on both data acquisition and labeling costs. We thoroughly evaluate this assumption and find that the attacker often does not. This is because current attacks implicitly rely on the adversary being able to sample from the victim model's data distribution. We thoroughly research factors influencing the success of model extraction. We discover that prior knowledge of the attacker, i.e., access to in-distribution data, dominates other factors like the attack policy the adversary follows to choose which queries to make to the victim model API. Our findings urge the community to redefine the adversarial goals of ME attacks as current evaluation methods misinterpret the ME performance.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Avital Shafran",
        "Ilia Shumailov",
        "Murat A. Erdogdu",
        "Nicolas Papernot"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/cc104c1c3daa07e298220c42eaf01dc24ff0c15a",
      "pdf_url": "https://arxiv.org/pdf/2310.01959",
      "publication_date": "2023-10-03",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "title": "DeepTheft: Stealing DNN Model Architectures through Power Side Channel",
      "abstract": "Deep Neural Network (DNN) models are often deployed in resource-sharing clouds as Machine Learning as a Service (MLaaS) to provide inference services. To steal model architectures that are of valuable intellectual properties, a class of attacks has been proposed via different side-channel leakage, posing a serious security challenge to MLaaS.Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to accurately recover complex DNN model architectures on general processors via the RAPL (Running Average Power Limit)-based power side channel. While unprivileged access to the RAPL has been disabled in bare-metal OSes, we observe that the RAPL is still legitimately accessible in a platform as a service, e.g., the latest docker environment of version 20.10.18 used in this work. However, an attacker can acquire only a low sampling rate (1 KHz) of the time-series energy traces from the RAPL interface, rendering existing techniques ineffective in stealing large and deep DNN models. To this end, we design a novel and generic learning-based framework consisting of a set of meta-models, based on which DeepTheft is demonstrated to have high accuracy in recovering a large number (thousands) of models architectures from different model families including the deepest ResNet152. Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75% in recovering network structures, and a weighted average F1 score of 99.60% in recovering diverse layer-wise hyperparameters. Besides, our proposed learning framework is general to other time-series side-channel signals. To validate its generalization, another existing side channel is exploited, i.e., CPU frequency. Different from RAPL, CPU frequency is accessible to unprivileged users in bare-metal OSes. By using our generic learning framework trained against CPU frequency traces, DeepTheft has shown similarly high attack performance in stealing model architectures.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yansong Gao",
        "Huming Qiu",
        "Zhi Zhang",
        "Binghui Wang",
        "Hua Ma",
        "A. Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Surya Nepal"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "pdf_url": "https://arxiv.org/pdf/2309.11894",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3886b3c22675da54b0d55a5bc16754e3399c979",
      "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations",
      "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
      "year": 2022,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Yongjie Wang",
        "Hangwei Qian",
        "C. Miao"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/f3886b3c22675da54b0d55a5bc16754e3399c979",
      "pdf_url": "https://arxiv.org/pdf/2205.06504",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
      "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
      "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Adam Dziedzic",
        "Muhammad Ahmad Kaleem",
        "Y. Lu",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/2ee4127a2a6aab51a03305d8a564693181bc6424",
      "pdf_url": "",
      "publication_date": "2022-01-23",
      "keywords_matched": [
        "steal model",
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "703c13bd668334c30ab821cedd1bb1a60c77286d",
      "title": "Even Constrained Governments Steal: The Domestic Politics of Transfer and Expropriation Risks",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "Benjamin A. T. Graham",
        "Noel Johnston",
        "Allison F. Kingsley"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/703c13bd668334c30ab821cedd1bb1a60c77286d",
      "pdf_url": "",
      "publication_date": "2015-03-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3813c369accb4b87b5882eec943511fe2e8b4767",
      "title": "Killing Two Birds with One Stone: Stealing Model and Inferring Attribute from BERT-based APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Lingjuan Lyu",
        "Xuanli He",
        "Fangzhao Wu",
        "Lichao Sun"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/3813c369accb4b87b5882eec943511fe2e8b4767",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a58e101142efdd9dd653b41d504152e940096be",
      "title": "Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks",
      "abstract": "An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "R. He"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/0a58e101142efdd9dd653b41d504152e940096be",
      "pdf_url": "https://arxiv.org/pdf/2210.15427",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "title": "Timing shift-based bi-residual network model for the detection of electricity stealing",
      "abstract": "With the increasing number of electricity stealing users, the interests of countries are jeopardized and it brings economic burden to the government. However, due to the small-scale stealing and its random time coherence, it is difficult to find electricity stealing users. To solve this issue, we first generate the hybrid dataset composed of real electricity data and specific electricity stealing data. Then, we put forward the timing shift-based bi-residual network (TS-BiResNet) model. It learns the features of electricity consumption data on two aspects, i.e., shallow features and deep features, and meanwhile takes time factor into consideration. The simulation results show that TS-BiResNet model can detect electricity stealing behaviors that are small scaled and randomly coherent with time. Besides, its detection accuracy is superior to the benchmark schemes, i.e., long short-term memory (LSTM), gated recurrent unit (GRU), combined convolutional neural network and LSTM (CNN-LSTM) and Bi-ResNet.",
      "year": 2021,
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "authors": [
        "Jie Lu",
        "Jingfu Li",
        "Wenjiang Feng",
        "Yongqi Zou",
        "Juntao Zhang",
        "Yuan Li"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/03b7ef4ed1de617e408ed31ee2f9b7c0cb3e4546",
      "pdf_url": "https://asp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13634-022-00865-4",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "title": "Noise Reduction Power Stealing Detection Model Based on Self-Balanced Data Set",
      "abstract": "In recent years, various types of power theft incidents have occurred frequently, and the training of the power-stealing detection model is susceptible to the influence of the imbalanced data set and the data noise, which leads to errors in power-stealing detection. Therefore, a power-stealing detection model is proposed, which is based on Improved Conditional Generation Adversarial Network (CWGAN), Stacked Convolution Noise Reduction Autoencoder (SCDAE) and Lightweight Gradient Boosting Decision Machine (LightGBM). The model performs Generation- Adversarial operations on the original unbalanced power consumption data to achieve the balance of electricity data, and avoids the interference of the imbalanced data set on classifier training. In addition, the convolution method is used to stack the noise reduction auto-encoder to achieve dimension reduction of power consumption data, extract data features and reduce the impact of random noise. Finally, LightGBM is used for power theft detection. The experiments show that CWGAN can effectively balance the distribution of power consumption data. Comparing the detection indicators of the power-stealing model with various advanced power-stealing models on the same data set, it is finally proved that the proposed model is superior to other models in the detection of power stealing.",
      "year": 2020,
      "venue": "Energies",
      "authors": [
        "Haiqing Liu",
        "Zhi-qiang Li",
        "Yuancheng Li"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/36a2854b9a81d7944fb9e0f6b3efd3fe49f3d2f8",
      "pdf_url": "https://www.mdpi.com/1996-1073/13/7/1763/pdf",
      "publication_date": "2020-04-07",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9356a216e5ae48c0007b66687a3da47fc2bea834",
      "title": "Perturbing Inputs to Prevent Model Stealing",
      "abstract": "We show how perturbing inputs to machine learning services (ML-service) deployed in the cloud can protect against model stealing attacks. In our formulation, there is an ML-service that receives inputs from users and returns the output of the model. There is an attacker that is interested in learning the parameters of the ML-service. We use the linear and logistic regression models to illustrate how strategically adding noise to the inputs fundamentally alters the attacker\u2019s estimation problem. We show that even with infinite samples, the attacker would not be able to recover the true model parameters. We focus on characterizing the trade-off between the error in the attacker\u2019s estimate of the parameters with the error in the ML-service\u2019s output.",
      "year": 2020,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "J. Grana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9356a216e5ae48c0007b66687a3da47fc2bea834",
      "pdf_url": "https://arxiv.org/pdf/2005.05823",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "title": "A Low-Cost Image Encryption Method to Prevent Model Stealing of Deep Neural Network",
      "abstract": "Model stealing attack may happen by stealing useful data transmitted from embedded end to server end for an artificial intelligent systems. In this paper, we are interested in preventing model stealing of neural network for resource-constrained systems. We propose an Image Encryption based on Class Activation Map (IECAM) to encrypt information before transmitting in embedded end. According to class activation map, IECAM chooses certain key areas of the image to be encrypted with the purpose of reducing the model stealing risk of neural network. With partly encrypted information, IECAM can greatly reduce the time overheads of encryption/decryption in both embedded and server ends, especially for big size images. The experimental results demonstrate that our method can significantly reduce time overheads of encryption/decryption and the risk of model stealing compared with traditional methods.",
      "year": 2020,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Wei Jiang",
        "Zicheng Gong",
        "Jinyu Zhan",
        "Zhiyuan He",
        "Weijia Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "pdf_url": "",
      "publication_date": "2020-05-28",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1505861411b029330a48d59982b58f1699f6e0d3",
      "title": "Extracting model equations from experimental data",
      "abstract": null,
      "year": 2000,
      "venue": "",
      "authors": [
        "R. Friedrich",
        "S. Siegert",
        "J. Peinke",
        "S. L\u00fcck",
        "M. Siefert",
        "Michael Lindemann",
        "J. Raethjen",
        "G. Deuschl",
        "G. Pfister"
      ],
      "citation_count": 203,
      "url": "https://www.semanticscholar.org/paper/1505861411b029330a48d59982b58f1699f6e0d3",
      "pdf_url": "",
      "publication_date": "2000-06-19",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ec111a0490ce1eb557d13a8dc2273a736f5d6de",
      "title": "Extracting Model Clones from Conceptual Schemas",
      "abstract": null,
      "year": 2006,
      "venue": "",
      "authors": [
        "Evanthia Faliagka",
        "Maria Rigou",
        "S. Sirmakessis",
        "G. Tzimas"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7ec111a0490ce1eb557d13a8dc2273a736f5d6de",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "03439c544b98069f51fe9c6b72840239ae77650f",
      "title": "FedRight: An Effective Model Copyright Protection for Federated Learning",
      "abstract": "Federated learning (FL), an effective distributed machine learning framework, implements model training and meanwhile protects local data privacy. It has been applied to a broad variety of practice areas due to its great performance and appreciable profits. Who owns the model, and how to protect the copyright has become a real problem. Intuitively, the existing property rights protection methods in centralized scenarios (e.g., watermark embedding and model fingerprints) are possible solutions for FL. But they are still challenged by the distributed nature of FL in aspects of the no data sharing, parameter aggregation, and federated training settings. For the first time, we formalize the problem of copyright protection for FL, and propose FedRight to protect model copyright based on model fingerprints, i.e., extracting model features by generating adversarial examples as model fingerprints. FedRight outperforms previous works in four key aspects: (i) Validity: it extracts model features to generate transferable fingerprints to train a detector to verify the copyright of the model. (ii) Fidelity: it is with imperceptible impact on the federated training, thus promising good main task performance. (iii) Robustness: it is empirically robust against malicious attacks on copyright protection, i.e., fine-tuning, model pruning, and adaptive attacks. (iv) Black-box: it is valid in the black-box forensic scenario where only application programming interface calls to the model are available. Extensive evaluations across 3 datasets and 9 model structures demonstrate FedRight's superior fidelity, validity, and robustness.",
      "year": 2023,
      "venue": "Computers & security",
      "authors": [
        "Jinyin Chen",
        "Mingjun Li",
        "Haibin Zheng"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/03439c544b98069f51fe9c6b72840239ae77650f",
      "pdf_url": "http://arxiv.org/pdf/2303.10399",
      "publication_date": "2023-03-18",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9487d29364645c2f086387ff817ad5fd14b33c41",
      "title": "A Comprehensive Defense Framework Against Model Extraction Attacks",
      "abstract": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9487d29364645c2f086387ff817ad5fd14b33c41",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "650f6db095276ca03d03f4951587d7383ca3b39d",
      "title": "ModelGuard: Information-Theoretic Defense Against Model Extraction Attacks",
      "abstract": null,
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minxue Tang",
        "Anna Dai",
        "Louis DiValentin",
        "Aolin Ding",
        "Amin Hass",
        "Neil Zhenqiang Gong",
        "Yiran Chen",
        "Helen Li"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/650f6db095276ca03d03f4951587d7383ca3b39d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40b40e942db469663609ad6c1911cca235079434",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40b40e942db469663609ad6c1911cca235079434",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "66431dd64545057c084c9f83a87ad0eb7c609f92",
      "title": "Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors",
      "abstract": "Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.",
      "year": 2024,
      "venue": "ECML/PKDD",
      "authors": [
        "Satoru Koda",
        "I. Morikawa"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66431dd64545057c084c9f83a87ad0eb7c609f92",
      "pdf_url": "",
      "publication_date": "2024-11-19",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "title": "MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction",
      "abstract": "The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes\"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Gurve",
        "S. Behera",
        "Satyadev Ahlawat",
        "Yamuna Prasad"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "title": "DAV: An Adaptive Defense Framework for Model Extraction Attacks",
      "abstract": "Machine learning platforms offer paid APIs to enable personalized inference services. However, model extraction attacks greatly threaten their intellectual property rights. Malicious users can create query samples using proxy datasets or generative models to train a clone model. Existing defense approaches usually focus on models that return soft-labels, and cannot effectively handle extracting attacks against hard-label models. In this paper, we propose an adaptive defense framework named DAV, which consists of a malicious query detector and an adaptive perturbation mechanism. Two perturbation strategies can be selected based on the detection results and the malicious query rate within the buffer queue, including accuracy-preserving perturbation and maximum-minimum probability inverse perturbation. Comprehensive experimental results show that DAV can significantly reduce the accuracy of the clone model with little impact on the performance of the victim model and benign queries, no matter whether the returned probabilities are for soft-label or hard-label.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Peng Sui",
        "Jiapeng Zhou",
        "Yu Chen",
        "Youhuizi Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ae1157c1fba9603a98566209bc266f6b7a534629",
      "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
      "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful machine learning models through cloud-based APIs, offering scalability and ease of deployment. However, these services are vulnerable to model extraction attacks, where adversaries repeatedly query the application programming interface (API) to reconstruct a functionally similar model, compromising intellectual property and security. Despite various defense strategies being proposed, many suffer from high computational costs, limited adaptability to evolving attack techniques, and a reduction in performance for legitimate users. In this paper, we introduce a Resilient Adaptive Defense Framework for Model Extraction Attack Protection (RADEP), a multifaceted defense framework designed to counteract model extraction attacks through a multi-layered security approach. RADEP employs progressive adversarial training to enhance model resilience against extraction attempts. Malicious query detection is achieved through a combination of uncertainty quantification and behavioral pattern analysis, effectively identifying adversarial queries. Furthermore, we develop an adaptive response mechanism that dynamically modifies query outputs based on their suspicion scores, reducing the utility of stolen models. Finally, ownership verification is enforced through embedded watermarking and backdoor triggers, enabling reliable identification of unauthorized model use. Experimental evaluations demonstrate that RADEP significantly reduces extraction success rates while maintaining high detection accuracy with minimal impact on legitimate queries. Extensive experiments show that RADEP effectively defends against model extraction attacks and remains resilient even against adaptive adversaries, making it a reliable security framework for MLaaS models.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Amit Chakraborty",
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae1157c1fba9603a98566209bc266f6b7a534629",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "08e1f102344bc341e8109a5c23f78093a4c53323",
      "title": "Protecting Object Detection Models from Model Extraction Attack via Feature Space Coverage",
      "abstract": "The model extraction attack is an attack pattern aimed at stealing well-trained machine learning models' functionality or privacy information. With the gradual popularization of AI-related technologies in daily life, various well-trained models are being deployed. As a result, these models are considered valuable assets and attractive to model extraction attackers. Currently, the academic community primarily focuses on defense for model extraction attacks in the context of classification, with little attention to the more commonly used task scenario of object detection. Therefore, we propose a detection framework targeting model extraction attacks against object detection models in this paper. The framework first locates suspicious users based on feature coverage in query traffic and uses an active verification module to confirm whether the identified suspicious users are attackers. Through experiments conducted in multiple task scenarios, we validate the effectiveness and detection efficiency of the proposed method.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Zeyu Li",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/08e1f102344bc341e8109a5c23f78093a4c53323",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a273c40cc1f1d7096efe40d62fe28befa524245c",
      "title": "Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks",
      "abstract": "The increasing prevalence of Deep Neural Networks (DNNs) in cloud-based services has led to their widespread use through various APIs. However, recent studies reveal the susceptibility of these public APIs to model extraction attacks, where adversaries attempt to create a local duplicate of the private model using data and API-generated predictions. Existing defense methods often involve perturbing prediction distributions to hinder an attacker's training goals, inadvertently affecting API utility. In this study, we extend the concept of digital watermarking to protect DNNs' APIs. We suggest embedding a watermark into the safeguarded APIs; thus, any model attempting to copy will inherently carry the watermark, allowing the defender to verify any suspicious models. We propose a simple yet effective framework to increase watermark transferability. By requiring the model to memorize the preset watermarks in the final decision layers, we significantly enhance the transferability of watermarks. Comprehensive experiments show that our proposed framework not only successfully watermarks APIs but also maintains their utility.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Ruixiang Tang",
        "Hongye Jin",
        "Mengnan Du",
        "Curtis Wigington",
        "R. Jain",
        "Xia Hu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a273c40cc1f1d7096efe40d62fe28befa524245c",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "title": "The Limits of Provable Security Against Model Extraction",
      "abstract": null,
      "year": 2022,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2d6f41bb62b116978dee9083c39ac5c17a586fa2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "253a59d979d560456c2984742466b796a983da0e",
      "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging k-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments. Our source code is available at https://github.com/LabRAI/ATOM.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhan Cheng",
        "Bolin Shen",
        "Tianming Sha",
        "Yuan Gao",
        "Shibo Li",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/253a59d979d560456c2984742466b796a983da0e",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "title": "SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries",
      "abstract": "Given black-box access to the prediction API, model extraction attacks can steal the functionality of models deployed in the cloud. In this paper, we introduce the SEAT detector, which detects black-box model extraction attacks so that the defender can terminate malicious accounts. SEAT has a similarity encoder trained by adversarial training. Using the similarity encoder, SEAT detects accounts that make queries that indicate a model extraction attack in progress and cancels these accounts. We evaluate our defense against existing model extraction attacks and against new adaptive attacks introduced in this paper. Our results show that even against adaptive attackers, SEAT increases the cost of model extraction attacks by 3.8 times to 16 times.",
      "year": 2021,
      "venue": "AISec@CCS",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3474369.3486863",
      "publication_date": "2021-11-15",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models",
      "abstract": "Model extraction attacks aim to replicate the functionality of a black-box model through query access, threatening the intellectual property (IP) of machine-learning-as-a-service (MLaaS) providers. Defending against such attacks is challenging, as it must balance efficiency, robustness, and utility preservation in the real-world scenario. Despite the recent advances, most existing defenses presume that attacker queries have out-of-distribution (OOD) samples, enabling them to detect and disrupt suspicious inputs. However, this assumption is increasingly unreliable, as modern models are trained on diverse datasets and attackers often operate under limited query budgets. As a result, the effectiveness of these defenses is significantly compromised in realistic deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel optimization problem that simultaneously preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. Our framework combines data augmentation to simulate attacker queries with an ensemble of heterogeneous distilled models to improve robustness and diversity. We further provide a tractable approximation algorithm and derive theoretical error bounds to characterize defense effectiveness. Extensive experiments across various settings validate the utility-preserving and extraction-resistant properties of our proposed defense strategy. Our code is available at https://github.com/LabRAI/MISLEADER.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xueqi Cheng",
        "Minxing Zheng",
        "Shixiang Zhu",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "title": "Model Extraction Attack and Its Countermeasure for Denoising Diffusion Implicit Models",
      "abstract": "Recently, the threat of cyber attacks against machine learning models has been increasing. Typical examples include Model Extraction Attack (MEA), which steals the functionality of a victim model by creating its clone model that has almost the same functionality. Thus, the literature has studied MEA and its defense methods, mainly focusing on image recognition models. However, no existing studies evaluate the risk of MEA on diffusion-based image generation models, despite the recent advances and widespread use of image generation AI services powered by diffusion models. In this paper, we first demonstrate the feasibility of MEA on DDIM, one of the most common diffusion-based image generation models. Then, as a countermeasure, we propose a defense method that detects clone models of DDIM. In the proposed method, we add a small number of out-of-distribution images, referred to as \u201cmarking images\u201d, to the training dataset of a victim DDIM model. This technique provides the property of occasionally generating marking images for the victim model. This property works as a watermark and is inherited by the clone models, being used as a clue for detecting them. In the results of our experiments conducted on face, fruit, and church image datasets, the proposed defense method can correctly detect all clone models without seriously degrading the usability of victim DDIM models.",
      "year": 2025,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Hayato Shoji",
        "Kazuaki Nakamura"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "title": "Augmenting Model Extraction Attacks Against Disruption-Based Defenses",
      "abstract": "Existing research has demonstrated that deep neural networks are susceptible to model extraction attacks, where an attacker can construct a substitute model with similar functionality to the victim model by querying the black-box victim model. To counter such attacks, various disruption-based defenses have been proposed. These defenses disrupt the output results of queries before returning them to potential attackers. In this paper, we propose the first defense-penetrating model extraction attack framework, aimed at breaking disruption-based defense methods. Our proposed attack framework comprises two key modules: disruption detection and disruption recovery, which can be integrated into generic model extraction attacks. Specifically, the disruption detection module uses a novel meta-learning-based algorithm to infer the defense strategy employed by the defender, by learning the key differences between the distributions of disrupted and undisrupted query results. Once the defense method is inferred, the disruption recovery module is designed to restore clean query results from the disrupted query results, using a carefully-designed generative model. We conducted extensive experiments on 5 commonly-used datasets to evaluate the effectiveness of our proposed framework. The results demonstrate that the substitute model accuracy of current model extraction attacks can be significantly improved by up to 82.42%, even when faced with four state-of-the-art model extraction defenses. Moreover, our attack approach shows promising results in penetrating unknown defenses in real-world cloud service APIs hosted by Microsoft Azure and Face++.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xueluan Gong",
        "Shuaike Li",
        "Yanjiao Chen",
        "Mingzhe Li",
        "Rubin Wei",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "15cb47e68968064996190efae5a5f875af516ac9",
      "title": "POSTER: Disappearing Ink: How Partial Model Extraction Erases Watermarks",
      "abstract": "Deep neural networks have become invaluable intellectual property in machine learning. To deter model theft, Watermarking has emerged as a prominent defense by embedding hidden \u201ctrigger sets\u201d that aid in ownership verification. However, current watermarking solutions primarily address scenarios where adversaries steal the entire model. In this paper, we reveal a critical gap: partial model extraction, where only a subset of classes is stolen, substantially degrading the watermark\u2019s reliability. We introduce two attacks, Partial Model Extraction and Partial Knowledge Distillation, which reduce watermark accuracy by up to 80% while retaining strong performance on the stolen classes. Through extensive experiments on CIFAR10 and CIFAR100 against two state-of-the-art watermarking schemes, we demonstrate the need for more robust watermarking strategies that resist partial-class theft.",
      "year": 2025,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Venkata Sai Pranav Bachina",
        "Ankit Gangwal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/15cb47e68968064996190efae5a5f875af516ac9",
      "pdf_url": "",
      "publication_date": "2025-08-24",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "645e890034126dfc08484d3509b3493e85fbd603",
      "title": "Stateful Detection of Model Extraction Attacks",
      "abstract": "Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Kanade",
        "S. Shevade"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/645e890034126dfc08484d3509b3493e85fbd603",
      "pdf_url": "",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "title": "Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs",
      "abstract": null,
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0242acadf4940cf94596b54f7fd6fb75af7bb20d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "title": "QUEEN: Query Unlearning Against Model Extraction",
      "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://github.com/MaraPapMann/QUEEN.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Bo Liu",
        "Derui Wang",
        "Wanlei Zhou",
        "Minhui Xue"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "pdf_url": "http://arxiv.org/pdf/2407.01251",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bd04e8b6d726bdfa51f4d10b9c6693075c99afee",
      "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
      "abstract": "Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call \"Rosetta Neurons\" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, Big-GAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Amil Dravid",
        "Yossi Gandelsman",
        "Alexei A. Efros",
        "Assaf Shocher"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/bd04e8b6d726bdfa51f4d10b9c6693075c99afee",
      "pdf_url": "https://arxiv.org/pdf/2306.09346",
      "publication_date": "2023-06-15",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f0c057ebc71dc8804d503495f1a35c9cfb9b86f",
      "title": "Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models",
      "abstract": "With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their original non-sparsified counterpart, and that the performance of original and sparsified model is highly correlated. Finally, all models of the model zoos and their sparsified model twins are publicly available: modelzoos.cc.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "D. Honegger",
        "Konstantin Sch\u00fcrholt",
        "Damian Borth"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/4f0c057ebc71dc8804d503495f1a35c9cfb9b86f",
      "pdf_url": "http://arxiv.org/pdf/2304.13718",
      "publication_date": "2023-04-26",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "24253eb1c425de4c4e4c9946ed1b6eb5bdc8fba9",
      "title": "Jailbreaking Attack against Multimodal Large Language Model",
      "abstract": "This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \\emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \\textbf{Warning: some content generated by language models may be offensive to some readers.}",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhenxing Niu",
        "Haodong Ren",
        "Xinbo Gao",
        "Gang Hua",
        "Rong Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/24253eb1c425de4c4e4c9946ed1b6eb5bdc8fba9",
      "pdf_url": "",
      "publication_date": "2024-02-04",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d35927e0b346ab7e3da89295c24bf35e25d81968",
      "title": "A Model Zoo on Phase Transitions in Neural Networks",
      "abstract": "Using the weights of trained Neural Network (NN) models as data modality has recently gained traction as a research field - dubbed Weight Space Learning (WSL). Multiple recent works propose WSL methods to analyze models, evaluate methods, or synthesize weights. Weight space learning methods require populations of trained models as datasets for development and evaluation. However, existing collections of models - called `model zoos'- are unstructured or follow a rudimentary definition of diversity. In parallel, work rooted in statistical physics has identified phases and phase transitions in NN models. Models are homogeneous within the same phase but qualitatively differ from one phase to another. We combine the idea of `model zoos'with phase information to create a controlled notion of diversity in populations. We introduce 12 large-scale zoos that systematically cover known phases and vary over model architecture, size, and datasets. These datasets cover different modalities, such as computer vision, natural language processing, and scientific ML. For every model, we compute loss landscape metrics and validate full coverage of the phases. With this dataset, we provide the community with a resource with a wide range of potential applications for WSL and beyond. Evidence suggests the loss landscape phase plays a role in applications such as model training, analysis, or sparsification. We demonstrate this in an exploratory study of the downstream methods like transfer learning or model weights averaging.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Konstantin Sch\u00fcrholt",
        "L\u00e9o Meynent",
        "Yefan Zhou",
        "Haiquan Lu",
        "Yaoqing Yang",
        "Damian Borth"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d35927e0b346ab7e3da89295c24bf35e25d81968",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f85a33470b57bc3f559e7ba8714c4e932625cf2f",
      "title": "Model zoo",
      "abstract": null,
      "year": 1984,
      "venue": "Nature",
      "authors": [
        "T. Beardsley"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/f85a33470b57bc3f559e7ba8714c4e932625cf2f",
      "pdf_url": "https://www.nature.com/articles/311289b0.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50f257d6f72641770133024a9a37f2c1d720165c",
      "title": "The SWISS-MODEL Repository and associated resources",
      "abstract": "SWISS-MODEL Repository (http://swissmodel.expasy.org/repository/) is a database of 3D protein structure models generated by the SWISS-MODEL homology-modelling pipeline. The aim of the SWISS-MODEL Repository is to provide access to an up-to-date collection of annotated 3D protein models generated by automated homology modelling for all sequences in Swiss-Prot and for relevant models organisms. Regular updates ensure that target coverage is complete, that models are built using the most recent sequence and template structure databases, and that improvements in the underlying modelling pipeline are fully utilised. As of September 2008, the database contains 3.4 million entries for 2.7 million different protein sequences from the UniProt database. SWISS-MODEL Repository allows the users to assess the quality of the models in the database, search for alternative template structures, and to build models interactively via SWISS-MODEL Workspace (http://swissmodel.expasy.org/workspace/). Annotation of models with functional information and cross-linking with other databases such as the Protein Model Portal (http://www.proteinmodelportal.org) of the PSI Structural Genomics Knowledge Base facilitates the navigation between protein sequence and structure resources.",
      "year": 2008,
      "venue": "Nucleic Acids Res.",
      "authors": [
        "Florian Kiefer",
        "Konstantin Arnold",
        "Michael K\u00fcnzli",
        "L. Bordoli",
        "T. Schwede"
      ],
      "citation_count": 2107,
      "url": "https://www.semanticscholar.org/paper/50f257d6f72641770133024a9a37f2c1d720165c",
      "pdf_url": "https://academic.oup.com/nar/article-pdf/37/suppl_1/D387/3272538/gkn750.pdf",
      "publication_date": "2008-10-18",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bd057e3de7a5ea76015da6f905a0f1c9b4efc2f6",
      "title": "The SWISS-MODEL Repository\u2014new features and functionality",
      "abstract": "SWISS-MODEL Repository (SMR) is a database of annotated 3D protein structure models generated by the automated SWISS-MODEL homology modeling pipeline. It currently holds >400 000 high quality models covering almost 20% of Swiss-Prot/UniProtKB entries. In this manuscript, we provide an update of features and functionalities which have been implemented recently. We address improvements in target coverage, model quality estimates, functional annotations and improved in-page visualization. We also introduce a new update concept which includes regular updates of an expanded set of core organism models and UniProtKB-based targets, complemented by user-driven on-demand update of individual models. With the new release of the modeling pipeline, SMR has implemented a REST-API and adopted an open licencing model for accessing model coordinates, thus enabling bulk download for groups of targets fostering re-use of models in other contexts. SMR can be accessed at https://swissmodel.expasy.org/repository.",
      "year": 2016,
      "venue": "Nucleic Acids Res.",
      "authors": [
        "S. Bienert",
        "A. Waterhouse",
        "T. Beer",
        "G. Tauriello",
        "G. Studer",
        "L. Bordoli",
        "T. Schwede"
      ],
      "citation_count": 1451,
      "url": "https://www.semanticscholar.org/paper/bd057e3de7a5ea76015da6f905a0f1c9b4efc2f6",
      "pdf_url": "https://academic.oup.com/nar/article-pdf/45/D1/D313/8846950/gkw1132.pdf",
      "publication_date": "2016-11-28",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f84c8e0d60ae2e835c6e9aa9936b1031cb021bd6",
      "title": "Model annotation and discovery with the Physiome Model Repository",
      "abstract": "Mathematics and Phy sics-based simulation models have the potential to help interpret and encapsulate biological phenomena in a computable and reproducible form. Similarly, comprehensive descriptions of such models help to ensure that such models are accessible, discoverable, and reusable. To this end, researchers have developed tools and standards to encode mathematical models of biological systems enabling reproducibility and reuse, tools and guidelines to facilitate semantic description of mathematical models, and repositories in which to archive, share, and discover models. Scientists can leverage these resources to investigate specific questions and hypotheses in a more efficient manner. We have comprehensively annotated a cohort of models with biological semantics. These annotated models are freely available in the Physiome Model Repository (PMR). To demonstrate the benefits of this approach, we have developed a web-based tool which enables users to discover models relevant to their work, with a particular focus on epithelial transport. Based on a semantic query, this tool will help users discover relevant models, suggesting similar or alternative models that the user may wish to explore or use. The semantic annotation and the web tool we have developed is a new contribution enabling scientists to discover relevant models in the PMR as candidates for reuse in their own scientific endeavours. This approach demonstrates how semantic web technologies and methodologies can contribute to biomedical and clinical research. The source code and links to the web tool are available at https://github.com/dewancse/model-discovery-tool",
      "year": 2018,
      "venue": "BMC Bioinformatics",
      "authors": [
        "D. Sarwar",
        "R. Kalbasi",
        "J. Gennari",
        "B. Carlson",
        "M. Neal",
        "B. Bono",
        "K. Atalag",
        "P. Hunter",
        "D. Nickerson"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/f84c8e0d60ae2e835c6e9aa9936b1031cb021bd6",
      "pdf_url": "https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-019-2987-y",
      "publication_date": "2018-12-18",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dccbe314b61685077ca90bdef05183db4ee2e6c2",
      "title": "Evolution of computational models in BioModels Database and the Physiome Model Repository",
      "abstract": "BackgroundA useful model is one that is being (re)used. The development of a successful model does not finish with its publication. During reuse, models are being modified, i.e. expanded, corrected, and refined. Even small changes in the encoding of a model can, however, significantly affect its interpretation. Our motivation for the present study is to identify changes in models and make them transparent and traceable.MethodsWe analysed 13734 models from BioModels Database and the Physiome Model Repository. For each model, we studied the frequencies and types of updates between its first and latest release. To demonstrate the impact of changes, we explored the history of a Repressilator model in BioModels Database.ResultsWe observed continuous updates in the majority of models. Surprisingly, even the early models are still being modified. We furthermore detected that many updates target annotations, which improves the information one can gain from models. To support the analysis of changes in model repositories we developed MoSt, an online tool for visualisations of changes in models. The scripts used to generate the data and figures for this study are available from GitHub github.com/binfalse/BiVeS-StatsGenerator and as a Docker image at hub.docker.com/r/binfalse/bives-statsgenerator. The website most.bio.informatik.uni-rostock.de provides interactive access to model versions and their evolutionary statistics.ConclusionThe reuse of models is still impeded by a lack of trust and documentation. A detailed and transparent documentation of all aspects of the model, including its provenance, will improve this situation. Knowledge about a model\u2019s provenance can avoid the repetition of mistakes that others already faced. More insights are gained into how the system evolves from initial findings to a profound understanding. We argue that it is the responsibility of the maintainers of model repositories to offer transparent model provenance to their users.",
      "year": 2018,
      "venue": "BMC Systems Biology",
      "authors": [
        "Martin Scharm",
        "Tom Gebhardt",
        "Vasundra Tour\u00e9",
        "Andrea Bagnacani",
        "Ali Salehzadeh-Yazdi",
        "O. Wolkenhauer",
        "Dagmar Waltemath"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/dccbe314b61685077ca90bdef05183db4ee2e6c2",
      "pdf_url": "https://doi.org/10.1186/s12918-018-0553-2",
      "publication_date": "2018-04-12",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9045a8c8dff62eee57d7e4332e9b6c8f56574bc",
      "title": "Machine learning model repository",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "A. Teredesai",
        "James Marquardt",
        "Chris James Rizzuto",
        "Tyler Hughes"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9045a8c8dff62eee57d7e4332e9b6c8f56574bc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "682bae429d9c27ba958131eb3b9394153bc5ee7d",
      "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study",
      "abstract": "Background: The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. Aims: This paper analyzes the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face. Hugging Face is the most popular repository for pretrained ML models. We aim to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. Method: We conduct the first repository mining study on the Hugging Face Hub API on carbon emissions and answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? Results: Key findings from the study include a stalled proportion of carbon emissions-reporting models, a slight decrease in reported carbon footprint on Hugging Face over the past 2 years, and a continued dominance of NLP as the main application domain reporting emissions. The study also uncovers correlations between carbon emissions and various attributes, such as model size, dataset size, ML application domains and performance metrics. Conclusions: The results emphasize the need for software measurements to improve energy reporting practices and the promotion of carbon-efficient model development within the Hugging Face community. To address this issue, we propose two classifications: one for categorizing models based on their carbon emission reporting practices and another for their carbon efficiency. With these classification proposals, we aim to encourage transparency and sustainable model development within the ML community.",
      "year": 2023,
      "venue": "International Symposium on Empirical Software Engineering and Measurement",
      "authors": [
        "Joel Casta\u00f1o",
        "Silverio Mart'inez-Fern'andez",
        "Xavier Franch",
        "J. Bogner"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/682bae429d9c27ba958131eb3b9394153bc5ee7d",
      "pdf_url": "https://arxiv.org/pdf/2305.11164",
      "publication_date": "2023-05-18",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16aae5d0dc07c04f249b1f52295f7f29eaddcbaf",
      "title": "EMFStore: a model repository for EMF models",
      "abstract": null,
      "year": 2010,
      "venue": "2010 ACM/IEEE 32nd International Conference on Software Engineering",
      "authors": [
        "M. K\u00f6gel",
        "Jonas Helming"
      ],
      "citation_count": 123,
      "url": "https://www.semanticscholar.org/paper/16aae5d0dc07c04f249b1f52295f7f29eaddcbaf",
      "pdf_url": "",
      "publication_date": "2010-05-01",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fb5a868d6ac7de83fec15bab244e29dff22c09e2",
      "title": "Large Scale Protein Modeling and Model Repository",
      "abstract": null,
      "year": 1997,
      "venue": "Intelligent Systems in Molecular Biology",
      "authors": [
        "M. Peitsch"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/fb5a868d6ac7de83fec15bab244e29dff22c09e2",
      "pdf_url": "",
      "publication_date": "1997-06-21",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c038cfcc5110a44b97074a247f7eb6e608c7c118",
      "title": "The Model Repository of the Models of Infectious Disease Agent Study",
      "abstract": "The model repository (MREP) is a relational database management system (RDBMS) developed under the auspices of models of infectious disease agent study (MIDAS). The purpose of the MREP is to organize and catalog the models, results, and suggestions for using the MIDAS and to store them in a way to allow users to run models from an access-controlled disease MREP. The MREP contains source and object code of disease models developed by infectious disease modelers and tested in a production environment. Different versions of models used to describe various aspects of the same disease are housed in the repository. Models are linked to their developers and different versions of the codes are tied to Subversion, a version control tool. An additional element of the MREP will be to house, manage, and control access to a disease model results warehouse, which consists of output generated by the models contained in the MREP. The result tables and files are linked to the version of the model and the input parameters that collectively generated the results. The result tables are warehoused in a relational database that permits them to be easily identified, categorized, and downloaded.",
      "year": 2008,
      "venue": "IEEE Transactions on Information Technology in Biomedicine",
      "authors": [
        "P. Cooley",
        "D. Roberts",
        "V. Bakalov",
        "S. Bikmal",
        "S. Cantor",
        "T. Costandine",
        "L. Ganapathi",
        "B. Golla",
        "G. Grubbs",
        "C. Hollingsworth",
        "Sheping Li",
        "Ying Qin",
        "William Savage",
        "D. Simoni",
        "E. Solano",
        "D. Wagener"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c038cfcc5110a44b97074a247f7eb6e608c7c118",
      "pdf_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2741407",
      "publication_date": "2008-07-01",
      "keywords_matched": [
        "model repository"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "87fda70af42de7b1ae6776c2055f3a84d8c65037",
      "title": "GDALR: An Efficient Model Duplication Attack on Black Box Machine Learning Models",
      "abstract": "Trained Machine learning models are core components of proprietary products. Business models are entirely built around these ML powered products. Such products are either delivered as a software package (containing the trained model) or they are deployed on cloud with restricted API access for prediction. In ML-as-a-service, users are charged per-query or per-hour basis, generating revenue for businesses. Models deployed on cloud could be vulnerable to Model Duplication attacks. Researchers found ways to exploit these services and clone the functionalities of black box models hidden in the cloud by continuously querying the provided APIs. After successful execution of attack, the attacker does not require to pay the cloud service provider. Worst case scenario, attackers can also sell the cloned model or use them in their business model.Traditionally attackers use convex optimization algorithm like Gradient Descent with appropriate hyper-parameters to train their models. In our research we propose a modification to traditional approach called as GDALR (Gradient Driven Adaptive Learning Rate) that dynamically updates the learning rate based on the gradient values. This results in stealing the target model in comparatively less number of epochs, decreasing the time and cost, hence increasing the efficiency of the attack. This shows that sophisticated attacks can be launched for stealing the black box machine learning models which increases risk for MLaaS based businesses.",
      "year": 2019,
      "venue": "2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN)",
      "authors": [
        "Nikhil Joshi",
        "Rewanth Tammana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/87fda70af42de7b1ae6776c2055f3a84d8c65037",
      "pdf_url": "",
      "publication_date": "2019-03-01",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "81c359a570b6d7de48aecd0989c496e0ad6fee73",
      "title": "Model Provenance via Model DNA",
      "abstract": "Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.",
      "year": 2023,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Xin Mu",
        "Yu Wang",
        "Yehong Zhang",
        "Jiaqi Zhang",
        "Haibo Wang",
        "Yang Xiang",
        "Yue Yu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/81c359a570b6d7de48aecd0989c496e0ad6fee73",
      "pdf_url": "https://arxiv.org/pdf/2308.02121",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e2691aa28954a7cf890cfa27e90f69622e45efa1",
      "title": "Model Provenance Testing for Large Language Models",
      "abstract": "Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ivica Nikoli\u0107",
        "Teodora Baluta",
        "Prateek Saxena"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e2691aa28954a7cf890cfa27e90f69622e45efa1",
      "pdf_url": "",
      "publication_date": "2025-02-02",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "58f2f5e1611578180be89f39ec38476758221f1a",
      "title": "The Tiny Java Library for Maintaining Model Provenance",
      "abstract": "We present a small library for maintaining the provenance of objects in a software model called The Tiny Java Library for Maintaining Model Provenance (TJLP). A unique characteristic of the library is that it may be applied to existing software models with minimal modification. The library allows the software developer to introduce the ability to move back (undo) and forward (redo) through an object\u2019s instance history with minor alteration of existing code. The requirement is that the model implements the Model interface. Finally, methods that are considered critical in the object\u2019s provenance are adorned with an Undoable annotation. The code necessary to maintain the object\u2019s history is automatically inserted into the critical, undoable-method bytecode when the class definition is loaded by an extended class loader. The states of the model objects are preserved both in memory and on disk to accommodate various computer system configurations. The library performs well for small to medium size models using the default settings, but it may be customized in order to perform better with larger models \u2013 especially if the model size approaches the RAM of the underlying computer system.",
      "year": 2018,
      "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference",
      "authors": [
        "M. Royer",
        "S. Chawathe"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/58f2f5e1611578180be89f39ec38476758221f1a",
      "pdf_url": "",
      "publication_date": "2018-11-01",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2f4a119883e599f3202a483453250d7874c3c322",
      "title": "An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry",
      "abstract": "Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Wenxin Jiang",
        "Nicholas M. Synovic",
        "Matt Hyatt",
        "Taylor R. Schorlemmer",
        "R. Sethi",
        "Yung-Hsiang Lu",
        "G. Thiruvathukal",
        "James C. Davis"
      ],
      "citation_count": 85,
      "url": "https://www.semanticscholar.org/paper/2f4a119883e599f3202a483453250d7874c3c322",
      "pdf_url": "https://arxiv.org/pdf/2303.02552",
      "publication_date": "2023-03-05",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4702c10903630c366dd4a1e0b5de78720dbc5365",
      "title": "Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns",
      "abstract": "Simulation experiments are typically conducted repeatedly during the model development process, for example, to revalidate if a behavioral property still holds after several model changes. Approaches for automatically reusing and generating simulation experiments can support modelers in conducting simulation studies in a more systematic and effective manner. They rely on explicit experiment specifications and, so far, on user interaction for initiating the reuse. Thereby, they are constrained to support the reuse of simulation experiments in a specific setting. Our approach now goes one step further by automatically identifying and adapting the experiments to be reused for a variety of scenarios. To achieve this, we exploit provenance graphs of simulation studies, which provide valuable information about the previous modeling and experimenting activities, and contain meta-information about the different entities that were used or produced during the simulation study. We define provenance patterns and associate them with a semantics, which allows us to interpret the different activities and construct transformation rules for provenance graphs. Our approach is implemented in a Reuse and Adapt framework for Simulation Experiments (RASE), which can interface with various modeling and simulation tools. In the case studies, we demonstrate the utility of our framework for (1) the repeated sensitivity analysis of an agent-based model of migration routes and (2) the cross-validation of two models of a cell signaling pathway.",
      "year": 2021,
      "venue": "ACM Transactions on Modeling and Computer Simulation",
      "authors": [
        "Pia Wilsdorf",
        "A. Wolpers",
        "Jason Hilton",
        "Fiete Haack",
        "A. Uhrmacher"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4702c10903630c366dd4a1e0b5de78720dbc5365",
      "pdf_url": "https://eprints.soton.ac.uk/470891/1/manuscript.pdf",
      "publication_date": "2021-09-14",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9863e1cb5ab60b0ad2892b0d003ffb2308ff187",
      "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
      "abstract": "This paper investigates the impact of model compression on the way Large Language Models (LLMs) process prompts, particularly concerning jailbreak resistance. We show that moderate WANDA pruning can enhance resistance to jailbreaking attacks without fine-tuning, while maintaining performance on standard benchmarks. To systematically evaluate this safety enhancement, we introduce a dataset of 225 harmful tasks across five categories. Our analysis of LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning benefits correlate with initial model safety levels. We interpret these results by examining changes in attention patterns and perplexity shifts, demonstrating that pruned models exhibit sharper attention and increased sensitivity to artificial jailbreak constructs. We extend our evaluation to the AdvBench harmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much safer on AdvBench prompts than on our dataset when evaluated with manual jailbreak attempts, and that pruning is effective against both automated attacks and manual jailbreaking on Advbench.",
      "year": 2024,
      "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
      "authors": [
        "Adib Hasan",
        "Ileana Rugina",
        "Alex Wang"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/f9863e1cb5ab60b0ad2892b0d003ffb2308ff187",
      "pdf_url": "",
      "publication_date": "2024-01-19",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1de2dcb5de694920f50f000a3795eb0ca54d57ab",
      "title": "LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model",
      "abstract": "It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "M. A. Shah",
        "Roshan Sharma",
        "Hira Dhamyal",
        "R. Olivier",
        "Ankit Shah",
        "Joseph Konan",
        "Dareen Alharthi",
        "Hazim T. Bukhari",
        "Massa Baali",
        "Soham Deshmukh",
        "Michael Kuhlmann",
        "Bhiksha Raj",
        "Rita Singh"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/1de2dcb5de694920f50f000a3795eb0ca54d57ab",
      "pdf_url": "https://arxiv.org/pdf/2310.04445",
      "publication_date": "2023-10-02",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "56c208850c4ef8ea95aa87a5d2d6157ba174fbb5",
      "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models",
      "abstract": "The advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Ge Luo",
        "Junqiang Huang",
        "Manman Zhang",
        "Zhenxing Qian",
        "Sheng Li",
        "Xinpeng Zhang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/56c208850c4ef8ea95aa87a5d2d6157ba174fbb5",
      "pdf_url": "",
      "publication_date": "2023-11-22",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5cd89738f05e75927a54794cfb029f6e4cf902f3",
      "title": "Benchmarks in antimicrobial peptide prediction are biased due to the selection of negative data",
      "abstract": "Abstract Antimicrobial peptides (AMPs) are a heterogeneous group of short polypeptides that target not only microorganisms but also viruses and cancer cells. Due to their lower selection for resistance compared with traditional antibiotics, AMPs have been attracting the ever-growing attention from researchers, including bioinformaticians. Machine learning represents the most cost-effective method for novel AMP discovery and consequently many computational tools for AMP prediction have been recently developed. In this article, we investigate the impact of negative data sampling on model performance and benchmarking. We generated 660 predictive models using 12 machine learning architectures, a single positive data set and 11 negative data sampling methods; the architectures and methods were defined on the basis of published AMP prediction software. Our results clearly indicate that similar training and benchmark data set, i.e. produced by the same or a similar negative data sampling method, positively affect model performance. Consequently, all the benchmark analyses that have been performed for AMP prediction models are significantly biased and, moreover, we do not know which model is the most accurate. To provide researchers with reliable information about the performance of AMP predictors, we also created a web server AMPBenchmark for fair model benchmarking. AMPBenchmark is available at http://BioGenies.info/AMPBenchmark.",
      "year": 2022,
      "venue": "Briefings Bioinform.",
      "authors": [
        "Katarzyna Sidorczuk",
        "Przemys\u0142aw Gagat",
        "Filip Pietluch",
        "Jakub Ka\u0142a",
        "D. Rafacz",
        "Laura B\u0105ka\u0142a",
        "Jadwiga S\u0142owik",
        "Rafa\u0142 Kolenda",
        "S. R\u00f6diger",
        "Legana C H W Fingerhut",
        "I. Cooke",
        "P. Mackiewicz",
        "Micha\u0142 Burdukiewicz"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/5cd89738f05e75927a54794cfb029f6e4cf902f3",
      "pdf_url": "https://academic.oup.com/bib/article-pdf/23/5/bbac343/45935232/bbac343.pdf",
      "publication_date": "2022-08-21",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}