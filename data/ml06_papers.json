{
  "updated": "2026-01-06",
  "total": 27,
  "owasp_id": "ML06",
  "owasp_name": "AI Supply Chain Attacks",
  "description": "Attacks on the machine learning supply chain, including pre-trained models,\n        model repositories, ML pipelines, and dependencies. This includes trojaned\n        pre-trained models, malicious model hubs, compromised ML libraries, and\n        attacks on the infrastructure used to develop and deploy ML systems.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "7044d075fba8c188f716a25618d522c808a67a96",
      "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
      "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7044d075fba8c188f716a25618d522c808a67a96",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah",
        "My T. Thai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1d358a789c5e73b17c6aa4541d386d4cda4ab8d0",
      "title": "Security Risks in AI Accelerators: Detecting RTL Vulnerabilities to Model Theft with Formal Verification",
      "abstract": null,
      "year": 2025,
      "venue": "IEEE European Test Symposium",
      "authors": [
        "Mohamed Shelkamy Ali",
        "Lucas Deutschmann",
        "Johannes M\u00fcller",
        "Anna Lena Duque Ant\u00f3n",
        "M. R. Fadiheh",
        "D. Stoffel",
        "Wolfgang Kunz"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1d358a789c5e73b17c6aa4541d386d4cda4ab8d0",
      "pdf_url": "",
      "publication_date": "2025-05-26",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cb626a77f1e0c634d557ca88af22547f21f16afa",
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and interactions between entities, is growing in scale and complexity. As a result, training state-of-the-art graph machine learning (GML) models have become increasingly resource-intensive, turning these models and data into invaluable Intellectual Property (IP). To address the resource-intensive nature of model training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an efficient solution by leveraging third-party cloud services for model development and management. However, deploying such models in GMLaaS also exposes them to potential threats from attackers. Specifically, while the APIs within a GMLaaS system provide interfaces for users to query the model and receive outputs, they also allow attackers to exploit and steal model functionalities or sensitive training data, posing severe threats to the safety of these GML models and the underlying graph data. To address these challenges, this survey systematically introduces the first taxonomy of threats and defenses at the level of both GML model and graph-structured data. Such a tailored taxonomy facilitates an in-depth understanding of GML IP protection. Furthermore, we present a systematic evaluation framework to assess the effectiveness of IP protection methods, introduce a curated set of benchmark datasets across various domains, and discuss their application scopes and future challenges. Finally, we establish an open-sourced versatile library named PyGIP, which evaluates various attack and defense techniques in GMLaaS scenarios and facilitates the implementation of existing benchmark methods. The library resource can be accessed at: https://labrai.github.io/PyGIP. We believe this survey will play a fundamental role in intellectual property protection for GML and provide practical recipes for the GML community.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cb626a77f1e0c634d557ca88af22547f21f16afa",
      "pdf_url": "",
      "publication_date": "2025-08-27",
      "keywords_matched": [
        "steal model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51f750009d83c9112509bed3946077b032d78ccc",
      "title": "Measuring the Vulnerability Disclosure Policies of AI Vendors",
      "abstract": "As AI is increasingly integrated into products and critical systems, researchers are paying greater attention to identifying related vulnerabilities. Effective remediation depends on whether vendors are willing to accept and respond to AI vulnerability reports. In this paper, we examine the disclosure policies of 264 AI vendors. Using a mixed-methods approach, our quantitative analysis finds that 36% of vendors provide no disclosure channel, and only 18% explicitly mention AI-related risks. Vulnerabilities involving data access, authorization, and model extraction are generally considered in-scope, while jailbreaking and hallucination are frequently excluded. Through qualitative analysis, we further identify three vendor postures toward AI vulnerabilities - proactive clarification (n = 46, include active supporters, AI integrationists, and back channels), silence (n = 115, include self-hosted and hosted vendors), and restrictive (n = 103). Finally, by comparing vendor policies against 1,130 AI incidents and 359 academic publications, we show that bug bounty policy evolution has lagged behind both academic research and real-world events.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yangheran Piao",
        "Jingjie Li",
        "Daniel W. Woods"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/51f750009d83c9112509bed3946077b032d78ccc",
      "pdf_url": "",
      "publication_date": "2025-09-07",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "title": "Towards Functional Safety of Neural Network Hardware Accelerators: Concurrent Out-of-Distribution Detection in Hardware Using Power Side-Channel Analysis",
      "abstract": "For AI hardware, functional safety is crucial, especially for neural network (NN) accelerators used in safety-critical systems. A key requirement for maintaining this safety is the precise detection of out-of-distribution (OOD) instances, which are inputs significantly distinct from the training data. Neglecting to integrate robust OOD detection may result in possible safety hazards, diminished performance, and inaccurate decision-making within NN applications. Existing methods for OOD detection have been explored for full-precision models. However, the evaluation of methods on quantized neural network (QNN), which are often deployed on hardware accelerators such as FPGAs, and on-device hardware realization of concurrent OOD detection (COD) is missing in literature. In this paper, we provide a novel approach to OOD detection for NN FPGA accelerators using power measurements. Utilizing the power side-channel through digital voltage sensors allows on-device OOD detection in a non-intrusive and concurrent manner, without relying on explicit labels or modifications to the underlying NN. Furthermore, our method allows OOD detection before the inference finishes. Additionally to the evaluation, we provide an efficient hardware implementation of COD on an actual FPGA.",
      "year": 2025,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Mahboobe Sadeghipourrudsari",
        "Dennis R. E. Gnad",
        "Mehdi B. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f32ac891e623a3c000bcee8e36f351f1ccfa2707",
      "pdf_url": "",
      "publication_date": "2025-01-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-21"
    },
    {
      "paper_id": "f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "title": "Digital Scapegoat: An Incentive Deception Model for Resisting Unknown APT Stealing Attacks on Critical Data Resource",
      "abstract": "It is a challenging problem to resist unknown advanced persistent threats (APTs) on stealing data resources in an information system of critical infrastructures, because APT attackers have very specific objectives and compromise the system stealthily and slowly. We observe that it is a necessary condition for APT attackers to achieve their campaigns via controlling unknown Trojans to access and exfiltrate critical files. We present a theoretical model called Digital Scapegoat (abbreviated as DS-IDep) that constructs an Incentive Deception defense schema to hijack the attacker\u2019s access to critical files and redirect it to avatar files without awareness. We propose a FlipIDep Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{F}$ </tex-math></inline-formula>) and a Markov Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{M}$ </tex-math></inline-formula>) to characterize completely the payoffs, equilibria, and best strategies from the perspective of the attacker and the defender respectively. We also design an exponential risk propagation model to evaluate the ability of DS-IDep to eliminate stealing impact when the risk is propagated between states. Theoretically, we can achieve the objective of stealing impact elimination (<inline-formula> <tex-math notation=\"LaTeX\">$L_{K} \\lt 0.001$ </tex-math></inline-formula>) when the ratio of incentive deception exceeds 0.7 (<inline-formula> <tex-math notation=\"LaTeX\">$\\eta \\gt 0.7$ </tex-math></inline-formula>) and the probability of an attack operation bypassing the defense surface is less than 0.1 (<inline-formula> <tex-math notation=\"LaTeX\">$r^{*}\\times \\mu \\lt 0.1$ </tex-math></inline-formula>) under Stackelberg strategies. We develop a kernel-level incentive deception defense surface according to the theoretical parameters of the DS-IDep. The experimental results show that DS-IDep can resist APT stealing attacks from unknown Trojans. We also evaluate the DS-IDep in five well-known software applications. It demonstrates that DS-IDep can address unknown attacks from compromised software with less than 10% performance overhead.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaochun Yun",
        "Guangjun Wu",
        "Shuhao Li",
        "Qi Song",
        "Zixian Tang",
        "Zhenyu Cheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45475dcf398e58811ac8487fbd72fe6777bd8676",
      "title": "Quantum Disturbance-based Photon Cloning Attack",
      "abstract": "Geopolitical concerns in Asia have intensified, driving countries to bolster their border security and upgrade their military capabilities. With new and complex threats emerging, governments must invest in advanced defense technologies to maintain strategic stability and deter large-scale conflicts. The deployment and development of advanced military technologies is one of the key focuses. Surveillance largely contributes to situational awareness, fortifying defense and response time, making it the perfect tool for drones, AI-driven surveillance systems, cyber warfare capabilities, and anti-ship missiles. Such technologies enable countries to detect, prevent, and counter threats efficiently, thus making them essential in 21st-century warfare. In addition to the arms race in conventional defense, we're seeing increasingly base forms of international competition, particularly around advanced technologies like space-based reconnaissance, quantum encryption, and hypersonic weapons.Additionally, employing features such as AI surveillance systems, smart defense systems, and civil forces will be needed to stop wars before they happen. Governments must ensure they are attending to the urgent and game-changing elements, such as early-warning systems, predictive analytics, and automated threat response technologies that make for speedy and effective crisis management. It\u2019s critical to tighten cybersecurity infrastructure, as cyberattacks against military and governmental networks are increasing. Modernization efforts for an agile and resilient military force require substantial investment in R&D and defense infrastructure. New developments in autonomous defense systems, robotics, and advanced missile systems are key pieces in deterring aggression and protecting national interests.",
      "year": 2025,
      "venue": "International Conference on Innovative Mechanisms for Industry Applications",
      "authors": [
        "Hui-Kai Su",
        "K.MahaRajan",
        "Sanmugasundaram R",
        "M.Jayalakshmi",
        "A. S. Nantha",
        "Wen-Kai Kuo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/45475dcf398e58811ac8487fbd72fe6777bd8676",
      "pdf_url": "",
      "publication_date": "2025-09-03",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "title": "Security Challenges and Mitigation Strategies in Generative AI Systems",
      "abstract": "This article examines the critical security challenges and mitigation strategies in generative AI systems. The article explores how these systems have transformed various sectors, particularly in financial markets and critical infrastructure, while introducing significant security concerns. The article analyzes various types of adversarial attacks, including input perturbation and backdoor attacks, and their impact on AI model performance. Additionally, it investigates model stealing threats and data privacy concerns in AI deployments. The article presents comprehensive mitigation strategies, including advanced defense mechanisms, enhanced protection frameworks, and secure access control implementations. The article findings demonstrate the effectiveness of integrated security approaches in protecting AI systems while maintaining operational efficiency. This article contributes to the growing body of knowledge on AI security by providing evidence-based strategies for protecting generative AI systems across different application domains.",
      "year": 2025,
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "authors": [
        "Satya Naga Mallika Pothukuchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "pdf_url": "https://doi.org/10.32628/cseit25112377",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "title": "Activation Functions Considered Harmful: Recovering Neural Network Weights through Controlled Channels",
      "abstract": "With high-stakes machine learning applications increasingly moving to untrusted end-user or cloud environments, safeguarding pre-trained model parameters becomes essential for protecting intellectual property and user privacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX, hold the promise to secure the internal state of machine learning applications even against compromised operating systems. However, we show that privileged software adversaries can exploit input-dependent memory access patterns in common neural network activation functions to extract secret weights and biases from an SGX enclave. Our attack leverages the SGX-Step framework to obtain a noise-free, instruction-granular page-access trace. In a case study of an 11-input regression network using the Tensorflow Microlite library, we demonstrate complete recovery of all first-layer weights and biases, as well as partial recovery of parameters from deeper layers under specific conditions. Our novel attack technique requires only 20 queries per input per weight to obtain all first-layer weights and biases with an average absolute error of less than 1%, improving over prior model stealing attacks. Additionally, a broader ecosystem analysis reveals the widespread use of activation functions with input-dependent memory access patterns in popular machine learning frameworks (either directly or via underlying math libraries). Our findings highlight the limitations of deploying confidential models in SGX enclaves and emphasise the need for stricter side-channel validation of machine learning implementations, akin to the vetting efforts applied to secure cryptographic libraries.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jesse Spielman",
        "David Oswald",
        "Mark Ryan",
        "Jo Van Bulck"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "pdf_url": "",
      "publication_date": "2025-03-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7f8163935429700fed6c5ff88417726cb4ea4884",
      "title": "Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods",
      "abstract": "As quantum computing advances, quantum circuit simulators serve as critical tools to bridge the current gap caused by limited quantum hardware availability. These simulators are typically deployed on cloud platforms, where users submit proprietary circuit designs for simulation. In this work, we demonstrate a novel timing side-channel attack targeting cloud- based quantum simulators. A co-located malicious process can observe fine-grained execution timing patterns to extract sensitive information about concurrently running quantum circuits. We systematically analyze simulator behavior using the QASMBench benchmark suite, profiling timing and memory characteristics across various circuit executions. Our experimental results show that timing profiles exhibit circuit-dependent patterns that can be effectively classified using pattern recognition techniques, enabling the adversary to infer circuit identities and compromise user confidentiality. We were able to achieve 88% to 99.9% identification rate of quantum circuits based on different datasets. This work highlights previously unexplored security risks in quantum simulation environments and calls for stronger isolation mechanisms to protect user workloads",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ben Dong",
        "Hui Feng",
        "Qian Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7f8163935429700fed6c5ff88417726cb4ea4884",
      "pdf_url": "",
      "publication_date": "2025-09-16",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "abstract": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "year": 2025,
      "venue": "",
      "authors": [
        "A. Foundjem",
        "L. Tidjon",
        "L. D. Silva",
        "Foutse Khomh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-31"
    },
    {
      "paper_id": "a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "title": "Trained to Leak: Hiding Trojan Side-Channels in Neural Network Weights",
      "abstract": "Applications driven by neural networks (NNs) have been advancing various work flows in industries and everyday life. FPGA accelerators are a popular low latency solution for NN inference in the cloud, edge devices and critical systems, offering efficiency and availability. Additionally, cloud FPGAs enable maximizing resource utilization by sharing one device with multiple users in a multi-tenant scenario. However, due to the high energy costs, hardware requirements and time consumption for training an NN, using machine learning services or acquiring pre-trained models has become increasingly popular. This creates a trust issue that potentially puts the privacy of the user at risk. Specifically, malicious mechanisms may be hidden in the weights of the NN. We show that by manipulating the training process of an NN, the power consumption and resulting leakage can be manipulated to correlate strongly with the networks output, allowing the reliable recovery of the classification results through remote power side-channel analysis. In comparison to power traces from a benign model, which leak less information, our trained-in Trojan Side-Channel enhances the credibility and reliability of the stolen outputs, making them more usable and valuable for malicious intent.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Vincent Meyers",
        "Michael Hefenbrock",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a55cc1ebaab3de336386f292cc4028f6e5586ac0",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "title": "Protecting Confidential Virtual Machines from Hardware Performance Counter Side Channels",
      "abstract": "In modern cloud platforms, it is becoming more important to preserve the privacy of guest virtual machines (VMs) from the untrusted host. To this end, Secure Encrypted Virtualization (SEV) is developed as a hardware extension to protect VMs by encrypting their memory pages and register states. Unfortunately, such confidential VMs are still vulnerable to micro-architectural side channels, and Hardware Performance Counters (HPCs) are a prominent information leakage source. To make matters worse, currently there is no systematic defense against the HPC side channels. We introduce Aegis, a unified framework for demystifying the inherent relations between the instruction execution and HPC event statistics, and defending VMs against HPC side channels with provable privacy guarantee and minimal performance overhead. Aegis consists of three modules. Application Profiler profiles the application offline and adopts information theory to quantitatively estimate the vulnerability of HPC events. Event Fuzzer leverages the fuzzing technique to automatically generate interesting inputs, i.e., instruction sequences, that can effectively alter the HPC observations. Event Obfuscator injects noisy instructions into the protected VM based on the differential privacy mechanisms for high efficiency and privacy. We present three case studies to demonstrate that Aegis can defeat different types of HPC side-channel attacks (i.e., website fingerprinting, DNN model extraction, keystroke sniffing). Evaluations show that Aegis can effectively decrease the attack accuracy from 90% to 2%, with only 3% overhead on the application execution time and 7% overhead on the CPU usage.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Xiaoxuan Lou",
        "Kangjie Chen",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "title": "Secure AI Systems: Emerging Threats and Defense Mechanisms",
      "abstract": "The capability of artificial intelligence (AI), increasingly embedded in critical domains, faces a complex array of security threats. It has motivated researchers to explore the security vulnerability of AI solutions and propose effective countermeasures. This article offers a comprehensive exploration of diverse attacks on AI models, including backdoors (Trojans), adversarial, fault injection, data poisoning, model inversion, model extraction, membership inference attacks, etc. These security vulnerabilities are classified into two broad categories, namely, Supply Chain Attacks and Runtime Attacks. We highlight threat models, attack strategies, and defenses to secure AI systems against these attacks. The work also underscores the significance of developing secure and robust AI models and their implementation to safeguard sensitive data and embedded systems. We present some emerging research directions on secure AI systems.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "S. Bhunia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "title": "Enhancing TinyML Security: Study of Adversarial Attack Transferability",
      "abstract": "The recent strides in artificial intelligence (AI) and machine learning (ML) have propelled the rise of TinyML, a paradigm enabling AI computations at the edge without dependence on cloud connections. While TinyML offers real-time data analysis and swift responses critical for diverse applications, its devices' intrinsic resource limitations expose them to security risks. This research delves into the adversarial vulnerabilities of AI models on resource-constrained embedded hardware, with a focus on Model Extraction and Evasion Attacks. Our findings reveal that adversarial attacks from powerful host machines could be transferred to smaller, less secure devices like ESP32 and Raspberry Pi. This illustrates that adversarial attacks could be extended to tiny devices, underscoring vulnerabilities, and emphasizing the necessity for reinforced security measures in TinyML deployments. This exploration enhances the comprehension of security challenges in TinyML and offers insights for safeguarding sensitive data and ensuring device dependability in AI-powered edge computing settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "pdf_url": "",
      "publication_date": "2024-07-16",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "20993724635d425f696e1c7e38c9e8da025ff757",
      "title": "FedMCT: A Federated Framework for Intellectual Property Protection and Malicious Client Tracking",
      "abstract": "In the era of big data, federated learning (FL) emerges as a solution to train models collectively without exposing individual data, maintaining similar accuracy to models trained on shared datasets. However, challenges arise with the advent of privacy inference attacks and model theft, posing significant threats to the privacy of FL models, especially regarding intellectual property (IP) protection. This paper introduces FedMCT (Federated Malicious Client Tracking), a novel framework addressing these challenges in the FL context. The FedMCT framework is a new approach to protect IP rights of FL clients and track cheaters, which can improve efficiency in resource-heterogeneous environments. By embedding unique watermarks or fingerprints in Deep Neural Network (DNN) models, we can protect model IP. We employ a configuration round before watermark embedding, segmenting clients based on performance for tiered model watermarking. We also propose a tiered watermarking and traitor tracking mechanism, which reduces the tracking time and ensures high traitor tracking efficiency. Extensive experiments validate our solution\u2019s efficacy in maintaining original model performance, watermark privacy, and detectability, robust against various attacks, demonstrating superior traitor tracing efficiency compared to existing frameworks.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Qianyi Chen",
        "Peijia Zheng",
        "Yusong Du",
        "Weiqi Luo",
        "Hongmei Liu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/20993724635d425f696e1c7e38c9e8da025ff757",
      "pdf_url": "",
      "publication_date": "2024-02-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "title": "Dual-Rail Precharge Logic-Based Side-Channel Countermeasure for DNN Systolic Array",
      "abstract": "Deep neural network (DNN) accelerators are widely used in cloud-edge-end and other application scenarios. Researchers recently focused on extracting secret information from DNN through side-channel attacks (SCAs), which substantially threaten AI security. In this brief, we propose a high-security, high-performance side-channel countermeasure using dual-rail precharge logic (DPL) for the DNN systolic array. By collecting and analyzing 5000 power traces, our proposed DPL-based systolic array provides a significantly lower correlation coefficient of 0.045. Through system-level side-channel security evaluation on field-programmable gate arrays (FPGAs), the DPL-based systolic array can effectively defend against weight extraction under power SCAs.",
      "year": 2024,
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiangmin Zhang",
        "Munkhbaatar Chinbat"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "title": "Securing Machine Learning: Understanding Adversarial Attacks and Bias Mitigation",
      "abstract": "This paper offers a comprehensive examination of adversarial vulnerabilities in machine learning (ML) models and strategies for mitigating fairness and bias issues. It analyses various adversarial attack vectors encompassing evasion, poisoning, model inversion, exploratory probes, and model stealing, elucidating their potential to compromise model integrity and induce misclassification or information leakage. In response, a range of defence mechanisms including adversarial training, certified defences, feature transformations, and ensemble methods are scrutinized, assessing their effectiveness and limitations in fortifying ML models against adversarial threats. Furthermore, the study explores the nuanced landscape of fairness and bias in ML, addressing societal biases, stereotypes reinforcement, and unfair treatment, proposing mitigation strategies like fairness metrics, bias auditing, de-biasing techniques, and human-in-the-loop approaches to foster fairness, transparency, and ethical AI deployment. This synthesis advocates for interdisciplinary collaboration to build resilient, fair, and trustworthy AI systems amidst the evolving technological paradigm.",
      "year": 2024,
      "venue": "International Journal of Innovative Science and Research Technology",
      "authors": [
        "Archit Lakhani",
        "Neyah Rohit"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "pdf_url": "https://doi.org/10.38124/ijisrt/ijisrt24jun1671",
      "publication_date": "2024-07-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "004764a194f3e85900a0b15d11c4a6955d6a616a",
      "title": "Robust and Minimally Invasive Watermarking for EaaS",
      "abstract": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1\\%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zongqi Wang",
        "Baoyuan Wu",
        "Jingyuan Deng",
        "Yujiu Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/004764a194f3e85900a0b15d11c4a6955d6a616a",
      "pdf_url": "",
      "publication_date": "2024-10-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21e6d0743c99e614ccff2533a591affcc3438e14",
      "title": "OML: A Primitive for Reconciling Open Access with Owner Control in AI Model Distribution",
      "abstract": "The current paradigm of AI model distribution presents a fundamental dichotomy: models are either closed and API-gated, sacrificing transparency and local execution, or openly distributed, sacrificing monetization and control. We introduce OML(Open-access, Monetizable, and Loyal AI Model Serving), a primitive that enables a new distribution paradigm where models can be freely distributed for local execution while maintaining cryptographically enforced usage authorization. We are the first to introduce and formalize this problem, introducing rigorous security definitions tailored to the unique challenge of white-box model protection: model extraction resistance and permission forgery resistance. We prove fundamental bounds on the achievability of OML properties and characterize the complete design space of potential constructions, from obfuscation-based approaches to cryptographic solutions. To demonstrate practical feasibility, we present OML 1.0, a novel OML construction leveraging AI-native model fingerprinting coupled with crypto-economic enforcement mechanisms. Through extensive theoretical analysis and empirical evaluation, we establish OML as a foundational primitive necessary for sustainable AI ecosystems. This work opens a new research direction at the intersection of cryptography, machine learning, and mechanism design, with critical implications for the future of AI distribution and governance.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Zerui Cheng",
        "Edoardo Contente",
        "Ben Finch",
        "Oleg Golev",
        "J. Hayase",
        "Andrew Miller",
        "Niusha Moshrefi",
        "Anshul Nasery",
        "Sandeep Nailwal",
        "Sewoong Oh",
        "Himanshu Tyagi",
        "P. Viswanath"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21e6d0743c99e614ccff2533a591affcc3438e14",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "92013fa050869d3deb74b3fc4ac0874ffbf1544e",
      "title": "A PSO-based Method to Test Deep Learning Library at API Level",
      "abstract": "In recent years, deep learning (DL) is widely used in various fields. DL library bugs could result in security issues and even some losses like data loss and model stealing. As a result, testing DL libraries is the focus of an increasing number of studies. However, there are still issues with these works, such as poor test sample selection and overly general test oracle, which result in ineffective and insufficient testing. In this paper, we present a LEAPI-PSO method based on particle swarm optimization (PSO) algorithm for testing DL libraries at API level, which tackles the inadequacies of existing testing techniques. LEAPI-PSO initially chooses high-quality seed samples by using input coverage and the seed progeny tree. Then, by using PSO, LEAPI-PSO generates test samples that are more likely to reveal API bugs. Based on eight mutation strategies, LEAPI-PSO can produce rich and varied test samples and input them into the API to check for bugs using test oracle. The two most popular DL libraries, PyTorch and Tensorflow, have been used in this study to validate and evaluate LEAPI-PSO. The result shows that LEAPI-PSO is capable to successfully find bugs including crashes, logical errors, and documentation errors. We report 115 bugs to the DL library developers, 95 of which are confirmed, and 63 of which are fixed.",
      "year": 2024,
      "venue": "Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering",
      "authors": [
        "Shuyan Liao",
        "Chun Shan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/92013fa050869d3deb74b3fc4ac0874ffbf1544e",
      "pdf_url": "",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "title": "Security Concerns of Machine Learning Hardware",
      "abstract": "AI-as-a-Service (AIaaS) has been emerging with model providers deploying their models on cloud and model consumers using the model. Recently, ML models are being deployed on edge devices to improve cost and response time. The widespread usage of machine learning has made the study of security in the context of Machine Learning (ML) very critical. Model extraction attacks focuses on extracting model parameters such as weights and biases which can be used to clone a ML target model deployed on the cloud or on an edge device hardware. This paper explores different types of attacks on ML models primarily focusing on model extraction attacks on ML hardware such as scan-chain and side-channel attacks. The paper present an analysis of various such attacks and their countermeasures. Possible future directions of work are also discussed.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Nilotpola Sarma",
        "E. Bhawani",
        "E. Reddy",
        "C. Karfa"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "044758b8410c8bec7a542febbb8fb613d666cd1d",
      "title": "Exploring AI Attacks on Hardware Accelerated Targets",
      "abstract": "Artificial Intelligence have become integral to various applications, ranging from image recognition to natural language processing. To meet the increasing demand for real-time and low-power AI inference Hardware accelerated Embedded Systems such as Field Programmable Gate Arrays have emerged as a popular hardware platform. However, the deployment of AI models on Embedded AI Systems introduces new security concerns. AI attacks on such Embedded AI based systems pose significant risks to the integrity, confidentiality, and availability of AI applications. In our experiment, we conducted attacks such as the Model Extraction Attack and Evasion Attack for AI on Embedded Systems. These experimental results highlights the critical importance of implementing strong security measures to protect AI models running on Hardware accelerated Embedded AI Systems, ensuring their ability to withstand potential threats.",
      "year": 2023,
      "venue": "2023 IEEE 2nd International Conference on Data, Decision and Systems (ICDDS)",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/044758b8410c8bec7a542febbb8fb613d666cd1d",
      "pdf_url": "",
      "publication_date": "2023-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "title": "NeurObfuscator: A Full-stack Obfuscation Tool to Mitigate Neural Architecture Stealing",
      "abstract": "Neural network stealing attacks have posed grave threats to neural network model deployment. Such attacks can be launched by extracting neural architecture information, such as layer sequence and dimension parameters, through leaky side-channels. To mitigate such attacks, we propose NeurObfuscator, a full-stack obfuscation tool to obfuscate the neural network architecture while preserving its functionality with very limited performance overhead. At the heart of this tool is a set of obfuscating knobs, including layer branching, layer widening, selective fusion and schedule pruning, that increase the number of operators, reduce/increase the latency, and number of cache and DRAM accesses. A genetic algorithm-based approach is adopted to orchestrate the combination of obfuscating knobs to achieve the best obfuscating effect on the layer sequence and dimension parameters so that the architecture information cannot be successfully extracted. Results on sequence obfuscation show that the proposed tool obfuscates a ResNet-18 ImageNet model to a totally different architecture (with 44 layer difference) without affecting its functionality with only 2% overall latency overhead. For dimension obfuscation, we demonstrate that an example convolution layer with 64 input and 128 output channels can be obfuscated to generate a layer with 207 input and 93 output channels with only a 2% latency overhead.",
      "year": 2021,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Jingtao Li",
        "Zhezhi He",
        "A. S. Rakin",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "pdf_url": "https://arxiv.org/pdf/2107.09789",
      "publication_date": "2021-07-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "title": "Neural Network Stealing via Meltdown",
      "abstract": "Deep learning services are now deployed in various fields on top of cloud infrastructures. In such cloud environment, virtualization technology provides logically independent and isolated computing space for each tenant. However, recent studies demonstrate that by leveraging vulnerabilities of virtualization techniques and shared processor architectures in the cloud system, various side-channels can be established between cloud tenants. In this paper, we propose a novel attack scenario that can steal internal information of deep learning models by exploiting the Meltdown vulnerability in a multitenant system environment. On the basis of our experiment, the proposed attack method could extract internal information of a TensorFlow deep learning service with 92.875% accuracy and 1.325kB/s extraction speed.",
      "year": 2021,
      "venue": "International Conference on Information Networking",
      "authors": [
        "Hoyong Jeong",
        "Dohyun Ryu",
        "Junbeom Hur"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "pdf_url": "",
      "publication_date": "2021-01-13",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "title": "Proposed Guidelines for the Responsible Use of Explainable Machine Learning.",
      "abstract": "Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and security audits of ML models. Explainable ML (i.e. explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services. However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful black-boxes, e.g. fairwashing or scaffolding, and for other malevolent purposes like stealing models and sensitive training data. To promote best-practice discussions for this already in-flight technology, this short text presents internal definitions and a few examples before covering the proposed guidelines. This text concludes with a seemingly natural argument for the use of interpretable models and explanatory, debugging, and disparate impact testing methods in life- or mission-critical ML systems.",
      "year": 2019,
      "venue": "",
      "authors": [
        "Patrick Hall",
        "Navdeep Gill",
        "N. Schmidt"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/4ea06d39ff6c8f75c3a1e12c81b1595f72effdeb",
      "pdf_url": "",
      "publication_date": "2019-06-08",
      "keywords_matched": [
        "stealing model"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}