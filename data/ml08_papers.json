{
  "updated": "2026-01-06",
  "total": 578,
  "owasp_id": "ML08",
  "owasp_name": "Model Skewing",
  "description": "Inducing bias and unfairness in ML models",
  "keywords": [
    "model stealing",
    "model extraction",
    "model theft",
    "steal model",
    "stealing model",
    "extract model",
    "model stealing attack",
    "model extraction attack",
    "neural network extraction attack",
    "knockoff nets",
    "knockoff net",
    "copycat CNN",
    "copycat model",
    "imitation attack",
    "clone model",
    "cloning attack",
    "stealing machine learning",
    "steal ML model",
    "steal ML models",
    "steal neural network",
    "DNN model stealing",
    "DNN extraction",
    "stealing deep learning",
    "LLM stealing",
    "LLM extraction",
    "stealing language model",
    "stealing functionality",
    "functionality stealing",
    "black-box model stealing",
    "blackbox model extraction",
    "model stealing defense",
    "model extraction defense",
    "prevent model stealing",
    "protect model extraction",
    "side-channel model extraction",
    "side-channel neural network",
    "timing attack neural network",
    "cache attack DNN",
    "power analysis neural network",
    "electromagnetic neural network",
    "DNN weights leakage",
    "neural network weight extraction",
    "reverse engineer neural network",
    "reverse engineering DNN",
    "cryptanalytic extraction neural",
    "API model extraction",
    "query-based model stealing",
    "prediction API stealing"
  ],
  "note": "Filtered for Model Skewing",
  "papers": [
    {
      "paper_id": "eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Y. Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/eead229e073ee9f1221144e60ee2d90a7174eaf2",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "title": "CipherSteal: Stealing Input Data from TEE-Shielded Neural Networks with Ciphertext Side Channels",
      "abstract": "Shielding neural networks (NNs) from untrusted hosts with Trusted Execution Environments (TEEs) has been increasingly adopted. Nevertheless, this paper shows that the confidentiality of NNs and user data is compromised by the recently disclosed ciphertext side channels in TEEs, which leak memory write patterns of TEE-shielded NNs to malicious hosts. While recent works have used ciphertext side channels to recover cryptographic key bits, the technique does not apply to NN inputs which are more complex and only have partial information leaked. We propose an automated input recovery framework, CipherSteal, and for the first time demonstrate the severe threat of ciphertext side channels to NN inputs. CipherSteal novelly recasts the input recovery as a two-step approach \u2014 information transformation and reconstruction \u2014 and proposes optimizations to fully utilize partial input information leaked in ciphertext side channels. We evaluate CipherSteal on diverse NNs (e.g., Transformer) and image/video inputs, and successfully recover visually identical inputs under different levels of attacker's pre-knowledge towards the target NNs and their inputs. We comprehensively evaluate two popular NN frameworks, TensorFlow and PyTorch, and NN executables generated by two recent NN compilers, TVM and Glow, and study their different attack surfaces. Moreover, we further steal the target NN's functionality by training a surrogate NN with our recovered inputs, and also leverage the surrogate NN to generate \u201cwhite-box\u201d adversarial examples, effectively manipulating the target NN's predictions.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuanyuan Yuan",
        "Zhibo Liu",
        "Sen Deng",
        "Yanzuo Chen",
        "Shuai Wang",
        "Yinqian Zhang",
        "Zhendong Su"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "37ef78b60cb09e136e13df735fc58ade8348d671",
      "title": "Finding the PISTE: Towards Understanding Privacy Leaks in Vertical Federated Learning Systems",
      "abstract": "Vertical Federated Learning (VFL) is a collaborative learning paradigm where participants share the same sample space while splitting the feature space. In VFL, local participants host their bottom models for feature extraction and collaboratively train a classifier by exchanging intermediate results with the server owning the labels. Both local training data and bottom models contain privacy-sensitive information and are considered the intellectual property of each participant, and thus should be protected by the design of VFL. Our study exposes the fundamental susceptibility of VFL systems to privacy leaks, which arise from the collaboration between the server and clients during both training and testing. Based on our findings, we propose PISTE, a model-agnostic framework of privacy stealing attacks against VFL. PISTE delivers three privacy inference attacks, i.e., model stealing, data reconstruction, and property inference attacks on five benchmark datasets and four different model architectures. We further discuss four potential countermeasures. Experimental results show that all of them cannot prevent all three privacy stealing attacks in PISTE. In summary, our study demonstrates the inherent yet rarely uncovered vulnerability of VFL on leaking data and model privacy.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiangru Xu",
        "Wei Wang",
        "Zheng Chen",
        "Bin Wang",
        "Chao Li",
        "Li Duan",
        "Zhen Han",
        "Yufei Han"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/37ef78b60cb09e136e13df735fc58ade8348d671",
      "pdf_url": "",
      "publication_date": "2025-03-01",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7044d075fba8c188f716a25618d522c808a67a96",
      "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
      "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by enabling adversaries to steal models, exposing intellectual property and training data. With the increasing deployment of machine learning models in distributed computing environments, including cloud, edge, and federated learning settings, each paradigm introduces distinct vulnerabilities and challenges. Without a unified perspective on MEAs across these distributed environments, organizations risk fragmented defenses, inadequate risk assessments, and substantial economic and privacy losses. This survey is motivated by the urgent need to understand how the unique characteristics of cloud, edge, and federated deployments shape attack vectors and defense requirements. We systematically examine the evolution of attack methodologies and defense mechanisms across these environments, demonstrating how environmental factors influence security strategies in critical sectors such as autonomous vehicles, healthcare, and financial services. By synthesizing recent advances in MEAs research and discussing the limitations of current evaluation practices, this survey provides essential insights for developing robust and adaptive defense strategies. Our comprehensive approach highlights the importance of integrating protective measures across the entire distributed computing landscape to ensure the secure deployment of machine learning models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7044d075fba8c188f716a25618d522c808a67a96",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f94fe2776e4258650baffb9b0100518076aacdad",
      "title": "Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment",
      "abstract": "Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on image classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-Steal), the first stealing attack against medical MLLMs. ADA-Steal relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yaling Shen",
        "Zhixiong Zhuang",
        "Kun Yuan",
        "Maria-Irina Nicolae",
        "N. Navab",
        "N. Padoy",
        "Mario Fritz"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f94fe2776e4258650baffb9b0100518076aacdad",
      "pdf_url": "",
      "publication_date": "2025-02-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "253a59d979d560456c2984742466b796a983da0e",
      "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging k-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments. Our source code is available at https://github.com/LabRAI/ATOM.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Zhan Cheng",
        "Bolin Shen",
        "Tianming Sha",
        "Yuan Gao",
        "Shibo Li",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/253a59d979d560456c2984742466b796a983da0e",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "title": "SIGFinger: A Subtle and Interactive GNN Fingerprinting Scheme Via Spatial Structure Inference Perturbation",
      "abstract": "There have been significant improvements in intellectual property (IP) protection for deep learning models trained on euclidean data. However, the complex and irregular graph-structured data in non-euclidean space poses a huge challenge to the IP protection of graph neural networks (GNNs). To address this issue, we propose a subtle and interactive GNN fingerprinting scheme through spatial structure inference perturbation, which captures the stable coordination patterns of fingerprint to guarantee the reliability of copyright verification. Specifically, the data augmentation based on adaptive graph diffusion is first exploited to generate more samples, which enables the exploration of fingerprint information from coarse to fine. Subsequently, the graph-structured data are manipulated by multi-constrained spectral clustering to analyze intrinsic and extrinsic structure correlations in a causal inference manner. Ultimately, the cycle-consistent statistical optimization is performed to determine the copyright of GNN models from both intra-graph and inter-graph perspectives. Extensive experiments show that our proposed scheme can effectively verify the IP of GNN models on various challenging graph-structured datasets. Furthermore, we reveal that the space causality inference can facilitate the acquisition of inherent structural information, which improves the quality and robustness of the fingerprint under model modification operations and other model stealing attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Ju Jia",
        "Renjie Li",
        "Cong Wu",
        "Siqi Ma",
        "Lina Wang",
        "Rebert H. Deng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/81ac6ca9303c4ffd3fceaa9c77a157312b5ff988",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "95b993a3b281c920589fb5b158ff07009ff628b9",
      "title": "CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable utility across diverse applications, and their growing complexity has made Machine Learning as a Service (MLaaS) a viable platform for scalable deployment. However, this accessibility also exposes GNN to serious security threats, most notably model extraction attacks (MEAs), in which adversaries strategically query a deployed model to construct a high-fidelity replica. In this work, we evaluate the vulnerability of GNNs to MEAs and explore their potential for cost-effective model acquisition in non-adversarial research settings. Importantly, adaptive node querying strategies can also serve a critical role in research, particularly when labeling data is expensive or time-consuming. By selectively sampling informative nodes, researchers can train high-performing GNNs with minimal supervision, which is particularly valuable in domains such as biomedicine, where annotations often require expert input. To address this, we propose a node querying strategy tailored to a highly practical yet underexplored scenario, where bulk queries are prohibited, and only a limited set of initial nodes is available. Our approach iteratively refines the node selection mechanism over multiple learning cycles, leveraging historical feedback to improve extraction efficiency. Extensive experiments on benchmark graph datasets demonstrate our superiority over comparable baselines on accuracy, fidelity, and F1 score under strict query-size constraints. These results highlight both the susceptibility of deployed GNNs to extraction attacks and the promise of ethical, efficient GNN acquisition methods to support low-resource research environments.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zebin Wang",
        "Menghan Lin",
        "Bolin Shen",
        "Ken Anderson",
        "Molei Liu",
        "Tianxi Cai",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/95b993a3b281c920589fb5b158ff07009ff628b9",
      "pdf_url": "",
      "publication_date": "2025-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "633f03f1eea4387f02e826e4768841ef10190446",
      "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
      "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jiadong Lou",
        "Xu Yuan",
        "Rui Zhang",
        "Xingliang Yuan",
        "Neil Gong",
        "Nianfeng Tzeng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/633f03f1eea4387f02e826e4768841ef10190446",
      "pdf_url": "",
      "publication_date": "2025-01-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "26be7a4a13776ac194912a70e97783bf2e587c24",
      "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
      "abstract": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
      "year": 2025,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/26be7a4a13776ac194912a70e97783bf2e587c24",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "title": "Balancing Security and Efficiency in GAI-Driven Semantic Communication: Challenges, Solutions, and Future Paths",
      "abstract": "The convergence of artificial intelligence (AI) and wireless communications has driven the emergence of semantic communication (SC), a paradigm that prioritizes context-aware semantic exchange over traditional bit-level transmission. Although enhancing efficiency and task-specific reliability, this advancing capability is accompanied by significant security challenges that remain underexplored. In this paper, we provide an overview of security challenges in SC systems, with a particular focus on the confidentiality, integrity, and availability of the wireless transmission and generative AI (GAI) models. To defend against risks of model confidentiality compromise and semantic feature leakage, we propose a solution integrating trusted execution environments (TEEs) for secure model inference and adversarial cryptography for the protection of semantics over realistic wireless channels. Test results show it achieves close-to-black-box attack resistance in model stealing effectiveness, and the BLEU scores of eavesdropping attackers are effectively reduced to below 0.1 across various SNR levels. Finally, we discuss potential open issues and solutions for enhancing the SC security, paving the way for future research in this critical area. The proposed framework demonstrates promising results in enhancing both model and data confidentiality, contributing to the development of secure SC systems for 6G networks.",
      "year": 2025,
      "venue": "IEEE Network",
      "authors": [
        "Qianyun Zhang",
        "Jiting Shi",
        "Weihao Zeng",
        "Xinyu Xu",
        "Zhenyu Guan",
        "Shufeng Li",
        "Zhijing Qin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ef83ddc62cfc6a7caa37ea87a985eed7a9fede85",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
      "abstract": "Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zhenyuan Guo",
        "Yi Shi",
        "Wenlong Meng",
        "Chen Gong",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/dd37ddad0b07a5f9e38c117f0fb876735062211d",
      "pdf_url": "",
      "publication_date": "2025-02-17",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c167947e7fe8fd78118504627930f1935b11924",
      "title": "MER-Inspector: Assessing Model Extraction Risks from An Attack-Agnostic Perspective",
      "abstract": "Information leakage issues in machine learning-based Web applications have attracted increasing attention. While the risk of data privacy leakage has been rigorously analyzed, the theory of model function leakage, known as Model Extraction Attacks (MEAs), has not been well studied. In this paper, we are the first to understand MEAs theoretically from an attack-agnostic perspective and to propose analytical metrics for evaluating model extraction risks. By using the Neural Tangent Kernel (NTK) theory, we formulate the linearized MEA as a regularized kernel classification problem and then derive the fidelity gap and generalization error bounds of the attack performance. Based on these theoretical analyses, we propose a new theoretical metric called Model Recovery Complexity (MRC), which measures the distance of weight changes between the victim and surrogate models to quantify risk. Additionally, we find that victim model accuracy, which shows a strong positive correlation with model extraction risk, can serve as an empirical metric. By integrating these two metrics, we propose a framework, namely Model Extraction Risk Inspector (MER-Inspector), to compare the extraction risks of models under different model architectures by utilizing relative metric values. We conduct extensive experiments on 16 model architectures and 5 datasets. The experimental results demonstrate that the proposed metrics have a high correlation with model extraction risks, and MER-Inspector can accurately compare the extraction risks of any two models with up to 89.58%.",
      "year": 2025,
      "venue": "The Web Conference",
      "authors": [
        "Xinwei Zhang",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Bai",
        "Huadi Zheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4c167947e7fe8fd78118504627930f1935b11924",
      "pdf_url": "",
      "publication_date": "2025-04-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "title": "Adversarial Autoencoder based Model Extraction Attacks for Collaborative DNN Inference at Edge",
      "abstract": "Deep neural networks (DNNs) are influencing a wide range of applications from safety-critical to security-sensitive use cases. In many such use cases, the DNN inference process relies on distributed systems involving IoT devices and edge/cloud severs as participants where a pre-trained DNN model is partitioned/split onto multiple parts and the participants collaboratively execute them. However, often such collaboration requires dynamic DNN partitioning information to be exchanged among the participants over unsecured network or via relays/hops which can lead to novel privacy vulnerabilities. In this paper, we propose a DNN model extraction attack that exploits such vulnerabilities to not only extract the original input data, but also reconstruct the entire victim DNN model. Specifically, the proposed attack model utilizes extracted/leaked data and adversarial autoencoders to generate and train a shadow model that closely mimics the behavior of the original victim model. The proposed attack is query-free and does not require the attacker to have any prior information about the victim model and input data. Using an IoT-edge hardware testbed running collaborative DNN inference, we demonstrate the effectiveness of the proposed attack model in extracting the victim model with high levels of certainty across many realistic scenarios.",
      "year": 2025,
      "venue": "IEEE/IFIP Network Operations and Management Symposium",
      "authors": [
        "Manal Zneit",
        "Xiaojie Zhang",
        "Motahare Mounesan",
        "S. Debroy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a1dbeef30b37363111eb815ff7fd2a0b8e7da83c",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d3384cb78be25ed28a3544f175cab59236093dd",
      "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
      "abstract": "Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yixu Wang",
        "Tianle Gu",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d3384cb78be25ed28a3544f175cab59236093dd",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "title": "Towards Effective Prompt Stealing Attack against Text-to-Image Diffusion Models",
      "abstract": "Text-to-Image (T2I) models, represented by DALL$\\cdot$E and Midjourney, have gained huge popularity for creating realistic images. The quality of these images relies on the carefully engineered prompts, which have become valuable intellectual property. While skilled prompters showcase their AI-generated art on markets to attract buyers, this business incidentally exposes them to \\textit{prompt stealing attacks}. Existing state-of-the-art attack techniques reconstruct the prompts from a fixed set of modifiers (i.e., style descriptions) with model-specific training, which exhibit restricted adaptability and effectiveness to diverse showcases (i.e., target images) and diffusion models. To alleviate these limitations, we propose Prometheus, a training-free, proxy-in-the-loop, search-based prompt-stealing attack, which reverse-engineers the valuable prompts of the showcases by interacting with a local proxy model. It consists of three innovative designs. First, we introduce dynamic modifiers, as a supplement to static modifiers used in prior works. These dynamic modifiers provide more details specific to the showcases, and we exploit NLP analysis to generate them on the fly. Second, we design a contextual matching algorithm to sort both dynamic and static modifiers. This offline process helps reduce the search space of the subsequent step. Third, we interact with a local proxy model to invert the prompts with a greedy search algorithm. Based on the feedback guidance, we refine the prompt to achieve higher fidelity. The evaluation results show that Prometheus successfully extracts prompts from popular platforms like PromptBase and AIFrog against diverse victim models, including Midjourney, Leonardo.ai, and DALL$\\cdot$E, with an ASR improvement of 25.0\\%. We also validate that Prometheus is resistant to extensive potential defenses, further highlighting its severity in practice.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shiqian Zhao",
        "Chong Wang",
        "Yiming Li",
        "Yihao Huang",
        "Wenjie Qu",
        "Siew-Kei Lam",
        "Yi Xie",
        "Kangjie Chen",
        "Jie Zhang",
        "Tianwei Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0c72a0508a3f3944660e786e83a7f4d56c50ecf1",
      "pdf_url": "",
      "publication_date": "2025-08-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "title": "TSQP: Safeguarding Real-Time Inference for Quantization Neural Networks on Edge Devices",
      "abstract": "Quantization Neural Networks (QNNs) has been widely adopted in resource-constrained edge devices due to their real-time capabilities and low resource requirement. However, concerns have arisen regarding that deployed models are white-box available to model thefts. To address this issue, TEE-shielded secure inference has been introduced as a secure and efficient solution. Nevertheless, existing methods neglect the compatibility with 8-bit quantized computation, which leads to severe integer overflow issue during inference. This issue could result a disastrous degradation in QNNs (to random guessing level), completely destroying model utility. Moreover, the model confidentiality and inference integrity also face a substantial threat due to the limited data representation space. To safeguard accurate and efficient inference for QNNs, TEE-Shielded QNN Partition (TSQP) are proposed, which presents three key insights: Firstly, Quantization Manager is designed to convert white-box inference to black-box by shielding critical scales in TEE. Additionally, overflow concerns are effectively addressed using reduced-range approaches. Secondly, by leveraging the Information Bottleneck theory to enhance model training, we introduce Parameter De-Similarity to defend against powerful Model Stealing attacks that existing methods are vulnerable to. Thirdly, the Integrity Monitor is suggested to detect inference integrity breaches in an oblivious manner. In contrast, existing method can be bypassed due to the lack of obliviousness. Experimental results demonstrate that proposed TSQP maintains high accuracy and achieves accurate integrity breaches detection. Our method achieves more than $8\\times$ speedup compared to full TEE inference, while reducing Model Stealing attacks accuracy from $3.99\\times$ to $1.29\\times$. To our best knowledge, proposed method is the first TEE-shielded secure inference solution that achieves model confidentiality, inference integrity and model utility on QNNs.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yu Sun",
        "Gaojian Xiong",
        "Jianhua Liu",
        "Zheng Liu",
        "Jian Cui"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f961eb51aed4d1db5b246ac4cc842e0faa820ca2",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5bce864b579b376c028ec40a8fec0f999b005d0e",
      "title": "Attack and defense techniques in large language models: A survey and new perspectives",
      "abstract": "Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attacks, optimized attacks, model theft, as well as attacks on LLM applications, detailing their mechanisms and implications. Consequently, we analyze defense strategies, such as prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open issues, including the need for adaptive scalable defenses, adversarial attack detection, generalized defense mechanisms, and ethical and bias concerns. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",
      "year": 2025,
      "venue": "Neural Networks",
      "authors": [
        "Zhiyu Liao",
        "Kang Chen",
        "Y. Lin",
        "Kangkang Li",
        "Yunxuan Liu",
        "Hefeng Chen",
        "Xingwang Huang",
        "Yuanhui Yu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5bce864b579b376c028ec40a8fec0f999b005d0e",
      "pdf_url": "",
      "publication_date": "2025-05-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "title": "Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers\u2019 datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Wenhan Chang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b31459472aa34e8711fe845acaab9b7b4a74f1d8",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "369d7792462ab184ed6dc53cab70b9b101d9d034",
      "title": "Sim4Rec: Data-Free Model Extraction Attack on Sequential Recommendation",
      "abstract": "Model extraction attack shows promising performance in revealing sequential recommendation (SeqRec) robustness, e.g., as an upstream task of transfer-based attack to provide optimization feedback for downstream attacks. However, existing work either heavily relies on impractical prior knowledge or has impressive attack performance. In this paper, we focus on data-free model extraction attack on SeqRec, which aims to efficiently train a surrogate model that closely imitates the target model in a practical setting. Conducting such an attack is challenging. First, imitating sequential training data for accurate model extraction is hard without prior knowledge. Second, limited queries for the target model require the attack to be efficient. To address these challenges, we propose a novel adversarial framework Sim4Rec which includes two modules, i.e., controllable sequence generation and reinforced adversarial distillation. The former allows a sequential generator to produce synthetic data similar to training data through pre-training with controllable generated samples. The latter efficiently extracts the target model via reinforced adversarial knowledge distillation. Extensive experiments demonstrate the advancement of Sim4Rec.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yihao Wang",
        "Jiajie Su",
        "Chaochao Chen",
        "Meng Han",
        "Chi Zhang",
        "Jun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/369d7792462ab184ed6dc53cab70b9b101d9d034",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b5caff453174871e8c9b374b72659a7c63fa830",
      "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
      "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains, with well-trained QNNs representing critical intellectual property often deployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has examined QNN model extraction attacks using classical and emerging quantum strategies. These attacks involve adversaries querying QNNaaS platforms to obtain labeled data for training local substitute QNNs that replicate the functionality of cloud-based models. However, existing approaches have largely over-looked the impact of varying quantum noise inherent in noisy intermediate-scale quantum (NISQ) computers, limiting their effectiveness in real-world settings. To address this limitation, we propose the CopyQNN framework, which employs a three-step data cleaning method to eliminate noisy data based on its noise sensitivity. This is followed by the integration of contrastive and transfer learning within the quantum domain, enabling efficient training of substitute QNNs using a limited but cleaned set of queried data. Experimental results on NISQ computers demonstrate that a practical implementation of CopyQNN significantly outperforms state-of-the-art QNN extraction attacks, achieving an average performance improvement of 8.73% across all tasks while reducing the number of required queries by 90\u00d7, with only a modest increase in hardware overhead.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Leyi Zhao",
        "Xuhong Zhang",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1b5caff453174871e8c9b374b72659a7c63fa830",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "title": "Delving into Cryptanalytic Extraction of PReLU Neural Networks",
      "abstract": "The machine learning problem of model extraction was first introduced in 1991 and gained prominence as a cryptanalytic challenge starting with Crypto 2020. For over three decades, research in this field has primarily focused on ReLU-based neural networks. In this work, we take the first step towards the cryptanalytic extraction of PReLU neural networks, which employ more complex nonlinear activation functions than their ReLU counterparts. We propose a raw output-based parameter recovery attack for PReLU networks and extend it to more restrictive scenarios where only the top-m probability scores are accessible. Our attacks are rigorously evaluated through end-to-end experiments on diverse PReLU neural networks, including models trained on the MNIST dataset. To the best of our knowledge, this is the first practical demonstration of PReLU neural network extraction across three distinct attack scenarios.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yi Chen",
        "Xiaoyang Dong",
        "Ruijie Ma",
        "Yan Shen",
        "Anyu Wang",
        "Hongbo Yu",
        "Xiaoyun Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e569cd7a6612a86e89c2e09c36f75fdcce6dd453",
      "pdf_url": "",
      "publication_date": "2025-09-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b6603145f57b294c2e5dff92efe136568ab8cf52",
      "title": "Securing RFID With GNN: A Real-Time Tag Cloning Attack Detection System",
      "abstract": "In the field of RFID systems, cloning attacks that replicate authentic tags to deceive readers pose a significant threat to corporate security, potentially leading to financial losses and reputational damage. Many existing solutions struggle to mitigate this threat without altering the Medium Access Control (MAC) layer protocols or integrating additional hardware resources, which are impractical adjustments for commercial off-the-shelf (COTS) RFID devices. This paper introduces an innovative system framework that leverages Graph Neural Network to detect RFID tag cloning attacks without the need to change the MAC protocols or add hardware resources. The system can automatically uncover implicit topological structures from RFID signal data and adaptively capture complex inter-signal relationships. By constructing dynamic graph and employing Graph Attention Network, this framework not only captures deep data correlations that traditional detection methods cannot identify but also demonstrates exceptional accuracy and robustness in experiments. Experimental results have proven that the framework maintains stable performance even when the training and testing data distributions are mismatched, as verified in both static and dynamic tag cloning attack scenarios. Furthermore, the framework effectively identifies anomalous behavior by comprehensively considering precision, recall, and F1 scores, especially when dealing with highly imbalanced datasets.",
      "year": 2025,
      "venue": "IEEE Open Journal of the Communications Society",
      "authors": [
        "Bojun Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b6603145f57b294c2e5dff92efe136568ab8cf52",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
      "abstract": "Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaixiang Zhao",
        "Lincan Li",
        "Kaize Ding",
        "Neil Zhenqiang Gong",
        "Yue Zhao",
        "Yushun Dong"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0daf186b202c94263c68c6ffd19c3539c080a1f2",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
      "abstract": "Generative AI technology has become increasingly integrated into our daily lives, offering powerful capabilities to enhance productivity. However, these same capabilities can be exploited by adversaries for malicious purposes. While existing research on adversarial applications of generative AI predominantly focuses on cyberattacks, less attention has been given to attacks targeting deep learning models. In this paper, we introduce the use of generative AI for facilitating model-related attacks, including model extraction, membership inference, and model inversion. Our study reveals that adversaries can launch a variety of model-related attacks against both image and text models in a data-free and black-box manner, achieving comparable performance to baseline methods that have access to the target models' training data and parameters in a white-box manner. This research serves as an important early warning to the community about the potential risks associated with generative AI-powered attacks on deep learning models.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Shang Wang",
        "Bo Liu",
        "Leo Yu Zhang",
        "Wanlei Zhou",
        "Yang Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f2b0fb9334fb9e490213ac315dbf52e9fbdbf93c",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
      "abstract": "The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ganghua Wang",
        "Yuhong Yang",
        "Jie Ding"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e9534b0f74ff371aa086ecc30d95a633ecad7ddc",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "title": "Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging",
      "abstract": "Model merging has emerged as a promising approach for updating large language models (LLMs) by integrating multiple domain-specific models into a cross-domain merged model. Despite its utility and plug-and-play nature, unmonitored mergers can introduce significant security vulnerabilities, such as backdoor attacks and model merging abuse. In this paper, we identify a novel and more realistic attack surface where a malicious merger can extract targeted personally identifiable information (PII) from an aligned model with model merging. Specifically, we propose \\texttt{Merger-as-a-Stealer}, a two-stage framework to achieve this attack: First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries. The attacker then uploads this malicious model to the model merging conductor and obtains the merged model. Second, the attacker inputs direct PII-related queries to the merged model to extract targeted PII. Extensive experiments demonstrate that \\texttt{Merger-as-a-Stealer} successfully executes attacks against various LLMs and model merging methods across diverse settings, highlighting the effectiveness of the proposed framework. Given that this attack enables character-level extraction for targeted PII without requiring any additional knowledge from the attacker, we stress the necessity for improved model alignment and more robust defense mechanisms to mitigate such threats.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Lin Lu",
        "Zhigang Zuo",
        "Ziji Sheng",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7174cf188e9d7b5e1842c0d0517ba5df22bc6a8a",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "90360529f01cf996d62369fd0c47af3b1823c7f4",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attack poses a significant risk to Machine-Learning-as-a-Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying Black-Box (without internal insight) APIs. Despite FL\u2019s privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of the FL-based victim model to two types of model extraction attacks. For various federated clients built under NVFlare platform, we implemented ME attack across two deep-learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for various FL clients, the accuracy and fidelity of the extraction model are closely related to the size of the attack query set. Additionally, we explore a transfer learning-based approach where pre-trained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pre-trained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/90360529f01cf996d62369fd0c47af3b1823c7f4",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "268ce1e9492455e825a3894b0f2713d14e376d36",
      "title": "ADAGE: Active Defenses Against GNN Extraction",
      "abstract": "Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the observation that stealing a model's full functionality requires highly diverse queries to leak its behavior across the input space. Our defense monitors this query diversity and progressively perturbs outputs as the accumulated leakage grows. In contrast to prior work, ADAGE can prevent stealing across all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst preserving predictive performance on downstream tasks. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jing Xu",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/268ce1e9492455e825a3894b0f2713d14e376d36",
      "pdf_url": "",
      "publication_date": "2025-02-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
      "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical machine learning while introducing new attack surfaces rooted in the physical and algorithmic layers of quantum computing. Despite a growing body of research on individual attack vectors - ranging from adversarial poisoning and evasion to circuit-level backdoors, side-channel leakage, and model extraction - these threats are often analyzed in isolation, with unrealistic assumptions about attacker capabilities and system environments. This fragmentation hampers the development of effective, holistic defense strategies. In this work, we argue that QML security requires more structured modeling of the attack surface, capturing not only individual techniques but also their relationships, prerequisites, and potential impact across the QML pipeline. We propose adapting kill chain models, widely used in classical IT and cybersecurity, to the quantum machine learning context. Such models allow for structured reasoning about attacker objectives, capabilities, and possible multi-stage attack paths - spanning reconnaissance, initial access, manipulation, persistence, and exfiltration. Based on extensive literature analysis, we present a detailed taxonomy of QML attack vectors mapped to corresponding stages in a quantum-aware kill chain framework that is inspired by the MITRE ATLAS for classical machine learning. We highlight interdependencies between physical-level threats (like side-channel leakage and crosstalk faults), data and algorithm manipulation (such as poisoning or circuit backdoors), and privacy attacks (including model extraction and training data inference). This work provides a foundation for more realistic threat modeling and proactive security-in-depth design in the emerging field of quantum machine learning.",
      "year": 2025,
      "venue": "International Conference on Quantum Computing and Engineering",
      "authors": [
        "Pascal Debus",
        "Maximilian Wendlinger",
        "Kilian Tscharke",
        "Daniel Herr",
        "Cedric Br\u00fcgmann",
        "Daniel de Mello",
        "J. Ulmanis",
        "Alexander Erhard",
        "Arthur Schmidt",
        "Fabian Petsch"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "pdf_url": "",
      "publication_date": "2025-07-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be0d667c4b910dad74a39e135697d7d7957512e1",
      "title": "Side-Channel Analysis of Integrate-and-Fire Neurons Within Spiking Neural Networks",
      "abstract": "Spiking neural networks gain increasing attention in constraint edge devices due to event-based low-power operation and little resource usage. Such edge devices often allow physical access, opening the door for Side-Channel Analysis. In this work, we introduce a novel robust attack strategy on the neuron level to retrieve the trained parameters of an implemented spiking neural network. Utilizing horizontal correlation power analysis, we demonstrate how to recover the weights and thresholds of a feed-forward spiking neural network implementation. We verify our methodology with real-world measurements of localized electromagnetic emanations of an FPGA design. Additionally, we propose countermeasures against the introduced novel attack approach. We evaluate shuffling and masking as countermeasures to protect the implementation against our proposed attack and demonstrate their effectiveness and limitations.",
      "year": 2025,
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "authors": [
        "Matthias Probst",
        "Manuel Brosch",
        "G. Sigl"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/be0d667c4b910dad74a39e135697d7d7957512e1",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
      "abstract": "Decentralized training has become a resource-efficient framework to democratize the training of large language models (LLMs). However, the privacy risks associated with this framework, particularly due to the potential inclusion of sensitive data in training datasets, remain unexplored. This paper identifies a novel and realistic attack surface: the privacy leakage from training data in decentralized training, and proposes \\textit{activation inversion attack} (AIA) for the first time. AIA first constructs a shadow dataset comprising text labels and corresponding activations using public datasets. Leveraging this dataset, an attack model can be trained to reconstruct the training data from activations in victim decentralized training. We conduct extensive experiments on various LLMs and publicly available datasets to demonstrate the susceptibility of decentralized training to AIA. These findings highlight the urgent need to enhance security measures in decentralized training to mitigate privacy risks in training LLMs.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Chenxi Dai",
        "Lin Lu",
        "Pan Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bff5a24e045b0eb0dc50ccab16fcf5497d8817c9",
      "pdf_url": "",
      "publication_date": "2025-02-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
      "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah",
        "My T. Thai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6678e884da10e0d6cd763811069a813700b685a6",
      "title": "Exploring Query Efficient Data Generation Towards Data-Free Model Stealing in Hard Label Setting",
      "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. Instead, the adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (QEDG). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real MLaaS scenario and five datasets.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Gaozheng Pei",
        "Shaojie Lyu",
        "Ke Ma",
        "Pinci Yang",
        "Qianqian Xu",
        "Yingfei Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6678e884da10e0d6cd763811069a813700b685a6",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "title": "Examining the Threat Landscape: Foundation Models and Model Stealing",
      "abstract": "Foundation models (FMs) for computer vision learn rich and robust representations, enabling their adaptation to task/domain-specific deployments with little to no fine-tuning. However, we posit that the very same strength can make applications based on FMs vulnerable to model stealing attacks. Through empirical analysis, we reveal that models fine-tuned from FMs harbor heightened susceptibility to model stealing, compared to conventional vision architectures like ResNets. We hypothesize that this behavior is due to the comprehensive encoding of visual patterns and features learned by FMs during pre-training, which are accessible to both the attacker and the victim. We report that an attacker is able to obtain 94.28% agreement (matched predictions with victim) for a Vision Transformer based victim model (ViT-L/16) trained on CIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim, when using ViT-L/16 as the thief model. We arguably show, for the first time, that utilizing FMs for downstream tasks may not be the best choice for deployment in commercial APIs due to their susceptibility to model theft. We thereby alert model owners towards the associated security risks, and highlight the need for robust security measures to safeguard such models against theft. Code is available at https://github.com/rajankita/foundation_model_stealing.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ankita Raj",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a9d7ffd4856883122048dacf19c149f7ad92a85",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "title": "THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models",
      "abstract": "On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yujin Huang",
        "Zhi Zhang",
        "Qingchuan Zhao",
        "Xingliang Yuan",
        "Chunyang Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/edc7c0b636b1b5a9e268b4b554e915ac49e9b747",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "title": "MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models",
      "abstract": "Model extraction attacks aim to replicate the functionality of a black-box model through query access, threatening the intellectual property (IP) of machine-learning-as-a-service (MLaaS) providers. Defending against such attacks is challenging, as it must balance efficiency, robustness, and utility preservation in the real-world scenario. Despite the recent advances, most existing defenses presume that attacker queries have out-of-distribution (OOD) samples, enabling them to detect and disrupt suspicious inputs. However, this assumption is increasingly unreliable, as modern models are trained on diverse datasets and attackers often operate under limited query budgets. As a result, the effectiveness of these defenses is significantly compromised in realistic deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel optimization problem that simultaneously preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. Our framework combines data augmentation to simulate attacker queries with an ensemble of heterogeneous distilled models to improve robustness and diversity. We further provide a tractable approximation algorithm and derive theoretical error bounds to characterize defense effectiveness. Extensive experiments across various settings validate the utility-preserving and extraction-resistant properties of our proposed defense strategy. Our code is available at https://github.com/LabRAI/MISLEADER.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xueqi Cheng",
        "Minxing Zheng",
        "Shixiang Zhu",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f99ae2cfcb94dc21f4089be22c3b6daaa65eeeb9",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "title": "Real-world Edge Neural Network Implementations Leak Private Interactions Through Physical Side Channel",
      "abstract": "Neural networks have become a fundamental component of numerous practical applications, and their implementations, which are often accelerated by hardware, are integrated into all types of real-world physical devices. User interactions with neural networks on hardware accelerators are commonly considered privacy-sensitive. Substantial efforts have been made to uncover vulnerabilities and enhance privacy protection at the level of machine learning algorithms, including membership inference attacks, differential privacy, and federated learning. However, neural networks are ultimately implemented and deployed on physical devices, and current research pays comparatively less attention to privacy protection at the implementation level. In this paper, we introduce a generic physical side-channel attack, ScaAR, that extracts user interactions with neural networks by leveraging electromagnetic (EM) emissions of physical devices. Our proposed attack is implementation-agnostic, meaning it does not require the adversary to possess detailed knowledge of the hardware or software implementations, thanks to the capabilities of deep learning-based side-channel analysis (DLSCA). Experimental results demonstrate that, through the EM side channel, ScaAR can effectively extract the class label of user interactions with neural classifiers, including inputs and outputs, on the AMD-Xilinx MPSoC ZCU104 FPGA and Raspberry Pi 3 B. In addition, for the first time, we provide side-channel analysis on edge Large Language Model (LLM) implementations on the Raspberry Pi 5, showing that EM side channel leaks interaction data, and different LLM tokens can be distinguishable from the EM traces.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuoran Liu",
        "Senna van Hoek",
        "P'eter Horv'ath",
        "Dirk Lauret",
        "Xiaoyun Xu",
        "L. Batina"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97314cd9e8846f31e14fc8a7d9579c469abc3eba",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6971c737329185c98d62432dcbcbd36c03c04c6b",
      "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?",
      "abstract": "Deep Neural Networks (DNNs) have attracted significant attention, and their internal models are now considered valuable intellectual assets. Extracting these internal models through access to a DNN is conceptually similar to extracting a secret key via oracle access to a block cipher. Consequently, cryptanalytic techniques, particularly differential-like attacks, have been actively explored recently. ReLU-based DNNs are the most commonly and widely deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024) assume access to exact output logits, which are usually invisible, more recent works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting, where only the final classification result (e.g.,\"dog\"or\"car\") is available to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that model extraction is feasible in polynomial time even under this restricted setting. In this paper, we first show that the assumptions underlying their attack become increasingly unrealistic as the attack-target depth grows. In practice, satisfying these assumptions requires an exponential number of queries with respect to the attack depth, implying that the attack does not always run in polynomial time. To address this critical limitation, we propose a novel attack method called CrossLayer Extraction. Instead of directly extracting the secret parameters (e.g., weights and biases) of a specific neuron, which incurs exponential cost, we exploit neuron interactions across layers to extract this information from deeper layers. This technique significantly reduces query complexity and mitigates the limitations of existing model extraction approaches.",
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Akira Ito",
        "Takayuki Miura",
        "Yosuke Todo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6971c737329185c98d62432dcbcbd36c03c04c6b",
      "pdf_url": "",
      "publication_date": "2025-10-08",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cb626a77f1e0c634d557ca88af22547f21f16afa",
      "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
      "abstract": "Graph-structured data, which captures non-Euclidean relationships and interactions between entities, is growing in scale and complexity. As a result, training state-of-the-art graph machine learning (GML) models have become increasingly resource-intensive, turning these models and data into invaluable Intellectual Property (IP). To address the resource-intensive nature of model training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an efficient solution by leveraging third-party cloud services for model development and management. However, deploying such models in GMLaaS also exposes them to potential threats from attackers. Specifically, while the APIs within a GMLaaS system provide interfaces for users to query the model and receive outputs, they also allow attackers to exploit and steal model functionalities or sensitive training data, posing severe threats to the safety of these GML models and the underlying graph data. To address these challenges, this survey systematically introduces the first taxonomy of threats and defenses at the level of both GML model and graph-structured data. Such a tailored taxonomy facilitates an in-depth understanding of GML IP protection. Furthermore, we present a systematic evaluation framework to assess the effectiveness of IP protection methods, introduce a curated set of benchmark datasets across various domains, and discuss their application scopes and future challenges. Finally, we establish an open-sourced versatile library named PyGIP, which evaluates various attack and defense techniques in GMLaaS scenarios and facilitates the implementation of existing benchmark methods. The library resource can be accessed at: https://labrai.github.io/PyGIP. We believe this survey will play a fundamental role in intellectual property protection for GML and provide practical recipes for the GML community.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Lincan Li",
        "Bolin Shen",
        "Chenxi Zhao",
        "Yuxiang Sun",
        "Kaixiang Zhao",
        "Shirui Pan",
        "Yushun Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cb626a77f1e0c634d557ca88af22547f21f16afa",
      "pdf_url": "",
      "publication_date": "2025-08-27",
      "keywords_matched": [
        "steal model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "098c756f393dc1c3fafa8348fabc3a97410f4473",
      "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks",
      "abstract": "Neural network model extraction has emerged in recent years as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. A key step in this process is signature extraction, which aims to recover the absolute values of the network's weights layer by layer. Prior work, notably by Carlini et al. (2020), introduced a technique inspired by differential cryptanalysis to extract neural network parameters. However, their method suffers from several limitations that restrict its applicability to networks with a few layers only. Later works focused on improving sign extraction, but largely relied on the assumption that signature extraction itself was feasible. In this work, we revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues, greatly improving the efficiency of signature extraction. Our approach permits the extraction of much deeper networks than was previously possible. We validate our method through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth and accuracy. For instance, our extracted network matches the target network on at least 95% of the input space for each of the eight layers of a neural network trained on the CIFAR-10 dataset, while previous works could barely extract the first three layers. Our results represent a crucial step toward practical attacks on larger and more complex neural network architectures.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haolin Liu",
        "Adrien Siproudhis",
        "Samuel Experton",
        "Peter Lorenz",
        "Christina Boura",
        "Thomas Peyrin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/098c756f393dc1c3fafa8348fabc3a97410f4473",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0c77335d4c20505a99178221b377755a5ece20a",
      "title": "Security of Approximate Neural Networks against Power Side-channel Attacks",
      "abstract": "Emerging low-energy computing technologies, in particular approximate computing, are becoming increasingly relevant in key applications. A significant use case for these technologies is reduced energy consumption in Artificial Neural Networks (ANNs), an increasingly pressing concern with the rapid growth of AI deployments. It is essential we understand the security implications of approximate computing in an ANN context before this practice becomes commonplace. In this work, we examine the test case of approximate ANN processing elements (PE) in terms of information leakage via the power side channel. We perform a weight extraction correlation Power Analysis (CPA) attack under three approximation scenarios: overclocking, voltage scaling, and circuit level bitwise approximation. We demonstrate that as the degree of approximation increases the Signal to Noise Ratio (SNR) of power traces rapidly degrades. We show that the Measurement to Disclosure (MTD) increases for all approximate techniques. An MTD of 48 under precise computing is increased to at minimum 200 (bitwise approximate circuit at $\\mathbf{2 5 \\%}$ approximation), and under some approximation scenarios $\\gt1024$. i.e. an increase in attack difficulty of at least x4 and potentially over x20. A relative Security-Power-Delay (SPD) analysis reveals that, in addition to the across the board improvement vs precise computing, voltage and clock scaling both significantly outperform approximate circuits with voltage scaling as the highest performing technique.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Aditya Japa",
        "Jack Miskelly",
        "M. O'Neill",
        "Chongyan Gu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f0c77335d4c20505a99178221b377755a5ece20a",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "title": "Unlocking High-Fidelity Learning: Towards Neuron-Grained Model Extraction",
      "abstract": "Model extraction (ME) attacks replicate valuable closed-box machine learning (ML) models via malicious query interactions. Cutting-edge attacks focus on actively designing query samples to enhance model fidelity and imprudently adhere to the standard ML training approach. This causes a deviation from the true objective of learning a model over a task. In this article, we innovatively shift our focus from query selection to training process optimization, aiming to boost the similarity of the copy model with the victim model from neuron to model level. We leverage neuron matching theory to attain this objective and develop a general training booster framework, MEBooster, to fully exploit this theory. MEBooster comprises an initial bootstrapping phase that furnishes initial parameters and an optimal model architecture, followed by a post-processing phase that employs fine-tuning for enhanced neuron matching. Notably, MEBooster can seamlessly integrate with all existing model extraction attacks, enhancing their overall performance. Performance evaluation shows up to 58.10% fidelity gain in image classification. From a defender's perspective, we introduce a novel defensive strategy called Stochastic Norm Enlargement (SNE) to mitigate the risk of such attacks by enlarging the model parameters\u2019 norm property in training. Performance evaluation shows up to 58.81% extractability (i.e., fidelity) reduction.",
      "year": 2025,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Yaxin Xiao",
        "Haibo Hu",
        "Qingqing Ye",
        "Li Tang",
        "Zi Liang",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4a1c5a240e9ca7572f7d1545710f45c3df5f7b54",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4934c657457df8eae2b5193b1744f87906d37cfa",
      "title": "Side-channel attacks on convolutional neural networks based on the hybrid attention mechanism",
      "abstract": "In the field of security assessment of password chips, side-channel attacks are an important and effective means of extracting sensitive information by analysing the physical characteristics of the chip during operation, providing an important basis for security assessment. In recent years, deep learning technology has been widely used in the field of side-channel attacks, which can automatically learn and identify the physical leakage characteristics of the chip and improve the efficiency and accuracy of the attack. However, deep learning-based side-channel attacks may be disturbed by environmental noise during the training process, and there are also problems of model overfitting and slow convergence. In order to more effectively extract feature information in the trajectory to implement a side-channel attack, this paper proposes a new attention mechanism convolutional neural network model architecture. The model combines a convolutional neural network with an attention mechanism. It optimises the traditional CNN model by improving the convolutional layer and introducing a fused hybrid attention mechanism, enhancing the model's ability to capture global information to effectively extract relevant leaked information. Experimental results show that the model has good attack results on the ASCAD public dataset. Compared with other models, it requires 74.87% less power consumption for side-channel analysis, and the model accuracy is significantly improved. It solves the problems of overfitting and slow convergence speed, and can meet the requirements of side-channel modeling and analysis. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model. Design an efficient convolutional neural network architecture model with an integrated attention mechanism. Improve the CBAM module and optimize the network structure for side-channel attacks. Significantly improve the convergence speed and attack efficiency of the neural network side channel model.",
      "year": 2025,
      "venue": "Discover Applied Sciences",
      "authors": [
        "Tao Feng",
        "Huan Gao",
        "Xiaomin Li",
        "Chunyan Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4934c657457df8eae2b5193b1744f87906d37cfa",
      "pdf_url": "https://doi.org/10.1007/s42452-025-06854-0",
      "publication_date": "2025-04-24",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "title": "TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE",
      "abstract": "To safeguard user data privacy, on-device inference has emerged as a prominent paradigm on mobile and Internet of Things (IoT) devices. This paradigm involves deploying a model provided by a third party on local devices to perform inference tasks. However, it exposes the private model to two primary security threats: model stealing (MS) and membership inference attacks (MIA). To mitigate these risks, existing wisdom deploys models within Trusted Execution Environments (TEEs), which is a secure isolated execution space. Nonetheless, the constrained secure memory capacity in TEEs makes it challenging to achieve full model security with low inference latency. This paper fills the gap with TensorShield, the first efficient on-device inference work that shields partial tensors of the model while still fully defending against MS and MIA. The key enabling techniques in TensorShield include: (i) a novel eXplainable AI (XAI) technique exploits the model's attention transition to assess critical tensors and shields them in TEE to achieve secure inference, and (ii) two meticulous designs with critical feature identification and latency-aware placement to accelerate inference while maintaining security. Extensive evaluations show that TensorShield delivers almost the same security protection as shielding the entire model inside TEE, while being up to 25.35\u00d7 (avg. 5.85\u00d7) faster than the state-of-the-art work, without accuracy loss.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tong Sun",
        "Bowen Jiang",
        "Hailong Lin",
        "Borui Li",
        "Yixiao Teng",
        "Yi Gao",
        "Wei Dong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/57be40e5b844e37895cadcc5f9b729ecdc59b69d",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "title": "\u03b4-STEAL: LLM Stealing Attack with Local Differential Privacy",
      "abstract": "Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kieu Dang",
        "Phung Lai",
        "Nhathai Phan",
        "Yelong Shen",
        "Ruoming Jin",
        "Abdallah Khreishah"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4751cda9e628009ffaa86cfdaac218bef0eacd1e",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack",
        "LLM stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "title": "Inside the Mind of an Attacker: Review Sistematika Tujuan Pencurian Machine Learning Model",
      "abstract": "Abstrak - Pencurian model (model stealing) menjadi salah satu ancaman serius dalam penerapan machine learning modern, terutama pada layanan berbasis API dan cloud. Artikel ini mengulas secara sistematik berbagai tujuan di balik serangan pencurian model untuk memahami motif penyerang dan implikasinya bagi pengembang sistem. Metode penulisan berupa kajian literatur terkini yang mengklasifikasikan tujuan pencurian ke dalam delapan kategori utama: (1) pencurian properti internal seperti arsitektur, bobot, dan hyperparameter; (2) peniruan perilaku model untuk menghasilkan efektivitas setara dan konsistensi prediksi pada data normal maupun adversarial; (3) transfer pengetahuan untuk distillation dan deployment ringan; (4) serangan privasi berupa membership inference dan model inversion; (5) monetisasi dengan menjual model bajakan atau menyediakan layanan API ilegal; (6) pencurian kemampuan pertahanan adversarial untuk meningkatkan efektivitas serangan; (7) spionase industri untuk reverse engineering model pesaing; serta (8) penghindaran regulasi dengan mencuri model yang sudah tersertifikasi. Review ini menegaskan bahwa ancaman pencurian model tidak hanya merugikan secara teknis, tetapi juga membuka peluang eksploitasi ekonomi ilegal, kebocoran data sensitif, dan persaingan usaha tidak sehat. Pemahaman yang detail atas ragam tujuan ini diharapkan mendorong perancang sistem untuk mengembangkan strategi pertahanan yang lebih cermat dan menyeluruh.Kata kunci: Machine Learning; Model Stealing; API; Adversarial; Transfer Pengetahuan;\u00a0Abstract - Model stealing has become one of the most serious threats in modern machine learning applications, especially in API- and cloud-based services. This article systematically reviews the various objectives behind model stealing attacks to understand the attackers\u2019 motivations and their implications for system developers. The writing method is a current literature review that classifies model stealing objectives into eight main categories: (1) theft of internal properties such as architecture, weights, and hyperparameters; (2) imitation of model behavior to achieve comparable effectiveness and prediction consistency on both normal and adversarial data; (3) knowledge transfer for distillation and lightweight deployment; (4) privacy attacks through membership inference and model inversion; (5) monetization by selling stolen models or offering illegal API services; (6) stealing adversarial robustness to improve attack effectiveness; (7) industrial espionage for reverse engineering competitor models; and (8) regulatory evasion by stealing pre-certified models. This review emphasizes that model stealing threats are not merely technical issues but also open opportunities for illegal economic exploitation, leakage of sensitive data, and unfair business competition. A detailed understanding of these diverse objectives is expected to encourage system designers to develop more careful and comprehensive defense strategies.Keywords: Machine Learning; Model Stealing; API; Adversarial; Knowledge Transfer;",
      "year": 2025,
      "venue": "Jurnal Nasional Komputasi dan Teknologi Informasi (JNKTI)",
      "authors": [
        "Mulkan Fadhli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bbd216e8ca69d2583e1d203a4fdf8e9bf4e07f3d",
      "pdf_url": "",
      "publication_date": "2025-07-27",
      "keywords_matched": [
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "title": "Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks",
      "abstract": "Machine learning (ML) models were shown to be vulnerable to model stealing attacks, which lead to intellectual property infringement. Among other attack methods, substitute model training is an all-encompassing attack applicable to any machine learning model whose behaviour can be approximated from input-output queries. Whereas previous works mainly focused on improving the performance of substitute models by, e.g. developing a new substitute training method, there have been only limited ablation studies that try to understand the impact the strength of an attacker has on the substitute model's performance. As a result, different authors came to diverse, sometimes contradicting, conclusions. In this work, we exhaustively examine the ambivalent influence of different factors resulting from varying the attacker's capabilities and knowledge on a substitute training attack. Our findings suggest that some of the factors that have been considered important in the past are, in fact, not that influential; instead, we discover new correlations between attack conditions and success rate. In particular, we demonstrate that better-performing target models enable higher-fidelity attacks and explain the intuition behind this phenomenon. Further, we propose to shift the focus from the complexity of target models toward the complexity of their learning tasks. Therefore, for the substitute model, rather than aiming for a higher architecture complexity, we suggest focusing on getting data of higher complexity and an appropriate architecture. Finally, we demonstrate that even in the most limited data-free scenario, there is no need to overcompensate weak knowledge with unrealistic capabilities in the form of millions of queries. Our results often exceed or match the performance of previous attacks that assume a stronger attacker, suggesting that these stronger attacks are likely endangering a model owner's intellectual property to a significantly higher degree than shown until now.",
      "year": 2025,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2e50f4dc488356303b0f76a1db35d5e672ef3b28",
      "pdf_url": "",
      "publication_date": "2025-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b066eaa019bf6e94540dc54e735defb19d910bf8",
      "title": "Stealix: Model Stealing via Prompt Evolution",
      "abstract": "Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model's data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Hui-Po Wang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b066eaa019bf6e94540dc54e735defb19d910bf8",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5ad7f669afa84855e00c743422026c8c5e91e411",
      "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
      "abstract": "Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "Kathrin Grosse",
        "Andreas Rauber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5ad7f669afa84855e00c743422026c8c5e91e411",
      "pdf_url": "",
      "publication_date": "2025-08-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "title": "Model-Guardian: Protecting against Data-Free Model Stealing Using Gradient Representations and Deceptive Predictions",
      "abstract": "Model stealing attack is increasingly threatening the confidentiality of machine learning models deployed in the cloud. Recent studies reveal that adversaries can exploit data synthesis techniques to steal machine learning models even in scenarios devoid of real data, leading to data-free model stealing attacks. Existing defenses against such attacks suffer from limitations, including poor effectiveness, insufficient generalization ability, and low comprehensiveness. In response, this paper introduces a novel defense framework named Model-Guardian. Comprising two components, Data-Free Model Stealing Detector (DFMS-Detector) and Deceptive Predictions (DPreds), Model-Guardian is designed to address the shortcomings of current defenses with the help of the artifact properties of synthetic samples and gradient representations of samples. Extensive experiments on seven prevalent data-free model stealing attacks showcase the effectiveness and superior generalization ability of Model-Guardian, outperforming eleven defense methods and establishing a new state-of-the-art performance. Notably, this work pioneers the utilization of various GANs and diffusion models for generating highly realistic query samples in attacks, with Model-Guardian demonstrating accurate detection capabilities.",
      "year": 2025,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1d3483a45263d2f67dedba087ff267194c7cfa1d",
      "pdf_url": "",
      "publication_date": "2025-03-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16cfca0d74d329b2b3f2d927047468f09300e33e",
      "title": "CERBEROS: Compression-Based Efficient and Robust Optimized Security for Model Stealing Defense",
      "abstract": "Model stealing attacks pose an increasing threat to the confidentiality and intellectual property of artificial intelligence (AI) models. Existing defenses\u2013such as query monitoring, output perturbation, multi-model output variation, and post hoc verification\u2013fall short in on-device applications where models must run under strict memory and computation budgets. These approaches typically incur high memory or latency overhead due to their reliance on auxiliary models or additional inference-time processing. To address these limitations, we propose CERBEROS, a defense framework designed to achieve security against model stealing with deployability in resource-constrained environments. At its core, CERBEROS introduces a novel neural architecture with multiple classification heads trained jointly for output diversification, while sharing a single feature extraction backbone to minimize unnecessary memory usage. At inference, CERBEROS reveals the prediction of a randomly selected head, thereby misleading adversaries while preserving test accuracy for legitimate users, without requiring separate models or costly output modification. In addition, we integrate structured pruning into training to compress the backbone while retaining the classification heads. This ensures that functional diversity across heads remains achievable even under tight resource constraints. Our experiments show that CERBEROS effectively mitigates model replication attacks while consistently maintaining task performance across widely used convolutional neural networks and benchmark datasets. Furthermore, it achieves significant reductions in memory consumption and inference latency compared to prior defenses, offering a practical and efficient solution for securing on-device AI models.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Sohyun Keum",
        "Jeonghyun Lee",
        "Sangkyun Lee"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16cfca0d74d329b2b3f2d927047468f09300e33e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16743a0a8daad27538a0bc734ed42abca3a14289",
      "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
      "abstract": "Malicious users attempt to replicate commercial models functionally at low cost by training a clone model with query responses. It is challenging to timely prevent such model-stealing attacks to achieve strong protection and maintain utility. In this paper, we propose a novel non-parametric detector called Account-aware Distribution Discrepancy (ADD) to recognize queries from malicious users by leveraging account-wise local dependency. We formulate each class as a Multivariate Normal distribution (MVN) in the feature space and measure the malicious score as the sum of weighted class-wise distribution discrepancy. The ADD detector is combined with random-based prediction poisoning to yield a plug-and-play defense module named D-ADD for image classification models. Results of extensive experimental studies show that D-ADD achieves strong defense against different types of attacks with little interference in serving benign users for both soft and hard-label settings.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jian-Ping Mei",
        "Weibin Zhang",
        "Jie Chen",
        "Xuyun Zhang",
        "Tiantian Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/16743a0a8daad27538a0bc734ed42abca3a14289",
      "pdf_url": "",
      "publication_date": "2025-03-16",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "title": "Model Rake: A Defense Against Stealing Attacks in Split Learning",
      "abstract": "Split learning is a prominent framework for vertical federated learning, where multiple clients collaborate with a central server for model training by exchanging intermediate embeddings. Recently, it is shown that an adversarial server can exploit the intermediate embeddings to train surrogate models to replace the bottom models on the clients (i.e., model stealing). The surrogate models can also be used to reconstruct private training data of the clients (i.e., data stealing).\n\nTo defend against these stealing attacks, we propose Model Rake (i.e., Rake), which runs two bottom models on each client and differentiates their output spaces to make the two models distinct. Rake hinders the stealing attacks because it is difficult for a surrogate model to approximate two distinct bottom models. We prove that, under some assumptions, the surrogate model converges to the average of the two bottom models and thus will be inaccurate. Extensive experiments show that Rake is much more effective than existing methods in defending against both model and data stealing attacks, and the accuracy of normal model training is not affected.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Qinbo Zhang",
        "Xiao Yan",
        "Yanfeng Zhao",
        "Fangcheng Fu",
        "Quanqing Xu",
        "Yukai Ding",
        "Xiaokai Zhou",
        "Chuang Hu",
        "Jiawei Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e445ba60a5f0dd22cb9188df23c228c5ec7a1f45",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "title": "Digital Scapegoat: An Incentive Deception Model for Resisting Unknown APT Stealing Attacks on Critical Data Resource",
      "abstract": "It is a challenging problem to resist unknown advanced persistent threats (APTs) on stealing data resources in an information system of critical infrastructures, because APT attackers have very specific objectives and compromise the system stealthily and slowly. We observe that it is a necessary condition for APT attackers to achieve their campaigns via controlling unknown Trojans to access and exfiltrate critical files. We present a theoretical model called Digital Scapegoat (abbreviated as DS-IDep) that constructs an Incentive Deception defense schema to hijack the attacker\u2019s access to critical files and redirect it to avatar files without awareness. We propose a FlipIDep Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{F}$ </tex-math></inline-formula>) and a Markov Game model (<inline-formula> <tex-math notation=\"LaTeX\">$G_{M}$ </tex-math></inline-formula>) to characterize completely the payoffs, equilibria, and best strategies from the perspective of the attacker and the defender respectively. We also design an exponential risk propagation model to evaluate the ability of DS-IDep to eliminate stealing impact when the risk is propagated between states. Theoretically, we can achieve the objective of stealing impact elimination (<inline-formula> <tex-math notation=\"LaTeX\">$L_{K} \\lt 0.001$ </tex-math></inline-formula>) when the ratio of incentive deception exceeds 0.7 (<inline-formula> <tex-math notation=\"LaTeX\">$\\eta \\gt 0.7$ </tex-math></inline-formula>) and the probability of an attack operation bypassing the defense surface is less than 0.1 (<inline-formula> <tex-math notation=\"LaTeX\">$r^{*}\\times \\mu \\lt 0.1$ </tex-math></inline-formula>) under Stackelberg strategies. We develop a kernel-level incentive deception defense surface according to the theoretical parameters of the DS-IDep. The experimental results show that DS-IDep can resist APT stealing attacks from unknown Trojans. We also evaluate the DS-IDep in five well-known software applications. It demonstrates that DS-IDep can address unknown attacks from compromised software with less than 10% performance overhead.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaochun Yun",
        "Guangjun Wu",
        "Shuhao Li",
        "Qi Song",
        "Zixian Tang",
        "Zhenyu Cheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f2aaca56a4bbc4e9358f0af26e67f8cae4ebf476",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "title": "On Stealing Graph Neural Network Models",
      "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract a GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "Agnieszka Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d7acc4f16a3e3c33a9544bbe80a72ab218b23869",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5102a7c89adeba6818f90b1964106397a2fb7a37",
      "title": "Dynamic Gradient Compression and Attack Defense Strategy for Privacy Enhancement of Heterogeneous Data in Federated Learning",
      "abstract": "An innovatively constructed dynamic perception mechanism based on temporal and spatial dual dimensions is proposed. In particular, it dynamically adjusts the gradient compression ratio depending on the convergence rate of the model (e.g., using high compression ratio for fast convergence at early stage and fine-tuning later), which is realized by integrating loss variation rate and training cycle into compression ratio equation. In spatial dimension, this method adapts to heterogeneous data distributions by introducing a sample weight factor into the non-IID measurement index so that the heterogeneity of data can be quantified. A dynamic privacy budget allocation strategy based on data sensitivity matrix ensures adaptive noise injection and hierarchical encryption. In contrast to traditional methods, the anomaly detection module introduces high order statistical moments (skewness, kurtosis), combined with machine learning based attack classification methods, to detect gradient poisoning and model stealing attacks in real time.",
      "year": 2025,
      "venue": "2025 10th International Symposium on Advances in Electrical, Electronics and Computer Engineering (ISAEECE)",
      "authors": [
        "Conghui Wei",
        "Yaqian Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5102a7c89adeba6818f90b1964106397a2fb7a37",
      "pdf_url": "",
      "publication_date": "2025-06-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
      "abstract": "Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Linghui Zhu",
        "Yiming Li",
        "Haiqin Weng",
        "Yan Liu",
        "Tianwei Zhang",
        "Shu-Tao Xia",
        "Zhi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8caf3ee31a4d1d4d72f87997c104aaeb56e0a33",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed vision model adaptation, enabling the rapid deployment of customized models. However, the compactness of LoRA adaptations introduces new safety concerns, particularly their vulnerability to model extraction attacks. This paper introduces a new focus of model extraction attacks named LoRA extraction that extracts LoRA-adaptive models based on a public pre-trained model. We then propose a novel extraction method called StolenLoRA which trains a substitute model to extract the functionality of a LoRA-adapted model using synthetic data. StolenLoRA leverages a Large Language Model to craft effective prompts for data generation, and it incorporates a Disagreement-based Semi-supervised Learning (DSL) strategy to maximize information gain from limited queries. Our experiments demonstrate the effectiveness of StolenLoRA, achieving up to a 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where the attacker and victim models utilize different pre-trained backbones. These findings reveal the specific vulnerability of LoRA-adapted models to this type of extraction and underscore the urgent need for robust defense mechanisms tailored to PEFT methods. We also explore a preliminary defense strategy based on diversified LoRA deployments, highlighting its potential to mitigate such attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixu Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ced0ca56a1f623ebe1860f2551ee5b8011c9b87",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "056036a3ce3a21daf80167dee622b6b515f9490e",
      "title": "Black-box model functionality stealing for Vietnamese sentiment analysis",
      "abstract": "Black-box deep learning models often keep critical components such as model architecture, hyperparameters, and training data confidential, allowing users to observe only the inputs and outputs without understanding their internal workings. Consequently, there is growing interested in developing \"knockoff\" models that replicate the behavior of these black-box models without direct access to internal details. We have conducted extensive studies on function extraction attacks targeting English text sentiment analysis models. By employing random or adaptive sampling methods, we have successfully reconstructed knockoff models that achieve functionality equivalent to the original models with high similarity. In this study, we extend our investigation to sentiment analysis datasets in Vietnamese. Experimental results demonstrate that for black-box models in Vietnamese text sentiment analysis, our method remains effective, successfully constructing models with equivalent functionality.",
      "year": 2025,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Viet-Binh Do",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/056036a3ce3a21daf80167dee622b6b515f9490e",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "title": "Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks",
      "abstract": "The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model's functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model's training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at https://github.com/rajankita/QueryWise.",
      "year": 2025,
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "authors": [
        "Ankita Raj",
        "Harsh Swaika",
        "Deepankar Varma",
        "Chetan Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bb3cf7ad6f2528490b40a8266ca1fa4dc5b929f4",
      "pdf_url": "",
      "publication_date": "2025-06-24",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "title": "DeepAW: A Customized DNN Watermarking Scheme Against Unreliable Participants",
      "abstract": "Training DNNs requires large amounts of labeled data, costly computational resources, and tremendous human effort, resulting in such models being a valuable commodity. In collaborative learning scenarios, unreliable participants are widespread due to data collected from a diverse set of end-users that differ in quality and quantity. It is important to note that failure to take into account the contributions of all participants in the collaborative model training process when sharing the model with them could potentially result in a deterioration in collaborative efforts. In this paper, we propose a customized DNN watermarking scheme to safeguard the model ownership, namely DeepAW, achieving robustness to model stealing attacks and collaborative fairness in the presence of unreliable participants. Specifically, DeepAW leverages the tightly binding between the embedded watermarking and the model performance to defend against the model stealing attacks, resulting in the sharp decline of the model performance encountering any attempt at watermarking modification. DeepAW achieves collaborative fairness by detecting unreliable participants and customizing the model performance according to the participants' contributions. Furthermore, we set up three model stealing attacks and four types of unreliable participants. The experimental results demonstrate the effectiveness, robustness, and collaborative fairness of DeepAW.",
      "year": 2025,
      "venue": "IEEE Transactions on Network Science and Engineering",
      "authors": [
        "Shen Lin",
        "Xiaoyu Zhang",
        "Xu Ma",
        "Xiaofeng Chen",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c100542ca883890adf1fa185e07e13e9f4e5ec46",
      "pdf_url": "",
      "publication_date": "2025-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "title": "Catch the Star: Weight Recovery Attack Using Side-Channel Star Map Against DNN Accelerator",
      "abstract": "The rapid development of Artificial Intelligence (AI) technology must be connected to the arithmetic support of high-performance hardware. However, when the deep neural network (DNN) accelerator performs inference tasks at the edge end, the sensitive data of DNN will generate leakage through side-channel information. The adversary can recover the model structure and weight parameters of DNN by using the side-channel information, which seriously affects the protection of necessary intellectual property (IP) of DNN, so the hardware security of the DNN accelerator is critical. In the current research of Side-channel attack (SCA) for matrix multiplication units, such as systolic arrays, the linear multiplication operation leads to a more extensive weights search space for the SCA, and extracting all the weight parameters requires higher attack conditions. This article proposes a new power SCA method, which includes a Collision-Correlation Power Analysis (Collision-CPA) and Correlation-based Weight Search Algorithm (C-WSA) to address the problem. The Collision-CPA reduces the attack conditions for the SCA by building multiple Hamming Distance (HD)-based power leakage models for the systolic array. Meanwhile, the C-WSA dramatically reduces the weights search space. In addition, the concept of a Side-channel star map (SCSM) is proposed for the first time in this article, and the adversary can quickly and accurately locate the correct weight information in the SCSM. Through experiments, we recover all the weight parameters of a $3\\times 3$ systolic array based on 100000 power traces, in which the weight search space is reduced by up to 97.7%. For the DNN accelerator at the edge, especially the systolic array structure, our proposed novel SCA aligns more with practical attack scenarios, with lower attack conditions, and higher attack efficiency.",
      "year": 2025,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiang Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/23bb050bbdf0d710f9f6f665e227b25c1d6ee034",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "title": "MACPruning: Dynamic Operation Pruning to Mitigate Side-Channel DNN Model Extraction",
      "abstract": "As deep learning gains popularity, edge IoT devices have seen proliferating deployment of pre-trained Deep Neural Network (DNN) models. These DNNs represent valuable intellectual property and face significant confidentiality threats from side-channel analysis (SCA), particularly non-invasive Differential Electromagnetic (EM) Analysis (DEMA), which retrieves individual model parameters from EM traces collected during model inference. Traditional SCA mitigation methods, such as masking and shuffling, can still be applied to DNN inference, but will incur significant performance degradation due to the large volume of operations and parameters. Based on the insight that DNN models have high redundancy and are robust to input variation, we introduce MACPruning, a novel lightweight defense against DEMA-based parameter extraction attacks, exploiting specific characteristics of DNN execution. The design principle of MACPruning is to randomly deactivate input pixels and prune the operations (typically multiply-accumulate-MAC) on those pixels. The technique removes certain leakages and overall redistributes weight-dependent EM leakages temporally, and thus effectively mitigates DEMA. To maintain DNN performance, we propose an importance-aware pixel map that preserves critical input pixels, keeping randomness in the defense while minimizing its impact on DNN performance due to operation pruning. We conduct a comprehensive security analysis of MACPruning on various datasets for DNNs on edge devices. Our evaluations demonstrate that MACPruning effectively reduces EM leakages with minimal impact on the model accuracy and negligible computational overhead.",
      "year": 2025,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Ruyi Ding",
        "Gongye Cheng",
        "Davis Ranney",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e685da2d597d7b478a514bf26d78f5d9131e6b98",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2c361d27c70f4b61da39732c338c54145a4cb82",
      "title": "Exploiting Power Side-Channel Vulnerabilities in XGBoost Accelerator",
      "abstract": "XGBoost (eXtreme Gradient Boosting), a widelyused decision tree algorithm, plays a crucial role in applications such as ransomware and fraud detection. While its performance is well-established, its security against model extraction on hardware platforms like Field Programmable Gate Arrays (FPGAs) has not been fully explored. In this paper, we demonstrate a significant vulnerability where sensitive model data can be leaked from an XGBoost implementation through side-channel attacks (SCAs). By analyzing variations in power consumption, we show how an attacker can infer node features within the XGBoost model, leading to the extraction of critical data. We conduct an experiment using the XGBoost accelerator FAXID on the Sakura-X platform, demonstrating a method to deduce model decisions by monitoring power consumptions. The results show that on average 367k tests are sufficient to leak sensitive values. Our findings underscore the need for improved hardware and algorithmic protections to safeguard machine learning models from these types of attacks.",
      "year": 2025,
      "venue": "Design Automation Conference",
      "authors": [
        "Yimeng Xiao",
        "Archit Gajjar",
        "Aydin Aysu",
        "Paul Franzon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2c361d27c70f4b61da39732c338c54145a4cb82",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a56e4103911d723020bb99bf63f5bb452b236e9",
      "title": "PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization",
      "abstract": "Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner",
      "year": 2025,
      "venue": "",
      "authors": [
        "Mingzhe Li",
        "Renhao Zhang",
        "Zhiyang Wen",
        "Siqi Pan",
        "Bruno Castro da Silva",
        "Juan Zhai",
        "Shiqing Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a56e4103911d723020bb99bf63f5bb452b236e9",
      "pdf_url": "",
      "publication_date": "2025-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-02"
    },
    {
      "paper_id": "f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "title": "Middleware Architecture for the Management and Mitigation of OWASP ML05: Model Theft in IoT Machine Learning Networks",
      "abstract": "The increasing integration of machine learning (ML) models into Internet of Things (IoT) applications has led to notable advancements in automation and decision-making. However, these models are vulnerable to modern attack vectors recognized by the OWASP Top 10 for Large Language Model Applications, specifically ML05: Model Theft, where adversaries gain unauthorized access to model parameters and training data, compromising intellectual property and sensitive information. Such threats are particularly concerning in IoT environments due to their distributed nature and resource limitations. This paper proposes a middleware architecture for the management and mitigation of model theft risks by incorporating encryption, access control, obfuscation, watermarking, continuous monitoring, and service assurance programmability. By strengthening the security management framework of ML models deployed in IoT, the proposed architecture aims to protect against theft, ensure data confidentiality, and maintain network resilience. The approach includes detailed mathematical models and an evaluation of existing security measures, demonstrating the architecture's effectiveness in diverse IoT deployments, such as telemedicine and smart cities.",
      "year": 2025,
      "venue": "Global",
      "authors": [
        "Yair Enrique Rivera Julio",
        "\u00c1ngel Pinto",
        "Nelson A. P\u00e9rez-Garc\u00eda",
        "M\u00f3nica-Karel Huerta",
        "C\u00e9sar Viloria-N\u00fa\u00f1ez",
        "Marvin Luis P\u00e9rez Cabrera",
        "Frank Ibarra Hern\u00e1ndez",
        "Juan Manuel Torres Tovio",
        "Erwin J. Sacoto-Cabrera"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f9e06ecdefac93f1c0dc26436ed73c0e708d4307",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45475dcf398e58811ac8487fbd72fe6777bd8676",
      "title": "Quantum Disturbance-based Photon Cloning Attack",
      "abstract": "Geopolitical concerns in Asia have intensified, driving countries to bolster their border security and upgrade their military capabilities. With new and complex threats emerging, governments must invest in advanced defense technologies to maintain strategic stability and deter large-scale conflicts. The deployment and development of advanced military technologies is one of the key focuses. Surveillance largely contributes to situational awareness, fortifying defense and response time, making it the perfect tool for drones, AI-driven surveillance systems, cyber warfare capabilities, and anti-ship missiles. Such technologies enable countries to detect, prevent, and counter threats efficiently, thus making them essential in 21st-century warfare. In addition to the arms race in conventional defense, we're seeing increasingly base forms of international competition, particularly around advanced technologies like space-based reconnaissance, quantum encryption, and hypersonic weapons.Additionally, employing features such as AI surveillance systems, smart defense systems, and civil forces will be needed to stop wars before they happen. Governments must ensure they are attending to the urgent and game-changing elements, such as early-warning systems, predictive analytics, and automated threat response technologies that make for speedy and effective crisis management. It\u2019s critical to tighten cybersecurity infrastructure, as cyberattacks against military and governmental networks are increasing. Modernization efforts for an agile and resilient military force require substantial investment in R&D and defense infrastructure. New developments in autonomous defense systems, robotics, and advanced missile systems are key pieces in deterring aggression and protecting national interests.",
      "year": 2025,
      "venue": "International Conference on Innovative Mechanisms for Industry Applications",
      "authors": [
        "Hui-Kai Su",
        "K.MahaRajan",
        "Sanmugasundaram R",
        "M.Jayalakshmi",
        "A. S. Nantha",
        "Wen-Kai Kuo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/45475dcf398e58811ac8487fbd72fe6777bd8676",
      "pdf_url": "",
      "publication_date": "2025-09-03",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "86c0661fdb12bcde0709e91416510fda72f10f13",
      "title": "A Comprehensive Survey of Model Extraction Attacks: Current Trends, Defenses, and Future Directions",
      "abstract": "Model extraction attacks pose a significant threat to Machine Learning (ML) systems, especially in cloud-based services like Machine Learning as a Service (MLaaS). Attacks aim to steal proprietary models by replicating their functionality or extracting their internal parameters. This paper reviews model extraction attack types, examining existing defensive techniques and weaknesses in current defenses. Promising defense mechanisms are discussed, including adaptive privacy budgets, hybrid defense strategies, combining multiple methods, and hardwarebased security solutions. Emerging attack models like collaborative attacks in federated learning environments are explored. Future research focuses on adaptive defenses and Artificial Intelligence (AI)-driven detection methods to improve model robustness and contribute to more resilient machine learning systems.",
      "year": 2025,
      "venue": "2025 1st International Conference on Secure IoT, Assured and Trusted Computing (SATC)",
      "authors": [
        "Quazi Rian Hasnaine",
        "Yaodan Hu",
        "Mohamed I. Ibrahem",
        "Mostafa M. Fouda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/86c0661fdb12bcde0709e91416510fda72f10f13",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68f0633a0be1a7009c4caf765529d898b2450044",
      "title": "SONNI: Secure Oblivious Neural Network Inference",
      "abstract": "In the standard privacy-preserving Machine learning as-a-service (MLaaS) model, the client encrypts data using homomorphic encryption and uploads it to a server for computation. The result is then sent back to the client for decryption. It has become more and more common for the computation to be outsourced to third-party servers. In this paper we identify a weakness in this protocol that enables a completely undetectable novel model-stealing attack that we call the Silver Platter attack. This attack works even under multikey encryption that prevents a simple collusion attack to steal model parameters. We also propose a mitigation that protects privacy even in the presence of a malicious server and malicious client or model provider (majority dishonest). When compared to a state-of-the-art but small encrypted model with 32k parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while batching capability is reduced by 0.2%. Our approach uses a novel results-checking protocol that ensures the computation was performed correctly without violating honest clients' data privacy. Even with collusion between the client and the server, they are unable to steal model parameters. Additionally, the model provider cannot learn any client data if maliciously working with the server.",
      "year": 2025,
      "venue": "International Conference on Security and Cryptography",
      "authors": [
        "Luke Sperling",
        "Sandeep S. Kulkarni"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68f0633a0be1a7009c4caf765529d898b2450044",
      "pdf_url": "",
      "publication_date": "2025-04-26",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "title": "Detecting Generative Model Inversion Attacks for Protecting Intellectual Property of Deep Neural Networks",
      "abstract": "Recently, protecting the Intellectual Property (IP) of deep neural networks (DNNs) has attracted attention from researchers. This is because training DNN models can be costly especially when acquiring and labeling training data require domain expertise. DNN watermarking and fingerprinting are two techniques proposed to prevent DNN IP infringement. Although these two techniques achieve high performance on defending against previously proposed DNN stealing attacks, researchers recently show that both of them are ineffective against generative model inversion attacks. Specifically, an adversary inverts training data from well-trained DNNs and uses the inverted data to train DNNs from scratch such that DNN watermarking and fingerprinting are both bypassed. This novel model stealing strategy shows that data inverted from victim models can be effectively exploited by adversaries, which poses a new threat to the IP protection of DNNs. To combat this new threat, one potential solution is to enable defenders to prove ownership on data inverted from models being protected. If the training data of a suspected model, which can be disclosed via the judicial process, are proven to be data inverted from victim models, then IP infringement is detected. This research direction is currently underexplored. In this paper, we fill the gap in the literature to investigate countermeasures against this emerging threat. We propose a simple but effective method, called InverseDataInspector (IDI), to detect whether data are inverted from victim models. Specifically, our method first extracts features from both the inverted data and victim models. These features are then combined and used for training classifiers. Experimental results demonstrate that our method achieves high performance on detecting inverted data and also generalizes to new generative model inversion methods that are not seen when training classifiers.",
      "year": 2025,
      "venue": "Journal of Artificial Intelligence Research",
      "authors": [
        "Yiding Yu",
        "W. Zong",
        "Wenjing Su",
        "Yang-Wai Chow",
        "Willy Susilo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/dc1d5b64a1b8bf876b4cd518f3786b6332dbc9b6",
      "pdf_url": "",
      "publication_date": "2025-10-28",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "title": "Knockoff Branch: Model Stealing Attack via Adding Neurons in the Pre-Trained Model",
      "abstract": "We introduce Knockoff Branch: adding few neurons as a knockoff container for learning stolen features. Model stealing attacks extract the functionality from the victim model by querying APIs. Prior work substantially enhanced transferability and improved query efficiency between the adversary model and the victim model. However, there is still a limited understanding of the knockoff itself. For knockoff, the model is either compared to the same type but with different structures or different types and capacities. For this reason, we propose a framework to analyze the knockoff quality for a single model, specifically reinvestigating transformer-based extraction. We observed that 1) when the adversary can access the public pretrained model, full fine-tuning is not necessary. This allows a knockoff to require only about 0.5% of trainable parameters and 20 epochs. 2) Although querying by out-of-distribution datasets leads to a sub-optimal knockoff, this issue can be mitigated by scaling branch features, even without using complicated sampling strategies. Our proposed method is lightweight and achieves high accuracy, at most similar to white-box knowledge distillation (higher performance than the victim model). https://github.com/onlyin-hung/knockoff-branch.",
      "year": 2025,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Li-Ying Hung",
        "Cooper Cheng-Yuan Ku"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/64bebdbe7fa9a6b173533e64c35960e0378ea21a",
      "pdf_url": "",
      "publication_date": "2025-02-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "title": "Too Clever by Half: Detecting Sampling-based Model Stealing Attacks by Their Own Cleverness",
      "abstract": "Machine learning as a service (MLaaS) has gained significant popularity and market traction in recent years, driven by advancements in Artificial Intelligence particularly Generative AI (GAI). However, MLaaS faces severe challenges from sampling-based model stealing attacks (MSAs), where attackers strategically query the targeted ML models provided by MLaaS providers to minimize the query burden while closely replicating the model\u2019s functionality. Such MSAs pose severe consequences, including intellectual property (IP) theft and potential leakage of private training data. Unfortunately, existing defenses either sacrifice model utility or fail to generalize across diverse MSAs.In this paper, we propose DIARY, an innovative detection method specifically tailored to sampling-based MSAs by exploiting their inherent sophistication. Our key insight is that \u2018clever\u2019 malicious queries tend to extract more information from the targeted (victim) model than typical benign queries, as these attacks iteratively refine their queries by examining and analyzing prior queries and the corresponding responses. Hence we design DIARY to extract timing dependence within a query sequence and incorporate contrastive learning for properly characterizing such dependency that holds for different sampling-based MSAs. Comprehensive evaluations using five different sampling-based MSAs and two state-of-the-art defense baselines across four popular datasets consistently validate DIARY\u2019s superior performance.",
      "year": 2025,
      "venue": "IEEE International Conference on Distributed Computing Systems",
      "authors": [
        "Xin Yao",
        "Chenyang Wang",
        "Yimin Chen",
        "Kecheng Huang",
        "Jiawei Guo",
        "Ming Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bc15874a504bc3abf610ce66fc8bc8d5e8d6e7ac",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "title": "DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks",
      "abstract": "Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao",
        "Xin Zhao",
        "He Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a659e69b704b4f215bb775ae54f6bfe8411dd8db",
      "pdf_url": "",
      "publication_date": "2025-11-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "title": "A Method for Stealing Traffic Detection Models Based on Equivalent Feature Sets",
      "abstract": "Machine learning models have shown excellent performance in the field of traffic detection. However, they also face new security challenges. This study focuses on model stealing attacks in the traffic detection field. Existing model stealing methods are difficult to implement in real - world network environments. They either cannot be applied to the traffic detection field or require prior knowledge of the target model's feature set. To achieve the stealing of traffic detection models in a truly black - box scenario and overcome the strong assumption of relying on the known target model feature set in existing methods, this study proposes a method for stealing traffic detection models based on feature inference. This study introduces the concept of \u201cequivalent feature set\u201d, analyzes the prediction logic of the target model and the characteristics of traffic data through two feature inference algorithms, constructs an equivalent feature set, and uses it as the feature set for training a substitute model. The results of stealing experiments on multiple target models show that in most experimental settings, the stealing rate exceeds 85%, with a maximum of 96.64%, demonstrating the high efficiency and stability of the method. At the same time, experimental verification shows that the equivalent feature set can accurately capture key features, significantly improving the stealing effect of the substitute model, with an improvement amplitude of more than 40.48%.",
      "year": 2025,
      "venue": "International Conference Civil Engineering and Architecture",
      "authors": [
        "Long Meng",
        "Bin Lu",
        "Xu Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fdf69d7c5067bd1eddba2cc577b72be20a3546b0",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a744498909536ee4a0752f3fe2430c13268a2b82",
      "title": "Siamese: Stealing Fine-Tuned Visual Foundation Models via Diversified Prompting",
      "abstract": "Visual foundation models, characterized by their robust generalization and adaptability, serve as the basis for a wide array of downstream tasks. When fine-tuned for specific tasks, these models encapsulate confidential and valuable task-specific knowledge, making them prime targets for model stealing (MS) attacks. While recent efforts have exposed MS threats in practical scenarios such as data-free and hard-label contexts, these attacks predominantly target traditional victim models trained from scratch. Fine-tuned visual foundation models, pre-trained on vast and diverse datasets and then fine-tuned on downstream tasks, present significant challenges for traditional MS attacks to extract task-specific knowledge. In this paper, we introduce an innovative MS attack, named SIAMESE, to steal fine-tuned visual foundation models under black-box, data-free, and hard-label settings. The core approach of SIAMESE involves constructing a stolen model using a foundation model that is efficiently and concurrently fine-tuned with multiple diversified soft prompts. To integrate the knowledge derived from these prompts, we propose a novel and tractable loss function that analyzes the output distributions while enforcing orthogonality among the prompts to minimize interference. Additionally, a unique alignment module enhances SIAMESE by synchronizing interpretations between the victim and stolen models. Extensive experiments validate that SIAMESE outperforms state-of-the-art baseline attacks over 10% in accuracy, exposing the heightened vulnerability of fine-tuned visual foundation models to MS threats.",
      "year": 2025,
      "venue": "Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing",
      "authors": [
        "Madhureeta Das",
        "Gaurav Bagwe",
        "Miao Pan",
        "Kaichen Yang",
        "X. Yuan",
        "Lan Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a744498909536ee4a0752f3fe2430c13268a2b82",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "title": "Security Challenges and Mitigation Strategies in Generative AI Systems",
      "abstract": "This article examines the critical security challenges and mitigation strategies in generative AI systems. The article explores how these systems have transformed various sectors, particularly in financial markets and critical infrastructure, while introducing significant security concerns. The article analyzes various types of adversarial attacks, including input perturbation and backdoor attacks, and their impact on AI model performance. Additionally, it investigates model stealing threats and data privacy concerns in AI deployments. The article presents comprehensive mitigation strategies, including advanced defense mechanisms, enhanced protection frameworks, and secure access control implementations. The article findings demonstrate the effectiveness of integrated security approaches in protecting AI systems while maintaining operational efficiency. This article contributes to the growing body of knowledge on AI security by providing evidence-based strategies for protecting generative AI systems across different application domains.",
      "year": 2025,
      "venue": "International Journal of Scientific Research in Computer Science Engineering and Information Technology",
      "authors": [
        "Satya Naga Mallika Pothukuchi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/557b82a8e54356b25b20fd96bd86dc449d8d10bc",
      "pdf_url": "https://doi.org/10.32628/cseit25112377",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "title": "Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses",
      "abstract": "Quantum Machine Learning (QML) integrates quantum computing with classical machine learning, primarily to solve classification, regression and generative tasks. However, its rapid development raises critical security challenges in the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines adversarial threats unique to QML systems, focusing on vulnerabilities in cloud-based deployments, hybrid architectures, and quantum generative models. Key attack vectors include model stealing via transpilation or output extraction, data poisoning through quantum-specific perturbations, reverse engineering of proprietary variational quantum circuits, and backdoor attacks. Adversaries exploit noise-prone quantum hardware and insufficiently secured QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership, and functionality. Defense mechanisms leverage quantum properties to counter these threats. Noise signatures from training hardware act as non-invasive watermarks, while hardware-aware obfuscation techniques and ensemble strategies disrupt cloning attempts. Emerging solutions also adapt classical adversarial training and differential privacy to quantum settings, addressing vulnerabilities in quantum neural networks and generative architectures. However, securing QML requires addressing open challenges such as balancing noise levels for reliability and security, mitigating cross-platform attacks, and developing quantum-classical trust frameworks. This chapter summarizes recent advances in attacks and defenses, offering a roadmap for researchers and practitioners to build robust, trustworthy QML systems resilient to evolving adversarial landscapes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Archisman Ghosh",
        "Satwik Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ab3cd21bb78472e4fe7f4568b025abc76e3f59a9",
      "pdf_url": "",
      "publication_date": "2025-06-26",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "title": "DAV: An Adaptive Defense Framework for Model Extraction Attacks",
      "abstract": "Machine learning platforms offer paid APIs to enable personalized inference services. However, model extraction attacks greatly threaten their intellectual property rights. Malicious users can create query samples using proxy datasets or generative models to train a clone model. Existing defense approaches usually focus on models that return soft-labels, and cannot effectively handle extracting attacks against hard-label models. In this paper, we propose an adaptive defense framework named DAV, which consists of a malicious query detector and an adaptive perturbation mechanism. Two perturbation strategies can be selected based on the detection results and the malicious query rate within the buffer queue, including accuracy-preserving perturbation and maximum-minimum probability inverse perturbation. Comprehensive experimental results show that DAV can significantly reduce the accuracy of the clone model with little impact on the performance of the victim model and benign queries, no matter whether the returned probabilities are for soft-label or hard-label.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Peng Sui",
        "Jiapeng Zhou",
        "Yu Chen",
        "Youhuizi Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b7ac762bbb6a90d1ed6b0ede09a2410cff9f1961",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ae1157c1fba9603a98566209bc266f6b7a534629",
      "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
      "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful machine learning models through cloud-based APIs, offering scalability and ease of deployment. However, these services are vulnerable to model extraction attacks, where adversaries repeatedly query the application programming interface (API) to reconstruct a functionally similar model, compromising intellectual property and security. Despite various defense strategies being proposed, many suffer from high computational costs, limited adaptability to evolving attack techniques, and a reduction in performance for legitimate users. In this paper, we introduce a Resilient Adaptive Defense Framework for Model Extraction Attack Protection (RADEP), a multifaceted defense framework designed to counteract model extraction attacks through a multi-layered security approach. RADEP employs progressive adversarial training to enhance model resilience against extraction attempts. Malicious query detection is achieved through a combination of uncertainty quantification and behavioral pattern analysis, effectively identifying adversarial queries. Furthermore, we develop an adaptive response mechanism that dynamically modifies query outputs based on their suspicion scores, reducing the utility of stolen models. Finally, ownership verification is enforced through embedded watermarking and backdoor triggers, enabling reliable identification of unauthorized model use. Experimental evaluations demonstrate that RADEP significantly reduces extraction success rates while maintaining high detection accuracy with minimal impact on legitimate queries. Extensive experiments show that RADEP effectively defends against model extraction attacks and remains resilient even against adaptive adversaries, making it a reliable security framework for MLaaS models.",
      "year": 2025,
      "venue": "International Conference on Wireless Communications and Mobile Computing",
      "authors": [
        "Amit Chakraborty",
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "E. Bowen",
        "Sachin Shetty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae1157c1fba9603a98566209bc266f6b7a534629",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "title": "Augmenting Model Extraction Attacks Against Disruption-Based Defenses",
      "abstract": "Existing research has demonstrated that deep neural networks are susceptible to model extraction attacks, where an attacker can construct a substitute model with similar functionality to the victim model by querying the black-box victim model. To counter such attacks, various disruption-based defenses have been proposed. These defenses disrupt the output results of queries before returning them to potential attackers. In this paper, we propose the first defense-penetrating model extraction attack framework, aimed at breaking disruption-based defense methods. Our proposed attack framework comprises two key modules: disruption detection and disruption recovery, which can be integrated into generic model extraction attacks. Specifically, the disruption detection module uses a novel meta-learning-based algorithm to infer the defense strategy employed by the defender, by learning the key differences between the distributions of disrupted and undisrupted query results. Once the defense method is inferred, the disruption recovery module is designed to restore clean query results from the disrupted query results, using a carefully-designed generative model. We conducted extensive experiments on 5 commonly-used datasets to evaluate the effectiveness of our proposed framework. The results demonstrate that the substitute model accuracy of current model extraction attacks can be significantly improved by up to 82.42%, even when faced with four state-of-the-art model extraction defenses. Moreover, our attack approach shows promising results in penetrating unknown defenses in real-world cloud service APIs hosted by Microsoft Azure and Face++.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xueluan Gong",
        "Shuaike Li",
        "Yanjiao Chen",
        "Mingzhe Li",
        "Rubin Wei",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0912c3c7cbcc0ffd85d5334848b17e9128eaf5d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "15cb47e68968064996190efae5a5f875af516ac9",
      "title": "POSTER: Disappearing Ink: How Partial Model Extraction Erases Watermarks",
      "abstract": "Deep neural networks have become invaluable intellectual property in machine learning. To deter model theft, Watermarking has emerged as a prominent defense by embedding hidden \u201ctrigger sets\u201d that aid in ownership verification. However, current watermarking solutions primarily address scenarios where adversaries steal the entire model. In this paper, we reveal a critical gap: partial model extraction, where only a subset of classes is stolen, substantially degrading the watermark\u2019s reliability. We introduce two attacks, Partial Model Extraction and Partial Knowledge Distillation, which reduce watermark accuracy by up to 80% while retaining strong performance on the stolen classes. Through extensive experiments on CIFAR10 and CIFAR100 against two state-of-the-art watermarking schemes, we demonstrate the need for more robust watermarking strategies that resist partial-class theft.",
      "year": 2025,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Venkata Sai Pranav Bachina",
        "Ankit Gangwal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/15cb47e68968064996190efae5a5f875af516ac9",
      "pdf_url": "",
      "publication_date": "2025-08-24",
      "keywords_matched": [
        "model extraction",
        "model theft"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "title": "Model Extraction Attack and Its Countermeasure for Denoising Diffusion Implicit Models",
      "abstract": "Recently, the threat of cyber attacks against machine learning models has been increasing. Typical examples include Model Extraction Attack (MEA), which steals the functionality of a victim model by creating its clone model that has almost the same functionality. Thus, the literature has studied MEA and its defense methods, mainly focusing on image recognition models. However, no existing studies evaluate the risk of MEA on diffusion-based image generation models, despite the recent advances and widespread use of image generation AI services powered by diffusion models. In this paper, we first demonstrate the feasibility of MEA on DDIM, one of the most common diffusion-based image generation models. Then, as a countermeasure, we propose a defense method that detects clone models of DDIM. In the proposed method, we add a small number of out-of-distribution images, referred to as \u201cmarking images\u201d, to the training dataset of a victim DDIM model. This technique provides the property of occasionally generating marking images for the victim model. This property works as a watermark and is inherited by the clone models, being used as a clue for detecting them. In the results of our experiments conducted on face, fruit, and church image datasets, the proposed defense method can correctly detect all clone models without seriously degrading the usability of victim DDIM models.",
      "year": 2025,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Hayato Shoji",
        "Kazuaki Nakamura"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/504b2d98d26dfc4bf46b53de5c5c30acfcd0082a",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
      "abstract": "Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhuochen Yang",
        "Fok Kar Wai",
        "V. Thing"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb09eaceb17a4813fd8ce3496d01e1c78c6cec06",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "80b18d8f790620e175cde50af13e78d03ec424b8",
      "title": "The Implementation of Neural Network Encryption Algorithms for Side-Channel Attack Protection",
      "abstract": "\u00a0With the rapid development of deep learning in the field of side-channel analysis, neural network-trained encryption algorithms have demonstrated numerous advantages, providing novel ideas for resisting side-channel attacks. We implemented a bit neural network-based block encryption scheme resistant to side-channel attacks. Experimental verification shows that the complete algorithm combined with this scheme exhibits correctness, reliability, efficiency, and resistance to side-channel attacks. This scheme has two significant advantages: First, bit networks can achieve functions that multilayer perceptrons (MLPs) cannot perform. For example, using the AES encryption algorithm, we successfully reduced the column mixing network loss during MLP training from 0.25 to 0. Second, bit networks can integrate with MLPs without intermediate value leakage issues. Once combined with MLPs, the generalization capability of the AES round operation model is significantly enhanced, while ensuring that the number",
      "year": 2025,
      "venue": "Journal of Computing and Electronic Information Management",
      "authors": [
        "Bo Chen",
        "Yi Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/80b18d8f790620e175cde50af13e78d03ec424b8",
      "pdf_url": "",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "63fb4344b3b77b8b23b89c732d31274520c8f026",
      "title": "Exploring A Model Type Detection Attack against Machine Learning as A Service",
      "abstract": "Recently, Machine-Learning-as-a-Service (MLaaS) systems are reported to be vulnerable to varying novel attacks, e.g., model extraction attacks and adversarial examples. However, in our investigation, we notice that the majority of MLaas attacks are not as threatening as expected due to model-type-sensitive problem. Literally speaking, many MLaaS attacks are designed for only a specific type of models. Without model type info as default prior knowledge, these attacks suffer from great performance degradation, or even become infeasible! In this paper, we demonstrate a novel attack method, named SNOOPER, to resolve the model-type-sensitive problem of MLaaS attacks. Specifically, SNOOPER is integrated with multiple self-designed model-type-detection modules. Each module can judge whether a given black-box model belongs to a specific type of models by analyzing its query-response pattern. Then, after proceeding with all modules, the attacker can know the type of its target model in the querying stage, and accordingly choose the optimal attack method. Also, to save budget, the queries can be re-used in the latter attack stage. We call such a kind of attack as model-type-detection attack. Finally, we experiment with SNOOPER on some popular model classes, including decision trees, linear models, non-linear models and neural networks. The results show that SNOOPER is capable of detecting the model type with more than 90% accuracy.",
      "year": 2025,
      "venue": "Journal of Intelligent Computing and Networking",
      "authors": [
        "Yilong Yang",
        "Xinjing Liu",
        "Ruidong Han",
        "Yang Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/63fb4344b3b77b8b23b89c732d31274520c8f026",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d66b4910d4c16463e1859035cca94bbefbd76f92",
      "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
      "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhengchunmin Dai",
        "Jiaxiong Tang",
        "Peng Sun",
        "Honglong Chen",
        "Liantao Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d66b4910d4c16463e1859035cca94bbefbd76f92",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Shufan Yang",
        "Zifeng Cheng",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Cong Wang",
        "Shiping Ge",
        "Yuchen Fu",
        "Qing Gu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c39a0348fc04f387ebdf91079d0b8b79d0984ce5",
      "pdf_url": "",
      "publication_date": "2025-11-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "title": "Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks",
      "abstract": "Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks. For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yaxin Xiao",
        "Qingqing Ye",
        "Zi Liang",
        "Haoyang Li",
        "Ronghua Li",
        "Huadi Zheng",
        "Haibo Hu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0f6d3176161c5dfe0ec6642afa5e75231bc519dd",
      "pdf_url": "",
      "publication_date": "2025-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ff22d38603caf509358f50ef67244b039c63a5b1",
      "title": "Budget and Frequency Controlled Cost-Aware Model Extraction Attack on Sequential Recommenders",
      "abstract": "Sequential recommenders are integral to many applications yet remain vulnerable to model extraction attacks, in which adversaries can recover information about the deployed model by issuing queries to a black-box without internal access. From the attacker's perspective, existing studies impose a fixed and limited query budget but overlook optimal allocation, resulting in redundant or low-value requests. Furthermore, the scarce data obtained through these costly queries is typically handled by crude random sampling, resulting in low diversity and information coverage with actual data. In this paper, we propose a novel approach, named Budget and Frequency Controlled Cost-Aware Model Extraction Attack (BECOME), for extracting black-box sequential recommenders, which extends the standard extraction framework with two cost-aware innovations: Feedback-Driven Dynamic Budgeting periodically evaluates the victim model to refine query allocation and steer sequence generation adaptively. Rank-Aware Frequency Controlling integrates frequency constraints with ranking guidance in the next-item sampler to select high-value items and broaden information coverage. Experiments on public datasets and representative sequential recommender architectures demonstrate that our method achieves superior extraction performance. Our code is released at https://github.com/Loche2/BECOME.",
      "year": 2025,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Lei Zhou",
        "Min Gao",
        "Zongwei Wang",
        "Yibing Bai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ff22d38603caf509358f50ef67244b039c63a5b1",
      "pdf_url": "",
      "publication_date": "2025-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "112442beb4161e251a44ea532893c8a32eb17ba3",
      "title": "Ownership Infringement Detection for Generative Adversarial Networks Against Model Stealing",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process to verify the ownership of a GAN. In this article, we show that these methods are not robust to emerging model extraction attacks. Then, we propose a new method GAN-Guards which utilizes the common characteristics of a target model and its stolen models for ownership infringement detection. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method achieves superior detection performance, compared with the watermark-based and fingerprint-based methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, and adaptive attacks.",
      "year": 2025,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/112442beb4161e251a44ea532893c8a32eb17ba3",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8cf823eb4c9309221953480a9b98c7700136c546",
      "title": "SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment",
      "abstract": "With the increasing deployment of Large Language Models (LLMs) on mobile and edge platforms, securing them against model extraction attacks has become a pressing concern. However, protecting model privacy without sacrificing the performance benefits of untrusted AI accelerators, such as GPUs, presents a challenging trade-off. In this paper, we initiate the study of high-performance execution on LLMs and present SecureInfer, a hybrid framework that leverages a heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate privacy-critical components while offloading compute-intensive operations to untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts an information-theoretic and threat-informed partitioning strategy: security-sensitive components, including non-linear layers, projection of attention head, FNN transformations, and LoRA adapters, are executed inside an SGX enclave, while other linear operations (matrix multiplication) are performed on the GPU after encryption and are securely restored within the enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and evaluate it across performance and security metrics. Our results show that SecureInfer offers strong security guarantees with reasonable performance, offering a practical solution for secure on-device model inference.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tushar Nayan",
        "Ziqi Zhang",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8cf823eb4c9309221953480a9b98c7700136c546",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "title": "Stealing AI Model Weights Through Covert Communication Channels",
      "abstract": "AI models are often regarded as valuable intellectual property due to the high cost of their development, the competitive advantage they provide, and the proprietary techniques involved in their creation. As a result, AI model stealing attacks pose a serious concern for AI model providers. In this work, we present a novel attack targeting wireless devices equipped with AI hardware accelerators. The attack unfolds in two phases. In the first phase, the victim's device is compromised with a hardware Trojan (HT) designed to covertly leak model weights through a hidden communication channel, without the victim realizing it. In the second phase, the adversary uses a nearby wireless device to intercept the victim's transmission frames during normal operation and incrementally reconstruct the complete weight matrix. The proposed attack is agnostic to both the AI model architecture and the hardware accelerator used. We validate our approach through a hardware-based demonstration involving four diverse AI models of varying types and sizes. We detail the design of the HT and the covert channel, highlighting their stealthy nature. Additionally, we analyze the impact of bit error rates on the reception and propose an error mitigation technique. The effectiveness of the attack is evaluated based on the accuracy of the reconstructed models with stolen weights and the time required to extract them. Finally, we explore potential defense mechanisms.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Valentin Barbaza",
        "Al\u00e1n Rodrigo D\u00edaz Rizo",
        "Hassan Aboushady",
        "Spyridon Raptis",
        "H. Stratigopoulos"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a1a88758960b51b8f7b47d8d429df74c28f12b6c",
      "pdf_url": "",
      "publication_date": "2025-09-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "title": "Stabilizing Data-Free Model Extraction",
      "abstract": "Model extraction is a severe threat to Machine Learning-as-a-Service systems, especially through data-free approaches, where dishonest users can replicate the functionality of a black-box target model without access to realistic data. Despite recent advancements, existing data-free model extraction methods suffer from the oscillating accuracy of the substitute model. This oscillation, which could be attributed to the constant shift in the generated data distribution during the attack, makes the attack impractical since the optimal substitute model cannot be determined without access to the target model's in-distribution data. Hence, we propose MetaDFME, a novel data-free model extraction method that employs meta-learning in the generator training to reduce the distribution shift, aiming to mitigate the substitute model's accuracy oscillation. In detail, we train our generator to iteratively capture the meta-representations of the synthetic data during the attack. These meta-representations can be adapted with a few steps to produce data that facilitates the substitute model to learn from the target model while reducing the effect of distribution shifts. Our experiments on popular baseline image datasets, MNIST, SVHN, CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current state-of-the-art data-free model extraction method while exhibiting a more stable substitute model's accuracy during the attack.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dat-Thinh Nguyen",
        "Kim-Hung Le",
        "Nhien-An Le-Khac"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6e068deff65d5f69213fd9a6bf3389dc514c1cf5",
      "pdf_url": "",
      "publication_date": "2025-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "title": "Extracting Proxy Models from Side-Channel Insights to Enhance Adversarial Attacks on Black-Box DNNs",
      "abstract": "Side-channel information leakage can be exploited to reverse engineer critical architectural details of a target DNN model executing on a hardware accelerator. However, using these details to apply a practical adversarial attack remains a significant challenge. In this paper, we first introduce a novel approach to analyze side-channel data and extract detailed architectural information of DNN models, including accurate prediction of layer hyperparameters and inter-layer skip connections. Next, we develop techniques to construct effective proxy models from this information. We then leverage white-box access to these proxies to generate adversarial examples capable of effectively deceiving the target DNN model. We illustrate our techniques using popular DNNs as target models, and demonstrate that the constructed proxy models achieve up to 89.8% similarity in performance compared to the target models. Furthermore, we achieve adversarial transferability rates of up to 72.34% and induce up to 60.4% drop in accuracy in the target models using the crafted adversarial images. Compared to off-the-shelf substitute models, our method improves transferability by as much as 30% in untargeted adversarial attacks. Even when the target model is protected by a state-of-the-art denoiser, our proxy models generate 5.5% more transferable adversarial examples compared to other substitute models in untargeted adversarial attacks.",
      "year": 2025,
      "venue": "Proceedings of the 11th ACM Cyber-Physical System Security Workshop",
      "authors": [
        "Srivatsan Chandrasekar",
        "Likith Anaparty",
        "Siew-Kei Lam",
        "Vivek Chaturvedi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/68574082c3b466126b0c2f6a15f3cbd7d4d5c51b",
      "pdf_url": "",
      "publication_date": "2025-08-25",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms",
      "abstract": "Online collaborative medical prediction platforms offer convenience and real-time feedback by leveraging massive electronic health records. However, growing concerns about privacy and low prediction quality can deter patient participation and doctor cooperation. In this paper, we first clarify the privacy attacks, namely attribute attacks targeting patients and model extraction attacks targeting doctors, and specify the corresponding privacy principles. We then propose a privacy-preserving mechanism and integrate it into a novel one-shot distributed learning framework, aiming to simultaneously meet both privacy requirements and prediction performance objectives. Within the framework of statistical learning theory, we theoretically demonstrate that the proposed distributed learning framework can achieve the optimal prediction performance under specific privacy requirements. We further validate the developed privacy-preserving collaborative medical prediction platform through both toy simulations and real-world data experiments.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shao-Bo Lin",
        "Xiaotong Liu",
        "Yao Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6ee2e687a10d8045c1ffe4293535e76b293de70",
      "pdf_url": "",
      "publication_date": "2025-07-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "title": "BarkBeetle: Stealing Decision Tree Models with Fault Injection",
      "abstract": "Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qifan Wang",
        "Jonas Sander",
        "Minmin Jiang",
        "Thomas Eisenbarth",
        "David Oswald"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/25245b57c2f0ce2f7a7bae6f55efb945ed7b19c9",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9d0f0fb82a339debb181382f725baa278bb202d7",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d0f0fb82a339debb181382f725baa278bb202d7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "title": "Hybrid Fingerprinting for Effective Detection of Cloned Neural Networks",
      "abstract": "As artificial intelligence plays an increasingly important role in decision-making within critical infrastructure, ensuring the authenticity and integrity of neural networks is crucial. This paper addresses the problem of detecting cloned neural networks. We present a method for identifying clones that employs a combination of metrics from both the information and physical domains: output predictions, probability score vectors, and power traces measured from the device running the neural network during inference. We compare the effectiveness of each metric individually, as well as in combination. Our results show that the effectiveness of both the information and the physical domain metrics is excellent for a clone that is a near replica of the target neural network. Furthermore, both the physical domain metric individually and the hybrid approach outperform the information domain metrics at detecting clones whose weights were extracted with low accuracy. The presented method offers a practical solution for verifying neural network authenticity and integrity. It is particularly useful in scenarios where neural networks are at risk of model extraction attacks, such as in cloud-based machine learning services.",
      "year": 2025,
      "venue": "IEEE International Symposium on Multiple-Valued Logic",
      "authors": [
        "Can Aknesil",
        "Elena Dubrova",
        "Niklas Lindskog",
        "Jakob Sternby",
        "H\u00e5kan Englund"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0b20c29e4fefc4acaa8e2bcf0f684ffa5f1863b0",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "title": "On the Interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction",
      "abstract": "Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.",
      "year": 2025,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine",
        "Rinad Akel",
        "Ihab Sbeity",
        "Silvia Giordano",
        "Marc Langheinrich",
        "Omran Ayoub"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ecbd364509711911af74dd0d1be4022bb7e1bec6",
      "pdf_url": "",
      "publication_date": "2025-05-13",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "title": "Activation Functions Considered Harmful: Recovering Neural Network Weights through Controlled Channels",
      "abstract": "With high-stakes machine learning applications increasingly moving to untrusted end-user or cloud environments, safeguarding pre-trained model parameters becomes essential for protecting intellectual property and user privacy. Recent advancements in hardware-isolated enclaves, notably Intel SGX, hold the promise to secure the internal state of machine learning applications even against compromised operating systems. However, we show that privileged software adversaries can exploit input-dependent memory access patterns in common neural network activation functions to extract secret weights and biases from an SGX enclave. Our attack leverages the SGX-Step framework to obtain a noise-free, instruction-granular page-access trace. In a case study of an 11-input regression network using the Tensorflow Microlite library, we demonstrate complete recovery of all first-layer weights and biases, as well as partial recovery of parameters from deeper layers under specific conditions. Our novel attack technique requires only 20 queries per input per weight to obtain all first-layer weights and biases with an average absolute error of less than 1%, improving over prior model stealing attacks. Additionally, a broader ecosystem analysis reveals the widespread use of activation functions with input-dependent memory access patterns in popular machine learning frameworks (either directly or via underlying math libraries). Our findings highlight the limitations of deploying confidential models in SGX enclaves and emphasise the need for stricter side-channel validation of machine learning implementations, akin to the vetting efforts applied to secure cryptographic libraries.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jesse Spielman",
        "David Oswald",
        "Mark Ryan",
        "Jo Van Bulck"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6033eb92723833c3ca2c478cfde3956e0d1e388a",
      "pdf_url": "",
      "publication_date": "2025-03-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
      "abstract": "Pre-trained models are valuable intellectual property, capturing both domain-specific and domain-invariant features within their weight spaces. However, model extraction attacks threaten these assets by enabling unauthorized source-domain inference and facilitating cross-domain transfer via the exploitation of domain-invariant features. In this work, we introduce **ProDiF**, a novel framework that leverages targeted weight space manipulation to secure pre-trained models against extraction attacks. **ProDiF** quantifies the transferability of filters and perturbs the weights of critical filters in unsecured memory, while preserving actual critical weights in a Trusted Execution Environment (TEE) for authorized users. A bi-level optimization further ensures resilience against adaptive fine-tuning attacks. Experimental results show that **ProDiF** reduces source-domain accuracy to near-random levels and decreases cross-domain transferability by 74.65\\%, providing robust protection for pre-trained models. This work offers comprehensive protection for pre-trained DNN models and highlights the potential of weight space manipulation as a novel approach to model security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tong Zhou",
        "Shijin Duan",
        "Gaowen Liu",
        "Charles Fleming",
        "R. Kompella",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/818a74f810f9f1b5bf7243279c97d22f57e97cbf",
      "pdf_url": "",
      "publication_date": "2025-03-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6c1f2957365c03221827f076190da814769a79b2",
      "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
      "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the\"oracle''queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Awa Khouna",
        "Julien Ferry",
        "Thibaut Vidal"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6c1f2957365c03221827f076190da814769a79b2",
      "pdf_url": "",
      "publication_date": "2025-02-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
      "abstract": "Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks. In this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from 193,252 to 1,857 with zero training cost. The code is available at https://github.com/NeurHT/NeurHT.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yixiao Xu",
        "Binxing Fang",
        "Rui Wang",
        "Yinghai Zhou",
        "Shouling Ji",
        "Yuan Liu",
        "Mohan Li",
        "Zhihong Tian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/385545515cfb1b63a6c5f81464e4ef2050cfd493",
      "pdf_url": "",
      "publication_date": "2025-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "17440e21a757f14e630d7443c89f018b79cc1754",
      "title": "StegGuard: Secrets Encoder and Decoder Act as Fingerprint of Self-Supervised Pretrained Model",
      "abstract": "In this work, we propose StegGuard, a novel fingerprinting mechanism to verify the ownership of a suspect pretrained model using steganography, where the pretrained model is obtained via self-supervised learning. A critical perspective in StegGuard is that the unique characteristic of the transformation from an image to an embedding, conducted by the pretrained model, can be equivalently captured by how an encoder embeds secrets into images and how a decoder extracts them from the embeddings with tolerable error. While each independently trained pretrained model has a distinct transformation, a piracy model exhibits a transformation similar to that of the victim. Based on these observations, StegGuard learns a pair of secrets encoder and decoder as the fingerprint of the victim model. Additionally, a Frequency domain channel attention Embedding block is introduced into the encoder to adaptively embed secrets into suitable frequency bands. During verification, if the secrets embedded into the query images can be extracted with an acceptable error from the embeddings of the query images, the suspect model is determined to be piracy; otherwise, it is deemed independent. Extensive experiments demonstrate that with as few as 100 query images, StegGuard achieves high piracy detection accuracy and robustness against model stealing attacks, including model extraction, fine-tuning, pruning, embedding noising and shuffle. Compared to existing methods, StegGuard consistently achieves lower p-values for piracy models (as low as 1e-14) and higher p-values for independent models (up to 0.99), confirming its effectiveness and reliability.",
      "year": 2025,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xingdong Ren",
        "Hanzhou Wu",
        "Yinggui Wang",
        "Haojie Liu",
        "Xiaofeng Lu",
        "Guangling Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/17440e21a757f14e630d7443c89f018b79cc1754",
      "pdf_url": "",
      "publication_date": "2025-09-15",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "title": "Explore the vulnerability of black-box models via diffusion models",
      "abstract": "Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Huajie Shao",
        "Ashley Gao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d47ec1770b15e2757e747b23b3789f4c24c91bb7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "135ea890e84a64ea2b6786cc795b94e214b70050",
      "title": "FlatD: Protecting Deep Neural Network Program from Reversing Attacks",
      "abstract": "The emergence of Deep Learning compilers provides automated optimization and compilation across Deep Learning frameworks and hardware platforms, which enhances the performance of AI service and benefits the deployment to edge devices and low-power processors. However, deep neural network (DNN) programs generated by Deep Learning compilers introduce a new attack interface. They are targeted by new model extraction attacks that can fully or partially rebuild the DNN model by reversing the DNN programs. Unfortunately, no defense countermeasure is designed to hinder this kind of attack. To address the issue, we investigate all of the state-of-the-art reversing-based model extraction attacks and identify an essential component shared across the frameworks. Based on this observation, we propose FlatD, the first defense framework for DNN programs toward reversing-based model extraction attacks. FlatD manipulates and conceals the original Control Flow Graphs of DNN programs based on Control Flow Flattening. Unlike traditional Control Flow Flattening, FlatD ensures the DNN programs are challenging for attackers to recover their Control Flow Graphs and gain necessary information statically. Our evaluation shows that, compared to the traditional Control Flow Flattening (O-LLVM), FlatD provides more effective and stealthy protection to DNN programs with similar performance and lower scale.",
      "year": 2025,
      "venue": "2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
      "authors": [
        "Jinquan Zhang",
        "Zihao Wang",
        "Dinghao Wu",
        "Pei Wang",
        "Rui Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/135ea890e84a64ea2b6786cc795b94e214b70050",
      "pdf_url": "",
      "publication_date": "2025-04-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7f8163935429700fed6c5ff88417726cb4ea4884",
      "title": "Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods",
      "abstract": "As quantum computing advances, quantum circuit simulators serve as critical tools to bridge the current gap caused by limited quantum hardware availability. These simulators are typically deployed on cloud platforms, where users submit proprietary circuit designs for simulation. In this work, we demonstrate a novel timing side-channel attack targeting cloud- based quantum simulators. A co-located malicious process can observe fine-grained execution timing patterns to extract sensitive information about concurrently running quantum circuits. We systematically analyze simulator behavior using the QASMBench benchmark suite, profiling timing and memory characteristics across various circuit executions. Our experimental results show that timing profiles exhibit circuit-dependent patterns that can be effectively classified using pattern recognition techniques, enabling the adversary to infer circuit identities and compromise user confidentiality. We were able to achieve 88% to 99.9% identification rate of quantum circuits based on different datasets. This work highlights previously unexplored security risks in quantum simulation environments and calls for stronger isolation mechanisms to protect user workloads",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ben Dong",
        "Hui Feng",
        "Qian Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7f8163935429700fed6c5ff88417726cb4ea4884",
      "pdf_url": "",
      "publication_date": "2025-09-16",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "title": "Misclassification-driven Fingerprinting for DNNs Using Frequency-aware GANs",
      "abstract": "Deep neural networks (DNNs) have become valuable assets due to their success in various tasks, but their high training costs also make them targets for model theft. Fingerprinting techniques are commonly used to verify model ownership, but existing methods either require training many additional models, leading to increased costs, or rely on GANs to generate fingerprints near decision boundaries, which may compromise image quality. To address these challenges, we propose a GAN-based fingerprint generation method that applies frequency-domain perturbations to normal samples, effectively creating fingerprints. This approach not only resists intellectual property (IP) threats, but also improves fingerprint acquisition efficiency while maintaining high imperceptibility. Extensive experiments demonstrate that our method achieves a state-of-the-art (SOTA) AUC of 0.98 on the Tiny-ImageNet dataset under IP removal attacks, outperforming existing methods by 8%, and consistently achieves the best ABP for three types of IP detection and erasure attacks on the GTSRB dataset. Our source code is available at https://github.com/wason981/Frequency-Fingerprinting.",
      "year": 2025,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Weixing Liu",
        "Shenghua Zhong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f0205fe725f9e9e58c5402b8579c864e9b19797b",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "title": "WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are increasingly deployed in real-world applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both previous methods enable ownership verification but do not convey additional information about the copy model. If the owner has multiple models, each model requires a distinct trigger graph. To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string in GNN models without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes in a graph. By assigning unique LDDE values to the edges and employing the LDDE sequence as the watermark, WGLE supports multi-bit capacity without relying on backdoor mechanisms. We evaluate WGLE on six public datasets across six mainstream GNN architectures, and compare WGLE with state-of-the-art GNN watermarking and fingerprinting methods. WGLE achieves 100% ownership verification accuracy, with an average fidelity degradation of only 1.41%. Additionally, WGLE exhibits robust resilience against potential attacks. The code is available in the repository.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tingzhi Li",
        "Xuefeng Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3f7a9b248785cac49c96a22ee4ba11d88235f9b",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
      "abstract": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Sohely Jahan",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-12"
    },
    {
      "paper_id": "5e91b9b928b33ce05273507cf8dea1ade2450598",
      "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
      "abstract": "Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Xuebin Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5e91b9b928b33ce05273507cf8dea1ade2450598",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
      "abstract": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "year": 2025,
      "venue": "",
      "authors": [
        "A. Foundjem",
        "L. Tidjon",
        "L. D. Silva",
        "Foutse Khomh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ad8a3f00e1b77aa19a71a7a166ee6ae533e7e3bc",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-31"
    },
    {
      "paper_id": "6a4dab913871b24c71bb3e66db20babe2a68880f",
      "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
      "abstract": "Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Kacem Khaled",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6a4dab913871b24c71bb3e66db20babe2a68880f",
      "pdf_url": "",
      "publication_date": "2025-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2026-01-02"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "title": "Watermark Stealing in Large Language Models",
      "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nikola Jovanovi'c",
        "Robin Staab",
        "Martin T. Vechev"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/c7af46b35061e856aa3332ac2eec6a7ccee0cb35",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9487d29364645c2f086387ff817ad5fd14b33c41",
      "title": "A Comprehensive Defense Framework Against Model Extraction Attacks",
      "abstract": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9487d29364645c2f086387ff817ad5fd14b33c41",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "title": "Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across numerous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously analyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open-source model inversion toolbox on GitHub (https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Fang",
        "Yixiang Qiu",
        "Hongyao Yu",
        "Wenbo Yu",
        "Jiawei Kong",
        "Baoli Chong",
        "Bin Chen",
        "Xuan Wang",
        "Shutao Xia"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/45967a64fdc7a3e20f47b24297844b93b9f3e3d9",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
      "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Shanglun Feng",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fe44bd072c2325eaa750990d148b27e42b7eb1d2",
      "pdf_url": "",
      "publication_date": "2024-03-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "title": "Unraveling Attacks to Machine-Learning-Based IoT Systems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This article embarks on a comprehensive exploration of the security threats arising from ML\u2019s integration into various facets of IoT, spanning various attack types, including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria, such as adversary models, attack targets, and key security attributes (confidentiality, integrity, and availability). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this article seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/3b82c1d871b0d5ab043e96cc4a73b77dfd03695e",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "title": "Side-Channel-Assisted Reverse-Engineering of Encrypted DNN Hardware Accelerator IP and Attack Surface Exploration",
      "abstract": "Deep Neural Networks (DNNs) have revolutionized numerous application domains with their unparalleled performance. As the models become larger and more complex, hardware DNN accelerators are increasingly popular. Field-Programmable Gate Array (FPGA)-based DNN accelerators offer near-Application Specific Integrated Circuit (ASIC) efficiency and exceptional flexibility, establishing them as one of the primary hardware platforms for rapidly evolving deep learning implementations, particularly on edge devices. This prominence renders them lucrative targets for attackers. Existing attacks aimed at compromising the confidentiality of DNN models deployed on FPGA DNN accelerators often assume complete knowledge of the accelerators. However, this assumption does not hold for real-world, proprietary, high-performance FPGA DNN accelerators. In this study, we introduce a comprehensive and effective reverse-engineering methodology for demystifying FPGA DNN accelerator soft Intellectual Property (IP) cores. We demonstrate its application on the cutting-edge AMD-Xilinx Deep Learning Processing Unit (DPU). Our method relies on schematic analysis and, innovatively, electromagnetic (EM) side-channel analysis to reveal the data flow and scheduling of the DNN accelerators. To the best of our knowledge, this research is the first successful endeavor to reverse-engineer a commercial encrypted DNN accelerator IP. Moreover, we investigate attack surfaces exposed by the reverse-engineering findings, including the successful recovery of DNN model architectures and extraction of model parameters. These outcomes pose a significant threat to real-world commercial FPGA-DNN acceleration systems. We discuss potential countermeasures and offer recommendations for FPGA-based IP protection.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Gongye Cheng",
        "Yukui Luo",
        "Xiaolin Xu",
        "Yunsi Fei"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/37bf4cce2a17e47480a857b235addbc73f5eb7fe",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "550a79a7e688347af18bf5752361c47f0af1cf40",
      "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/550a79a7e688347af18bf5752361c47f0af1cf40",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "title": "MEA-Defender: A Robust Watermark against Model Extraction Attack",
      "abstract": "Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Peizhuo Lv",
        "Hualong Ma",
        "Kai Chen",
        "Jiachen Zhou",
        "Shengzhi Zhang",
        "Ruigang Liang",
        "Shenchen Zhu",
        "Pan Li",
        "Yingjun Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/14b7cdf6349611cdc3d271c1672ef275e0d101ae",
      "pdf_url": "https://arxiv.org/pdf/2401.15239",
      "publication_date": "2024-01-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7bed6f6101204efdf04181aafa511ca55644b559",
      "title": "Data Stealing Attacks against Large Language Models via Backdooring",
      "abstract": "Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring. Our method mainly focuses on the scenario of model customization and is conducted in two phases, including backdoor training and backdoor activation, which allow for the extraction of private information without prior knowledge of the model\u2019s architecture or training data. During the model customization stage, attackers inject the backdoor into the pre-trained LLM by poisoning a small ratio of the training dataset. During the inference stage, attackers can extract private information from the third-party knowledge database by incorporating the pre-defined backdoor trigger. Our method leverages the customization process of LLMs, injecting a stealthy backdoor that can be triggered after deployment to retrieve private data. We demonstrate the effectiveness of our proposed attack through extensive experiments, achieving a notable attack success rate. Extensive experiments demonstrate the effectiveness of our stealing attack in popular LLM architectures, as well as stealthiness during normal inference.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Jiaming He",
        "Guanyu Hou",
        "Xinyue Jia",
        "Yangyang Chen",
        "Wenqi Liao",
        "Yinhang Zhou",
        "Rang Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/7bed6f6101204efdf04181aafa511ca55644b559",
      "pdf_url": "https://doi.org/10.3390/electronics13142858",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "85f707934e1630695fbfbdf1934a21760917416d",
      "title": "TinyPower: Side-Channel Attacks with Tiny Neural Networks",
      "abstract": "Side-channel attacks leverage correlations between power consumption and intermediate encryption results to infer encryption keys. Recent studies show that deep learning offers promising results in the context of side-channel attacks. However, neural networks utilized in deep-learning side-channel attacks are complex with a substantial number of parameters and consume significant memory. As a result, it is challenging to perform deep-learning side-channel attacks on resource-constrained devices. In this paper, we propose a framework, TinyPower, which leverages pruning to reduce the number of neural network parameters for side-channel attacks. Pruned neural networks obtained from our framework can successfully run side-channel attacks with significantly fewer parameters and less memory. Specifically, we focus on structured pruning over filters of Convolutional Neural Networks (CNNs). We demonstrate the effectiveness of structured pruning over power and EM traces of AES-128 running on microcontrollers (AVR XMEGA and ARM STM32) and FPGAs (Xilinx Artix-7). Our experimental results show that we can achieve a reduction rate of 98.8% (e.g., reducing the number of parameters from 53.1 million to 0.59 million) on a CNN and still recover keys on XMEGA. For STM32 and Artix-7, we achieve a reduction rate of 92.9% and 87.3% on a CNN respectively. We also demonstrate that our pruned CNNs can effectively perform the attack phase of side-channel attacks on a Raspberry Pi 4 with less than 2.5 millisecond inference time per trace and less than 41 MB memory usage per CNN.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Haipeng Li",
        "Mabon Ninan",
        "Boyang Wang",
        "J. M. Emmert"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/85f707934e1630695fbfbdf1934a21760917416d",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE, which can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security as the black-box guarantees with negligible overhead.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Sheng Cheng",
        "Xun Wang",
        "Jianwei Yin"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/df8401e2322f8f6f1b2cb8310cb048a939cce3d1",
      "pdf_url": "https://arxiv.org/pdf/2404.11121",
      "publication_date": "2024-04-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9af4ee4d5255739283cdd728864168a7e6336393",
      "title": "WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection",
      "abstract": "Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Yue Teng",
        "Ke He",
        "Qiongkai Xu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9af4ee4d5255739283cdd728864168a7e6336393",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "title": "DeMistify: Identifying On-Device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps",
      "abstract": "Mobile apps have become popular for providing artificial intelligence (AI) services via on-device machine learning (ML) techniques. Unlike accomplishing these AI services on remote servers traditionally, these on-device techniques process sensitive information required by AI services locally, which can mitigate the severe con-cerns of the sensitive data collection on the remote side. However, these on-device techniques have to push the core of ML expertise (e.g., models) to smartphones locally, which are still subject to similar vulnerabilities on the remote clouds and servers, especially when facing the model stealing attack. To defend against these attacks, developers have taken various protective measures. Unfor-tunately, we have found that these protections are still insufficient, and on-device ML models in mobile apps could be extracted and reused without limitation. To better demonstrate its inadequate protection and the feasibility of this attack, this paper presents DeMistify, which statically locates ML models within an app, slices relevant execution components, and finally generates scripts auto-matically to instrument mobile apps to successfully steal and reuse target ML models freely. To evaluate DeMistify and demonstrate its applicability, we apply it on 1,511 top mobile apps using on-device ML expertise for several ML services based on their install numbers from Google Play and DeMistify can successfully execute 1250 of them (82.73%). In addition, an in-depth study is conducted to understand the on-device ML ecosystem in the mobile application.",
      "year": 2024,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Pengcheng Ren",
        "Chaoshun Zuo",
        "Xiaofeng Liu",
        "Wenrui Diao",
        "Qingchuan Zhao",
        "Shanqing Guo"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/1e8b05c5b703f443b5470bef2b3aca1b064b1709",
      "pdf_url": "",
      "publication_date": "2024-02-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "title": "High-Frequency Matters: Attack and Defense for Image-Processing Model Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks with an almost 100% success rate. To counter this attack, we propose an adversarial framework for the watermarking network. The framework incorporates a specially-designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2024,
      "venue": "IEEE Transactions on Services Computing",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/d9588dc5028b7e66b5fec940fd9e31a8e0a6070b",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "title": "QUEEN: Query Unlearning Against Model Extraction",
      "abstract": "Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and using the query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://github.com/MaraPapMann/QUEEN.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Bo Liu",
        "Derui Wang",
        "Wanlei Zhou",
        "Minhui Xue"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/0e8594a161f1670e939a16699bd5cc8fd2f8335a",
      "pdf_url": "http://arxiv.org/pdf/2407.01251",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "title": "Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks",
      "abstract": "Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. Our findings revealed that model stealing attacks can produce clone models achieving up to 0.9 \u00d7 and 0.99 \u00d7 clone test accuracy when trained using Top-1 and Top-k labels, respectively (k: num_classes). To defend against these attacks, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Despite limited success with our defense techniques, it has led to an important discovery: QML models trained on noisy hardwares are naturally resistant to perturbation or obfuscation-based defenses or attacks.",
      "year": 2024,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Satwik Kundu",
        "Debarshi Kundu",
        "Swaroop Ghosh"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8f0679e01cb36c995fc5fbc9d364160bd72ce8cd",
      "pdf_url": "https://arxiv.org/pdf/2402.11687",
      "publication_date": "2024-02-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines",
      "abstract": "Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99%~7.35% across diverse datasets and VQC architectures.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhenxiao Fu",
        "Min Yang",
        "Cheng Chu",
        "Yilun Xu",
        "Gang Huang",
        "Fan Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/f3892c3b89b4b6fca4465308fcc9a99388eb019b",
      "pdf_url": "http://arxiv.org/pdf/2403.10790",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3434f582cc9c2818785e2920241d43d932625539",
      "title": "ModelShield: Adaptive and Robust Watermark Against Model Extraction Attack",
      "abstract": "Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kaiyi Pang",
        "Tao Qi",
        "Chuhan Wu",
        "Minhao Bai",
        "Minghu Jiang",
        "Yongfeng Huang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3434f582cc9c2818785e2920241d43d932625539",
      "pdf_url": "",
      "publication_date": "2024-05-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
      "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Chao Liu",
        "Boxi Chen",
        "Wei Shao",
        "Chris Zhang",
        "Kelvin Wong",
        "Yi Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6dc6c055a006b3b8bbbd10a44336877c9b190907",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c535f60659724b84d5a2169d434617ba49f005bf",
      "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
      "abstract": "Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qinfeng Li",
        "Yangfan Xie",
        "Tianyu Du",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Hao Peng",
        "Xinkui Zhao",
        "Xianwei Zhu",
        "Jianwei Yin",
        "Xuhong Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c535f60659724b84d5a2169d434617ba49f005bf",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "model stealing",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b892ac05369526432384a4cdf1d4d087f8bc45de",
      "title": "Beyond Slow Signs in High-fidelity Model Extraction",
      "abstract": "Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al. [1] further enhanced by Canales-Mart\\'inez et al. [2] for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Hanna Foerster",
        "Robert D. Mullins",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/b892ac05369526432384a4cdf1d4d087f8bc45de",
      "pdf_url": "",
      "publication_date": "2024-06-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
      "abstract": "The past few years have seen an intense research interest in the practical needs of the \u201cright to be forgotten\u201d, which has motivated researchers to develop machine unlearning methods to unlearn a fraction of training data and its lineage. While existing machine unlearning methods prioritize the protection of individuals\u2019 private data, they over-look investigating the unlearned models\u2019 susceptibility to adversarial attacks and security breaches. In this work, we uncover a novel security vulnerability of machine unlearning based on the insight that adversarial vulnerabilities can be bol-stered, especially for adversarially robust models. To exploit this observed vulnerability, we pro-pose a novel attack called Adv ersarial U nlearning A ttack (AdvUA), which aims to generate a small fraction of malicious unlearning requests during the unlearning process. AdvUA causes a significant reduction of adversarial robustness in the unlearned model compared to the original model, providing an entirely new capability for adversaries that is infeasible in conventional machine learning pipelines. Notably, we also show that AdvUA can effectively enhance model stealing attacks by extracting additional decision boundary information, further emphasizing the breadth and significance of our research. We also conduct both theoretical analysis and computational complexity of AdvUA. Extensive numerical studies are performed to demonstrate the effectiveness and efficiency of the proposed attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chenxu Zhao",
        "Wei Qian",
        "Yangyi Li",
        "Aobo Chen",
        "Mengdi Huai"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/ee51e69d40e027a6fd3fb2dca78d520e83737ae1",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-20"
    },
    {
      "paper_id": "93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
      "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Andrey V. Galichin",
        "Mikhail Aleksandrovich Pautov",
        "Alexey Zhavoronkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/93dc2c281fbf4d1f8886a83f6793864528fd48f8",
      "pdf_url": "",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "title": "Probabilistically Robust Watermarking of Neural Networks",
      "abstract": "As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Mikhail Aleksandrovich Pautov",
        "Nikita Bogdanov",
        "Stanislav Pyatkin",
        "Oleg Y. Rogov",
        "Ivan V. Oseledets"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3e0bc402f7c0a34964223e87646fa3da0b6d947a",
      "pdf_url": "https://arxiv.org/pdf/2401.08261",
      "publication_date": "2024-01-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "title": "TPUXtract: An Exhaustive Hyperparameter Extraction Framework",
      "abstract": "Model stealing attacks on AI/ML devices undermine intellectual property rights, compromise the competitive advantage of the original model developers, and potentially expose sensitive data embedded in the model\u2019s behavior to unauthorized parties. While previous research works have demonstrated successful side-channelbased model recovery in embedded microcontrollers and FPGA-based accelerators, the exploration of attacks on commercial ML accelerators remains largely unexplored. Moreover, prior side-channel attacks fail when they encounter previously unknown models. This paper demonstrates the first successful model extraction attack on the Google Edge Tensor Processing Unit (TPU), an off-the-shelf ML accelerator. Specifically, we show a hyperparameter stealing attack that can extract all layer configurations including the layer type, number of nodes, kernel/filter sizes, number of filters, strides, padding, and activation function. Most notably, our attack is the first comprehensive attack that can extract previously unseen models. This is achieved through an online template-building approach instead of a pre-trained ML-based approach used in prior works. Our results on a black-box Google Edge TPU evaluation show that, through obtained electromagnetic traces, our proposed framework can achieve 99.91% accuracy, making it the most accurate one to date. Our findings indicate that attackers can successfully extract various types of models on a black-box commercial TPU with utmost detail and call for countermeasures.",
      "year": 2024,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Ashley Kurian",
        "Anuj Dubey",
        "Ferhat Yaman",
        "Aydin Aysu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/74ed0e78bba8fda5a72d3db69593f4caeca82559",
      "pdf_url": "https://doi.org/10.46586/tches.v2025.i1.78-103",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16b5db7894dde1b49960f03e68b280ac895d86fc",
      "title": "Securing Data From Side-Channel Attacks: A Graph Neural Network-Based Approach for Smartphone-Based Side Channel Attack Detection",
      "abstract": "The widespread use of smartphones has brought convenience and connectivity to the fingertips of the masses. As a result, this has paved the way for potential security vulnerabilities concerning sensitive data, particularly by exploiting side-channel attacks. When typing on a smartphone\u2019s keyboard, its vibrations can be misused to discern the entered characters, thus facilitating side-channel attacks. These smartphone hardware sensors can capture such information while users input sensitive data like personal details, names, email addresses, age, bank details and passwords. This study presents a novel Graph Neural Network (GNN) approach to predict side-channel attacks on smartphone keyboards; different GNN architectures were used, including GNN, DeepGraphNet, Gradient Boosting (GB)+DeepGraphNet, Extreme Gradient Boosting (XGB)+DeepGraphNet and K-Nearest Neighbor (KNN)+DeepGraphNet. The proposed approach detects the side channel attack using vibrations produced while typing on the smartphone soft keyboard. The data was collected from three smartphone sensors, an accelerometer, gyroscope, and magnetometer, and evaluated this data using common evaluation measures such as accuracy, precision, recall, F1-score, ROC curves, confusion matrix and accuracy and loss curves. This study demonstrated that GNN architectures can effectively capture complex relationships in data, making them well-suited for analyzing patterns in smartphone sensor data. Likewise, this research aims to fill a crucial gap by enhancing data privacy in the information entered through smartphone keyboards, shielding it from side-channel attacks by providing an accuracy of 98.26%. Subsequently, the primary objective of this study is to assess the effectiveness of GNN architectures in this precise context. Similarly, the GNN model exhibits compelling performance, achieving accuracy, precision, recall, and f1 score metrics that showcase the model\u2019s effectiveness, with the highest values of 0.98, 0.98, 0.98, and 0.98, respectively. Significantly, the metrics mentioned in the study outperform those documented in the previous literature. Overall, the study contributes to the detection of side-channel smartphone attacks, which advances secure data practices.INDEX TERMS Graph neural networks (GNN), keystroke inference, motion sensors, machine learning, smartphone security, side-channel attacks.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Sidra Abbas",
        "Stephn Ojo",
        "Imen Bouazzi",
        "Gabriel Avelino Sampedro",
        "Abdullah Al Hejaili",
        "Ahmad S. Almadhor",
        "Rastislav Kulh\u00e1nek"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/16b5db7894dde1b49960f03e68b280ac895d86fc",
      "pdf_url": "https://doi.org/10.1109/access.2024.3465662",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "title": "Inversion-Guided Defense: Detecting Model Stealing Attacks by Output Inverting",
      "abstract": "Model stealing attacks involve creating copies of machine learning models that have similar functionalities to the original model without proper authorization. Such attacks raise significant concerns about the intellectual property of the machine learning models. Nonetheless, current defense mechanisms against such attacks tend to exhibit certain drawbacks, notably in terms of utility, and robustness. For example, watermarking-based defenses require victim models to be retrained for embedding watermarks, which can potentially impact the main task performance. Moreover, other defenses, especially fingerprinting-based methods, often rely on specific samples like adversarial examples to verify ownership of the target model. These approaches might prove less robust against adaptive attacks, such as model stealing with adversarial training. It remains unclear whether normal examples, as opposed to adversarial ones, can effectively reflect the characteristics of stolen models. To tackle these challenges, we propose a novel method that leverages a neural network as a decoder to inverse the suspicious model\u2019s outputs. Inspired by model inversion attacks, we argue that this decoding process will unveil hidden patterns inherent in the original outputs of the suspicious model. Drawing from these decoding outcomes, we calculate specific metrics to determine the legitimacy of the suspicious models. We validate the efficacy of our defense technique against diverse model stealing attacks, specifically within the domain of classification tasks based on deep neural networks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Shuai Zhou",
        "Tianqing Zhu",
        "Dayong Ye",
        "Wanlei Zhou",
        "Wei Zhao"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0beb8f26d688ce182cea25c361e0c3311b1afb69",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9fef6937c39de73fdc3531790e938274caa84de0",
      "title": "AugSteal: Advancing Model Steal With Data Augmentation in Active Learning Frameworks",
      "abstract": "With the proliferation of machine learning models in diverse applications, the issue of model security has increasingly become a focal point. Model steal attacks can cause significant financial losses to model owners and potentially threaten the security of their application scenarios. Traditional model steal attacks are primarily directed at soft-label black boxes, but their effectiveness significantly diminishes or even fails in hard-label scenarios. To address this, for hard-label black boxes, this study proposes an active learning-based Fusion Augmentation Model Stealing Framework (AugSteal). This framework initially utilizes large-scale irrelevant public datasets for deep filtering and feature extraction to generate reliable, diverse, and representative high-quality data subsets as the stealing dataset. Subsequently, we developed an adaptive active learning selection strategy that selects data samples with significant information gain for different black-box models, enhancing the attack\u2019s specificity and effectiveness. Finally, to further address the trade-off between query budget and steal precision, this paper designed a Fusion Augmentation training method constituted of two different loss functions, enabling the substitute model to closely approximate the decision distribution of the target black box.The comprehensive experimental results indicate that, compared to the current state-of-the-art attack methods, our approach achieved a maximum performance gain of 8.21% in functional similarity for the substitute models in simulated black-box scenarios CIFAR10, SVHN, CALTECH256, and the real-world application Tencent Cloud API.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Lijun Gao",
        "Wenjun Liu",
        "Kai Liu",
        "Jiehong Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9fef6937c39de73fdc3531790e938274caa84de0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
      "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fbf3119f69e29cebac200d1b0e9a2f7440e1845d",
      "pdf_url": "",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "title": "DeepCache: Revisiting Cache Side-Channel Attacks in Deep Neural Networks Executables",
      "abstract": "Deep neural networks (DNN) are increasingly deployed in heterogeneous hardware, including high-performance devices like GPUs and low-power devices like mobile/IoT CPUs, FPGAs, and accelerators. In order to unlock the full performance potential of various hardware, deep learning (DL) compilers automatically optimize DNN inference computations and compile DNN models into DNN executables for efficient computations across hardware backends. As valuable intellectual properties, DNN architectures are one primary attack target. Since previous works already demonstrate the abuse of cache side channels to steal DNN architectures from DL frameworks (e.g., PyTorch and TensorFlow), we first study using those known side-channel attacks against DNN executables. We find that attacking DNN executables presents unique challenges, and existing works can hardly apply. Particularly, DNN executables exhibit a standalone paradigm that largely reduces cache side channel attack surfaces. Meanwhile, cache side channels capture only limited behaviors of the whole DNN execution while facing daunting technical challenges (e.g., noise and low time resolution). However, we unveil a unique attack vector in DNN executables, such that the cache-aware optimizations, which are extensively employed by contemporary DL compilers to harvest the full potentials of hardware, would result in distinguishable DNN operator cache access patterns, making model architecture recovery possible. We propose DeepCache, an end-to-end side channel attack framework, to infer DNN model architectures from DNN executables. DeepCache \\ leverages cache side channels as the attacking primitives and combines contrastive learning and anomaly detection to enable precise inference. Our evaluation using the standard Prime+Probe shows that DeepCache \\ yields a high accuracy in exploiting complex DNN executables under both the basic L1 cache attack and the more practical but challenging last level cache (LLC) attack settings.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhibo Liu",
        "Yuanyuan Yuan",
        "Yanzuo Chen",
        "Sihang Hu",
        "Tianxiang Li",
        "Shuai Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1ed4e363c0d1caa38139b11bfbdf57ff7c3305b4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690241",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "title": "WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks",
      "abstract": "Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Anudeex Shetty",
        "Qiongkai Xu",
        "Jey Han Lau"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/7aed21ac2ca321b4bcaef9a32b2e5886425599e7",
      "pdf_url": "",
      "publication_date": "2024-08-29",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "title": "Privacy Implications of Explainable AI in Data-Driven Systems",
      "abstract": "Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.",
      "year": 2024,
      "venue": "xAI",
      "authors": [
        "Fatima Ezzeddine"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/62c9a0ed00f28ac6aedbee59ba11d26bb486216f",
      "pdf_url": "",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "94621729b6ca9e811d8a052dc3457e98df457676",
      "title": "Fully Exploiting Every Real Sample: SuperPixel Sample Gradient Model Stealing",
      "abstract": "Model stealing (MS) involves querying and observing the output of a machine learning model to steal its capabilities. The quality of queried data is crucial, yet obtaining a large amount of real data for MS is often challenging. Recent works have reduced reliance on real data by using generative models. However, when high-dimensional query data is required, these methods are impractical due to the high costs of querying and the risk of model collapse. In this work, we propose using sample gradients (SG) to enhance the utility of each real sample, as SG provides crucial guidance on the decision boundaries of the victim model. However, utilizing SG in the model stealing scenario faces two challenges: 1. Pixel-level gradient estimation requires ex-tensive query volume and is susceptible to defenses. 2. The estimation of sample gradients has a significant variance. This paper proposes Superpixel Sample Gradient stealing (SPSG) for model stealing under the constraint of limited real samples. With the basic idea of imitating the victim model's low-variance patch-level gradients instead ofpixel-level gradients, SPSG achieves efficient sample gradient es-timation through two steps. First, we perform patch-wise perturbations on query images to estimate the average gradient in different regions of the image. Then, we filter the gradients through a threshold strategy to reduce variance. Exhaustive experiments demonstrate that, with the same number of real samples, SPSG achieves accuracy, agreements, and adversarial success rate significantly surpassing the current state-of-the-art MS methods. Codes are available at https://github.com/zyI123456aBISPSG_attack.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yunlong Zhao",
        "Xiaoheng Deng",
        "Yijing Liu",
        "Xin-jun Pei",
        "Jiazhi Xia",
        "Wei Chen"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/94621729b6ca9e811d8a052dc3457e98df457676",
      "pdf_url": "https://arxiv.org/pdf/2406.18540",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "model stealing",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "389f1fad962f58799327a0112526452d2da5157d",
      "title": "TEXTKNOCKOFF: KNOCKOFF NETS FOR STEALING FUNCTIONALITY OF TEXT SENTIMENT MODELS",
      "abstract": "Most commercial machine learning models today are designed to require significant amounts of time, money, and human effort. Therefore, intrinsic information about the model (such as architecture, hyperparameters, and training data) needs to be kept confidential. These models are referred to as black boxes, and there is an increasing amount of research focused on both attacking and protecting them. Recent publications have often concentrated on the field of computer vision; in contrast, there is still relatively little research on methods for attacking black box models with textual data. This article introduces a research method for extracting the functionality of a black box model in the task of text sentiment analysis. The method has been effectively tested based on random sampling techniques to reconstruct a new model with equivalent functionality to the original model, achieving high accuracy (94.46% compared to 94.92%) and high similarity (96.82%).",
      "year": 2024,
      "venue": "Journal of Science and Technique",
      "authors": [
        "X. Pham"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/389f1fad962f58799327a0112526452d2da5157d",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "knockoff nets",
        "knockoff net",
        "stealing functionality"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f63f7d623e1738fbc314143a1ad1812045caffff",
      "title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) are recognized as potent tools for processing real-world data organized in graph structures. Especially inductive GNNs, which allow for the processing of graph-structured data without relying on predefined graph structures, are becoming increasingly important in a wide range of applications. As such these networks become attractive targets for model-stealing attacks where an adversary seeks to replicate the functionality of the targeted network. Significant efforts have been devoted to developing model-stealing attacks that extract models trained on images and texts. However, little attention has been given to stealing GNNs trained on graph data. This paper identifies a new method of performing unsupervised model-stealing attacks against inductive GNNs, utilizing graph contrastive learning and spectral graph augmentations to efficiently extract information from the targeted model. The new type of attack is thoroughly evaluated on six datasets and the results show that our approach outperforms the current state-of-the-art by Shen et al. (2021). In particular, our attack surpasses the baseline across all benchmarks, attaining superior fidelity and downstream accuracy of the stolen model while necessitating fewer queries directed toward the target model.",
      "year": 2024,
      "venue": "European Conference on Artificial Intelligence",
      "authors": [
        "Marcin Podhajski",
        "Jan Dubi'nski",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Pregowska",
        "Tomasz P. Michalak"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f63f7d623e1738fbc314143a1ad1812045caffff",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "title": "Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation",
      "abstract": "Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Di Mi",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Shengshan Hu",
        "Qi Zhong",
        "Haizhuan Yuan",
        "Shirui Pan"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1ab2ad9f96ffe88bb1e8817984ad78a1e9d7bf86",
      "pdf_url": "https://research-repository.griffith.edu.au/bitstreams/4224c508-a124-40eb-94f2-46c6b9821946/download",
      "publication_date": "2024-03-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "title": "Revisiting Black-box Ownership Verification for Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for processing graph-structured data, enabling applications in various domains. Yet, GNNs are vulnerable to model extraction attacks, imposing risks to intellectual property. To mitigate model extraction attacks, model ownership verification is considered an effective method. However, throughout a series of empirical studies, we found that the existing GNN ownership verification methods either mandate unrealistic conditions or present unsatisfactory accuracy under the most practical settings\u2014the black-box setting where the verifier only requires access to the final output (e.g., posterior probability) of the target model and the suspect model.Inspired by the studies, we propose a new, black-box GNN ownership verification method that involves local independent models and shadow surrogate models to train a classifier for performing ownership verification. Our method boosts the verification accuracy by exploiting two insights: (1) We consider the overall behaviors of the target model for decision-making, better utilizing its holistic fingerprinting; (2) We enrich the fingerprinting of the target model by masking a subset of features of its training data, injecting extra information to facilitate ownership verification.To assess the effectiveness of our proposed method, we perform an intensive series of evaluations with 5 popular datasets, 5 mainstream GNN architectures, and 16 different settings. Our method achieves nearly perfect accuracy with a marginal impact on the target model in all cases, significantly outperforming the existing methods and enlarging their practicality. We also demonstrate that our method maintains robustness against adversarial attempts to evade the verification.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ruikai Zhou",
        "Kang Yang",
        "Xiuling Wang",
        "Wendy Hui Wang",
        "Jun Xu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a35f48cf47cf57ee86246106e7e99d071e31b30e",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "title": "Protecting Confidential Virtual Machines from Hardware Performance Counter Side Channels",
      "abstract": "In modern cloud platforms, it is becoming more important to preserve the privacy of guest virtual machines (VMs) from the untrusted host. To this end, Secure Encrypted Virtualization (SEV) is developed as a hardware extension to protect VMs by encrypting their memory pages and register states. Unfortunately, such confidential VMs are still vulnerable to micro-architectural side channels, and Hardware Performance Counters (HPCs) are a prominent information leakage source. To make matters worse, currently there is no systematic defense against the HPC side channels. We introduce Aegis, a unified framework for demystifying the inherent relations between the instruction execution and HPC event statistics, and defending VMs against HPC side channels with provable privacy guarantee and minimal performance overhead. Aegis consists of three modules. Application Profiler profiles the application offline and adopts information theory to quantitatively estimate the vulnerability of HPC events. Event Fuzzer leverages the fuzzing technique to automatically generate interesting inputs, i.e., instruction sequences, that can effectively alter the HPC observations. Event Obfuscator injects noisy instructions into the protected VM based on the differential privacy mechanisms for high efficiency and privacy. We present three case studies to demonstrate that Aegis can defeat different types of HPC side-channel attacks (i.e., website fingerprinting, DNN model extraction, keystroke sniffing). Evaluations show that Aegis can effectively decrease the attack accuracy from 90% to 2%, with only 3% overhead on the application execution time and 7% overhead on the CPU usage.",
      "year": 2024,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Xiaoxuan Lou",
        "Kangjie Chen",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Tianwei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/be44a0e3c11ffe4944873eb415ced25ed36f7534",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "title": "Adversarial Machine Learning In Network Security: A Systematic Review Of Threat Vectors And Defense Mechanisms",
      "abstract": "Adversarial Machine Learning (AML) has emerged as a critical area of research within network security, addressing the evolving challenge of adversaries exploiting machine learning (ML) models. This systematic review adopts the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology to comprehensively examine threat vectors and defense mechanisms in AML. The study identifies, categorizes, and evaluates existing research focused on adversarial attacks targeting ML algorithms in network security applications, including evasion, poisoning, and model extraction attacks. By rigorously following the PRISMA guidelines, a systematic search across multiple scholarly databases yielded a robust dataset of peer-reviewed articles that were screened, reviewed, and analyzed for inclusion. The review outlines key adversarial techniques employed against ML systems, such as gradient-based attack strategies and black-box attacks and explores the underlying vulnerabilities in network security architectures. Additionally, it highlights defense mechanisms, including adversarial training, input preprocessing, and robust model design, discussing their efficacy and limitations in mitigating adversarial threats. The study also identifies critical gaps in current research, such as the lack of standardized benchmarking for adversarial defenses and the need for scalable and real-time AML solutions.",
      "year": 2024,
      "venue": "Innovatech Engineering Journal",
      "authors": [
        "Abdul Awal Mintoo",
        "Ashrafur Rahman Nabil",
        "Md Ashraful Alam",
        "Imran Ahmad"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "pdf_url": "",
      "publication_date": "2024-11-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b28136746a29ff698f57106c292d0d6e0181629",
      "title": "Efficient Data-Free Model Stealing with Label Diversity",
      "abstract": "Machine learning as a Service (MLaaS) allows users to query the machine learning model in an API manner, which provides an opportunity for users to enjoy the benefits brought by the high-performance model trained on valuable data. This interface boosts the proliferation of machine learning based applications, while on the other hand, it introduces the attack surface for model stealing attacks. Existing model stealing attacks have relaxed their attack assumptions to the data-free setting, while keeping the effectiveness. However, these methods are complex and consist of several components, which obscure the core on which the attack really depends. In this paper, we revisit the model stealing problem from a diversity perspective and demonstrate that keeping the generated data samples more diverse across all the classes is the critical point for improving the attack performance. Based on this conjecture, we provide a simplified attack framework. We empirically signify our conjecture by evaluating the effectiveness of our attack, and experimental results show that our approach is able to achieve comparable or even better performance compared with the state-of-the-art method. Furthermore, benefiting from the absence of redundant components, our method demonstrates its advantages in attack efficiency and query budget.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yiyong Liu",
        "Rui Wen",
        "Michael Backes",
        "Yang Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2b28136746a29ff698f57106c292d0d6e0181629",
      "pdf_url": "",
      "publication_date": "2024-03-29",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4595c8a7e1571c9974b2393ed127f05fabe72164",
      "title": "Model Stealing Detection for IoT Services Based on Multidimensional Features",
      "abstract": "Model stealing (MS) attacks pose a significant security concern for machine learning models on cloud platforms, as they can reconstruct a substitute model with limited effort to evade ownership. While detection-based methods show promise in preventing MS attacks, they often face practical challenges. Specifically, setting an appropriate threshold to distinguish malicious features from benign ones is a difficult task, often leading to a tradeoff between false alarm rates and detection accuracy. To address this challenge, we design a multidimensional feature extraction-and-distinction scheme called MED. It is achieved through a two-layer optimization: 1) the inner layer of extraction to maximize the difference of extracted multidimensional features between attack and benign samples and 2) the outer layer of distinction to maximize the accuracy of distinguishing malicious features automatically. Recognizing that different MS attacks result in varied features, we design a group of feature extraction functions in the inner layer optimization, which addresses the limitations of single-feature-based detection methods. Further, we employ three differently characterized models for distinction, enabling MED to distinguish different types of malicious features. Comprehensive experiments are conducted to evaluate the effectiveness of the proposed scheme: MED can detect all types of MS attacks with no more than 100 samples, with an average detection rate greater than 0.99.",
      "year": 2024,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Xinjing Liu",
        "Taifeng Liu",
        "Hao Yang",
        "Jiakang Dong",
        "Zuobin Ying",
        "Zhuo Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4595c8a7e1571c9974b2393ed127f05fabe72164",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a6854f91dbad0b39a0289872ac838545a9c18225",
      "title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming",
      "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.",
      "year": 2024,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "Leo Yu Zhang",
        "Chao Chen",
        "Shengshan Hu",
        "Asif Gill",
        "Shirui Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225",
      "pdf_url": "",
      "publication_date": "2024-12-09",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "title": "Exploring Zero-Day Attacks on Machine Learning and Deep Learning Algorithms",
      "abstract": "In the rapidly evolving field of artificial intelligence, machine learning (ML) and deep learning (DL) algorithms have emerged as powerful tools for solving complex problems in various domains, including cyber security. However, as these algorithms become increasingly prevalent, they also face new security challenges. One of the most significant of these challenges is the threat of zero-day attacks, which exploit unknown and unpredictable vulnerabilities in the algorithms or the data they process. \nThis paper provides a comprehensive overview of zero-day attacks on ML/DL algorithms, exploring their types, causes, effects, and potential countermeasures. The paper begins by introducing the concept and definition of zero-day attacks, providing a clear understanding of this emerging threat. It then reviews the existing research on zero-day attacks on ML/DL algorithms, focusing on three main categories: data poisoning attacks, adversarial input attacks, and model stealing attacks. Each of these attack types poses unique challenges and requires specific countermeasures. \nThe paper also discusses the potential impacts and risks of these attacks on various application domains. For instance, in facial expression recognition, an adversarial input attack could lead to misclassification of emotions, with serious implications for user experience and system integrity. In object classification, a data poisoning attack could cause the algorithm to misidentify critical objects, potentially endangering human lives in applications like autonomous driving. In satellite intersection recognition, a model stealing attack could compromise national security by revealing sensitive information. \nFinally, the paper presents some possible protection methods against zero-day attacks on ML/DL algorithms. These include anomaly detection techniques to identify unusual patterns in the data or the algorithm\u2019s behaviour, model verification and validation methods to ensure the algorithm\u2019s correctness and robustness, federated learning approaches to protect the privacy of the training data, and differential privacy techniques to add noise to the data or the algorithm\u2019s outputs to prevent information leakage. \nThe paper concludes by highlighting some open issues and future directions for research in this area, emphasizing the need for ongoing efforts to secure ML/DL algorithms against zero-day attacks.",
      "year": 2024,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "Marie Kov\u00e1\u0159ov\u00e1"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "pdf_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/2310/2134",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "be6dc29e9773c5242b0a44877df60af0feae9adb",
      "title": "Sample Correlation for Fingerprinting Deep Face Recognition",
      "abstract": "Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC.",
      "year": 2024,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "Yanbo Wang",
        "R. He"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/be6dc29e9773c5242b0a44877df60af0feae9adb",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b3744d928fcd84b7e2296d5983ba199a300955e0",
      "title": "Precise Extraction of Deep Learning Models via Side-Channel Attacks on Edge/Endpoint Devices",
      "abstract": "With growing popularity, deep learning (DL) models are becoming larger-scale, and only the companies with vast training datasets and immense computing power can manage their business serving such large models. Most of those DL models are proprietary to the companies who thus strive to keep their private models safe from the model extraction attack (MEA), whose aim is to steal the model by training surrogate models. Nowadays, companies are inclined to offload the models from central servers to edge/endpoint devices. As revealed in the latest studies, adversaries exploit this opportunity as new attack vectors to launch side-channel attack (SCA) on the device running victim model and obtain various pieces of the model information, such as the model architecture (MA) and image dimension (ID). Our work provides a comprehensive understanding of such a relationship for the first time and would benefit future MEA studies in both offensive and defensive sides in that they may learn which pieces of information exposed by SCA are more important than the others. Our analysis additionally reveals that by grasping the victim model information from SCA, MEA can get highly effective and successful even without any prior knowledge of the model. Finally, to evince the practicality of our analysis results, we empirically apply SCA, and subsequently, carry out MEA under realistic threat assumptions. The results show up to 5.8 times better performance than when the adversary has no model information about the victim model.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Younghan Lee",
        "Sohee Jun",
        "Yungi Cho",
        "Woorim Han",
        "Hyungon Moon",
        "Y. Paek"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b3744d928fcd84b7e2296d5983ba199a300955e0",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "title": "Layer Sequence Extraction of Optimized DNNs Using Side-Channel Information Leaks",
      "abstract": "Deep neural network (DNN) intellectual property (IP) models must be kept undisclosed to avoid revealing trade secrets. Recent works have devised machine learning techniques that leverage on side-channel information leakage of the target platform to reverse engineer DNN architectures. However, these works fail to perform successful attacks on DNNs that have undergone performance optimizations (i.e., operator fusion) using DNN compilers, e.g., Apache tensor virtual machine (TVM). We propose a two-phase attack framework to infer the layer sequences of optimized DNNs through side-channel information leakage. In the first phase, we use a recurrent network with multihead attention components to learn the intra and interlayer fusion patterns from GPU traces of TVM-optimized DNNs, in order to accurately predict the operation distribution. The second phase uses a model to learn the run-time temporal correlations between operations and layers, which enables the prediction of layer sequence. An encoding strategy is proposed to overcome the convergence issues faced by existing learning-based methods when inferring the layer sequences of optimized DNNs. Extensive experiments show that our learning-based framework outperforms state-of-the-art DNN model extraction techniques. Our framework is also the first to effectively reverse engineer both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) using side-channel leakage.",
      "year": 2024,
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Guiyuan Jiang",
        "Xinwang Liu",
        "Peilan He",
        "Siew-Kei Lam"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1abdfdd7cd2d325e09b82d4d3a5dee97000d27a0",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d678d169552a667531d66e09a75b4e4d13e3c044",
      "title": "Exploring the Efficacy of Learning Techniques in Model Extraction Attacks on Image Classifiers: A Comparative Study",
      "abstract": "In the rapidly evolving landscape of cybersecurity, model extraction attacks pose a significant challenge, undermining the integrity of machine learning models by enabling adversaries to replicate proprietary algorithms without direct access. This paper presents a comprehensive study on model extraction attacks towards image classification models, focusing on the efficacy of various Deep Q-network (DQN) extensions for enhancing the performance of surrogate models. The goal is to identify the most efficient approaches for choosing images that optimize adversarial benefits. Additionally, we explore synthetic data generation techniques, including the Jacobian-based method, Linf-projected Gradient Descent (LinfPGD), and Fast Gradient Sign Method (FGSM) aiming to facilitate the training of adversary models with enhanced performance. Our investigation also extends to the realm of data-free model extraction attacks, examining their feasibility and performance under constrained query budgets. Our investigation extends to the comparison of these methods under constrained query budgets, where the Prioritized Experience Replay (PER) technique emerges as the most effective, outperforming other DQN extensions and synthetic data generation methods. Through rigorous experimentation, including multiple trials to ensure statistical significance, this work provides valuable insights into optimizing model extraction attacks.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Dong Han",
        "Reza Babaei",
        "Shangqing Zhao",
        "Samuel Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d678d169552a667531d66e09a75b4e4d13e3c044",
      "pdf_url": "",
      "publication_date": "2024-04-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "title": "Secure AI Systems: Emerging Threats and Defense Mechanisms",
      "abstract": "The capability of artificial intelligence (AI), increasingly embedded in critical domains, faces a complex array of security threats. It has motivated researchers to explore the security vulnerability of AI solutions and propose effective countermeasures. This article offers a comprehensive exploration of diverse attacks on AI models, including backdoors (Trojans), adversarial, fault injection, data poisoning, model inversion, model extraction, membership inference attacks, etc. These security vulnerabilities are classified into two broad categories, namely, Supply Chain Attacks and Runtime Attacks. We highlight threat models, attack strategies, and defenses to secure AI systems against these attacks. The work also underscores the significance of developing secure and robust AI models and their implementation to safeguard sensitive data and embedded systems. We present some emerging research directions on secure AI systems.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Habibur Rahaman",
        "Atri Chatterjee",
        "S. Bhunia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/edfc6d47efc3cf83a8f9fb7fbb2dc02135f83846",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "title": "Power Side-Channel Analysis and Mitigation for Neural Network Accelerators based on Memristive Crossbars",
      "abstract": "The modern trend of exploring Artificial Intelligence (AI) in various industries, such as big data, edge computing, automobile, and medical applications, has increased tremendously. As functionalities grow, energy-efficient hardware for AI devices becomes crucial. To address that, Computation-in-Memory (CiM) using Non-Volatile Memories (NVMs) offers a promising solution. However, security is also an important concern in this computation paradigm. In this work, we analyze the vulnerability for power side-channel attacks on Multiply-Accumulate (MAC) operations implemented in CiM architecture based on emerging NVMs. Our results show that peripheral devices such as Analog-to-Digital Converters (ADCs) leak much more sensitive information than the crossbar itself because of its significant power consumption. Therefore, we propose a circuit-level countermeasure based on hiding for the ADCs of memristive CiM architecture to mitigate the power attacks. The efficiency of our proposed countermeasure is shown by both attacks and leakage assessment methodologies using a maximum of one million measurement traces.",
      "year": 2024,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Brojogopal Sapui",
        "M. Tahoori"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "befa5df856721b10c50d034594fbc7194d96e386",
      "title": "A Middle Path for On-Premises LLM Deployment: Preserving Privacy Without Sacrificing Model Confidentiality",
      "abstract": "Privacy-sensitive users require deploying large language models (LLMs) within their own infrastructure (on-premises) to safeguard private data and enable customization. However, vulnerabilities in local environments can lead to unauthorized access and potential model theft. To address this, prior research on small models has explored securing only the output layer within hardware-secured devices to balance model confidentiality and customization. Yet this approach fails to protect LLMs effectively. In this paper, we discover that (1) query-based distillation attacks targeting the secured top layer can produce a functionally equivalent replica of the victim model; (2) securing the same number of layers, bottom layers before a transition layer provide stronger protection against distillation attacks than top layers, with comparable effects on customization performance; and (3) the number of secured layers creates a trade-off between protection and customization flexibility. Based on these insights, we propose SOLID, a novel deployment framework that secures a few bottom layers in a secure environment and introduces an efficient metric to optimize the trade-off by determining the ideal number of hidden layers. Extensive experiments on five models (1.3B to 70B parameters) demonstrate that SOLID outperforms baselines, achieving a better balance between protection and downstream customization.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hanbo Huang",
        "Yihan Li",
        "Bowen Jiang",
        "Bowen Jiang",
        "Lin Liu",
        "Ruoyu Sun",
        "Zhuotao Liu",
        "Shiyu Liang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/befa5df856721b10c50d034594fbc7194d96e386",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "title": "Bio-Rollup: a new privacy protection solution for biometrics based on two-layer scalability-focused blockchain",
      "abstract": "The increased use of artificial intelligence generated content (AIGC) among vast user populations has heightened the risk of private data leaks. Effective auditing and regulation remain challenging, further compounding the risks associated with the leaks involving model parameters and user data. Blockchain technology, renowned for its decentralized consensus mechanism and tamper-resistant properties, is emerging as an ideal tool for documenting, auditing, and analyzing the behaviors of all stakeholders in machine learning as a service (MLaaS). This study centers on biometric recognition systems, addressing pressing privacy and security concerns through innovative endeavors. We conducted experiments to analyze six distinct deep neural networks, leveraging a dataset quality metric grounded in the query output space to quantify the value of the transfer datasets. This analysis revealed the impact of imbalanced datasets on training accuracy, thereby bolstering the system\u2019s capacity to detect model data thefts. Furthermore, we designed and implemented a novel Bio-Rollup scheme, seamlessly integrating technologies such as certificate authority, blockchain layer two scaling, and zero-knowledge proofs. This innovative scheme facilitates lightweight auditing through Merkle proofs, enhancing efficiency while minimizing blockchain storage requirements. Compared to the baseline approach, Bio-Rollup restores the integrity of the biometric system and simplifies deployment procedures. It effectively prevents unauthorized use through certificate authorization and zero-knowledge proofs, thus safeguarding user privacy and offering a passive defense against model stealing attacks.",
      "year": 2024,
      "venue": "PeerJ Computer Science",
      "authors": [
        "Jian Yun",
        "Yusheng Lu",
        "Xinyang Liu",
        "Jingdan Guan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4d5f1a09ddd9b3e984b97195e29c1c61a56e4d84",
      "pdf_url": "https://doi.org/10.7717/peerj-cs.2268",
      "publication_date": "2024-09-09",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "title": "Enhancing TinyML Security: Study of Adversarial Attack Transferability",
      "abstract": "The recent strides in artificial intelligence (AI) and machine learning (ML) have propelled the rise of TinyML, a paradigm enabling AI computations at the edge without dependence on cloud connections. While TinyML offers real-time data analysis and swift responses critical for diverse applications, its devices' intrinsic resource limitations expose them to security risks. This research delves into the adversarial vulnerabilities of AI models on resource-constrained embedded hardware, with a focus on Model Extraction and Evasion Attacks. Our findings reveal that adversarial attacks from powerful host machines could be transferred to smaller, less secure devices like ESP32 and Raspberry Pi. This illustrates that adversarial attacks could be extended to tiny devices, underscoring vulnerabilities, and emphasizing the necessity for reinforced security measures in TinyML deployments. This exploration enhances the comprehension of security challenges in TinyML and offers insights for safeguarding sensitive data and ensuring device dependability in AI-powered edge computing settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c3df2c0b6c68c4db4e946ed8432e7f7b2269ecf3",
      "pdf_url": "",
      "publication_date": "2024-07-16",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "title": "EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning",
      "abstract": "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA\".",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jingtao Li",
        "Xing Chen",
        "Li Yang",
        "A. S. Rakin",
        "Deliang Fan",
        "Chaitali Chakrabarti"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/467cabe6f85318ef74987895cbf2f1e46f5c1d01",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/29258/30374",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "title": "Defending Against Label-Only Attacks via Meta-Reinforcement Learning",
      "abstract": "Machine learning models are susceptible to a range of adversarial activities. These attacks are designed to either infer private information from the target model or deceive it. For instance, an attacker may attempt to discern if a given data example is from the model\u2019s training set (membership inference attacks) or create adversarial examples to mislead the model to make incorrect predictions (adversarial example attacks). Numerous defense methods have been proposed to counter these attacks. However, these methods typically share two common limitations. Firstly, most are not designed to address label-only attacks, which is a newly emerged kind of attacks that rely solely on the hard labels predicted by the target model. Secondly, they are often developed to mitigate specific attacks rather than universally various attacks. To address these limitations, this paper proposes a novel defense method that focuses on the most challenging attacks, i.e., label-only attacks, and can handle various types of label-only attacks. The key idea is to strategically modify the target model\u2019s predicted labels using a meta-reinforcement learning technique. This ensures that attackers receive incorrect labels while benign users continue to receive correct labels. Notably, the defender, i.e., the owner of the target model, can make effective decisions without knowledge of the attacker\u2019s behavior. The experimental results demonstrate that our proposed method is an effective defense against a range of attacks, including label-only model stealing, label-only membership inference, label-only model inversion, and label-only adversarial example attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Dayong Ye",
        "Tianqing Zhu",
        "Kun Gao",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d80e74e8ed6789b0901a3365f7185cb1ae9a991e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "20993724635d425f696e1c7e38c9e8da025ff757",
      "title": "FedMCT: A Federated Framework for Intellectual Property Protection and Malicious Client Tracking",
      "abstract": "In the era of big data, federated learning (FL) emerges as a solution to train models collectively without exposing individual data, maintaining similar accuracy to models trained on shared datasets. However, challenges arise with the advent of privacy inference attacks and model theft, posing significant threats to the privacy of FL models, especially regarding intellectual property (IP) protection. This paper introduces FedMCT (Federated Malicious Client Tracking), a novel framework addressing these challenges in the FL context. The FedMCT framework is a new approach to protect IP rights of FL clients and track cheaters, which can improve efficiency in resource-heterogeneous environments. By embedding unique watermarks or fingerprints in Deep Neural Network (DNN) models, we can protect model IP. We employ a configuration round before watermark embedding, segmenting clients based on performance for tiered model watermarking. We also propose a tiered watermarking and traitor tracking mechanism, which reduces the tracking time and ensures high traitor tracking efficiency. Extensive experiments validate our solution\u2019s efficacy in maintaining original model performance, watermark privacy, and detectability, robust against various attacks, demonstrating superior traitor tracing efficiency compared to existing frameworks.",
      "year": 2024,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Qianyi Chen",
        "Peijia Zheng",
        "Yusong Du",
        "Weiqi Luo",
        "Hongmei Liu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/20993724635d425f696e1c7e38c9e8da025ff757",
      "pdf_url": "",
      "publication_date": "2024-02-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "title": "MarginFinger: Controlling Generated Fingerprint Distance to Classification boundary Using Conditional GANs",
      "abstract": "Deep neural networks (DNNs) are widely employed across various domains, with their training costs making them crucial assets for model owners. However, the rise of Machine Learning as a Service has made models more accessible, but also increases the risk of leakage. Attackers can successfully steal models through internal leaks or API access, emphasizing the critical importance of protecting intellectual property. Several watermarking methods have been proposed, embedding secret watermarks of model owners into models. However, watermarking requires tampering with the model's training process to embed the watermark, which may lead to a decrease in utility. Recently, some fingerprinting techniques have emerged to generate fingerprint samples near the classification boundary to detect pirated models. Nevertheless, these methods lack distance constraints and suffer from high training costs. To address these issues, we propose to utilize conditional generative network to generate fingerprint data points, enabling a better exploration of the model's decision boundary. By incorporating margin loss during GAN training, we can control the distance between generated data points and classification boundary to ensure the robustness and uniqueness of our method. Moreover, our method does not require additional training of proxy models, enhancing the efficiency of fingerprint acquisition. To validate the effectiveness of our approach, we evaluate it on CIFAR-10 and Tiny-ImageNet, considering three types of model extraction attacks, fine-tuning, pruning, and transfer learning attacks. The results demonstrate that our method achieves ARUC values of 0.186 and 0.153 on CIFAR-10 and Tiny-ImageNet datasets, respectively, representing a remarkable improvement of 400% and 380% compared to the current leading baseline. The source code is available at https://github.com/wason981/MarginFinger.",
      "year": 2024,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Weixing Liu",
        "Sheng-hua Zhong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c26c3f657af13747b9160aaa3886ef9ecc27a028",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3652583.3658058",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "title": "Dual-Rail Precharge Logic-Based Side-Channel Countermeasure for DNN Systolic Array",
      "abstract": "Deep neural network (DNN) accelerators are widely used in cloud-edge-end and other application scenarios. Researchers recently focused on extracting secret information from DNN through side-channel attacks (SCAs), which substantially threaten AI security. In this brief, we propose a high-security, high-performance side-channel countermeasure using dual-rail precharge logic (DPL) for the DNN systolic array. By collecting and analyzing 5000 power traces, our proposed DPL-based systolic array provides a significantly lower correlation coefficient of 0.045. Through system-level side-channel security evaluation on field-programmable gate arrays (FPGAs), the DPL-based systolic array can effectively defend against weight extraction under power SCAs.",
      "year": 2024,
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "authors": [
        "Le Wu",
        "Liji Wu",
        "Xiangmin Zhang",
        "Munkhbaatar Chinbat"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/39380b900ca2f7692efd229c3b4f72f5fff6f2dc",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "title": "Streamlining DNN Obfuscation to Defend Against Model Stealing Attacks",
      "abstract": "Side-channel-based Deep Neural Network (DNN) model stealing has become a major concern with the advent of learning-based attacks. In respond to this threat, defence mechanisms have been presented to obfuscate the DNN execution, making it difficult to infer the correlation between side-channel information and DNN architecture. However, state-of-the-art (SOTA) DNN obfuscation is time-consuming, requires expert-level changes in existing DNN compilers (e.g., Tensor Virtual Machine (TVM)), and often relies on prior knowledge of the attack models. In this work, we study the impact of various obfuscation levels on the defence effectiveness, and present a streamlined DNN obfuscation process that is extremely fast and is agnostic to any attack models. Our study reveals that by just modifying the scheduling of DNN operations on the GPU, we can achieve comparable defense performance as the SOTA in an attack agnostic manner. We also propose a simple algorithm that determines an effective scheduling configuration for mitigating DNN model stealing at a fraction of a time required by SOTA obfuscation methods. Our method can be easily integrated into existing DNN compilers as a security feature, even by non-experts, to protect their DNN against side-channel attacks.",
      "year": 2024,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Yidan Sun",
        "Siew-Kei Lam",
        "Guiyuan Jiang",
        "Peilan He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/aee0bc1bc011d8da14aa209d0af984a9cc6b227f",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
      "abstract": "Deep reinforcement learning policies, which are integral to modern control systems, represent valuable intellectual property. The development of these policies demands considerable resources, such as domain expertise, simulation fidelity, and real-world validation. These policies are potentially vulnerable to model stealing attacks, which aim to replicate their functionality using only black-box access. In this paper, we propose Stealthy Imitation, the first attack designed to steal policies without access to the environment or knowledge of the input range. This setup has not been considered by previous model stealing methods. Lacking access to the victim's input states distribution, Stealthy Imitation fits a reward model that allows to approximate it. We show that the victim policy is harder to imitate when the distribution of the attack queries matches that of the victim. We evaluate our approach across diverse, high-dimensional control tasks and consistently outperform prior data-free approaches adapted for policy stealing. Lastly, we propose a countermeasure that significantly diminishes the effectiveness of the attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhixiong Zhuang",
        "Maria-Irina Nicolae",
        "Mario Fritz"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/2df09db143feb7aa0fc30ac548f0aaa4c628c3cd",
      "pdf_url": "",
      "publication_date": "2024-05-11",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
      "abstract": "Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Hui Sun",
        "Wanlei Zhou",
        "Philip S. Yu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/43bd351ab3d23bdb0ecc733cee5ad43db3dea075",
      "pdf_url": "http://arxiv.org/pdf/2406.16963",
      "publication_date": "2024-06-22",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models Through Adversarial Examples and Timing Side-Channels",
      "abstract": "Machine learning, with its myriad applications, has become an integral component of numerous AI systems. A common practice in this domain is the use of transfer learning, where a pre-trained model\u2019s architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it is crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present ArchWhisperer, a model fingerprinting attack approach based on the novel observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with model inference times is used to further enhance our attack in terms of attack effectiveness as well as query budget. ArchWhisperer is designed for typical user-level access in remote MLaaS environments and it exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20. This is a marked improvement compared to state-of-the-art works.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Shubhi Shukla",
        "Manaar Alam",
        "Pabitra Mitra",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a968c61fc5a14ce01db2a67b6ab87e30248a0a19",
      "pdf_url": "https://arxiv.org/pdf/2402.11953",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "title": "\"Yes, My LoRD.\" Guiding Language Model Extraction with Locality Reinforced Distillation",
      "abstract": "Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zi Liang",
        "Qingqing Ye",
        "Yanyun Wang",
        "Sen Zhang",
        "Yaxin Xiao",
        "Ronghua Li",
        "Jianliang Xu",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dca9242b227eebb3f052139dcb6a45c4dcbfde83",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "55414dc21b091006bf868b28008c9fc30fa38dca",
      "title": "Model Extraction Attack against On-device Deep Learning with Power Side Channel",
      "abstract": "The proliferation of on-device deep learning models in resource-constrained environments has led to significant advancements in privacy-preserving machine learning. However, the deployment of these models also introduces new security challenges, one of which is the vulnerability to model extraction attacks. In this paper, we investigate a novel attack with power side channel to extract on-device deep learning model deployed, which poses a substantial threat to on-device deep learning systems. By carefully monitoring power consumption during inference, an adversary can gain insights into the model\u2019s internal behavior, potentially compromising the model\u2019s intellectual property and sensitive data. Through experiments on a real-world embedded device (Jetson Nano) and various types of deep learning models, we demonstrate that the proposed attack can extract models with high fidelity. Based on experiments, we find that the power side channel-assisted model extraction attack can achieve high attacking success rate, up to 96.7% and 87.5% under close world and open world settings. This research sheds light on the evolving landscape of security threats in the context of on-device DL and provides valuable insights into safeguarding these models from potential adversaries.",
      "year": 2024,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Jiali Liu",
        "Han Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/55414dc21b091006bf868b28008c9fc30fa38dca",
      "pdf_url": "",
      "publication_date": "2024-04-03",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "title": "Unveiling Intellectual Property Vulnerabilities of GAN-Based Distributed Machine Learning through Model Extraction Attacks",
      "abstract": "Generative Adversarial Networks (GANs), as a cornerstone of artificial intelligence (AI), are widely recognized as the intellectual property (IP) of their owners, given the sensitivity of the training data and the commercial value tied to the models. Model extraction attacks, which aim to steal well-trained proprietary models, pose a significant threat to model IP. Nevertheless, current research predominately focuses on the context of machine learning as a service (MLaaS), where the emphasis lies in understanding the attack knowledge acquired through black-box API queries. This restricted perspective exposes a critical gap in investigating model extraction attacks within realistic distributed settings for generative tasks. In this work, we present the first investigation into model extraction attacks against GANs in distributed settings. We provide a comprehensive attack taxonomy, considering three different levels of knowledge the adversary can obtain in practice. Based on it, we introduce a novel model extraction attack named MoEx, which focuses on the GAN-based distributed learning scenario, i.e., Multi-Discriminator GANs, a typical asymmetric distributed setting. MoEx uses the objective function simulation, leveraging data exchanged during the learning process, to approximate the GAN generator owned by the server. We define two attack goals for MoEx, fidelity extraction and accuracy extraction. Then we comprehensively evaluate the effectiveness of MoEx's two goals with real-world datasets. Our results demonstrate its robust capabilities in extracting generators with high fidelity and accuracy compared with existing methods.",
      "year": 2024,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Mengyao Ma",
        "Shuofeng Liu",
        "M.A.P. Chamikara",
        "Mohan Baruwal Chhetri",
        "Guangdong Bai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/dfbbf3fa36cd03aa41d4170ba672e332f8295bd0",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "title": "Efficient and Effective Model Extraction",
      "abstract": "Model extraction aims to steal a functionally similar copy from a machine learning as a service (MLaaS) API with minimal overhead, typically for illicit profit or as a precursor to further attacks, posing a significant threat to the MLaaS ecosystem. However, recent studies have shown that model extraction is highly inefficient, particularly when the target task distribution is unavailable. In such cases, even substantially increasing the attack budget fails to produce a sufficiently similar replica, reducing the adversary\u2019s motivation to pursue extraction attacks. In this paper, we revisit the elementary design choices throughout the extraction lifecycle. We propose an embarrassingly simple yet dramatically effective algorithm, Efficient and Effective Model Extraction (E3), focusing on both query preparation and training routine. E3 achieves superior generalization compared to state-of-the-art methods while minimizing computational costs. For instance, with only 0.005\u00d7 the query budget and less than 0.2\u00d7 the runtime, E3 outperforms classical generative model based data-free model extraction by an absolute accuracy improvement of over 50% on CIFAR-10. Our findings underscore the persistent threat posed by model extraction and suggest that it could serve as a valuable benchmarking algorithm for future security evaluations.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hongyu Zhu",
        "Wentao Hu",
        "Sichu Liang",
        "Fangqi Li",
        "Wenwen Wang",
        "Shilin Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fbfb0ed07c0c8c84a959e0f4de6db90d5d65772f",
      "pdf_url": "",
      "publication_date": "2024-09-21",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
      "abstract": "We introduce Adversarial Sparse Teacher (AST), a robust defense method against distillation-based model stealing attacks. Our approach trains a teacher model using adversarial examples to produce sparse logit responses and increase the entropy of the output distribution. Typically, a model generates a peak in its output corresponding to its prediction. By leveraging adversarial examples, AST modifies the teacher model\u2019s original response, embedding a few altered logits into the output, while keeping the primary response slightly higher. Concurrently, all remaining logits are elevated to further increase the output distribution\u2019s entropy. All these complex manipulations are performed using an optimization function with our proposed Exponential Predictive Divergence (EPD) loss function. EPD allows us to maintain higher entropy levels compared to traditional KL divergence, effectively confusing attackers. Experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that AST outperforms state-of-the-art methods, providing effective defense against model stealing, while preserving high accuracy. The source codes are publicly available at https://github.com/codeofanon/AdversarialSparseTeacher",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "E. Y\u0131lmaz",
        "H. Keles"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7c1924e7f6a9c335ebb83c50996fa93e4bb62bcb",
      "pdf_url": "",
      "publication_date": "2024-03-08",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
      "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential. In this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.",
      "year": 2024,
      "venue": "ARES",
      "authors": [
        "Dirmanto Jap",
        "J. Breier",
        "Zdenko Lehock'y",
        "S. Bhasin",
        "Xiaolu Hou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4fa2c8c744b37ba9b227cb3de8cdec448c09c1d7",
      "pdf_url": "",
      "publication_date": "2024-07-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "99712e009858ba394bd119092bc2ebcc502a8892",
      "title": "DualCOS: Query-Efficient Data-Free Model Stealing with Dual Clone Networks and Optimal Samples",
      "abstract": "Although data-free model stealing attacks are free from reliance on real data, they suffer from limitations, including low accuracy and high query budgets, which restrict their practical feasibility. In this paper, we propose a novel data-free model stealing framework called DualCOS. As a whole, DualCOS is divided into two stages: interactive training and semi-supervised boosting. To optimize the usage of query budgets, we use a dual clone model architecture to address the challenge of querying victim model during generator training. We also introduce active learning-based sampling strategy and sample reuse mechanism to achieve an efficient query process. Furthermore, once query budget is exhausted, the semi-supervised boosting is employed to continue improving the final clone accuracy. Through extensive evaluations, we demonstrate the superiority of our proposed method in terms of accuracy and query efficiency, particularly in scenarios involving hard labels and multiple classes.",
      "year": 2024,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Yu Xuan",
        "Zhendong Zhao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/99712e009858ba394bd119092bc2ebcc502a8892",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "title": "Securing Machine Learning: Understanding Adversarial Attacks and Bias Mitigation",
      "abstract": "This paper offers a comprehensive examination of adversarial vulnerabilities in machine learning (ML) models and strategies for mitigating fairness and bias issues. It analyses various adversarial attack vectors encompassing evasion, poisoning, model inversion, exploratory probes, and model stealing, elucidating their potential to compromise model integrity and induce misclassification or information leakage. In response, a range of defence mechanisms including adversarial training, certified defences, feature transformations, and ensemble methods are scrutinized, assessing their effectiveness and limitations in fortifying ML models against adversarial threats. Furthermore, the study explores the nuanced landscape of fairness and bias in ML, addressing societal biases, stereotypes reinforcement, and unfair treatment, proposing mitigation strategies like fairness metrics, bias auditing, de-biasing techniques, and human-in-the-loop approaches to foster fairness, transparency, and ethical AI deployment. This synthesis advocates for interdisciplinary collaboration to build resilient, fair, and trustworthy AI systems amidst the evolving technological paradigm.",
      "year": 2024,
      "venue": "International Journal of Innovative Science and Research Technology",
      "authors": [
        "Archit Lakhani",
        "Neyah Rohit"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/04ba64269e3cedd3bf0117218d0c69cba2bd5abb",
      "pdf_url": "https://doi.org/10.38124/ijisrt/ijisrt24jun1671",
      "publication_date": "2024-07-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "title": "Genetic Improvement for DNN Security",
      "abstract": "Genetic improvement (GI) in Deep Neural Networks (DNNs) has traditionally enhanced neural architecture and trained DNN parameters. Recently, GI has supported large language models by optimizing DNN operator scheduling on accelerator clusters. However, with the rise of adversarial AI, particularly model extraction attacks, there is an unexplored potential for GI in fortifying Machine Learning as a Service (MLaaS) models. We suggest a novel application of GI \u2014 not to improve model performance, but to diversify operator parallelism for the purpose of a moving target defense against model extraction attacks. We discuss an application of GI to create a DNN model defense strategy that uses probabilistic isolation, offering unique benefits not present in current DNN defense systems.",
      "year": 2024,
      "venue": "International Genetic Improvement Workshop",
      "authors": [
        "Hunter Baxter",
        "Yu Huang",
        "Kevin Leach"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d6b246a94fc9347ad096c5762f743e45b6ab922c",
      "pdf_url": "",
      "publication_date": "2024-04-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "title": "Beowulf: Mitigating Model Extraction Attacks Via Reshaping Decision Regions",
      "abstract": "Machine Learning as a Service (MLaaS) enables resource-constrained users to access well-trained models through a publicly accessible Application Programming Interface (API) on a pay-per-query basis. Nevertheless, model owners may face the potential threats of model extraction attacks where malicious users replicate valuable commercial models based on query results. Existing defenses against model extraction attacks, however, either sacrifice prediction accuracy or fail to thwart more advanced attacks. In this paper, we propose a novel model extraction defense, dubbed Beowulf 1 , which draws inspiration from theoretical findings that models with complex and narrow decision regions are difficult to be reproduced. Rather than arbitrarily altering decision regions, which may jeopardize the predictive capacity of the victim model, we introduce a dummy class, carefully synthesized using both random and adversarial noises. The random noise broadens the coverage of the dummy class, and the adversarial noise impacts decision regions near decision boundaries with normal classes. To further improve the model utility, we propose to employ data augmentation methods to seamlessly integrate the dummy class and the normal classes. Extensive evaluations on CIFAR-10, GTSRB, CIFAR-100, and ImageNette datasets demonstrate that Beowulf can significantly reduce the extraction accuracy of 6 state-of-the-art model extraction attacks by as much as 80%. Moreover, we show that Beowulf is also robust to adaptive model extraction attacks.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xueluan Gong",
        "Rubin Wei",
        "Ziyao Wang",
        "Yuchen Sun",
        "Jiawen Peng",
        "Yanjiao Chen",
        "Qian Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9e3844cfb19402bde4df26cb1e1810faae8d5881",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670267",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c74aaca78aa0cfd0eff383f081cdd5440fb03098",
      "title": "Experimental Investigation of Side-Channel Attacks on Neuromorphic Spiking Neural Networks",
      "abstract": "This study investigates the reliability of commonly utilized digital spiking neurons and the potential side-channel vulnerabilities in neuromorphic systems that employ them. Through our experiments, we have successfully decoded the parametric information of Izhikevich and leaky integrate-and-fire (LIF) neuron-based spiking neural networks (SNNs) using differential power analysis. Furthermore, we have demonstrated the practical application of extracted information from the 92% accurate pretrained standard spiking convolution neural network classifier on the FashionMNIST dataset. These findings highlight the potential dangers of utilizing internal information for side-channel and denial-of-service attacks, even when using the usual input as the attack vector.",
      "year": 2024,
      "venue": "IEEE Embedded Systems Letters",
      "authors": [
        "Bhanprakash Goswami",
        "Tamoghno Das",
        "Manan Suri"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/c74aaca78aa0cfd0eff383f081cdd5440fb03098",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "title": "Enhancing Side-Channel Attacks Prediction using Convolutional Neural Networks",
      "abstract": "A common type of cyberattack is the side channel attack (SCA), which affects many devices and equipment connected to a network. These attacks have different types, like power attacks such as DPA, electromagnetic attacks, storage attacks, and others. Researchers and information security experts are concerned about SCA targeting devices, as they can lead to the loss and theft of important information. Using deep learning (DL) techniques in SCA analysis can be an efficient tool for detecting the SCA. Many previous works have tried to carry out the SCA in order to mitigate the impact of these attacks, but they encountered difficulties in detecting the SCA, whether in selecting the suitable dataset or applying the most efficient machine learning or deep learning techniques for achieving high performance. Therefore, we developed in this paper a deep learning-based model to detect SCAs using a dataset related to power attacks (the DPAv4 dataset) and the Convolutional Neural Networks (CNN) algorithm to train and test the DPAv4 dataset. The findings of our experiments showed a significant improvement in the performance of DL-based techniques in the detection of SCA. The proposed CNN-based model achieved an accuracy of 0.814 in detecting SCA and a loss rate of 0.581.",
      "year": 2024,
      "venue": "Automation, Control, and Information Technology",
      "authors": [
        "Khalid Alemerien",
        "Sadeq Al-Suhemat",
        "F. Alsuhimat",
        "Enshirah Altarawneh"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/de5840b27e0c9dc2888ae1badb41b9879a30e890",
      "pdf_url": "",
      "publication_date": "2024-09-19",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "title": "MTL-Leak: Privacy Risk Assessment in Multi-Task Learning",
      "abstract": "Multi-task learning (MTL) supports simultaneous training over multiple related tasks and learns the shared representation. While improving the generalization ability of training on a single task, MTL has higher privacy risk than traditional single-task learning because more sensitive information is extracted and learned in a correlated manner. Unfortunately, very few works have attempted to address the privacy risks posed by MTL. In this article, we first investigate such risk by designing model extraction attack (MEA) and membership inference attack (MIA) in MTL. Then we evaluate the privacy risks on six MTL model architectures and two popular MTL datasets, whose results show that both the number of tasks and the complexity of training data play an important role in the attack performance. Our investigation shows that MTL is more vulnerable than traditional single-task learning under both attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongyang Yan",
        "Anli Yan",
        "Li Hu",
        "Jiaming Liang",
        "Haibo Hu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/72df9bf57845936f81ce918adbc4b95b92aa1f9e",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "004764a194f3e85900a0b15d11c4a6955d6a616a",
      "title": "Robust and Minimally Invasive Watermarking for EaaS",
      "abstract": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1\\%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Zongqi Wang",
        "Baoyuan Wu",
        "Jingyuan Deng",
        "Yujiu Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/004764a194f3e85900a0b15d11c4a6955d6a616a",
      "pdf_url": "",
      "publication_date": "2024-10-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21c62f2ef68323099480c80318e48baae8b9098f",
      "title": "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack",
      "abstract": "Large language model (LLM) companies provide Embedding as a Service (EaaS) to assist the individual in efficiently dealing with downstream tasks such as text classification and recommendation. However, recent works reveal the risk of the model stealing attack, posing a financial threat to EaaS providers. To protect the copyright of EaaS, we propose GuardEmb, a dynamic embedding watermarking method, striking a balance between enhancing watermark detectability and preserving embedding functionality. Our approach involves selecting special tokens and perturbing embeddings containing these tokens to inject watermarks. Simultaneously, we train a verifier to detect these watermarks. In the event of an attacker attempting to replicate our EaaS for profit, their model inherits our watermarks. For watermark verification, we construct verification texts to query the suspicious EaaS, and the verifier identifies our watermarks within the responses, effectively tracing copyright infringement. Extensive experiments across diverse datasets showcase the high detectability of our watermark method, even in out-of-distribution scenarios, without compromising embedding functionality. Our code is publicly available at https://github. com/Melodramass/Dynamic-Watermark .",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Liaoyaqi Wang",
        "Minhao Cheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21c62f2ef68323099480c80318e48baae8b9098f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "title": "SwiftThief: Enhancing Query Efficiency of Model Stealing by Contrastive Learning",
      "abstract": "Model-stealing attacks are emerging as a severe threat to AI-based services because an adversary can create models that duplicate the functionality of the black-box AI models inside the services with regular query-based access. To avoid detection or query costs, the model-stealing adversary must consider minimizing the number of queries to obtain an accurate clone model. To achieve this goal, we propose SwiftThief, a novel model-stealing framework that utilizes both queried and unqueried data to reduce query complexity. In particular, SwiftThief uses contrastive learning, a recent technique for representation learning. We formulate a new objective function for model stealing consisting of self-supervised (for abundant unqueried inputs from public datasets) and soft-supervised (for queried inputs) contrastive losses, jointly optimized with an output matching loss (for queried inputs). In addition, we suggest a new sampling strategy to prioritize rarely queried classes to improve attack performance. Our experiments proved that SwiftThief could significantly enhance the efficiency of model-stealing attacks compared to the existing methods, achieving similar attack performance using only half of the query budgets of the competing approaches. Also, SwiftThief showed high competence even when a defense was activated for the victims.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a57be99734b62dfd2a54a15eb46c7359fdb7db58",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model stealing",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "title": "I Can Retrieve More than Images: Contrastive Stealing Attack against Deep Hashing Models",
      "abstract": "Deep hashing models have revolutionized traditional hashing methods by delivering superior performance, and have been applied in real-world applications such as Pinterest and Amazon, which are known as deep hashing-based retrieval systems. Behind their revolutionary representation capability, the requirements for training a deep hashing model expose it to the risks of potential model stealing attacks - a cheap way to mimic the well-trained hashing performance while circumventing the demanding requirements. Since the attacker is able to obtain the outputs of deep hashing models by querying the retrieval systems, the conventional stealing attacks relying on matching exact outputs can not be applied in this problem. In this paper, we propose a contrastive-based and GAN-enhanced stealing framework to leverage the informative knowledge of retrieved data. Our empirical results demonstrate that our stealing framework can train a substitute hashing model with a retrieval accuracy ranging from 80% to 110% of the target hashing model while utilizing significantly fewer training resources. Furthermore, we conduct attacks on the target hashing model using adversarial examples generated by the stolen model, resulting in an attack success rate that can be 3 times higher compared to attacks conducted without the substitute model. Finally, we leverage existing defense strategies to mitigate our attack, resulting in a stealing effectiveness decrease of no more than 4%.",
      "year": 2024,
      "venue": "2024 IEEE International Conference on Web Services (ICWS)",
      "authors": [
        "X. You",
        "Mi Zhang",
        "Jianwei Xu",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9c616cb25371fb66f3dacabafe1abc81a5fbaae5",
      "pdf_url": "",
      "publication_date": "2024-07-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "title": "GanTextKnockoff: stealing text sentiment analysis model functionality using synthetic data",
      "abstract": "Today, black-box machine learning models are often subject to extraction attacks that aim to retrieve their internal information. Black-box model extraction attacks are typically conducted by providing input data and, based on observing the output results, constructing a new model that functions equivalently to the original. This process is usually carried out by leveraging available data from public repositories or synthetic data generated by generative models. Most model extraction attack methods using synthetic data have been concentrated in the field of computer vision, with minimal research focused on model extraction in natural language processing. In this paper, we propose a method that utilizes synthetic textual data to construct a new model with high accuracy and similarity to the original black-box sentiment analysis model.",
      "year": 2024,
      "venue": "Journal of Military Science and Technology",
      "authors": [
        "Cong Pham",
        "Trung-Nguyen Hoang",
        "Cao-Truong Tran",
        "Viet-Binh Do"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/45b2b736cd8c66cb2bbb7efeb0907807d8f6fbe0",
      "pdf_url": "",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "title": "Quantum Neural Network Extraction Attack via Split Co-Teaching",
      "abstract": "Quantum Neural Networks (QNNs), now offered as QNN-as-a-Service (QNNaaS), have become key targets for model extraction attacks. Existing methods use ensemble learning to train substitute QNNs, but our analysis reveals significant limitations in real-world environments, where noise and cost constraints undermine their effectiveness. In this work, we introduce a novel attack, split co-teaching, which uses label variations to split queried data by noise sensitivity and employs co-teaching schemes to enhance extraction accuracy. The experimental results show that our approach outperforms classical extraction attacks by 6.5%~9.5% and existing QNN extraction methods by 0.1%~3.7% across various tasks.",
      "year": 2024,
      "venue": "2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9166ef92f94c521f1b5f78c51bac2dac805115a7",
      "pdf_url": "",
      "publication_date": "2024-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "title": "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality",
      "abstract": "Machine learning, particularly deep learning, is extensively applied in various real-life scenarios. However, recent research has highlighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. Most of the existing model stealing attacks rely on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, black-box, hard-label outputs, and there is a distribution shift during the testing phase. STMS achieves comparable or better stealing accuracy and generalization performance than prior works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can also be migrated to other areas of machine learning.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yunfei Yang",
        "Xiaojun Chen",
        "Zhendong Zhao",
        "Yu Xuan",
        "Bisheng Tang",
        "Xiaoying Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d011a91d041692e9915c99f4e0d3c9b2136bb3f",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "title": "MCD: Defense Against Query-Based Black-Box Surrogate Attacks",
      "abstract": "Deep neural networks (DNNs) is susceptible to surrogate attacks, where adversaries use surrogate data and corresponding outputs from the target model to build their own stolen model. Model stealing attacks jeopardize model privacy and model owners' commercial benefits. To address this issue, this paper proposes a hybrid protection approach-Maximize the confidence differences between benign samples and adversarial samples (MCD), to protect models from theft. Firstly, the LogitNorm approach is used to overcome the overconfidence problem in adversary query classification. Then, samples are divided into four groups according to ES and RS. Different groups are poisoned by different degrees. In addition to enhancing defensive performance and accounting for model integrity, the MCD uses a trigger to confirm the cloned model's owner. Experimental results show that the MCD defends against a variety of original models and attack techniques well. Against KnockoffNets and DFME attacks, the MCD yields an average defense performance of 54.58 % on five datasets, which is a great improvement over other defenses. Compared to other poisoning techniques, the Strong Poisoning (SP) module reduces the adversary's accuracy by 48.23 % on average. Additionally, the MCD overcomes the issue of OOD overconfidence while safeguarding the model accuracy in OOD detection and reduces the misclassification rate of ID samples for multiple OOD datasets.",
      "year": 2024,
      "venue": "IEEE International Conference on Systems, Man and Cybernetics",
      "authors": [
        "Yiwen Zou",
        "Wing W. Y. Ng",
        "Xueli Zhang",
        "Brick Loo",
        "Xingfu Yan",
        "Ran Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/931a9beaf77e0019bcfa2412b8210b83cb70aa1a",
      "pdf_url": "",
      "publication_date": "2024-10-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8d0fa6b361fc6319eac027da608a99f03b541111",
      "title": "Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters",
      "abstract": "Deep Neural Networks (DNNs) have made significant strides, but their susceptibility to backdoor attacks still remains a concern. Most defenses typically assume access to white-box models or poisoned data, requirements that are often not feasible in practice, especially for proprietary DNNs. Existing defenses in a black-box setting usually rely on confidence scores of DNN's predictions. However, this exposes DNNs to the risk of model stealing attacks, a significant concern for proprietary DNNs. In this paper, we introduce a novel strategy for detecting back-doors, focusing on a more realistic black-box scenario where only hard-label (i.e., without any prediction confidence) query access is available. Our strategy utilizes data flow dynamics in a computational environment during DNN inference to identify potential backdoor inputs and is agnostic of trigger types or their locations in the input. We observe that a clean image and its corresponding backdoor counterpart with a trigger induce distinct patterns across various microarchitectural activities during the inference phase. We exploit these variations captured by Hardware Performance Counters (HPCs) and use principles of the Gaussian Mixture Model to detect backdoor inputs. To the best of our knowledge, this is the first work that utilizes HPCs for detecting backdoors in DNNs. Extensive evaluation considering a range of benchmark datasets, DNN architectures, and trigger patterns shows the efficacy of the proposed method in distinguishing between clean and backdoor inputs using HPCs.",
      "year": 2024,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manaar Alam",
        "Yue Wang",
        "Michail Maniatakos"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8d0fa6b361fc6319eac027da608a99f03b541111",
      "pdf_url": "",
      "publication_date": "2024-03-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "title": "Extracting DNN Architectures via Runtime Profiling on Mobile GPUs",
      "abstract": "Deep Neural Networks (DNNs) have become invaluable intellectual property for AI providers due to advancements fueled by a decade of research and development. However, recent studies have demonstrated the effectiveness of model extraction attacks, which threaten this value by stealing DNN models. These attacks can lead to misuse of personal data, safety risks in critical systems, and the spread of misinformation. This paper explores model extraction attacks on DNN models deployed on mobile devices, using runtime profiles as a side-channel. Since mobile devices are resource constrained, DNN deployments require optimization efforts to reduce latency. The main hurdle in extracting DNN architectures in this scenario is that optimization techniques, such as operator-level and graph-level fusion, can obfuscate the association between runtime profile operators and their corresponding DNN layers, posing challenges for adversaries to accurately predict the computation performed. To overcome this, we propose a novel method analyzing GPU call profiles to identify the original DNN architecture. Our approach achieves full accuracy in extracting DNN architectures from a predefined set, even when layer information is obscured. For unseen architectures, a layer-by-layer hyperparameter extraction method guided by sub-layer patterns is introduced, also achieving high accuracy. This research achieves two firsts: 1) targeting mobile GPUs for DNN architecture extraction and 2) successfully extracting architectures from optimized models with fused layers.",
      "year": 2024,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Dong Hyub Kim",
        "Jonah O\u2019Brien Weiss",
        "Sandip Kundu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3f25ac027cb369ed864d3d7ef10a00b1f5877738",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
      "abstract": "With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as \\emph{multi-view data}, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. The code is available at: \\href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.",
      "year": 2024,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yuxuan Li",
        "Sarthak Kumar Maharana",
        "Yunhui Guo"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ff6c345c3d75b658760a19f9b368fd5266fa500c",
      "pdf_url": "",
      "publication_date": "2024-03-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "title": "TrustZoneTunnel: A Cross-World Pattern History Table-Based Microarchitectural Side-Channel Attack",
      "abstract": "ARM's TrustZone is a hardware-based trusted execution environment (TEE), prevalent in mobile devices, IoT edge systems, and autonomous systems. Within TrustZone, security-sensitive applications reside in a hardware-isolated secure world, protected from the normal-world's applications, OS, debugger, peripherals, and memory. However, microarchitectural side-channel vulnerabilities have been discovered on shared on-chip resources, such as caches and branch prediction unit (BPU). In this paper, we propose TrustZoneTunnel, the first Pattern History Table (PHT)-based side-channel attack on TrustZone, which is able to reveal the complete control flow of a trusted application in the secure world. We reverse-engineer the PHT indexing for ARM processors and develop key primitives for cross-world attacks, including well-controlled world-switching, PHT collision construction between two worlds, and precise PHT state-setting and checking functions. Furthermore, we introduce a novel model extraction attack against TrustZone based deep neural network, which can recover model parameters using only the side-channel leakage of vital branch instructions, obviating the need for model output or logits while prior research work requires such knowledge for model extraction.",
      "year": 2024,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Tianhong Xu",
        "A. A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/517a03d4025905d23928bbfdd4b1dfe595873ab3",
      "pdf_url": "",
      "publication_date": "2024-05-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d853215713815135bd419b5748c99c1ec03192ad",
      "title": "A New Approach in Mitigating Adversarial Attacks on Machine Learning",
      "abstract": "Machine learning is a powerful tool that has the potential to transform many industries, and thus is open to security attacks. Such attacks on machine learning algorithms are known as adversarial attacks. Adversarial attacks are designed to deceive or mislead machine learning models by introducing malicious input data, modifying existing data, or exploiting weaknesses in the algorithms used to train the models. These attacks can be targeted, deliberate, and sophisticated, leading to serious consequences such as incorrect decision-making, data breaches, and loss of intellectual property. Poisoning attacks, evasion attacks, model stealing, and model inversion attacks are some examples of adversarial attacks. At the moment, most researchers are focusing on a defense approach to mitigate these attacks. This approach aims to create a strong defense system that can detect and respond to attacks in real-time, prevent unauthorized access to systems and data, and mitigate the impact of security breaches. Unfortunately, this approach has some disadvantages, one of which is limited effectiveness. Despite the use of multiple defense measures, determined attackers can still find ways to breach systems and access sensitive data. This is due to the nature of the defense approach, which never addresses the root of the problem and thus can lead to the repetition of such attacks. In this paper, a new approach is proposed, namely using the forensic approach. The proposed approach will investigate attacks against machine learning, identify the root cause of the attack, determine the extent of the damage, and gather information that can be used to prevent similar incidents in the future.",
      "year": 2024,
      "venue": "IEEE Symposium on Wireless Technology and Applications",
      "authors": [
        "Abomakhleb Abdulruhman I Ahmad",
        "K. A. Jalil"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d853215713815135bd419b5748c99c1ec03192ad",
      "pdf_url": "",
      "publication_date": "2024-07-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "66431dd64545057c084c9f83a87ad0eb7c609f92",
      "title": "Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors",
      "abstract": "Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.",
      "year": 2024,
      "venue": "ECML/PKDD",
      "authors": [
        "Satoru Koda",
        "I. Morikawa"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/66431dd64545057c084c9f83a87ad0eb7c609f92",
      "pdf_url": "",
      "publication_date": "2024-11-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "title": "MisGUIDE : Defense Against Data-Free Deep Learning Model Extraction",
      "abstract": "The rise of Machine Learning as a Service (MLaaS) has led to the widespread deployment of machine learning models trained on diverse datasets. These models are employed for predictive services through APIs, raising concerns about the security and confidentiality of the models due to emerging vulnerabilities in prediction APIs. Of particular concern are model cloning attacks, where individuals with limited data and no knowledge of the training dataset manage to replicate a victim model's functionality through black-box query access. This commonly entails generating adversarial queries to query the victim model, thereby creating a labeled dataset. This paper proposes\"MisGUIDE\", a two-step defense framework for Deep Learning models that disrupts the adversarial sample generation process by providing a probabilistic response when the query is deemed OOD. The first step employs a Vision Transformer-based framework to identify OOD queries, while the second step perturbs the response for such queries, introducing a probabilistic loss function to MisGUIDE the attackers. The aim of the proposed defense method is to reduce the accuracy of the cloned model while maintaining accuracy on authentic queries. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed framework significantly enhances the resistance against state-of-the-art data-free model extraction in black-box settings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Gurve",
        "S. Behera",
        "Satyadev Ahlawat",
        "Yamuna Prasad"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3e24fcec8b51278cb2234560da6f6933f8fc7426",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "model extraction",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "title": "Advanced Side-Channel Profiling Attacks with Deep Neural Networks: A Hill Climbing Approach",
      "abstract": "Deep learning methods have significantly advanced profiling side-channel attacks. Finding the optimal set of hyperparameters for these models remains challenging. Effective hyperparameter optimization is crucial for training accurate neural networks. In this work, we introduce a novel hill climbing optimization algorithm that is specifically designed for deep learning in profiled side-channel analysis. This algorithm iteratively explores hyperparameter space using gradient-based techniques to make precise, localized adjustments. By incorporating performance feedback at each iteration, our approach efficiently converges on optimal hyperparameters, surpassing traditional Random Search methods. Extensive experiments\u2014covering protected implementations, leakage models, and various neural network architectures\u2014demonstrate that our hill climbing method consistently achieves superior performance in over 80% of test cases, predicting the secret key with fewer attack traces and outperforming both Random Search and state-of-the-art techniques.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Faisal Hameed",
        "Hoda Alkhzaimi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8ebbeb7248747d9ec63ccfd7117a03b0873bcc8b",
      "pdf_url": "https://doi.org/10.3390/electronics13173530",
      "publication_date": "2024-09-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "title": "SparseLeakyNets: Classification Prediction Attack Over Sparsity-Aware Embedded Neural Networks Using Timing Side-Channel Information",
      "abstract": "This letter explores security vulnerabilities in sparsity-aware optimizations for Neural Network (NN) platforms, specifically focusing on timing side-channel attacks introduced by optimizations such as skipping sparse multiplications. We propose a classification prediction attack that utilizes this timing side-channel information to mimic the NN's prediction outcomes. Our techniques were demonstrated for CIFAR-10, MNIST, and biomedical classification tasks using diverse dataflows and processing loads in timing models. The demonstrated results could predict the original classification decision with high accuracy.",
      "year": 2024,
      "venue": "IEEE computer architecture letters",
      "authors": [
        "Saurav Maji",
        "Kyungmi Lee",
        "A. Chandrakasan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d4e1bb8e5af5342a0fa1194b6e68b1f6d1ea307b",
      "pdf_url": "",
      "publication_date": "2024-01-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "title": "Proteus: Preserving Model Confidentiality during Graph Optimizations",
      "abstract": "Deep learning (DL) models have revolutionized numerous domains, yet optimizing them for computational efficiency remains a challenging endeavor. Development of new DL models typically involves two parties: the model developers and performance optimizers. The collaboration between the parties often necessitates the model developers exposing the model architecture and computational graph to the optimizers. However, this exposure is undesirable since the model architecture is an important intellectual property, and its innovations require significant investments and expertise. During the exchange, the model is also vulnerable to adversarial attacks via model stealing. This paper presents Proteus, a novel mechanism that enables model optimization by an independent party while preserving the confidentiality of the model architecture. Proteus obfuscates the protected model by partitioning its computational graph into subgraphs and concealing each subgraph within a large pool of generated realistic subgraphs that cannot be easily distinguished from the original. We evaluate Proteus on a range of DNNs, demonstrating its efficacy in preserving confidentiality without compromising performance optimization opportunities. Proteus effectively hides the model as one alternative among up to $10^{32}$ possible model architectures, and is resilient against attacks with a learning-based adversary. We also demonstrate that heuristic based and manual approaches are ineffective in identifying the protected model. To our knowledge, Proteus is the first work that tackles the challenge of model confidentiality during performance optimization. Proteus will be open-sourced for direct use and experimentation, with easy integration with compilers such as ONNXRuntime.",
      "year": 2024,
      "venue": "Conference on Machine Learning and Systems",
      "authors": [
        "Yubo Gao",
        "Maryam Haghifam",
        "Christina Giannoula",
        "Renbo Tu",
        "Gennady Pekhimenko",
        "Nandita Vijaykumar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/28e8ac6073a4abf2a220bc7951908c40d9d0b1f2",
      "pdf_url": "",
      "publication_date": "2024-04-18",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "title": "Exploring the Validity of Knockoff Nets Model Stealing Attack on Vgg16 Based on Different Models",
      "abstract": "Model stealing attacks represented by the Knockoff Nets method steal the intellectual property of AI models by black-box querying. Model stealing attacks on a wide range of deep learning models have attracted widespread attention in recent years. However, there has not been any research on stealing attacks based on common models such as VGG16, ResNet18, AlexNet, etc., especially since the research on the validity of the attack on the VGG16 model is still insufficient. Therefore, in this paper, three types of models, VGG16, ResNet18, and AlexNet, are used as the models for stealing, and the Knockoff Nets method is used to carry out stealing attacks on the pre-trained model of VGG16, which is capable of cat and dog image recognition. This paper analyzes the stealing similarity, stealing model accuracy and stealing training time so as to reflect the validity of stealing. The paper shows that Knockoff Nets based on three types of models, VGG16, ResNet18, and AlexNet, are all effective against the VGG16 model stealing attack, and the more similar the architectures of the stealing model and the victim's model are, the better the stealing effect is. In addition, to a certain extent, the stealing training time and the stealing model accuracy are affected by the architecture of model used to steal. This paper reveals the validity of the Knockoff Nets model stealing attack against VGG16 based on three types of models, namely VGG16, ResNet18, and AlexNet, to provide a reference for model security protection.",
      "year": 2024,
      "venue": "Applied and Computational Engineering",
      "authors": [
        "Yunxi Hei"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a9f0a5405f75e2596ebde4b0339dcc20b3b72db3",
      "pdf_url": "https://www.ewadirect.com/proceedings/ace/article/view/17285/pdf",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "model stealing",
        "stealing model",
        "model stealing attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "title": "Enhancing Data-Free Model Stealing Attack on Robust Models",
      "abstract": "Machine Learning Model Deployment as a Service (MLaaS) has surged in popularity, offering substantial business value. However, the significant resources and costs required to train models have raised concerns about Model Stealing Attacks (MSAs), where attackers create a clone model to replicate the knowledge of a victim model without access to its parameters. In data-free MSA, attackers also lack access to the training data for the victim model. In this setting, existing MSA methods rely on Generative Adversarial Networks (GANs) to generate images to query the victim model. However, GANs are known to suffer from model collapse, resulting in limited diversity in generated images. The lack of diversity in generated images will significantly impact the accuracy of the clone model, especially in stealing robust models trained with adversarial training. Recent studies have demonstrated that Denoising Diffusion Probabilistic Models (DDPMs) outperform GANs in generating images with greater diversity. In our data-free MSA framework, using DDPM as the generator to steal robust models significantly increases the effectiveness, improving the accuracy of the clone model from 21.34% to 60.23% compared to the GANs-based approach DFME, and requires fewer queries. We further use denoise diffusion GANs to address the problem of low sampling speed of DDPM, while retaining the advantage of its high sample diversity and obtaining better results.",
      "year": 2024,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jianping He",
        "Haichang Gao",
        "Yunyi Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0c713d0c069bfe03c945bfe7f67e9176ea8bcaff",
      "pdf_url": "",
      "publication_date": "2024-06-30",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9bedd67004ef344b801365ae28c09dce28410517",
      "title": "Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing",
      "abstract": "In learning-based functionality stealing, the attacker is trying to build a local model based on the victim's outputs. The attacker has to make choices regarding the local model's architecture, optimization method and, specifically for NLP models, subword vocabulary, such as BPE. On the machine translation task, we explore (1) whether the choice of the vocabulary plays a role in model stealing scenarios and (2) if it is possible to extract the victim's vocabulary. We find that the vocabulary itself does not have a large effect on the local model's performance. Given gray-box model access, it is possible to collect the victim's vocabulary by collecting the outputs (detokenized subwords on the output). The results of the minimum effect of vocabulary choice are important more broadly for black-box knowledge distillation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Vil\u00e9m Zouhar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9bedd67004ef344b801365ae28c09dce28410517",
      "pdf_url": "",
      "publication_date": "2024-01-29",
      "keywords_matched": [
        "model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
      "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jinyin Chen",
        "Haonan Ma",
        "Haibin Zheng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/274ead8c75880a252b6c92908dc329b0eb5f9f3f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "08e1f102344bc341e8109a5c23f78093a4c53323",
      "title": "Protecting Object Detection Models from Model Extraction Attack via Feature Space Coverage",
      "abstract": "The model extraction attack is an attack pattern aimed at stealing well-trained machine learning models' functionality or privacy information. With the gradual popularization of AI-related technologies in daily life, various well-trained models are being deployed. As a result, these models are considered valuable assets and attractive to model extraction attackers. Currently, the academic community primarily focuses on defense for model extraction attacks in the context of classification, with little attention to the more commonly used task scenario of object detection. Therefore, we propose a detection framework targeting model extraction attacks against object detection models in this paper. The framework first locates suspicious users based on feature coverage in query traffic and uses an active verification module to confirm whether the identified suspicious users are attackers. Through experiments conducted in multiple task scenarios, we validate the effectiveness and detection efficiency of the proposed method.",
      "year": 2024,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Zeyu Li",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/08e1f102344bc341e8109a5c23f78093a4c53323",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks",
      "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim's model (95.8% and 96.7% transfer rate).",
      "year": 2024,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Beno\u00eet Coqueret",
        "Mathieu Carbone",
        "Olivier Sentieys",
        "Gabriel Zaid"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/deef9baed03e2eb53aac92a38b5cfa6317dc1019",
      "pdf_url": "",
      "publication_date": "2024-11-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "title": "Security Concerns of Machine Learning Hardware",
      "abstract": "AI-as-a-Service (AIaaS) has been emerging with model providers deploying their models on cloud and model consumers using the model. Recently, ML models are being deployed on edge devices to improve cost and response time. The widespread usage of machine learning has made the study of security in the context of Machine Learning (ML) very critical. Model extraction attacks focuses on extracting model parameters such as weights and biases which can be used to clone a ML target model deployed on the cloud or on an edge device hardware. This paper explores different types of attacks on ML models primarily focusing on model extraction attacks on ML hardware such as scan-chain and side-channel attacks. The paper present an analysis of various such attacks and their countermeasures. Possible future directions of work are also discussed.",
      "year": 2024,
      "venue": "Asian Test Symposium",
      "authors": [
        "Nilotpola Sarma",
        "E. Bhawani",
        "E. Reddy",
        "C. Karfa"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9a4b5e07b9973e968ea746159bfe88c0801f854",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e1e53971438e42df0a29fa448b3398331d63f94a",
      "title": "Enabling Power Side-Channel Attack Simulation on Mixed-Signal Neural Network Accelerators",
      "abstract": "Due to the tremendous success of Deep Learning with neural networks (NNs) in recent years and the simultaneous leap of embedded, low-power devices (e.g. wearables, smart-phones, IoT, and smart sensors), enabling the inference of those NNs in power-constrained environments gave rise to specialized NN accelerators. One paradigm followed by many of those accelerators was the transition from digital domain computing towards performing operations in the analog domain, turning them from digital to mixed-signal NN accelerators. While power-efficiency and inference accuracy have been researched with increasing interest, security and protection against a side-channel attack (SCA) have not found much attention. However, side-channels pose a major security concern by allowing an attacker to steal valuable knowledge about proprietary NNs deployed on accelerators. In order to evaluate mixed-signal NNs accelerators concerning SCA robustness, its tendency to leak information through the side-channel needs investigation. In this work, we propose a methodology for enabling side-channel analysis of mixed-signal NNs accelerators, which shows reasonable accuracy in an early development stage. The approach enables the reuse of large portions of design sources for simulation and production while providing flexibility and fast development cycles for changes to the analog design.",
      "year": 2024,
      "venue": "Coins",
      "authors": [
        "Simon Wilhelmst\u00e4tter",
        "Joschua Conrad",
        "Devanshi Upadhyaya",
        "I. Polian",
        "M. Ortmanns"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e1e53971438e42df0a29fa448b3398331d63f94a",
      "pdf_url": "",
      "publication_date": "2024-07-29",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87a82c2177550ed7e10f635eadc8657e8f830c74",
      "title": "Time-Aware Face Anti-Spoofing with Rotation Invariant Local Binary Patterns and Deep Learning",
      "abstract": "Facial recognition systems have become an integral part of the modern world. These methods accomplish the task of human identification in an automatic, fast, and non-interfering way. Past research has uncovered high vulnerability to simple imitation attacks that could lead to erroneous identification and subsequent authentication of attackers. Similar to face recognition, imitation attacks can also be detected with Machine Learning. Attack detection systems use a variety of facial features and advanced machine learning models for uncovering the presence of attacks. In this work, we assess existing work on liveness detection and propose a novel approach that promises high classification accuracy by combining previously unused features with time-aware deep learning strategies.",
      "year": 2024,
      "venue": "IFIP International Information Security Conference",
      "authors": [
        "Moritz Finke",
        "Alexandra Dmitrienko"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/87a82c2177550ed7e10f635eadc8657e8f830c74",
      "pdf_url": "",
      "publication_date": "2024-08-27",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "title": "Camo-DNN: Layer Camouflaging to Protect DNNs against Timing Side-Channel Attacks",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose Camo $D N N$, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, Camo DNN performs various layer-obfuscation operations, e.g., layer branching layer deepening, etc., to alter the run-time traces while maintaining the functionality. Camo-DNN deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the architecture of DNN protected by Camo-DNN accurately. Further, we highlight that the adversarial attack on our obfuscated DNNs are unsuccessful.",
      "year": 2024,
      "venue": "IEEE International Symposium on On-Line Testing and Robust System Design",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "Ozgur Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ae86a7bdb3f8e8b10014ecfe47558e74222eb9f1",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "title": "CLUES: Collusive Theft of Conditional Generative Adversarial Networks",
      "abstract": "Conditional Generative Adversarial Networks (cGANs) are increasingly popular web-based synthesis services accessed through a query API, e.g., cGANs generate a cat image based on a \u201ccat\u201d query. However, cGAN-based synthesizers can be stolen via adversaries' queries, i.e., model thieves. The prevailing adversarial assumption is that thieves act independently: they query the deployed cGAN (i.e., the victim), and train a stolen cGAN using the images obtained from the victim. A popular anti-theft defense consists in throttling down the number of queries from any given user. We consider a more realistic adversarial scenario: model thieves collude to query the victim, and then train the stolen cGAN. Clues is a new collusive model stealing framework, enabling thieves to bypass throttle-based defenses and steal cGANs more efficiently than through individual efforts. Thieves collect queried images and train a stolen cGAN in a federated manner. We evaluate Clues on three image datasets, e.g., MNIST, FashionMNIST and CelebA. We experimentally show the scalability of the proposed attack strategies against the number of thieves and the queried images, the impact of a classical noise-based defense, a passive watermarking defense and a JPEG-based countermeasure. Our evaluation shows that such a collusive stealing strategy gets close to 4 units of Frechet Inception Distance from a victim model. Our code is readily available to the research community: https://zenodo.org/records/10224340.",
      "year": 2024,
      "venue": "IEEE International Symposium on Reliable Distributed Systems",
      "authors": [
        "Simon Queyrut",
        "V. Schiavoni",
        "Lydia Chen",
        "Pascal Felber",
        "Robert Birke"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e01dfe4abe3e952a9425a4fac38e673c008ae3e1",
      "pdf_url": "",
      "publication_date": "2024-09-30",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f98f66ade820ce31d5aa5099f60e856f52db700",
      "title": "AuthNet: Neural Network with Integrated Authentication Logic",
      "abstract": "Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuling Cai",
        "Fan Xiang",
        "Guozhu Meng",
        "Yinzhi Cao",
        "Kai Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f98f66ade820ce31d5aa5099f60e856f52db700",
      "pdf_url": "",
      "publication_date": "2024-05-24",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "title": "Invisible DNN Watermarking Against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) models are widely used in various fields, such as pattern recognition and natural language processing, and provide considerable commercial value to their owners. Embedding a digital watermark in the model allows the legitimate owner to detect unauthorized use of the model. However, the existing DNN watermarking methods are vulnerable to model extraction attacks since the watermark task and the original model task are independent. In this article, a novel collaborative DNN watermarking framework is proposed to defend against model extraction attacks by establishing cooperation between the watermark generation and embedding. Specifically, the trigger samples are not only imperceptible to ensure perceptual stealth security but also infused with target-label information to guide the following feature associations. In the process of watermark embedding, the feature representation of trigger samples is forced to be similar to that of the task distribution samples via feature coupling. Consequently, the trigger samples from our framework can be recognized in the stolen model as task distribution samples, so that the ownership of the model can be successfully verified. Extensive experiments on CIFAR10, CIFAR100, and ImageNet demonstrate the effectiveness and superior performance of the proposed watermarking framework against various model extraction attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Zuping Xi",
        "Zuomin Qu",
        "Wei Lu",
        "Xiangyang Luo",
        "Xiaochun Cao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ee29d72ded5dae1c3ae6d082a578ac4cc21b8634",
      "pdf_url": "",
      "publication_date": "2024-12-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "title": "Attack Data is Not Solely Paramount: A Universal Model Extraction Enhancement Method",
      "abstract": "Model extraction (ME) attacks, aiming to steal the functionality or parameters of the victim model, have become a widespread research topic. Most functional ME attack methodologies follow a uniform framework, which we summarize in three steps: initially choosing appropriate attack data, then querying the victim model with this data, and finally, training an incipient clone model based on the victim model\u2019s outputs. Despite much focus on data selection, the latter two steps have been somewhat neglected. Noticing this, we explore a method for the information of attack data labels to enhance the accuracy of the clone model. Specifically, we utilized the incipient clone model to identify similarities between the leaked private data and the attack data, subsequently appending the labels from the leaked data to those of the attack data. Then, we employed these modified attack data labels to fine-tune the incipient clone model, obtaining an enhanced clone model with higher accuracy. The enhancement was applied to three representative ME attack methodologies that primarily focus on the first step. Results show that the enhanced model reveals a higher accuracy than the three basic attacks. In summary, our approach suggests that future research should extend beyond data selection.",
      "year": 2024,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Chuang Liang",
        "Jie Huang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b0376b413ffaa728518fc0e6102ff31d0cb35e65",
      "pdf_url": "",
      "publication_date": "2024-12-17",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "title": "Evading VBA Malware Classification using Model Extraction Attacks and Stochastic Search Methods",
      "abstract": "Antivirus (AV) software that relies on learning-based methods is potentially vulnerable to adversarial attacks from threat actors. Threat actors can utilize model-extraction attacks against AV software to create a surrogate model. Malware samples can be tested against the surrogate model to determine how the target AV software will classify a given sample. Using a surrogate model speeds the process of malware development by allowing modifications to first be tested in feature space, which is significantly faster than performing modifications in code space. This work investigates performing evasion attacks against Windows Defender VBA malware classifier in an offline mode. The performance of five machine learning models is compared for their use as surrogate models. The models are reinforced by augmenting their training sets with samples that are generated by modifying existing samples. The results show that model performance is greatly improved with the augmented data and the best surrogate model achieved an accuracy of over 90% in predicting Defender\u2019s classifications. The best surrogate model is then used to test four search methods to find feature values to target when modifying malicious VBA samples to evade detection. The feature values found in feature space are used to guide modification of VBA samples in code space and then tested against Defender. Over 60% of the modified malicious-samples were able to evade detection after the targeted modifications based upon the results of the best search algorithm.",
      "year": 2024,
      "venue": "Computer Assisted Radiology and Surgery - International Congress and Exhibition",
      "authors": [
        "Brian Fehrman",
        "Francis Akowuah",
        "Randy Hoover"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c4b7d1a43c7c8b7ce8bb6f47edd5d61de13ea093",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "title": "On the Security Vulnerabilities of MRAM-Based in-Memory Computing Architectures Against Model Extraction Attacks",
      "abstract": "This paper studies the security vulnerabilities of embedded nonvolatile memory (eNVM)-based in-memory computing (IMC) architectures to model extraction attacks (MEAs). These attacks allow the reconstruction of private training data from trained model parameters thereby leaking sensitive user information. The presence of analog noise in eNVM-based IMC computation suggests that they may be intrinsically robust to MEA. However, we show that this conjecture is false. Specifically, we consider the scenario where an attacker aims to retrieve model parameters via input-output query access, and propose three attacks that exploit the statistics of the IMC computation. We demonstrate the efficacy of these attacks in extracting the model parameters of the last layer of a ResNet-20 network from the bitcell array of an MRAM-based IMC prototype in 22 nm process. Employing the proposed MEAs, the attacker obtains a CIFAR-10 accuracy within 0.1 % of that of a $N=64$ dimensional, $7 \\mathrm{b} \\times 4 \\mathrm{b}$ fixed-point digital baseline. To the best of our knowledge, this is the first work to demonstrate MEAs for eNVM-based IMC on a real-life IC prototype. Our results indicate the critical importance of investigating the security vulnerabilities of IMCs in general, and eNVM-based IMCs, in particular.",
      "year": 2024,
      "venue": "International Conference on Computer Aided Design",
      "authors": [
        "Saion K. Roy",
        "Naresh R. Shanbhag"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b6b0c38b86d8b8c430f901aed448eb512fb757e0",
      "pdf_url": "",
      "publication_date": "2024-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f5a16eb238e96accbe067c68886fc29d1202817f",
      "title": "Efficient Model Extraction via Boundary Sampling",
      "abstract": "This paper introduces a novel data-free model extraction attack that significantly advances the current state-of-the-art in terms of efficiency, accuracy, and effectiveness. Traditional black-box methods rely on using the victim's model as an oracle to label a vast number of samples within high-confidence areas. This approach not only requires an extensive number of queries but also results in a less accurate and less transferable model. In contrast, our method innovates by focusing on sampling low-confidence areas (along the decision boundaries) and employing an evolutionary algorithm to optimize the sampling process. These novel contributions allow for a dramatic reduction in the number of queries needed by the attacker by a factor of 10x to 600x while simultaneously improving the accuracy of the stolen model. Moreover, our approach improves boundary alignment, resulting in better transferability of adversarial examples from the stolen model to the victim's model (increasing the attack success rate from 60% to 82% on average). Finally, we accomplish all of this with a strict black-box assumption on the victim, with no knowledge of the target's architecture or dataset. We demonstrate our attack on three datasets with increasingly larger resolutions and compare our performance to four state-of-the-art model extraction attacks.",
      "year": 2024,
      "venue": "AISec@CCS",
      "authors": [
        "Maor Biton Dor",
        "Yisroel Mirsky"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f5a16eb238e96accbe067c68886fc29d1202817f",
      "pdf_url": "http://arxiv.org/pdf/2410.15429",
      "publication_date": "2024-10-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "title": "CaBaGe: Data-Free Model Extraction using ClAss BAlanced Generator Ensemble",
      "abstract": "Machine Learning as a Service (MLaaS) is often provided as a pay-per-query, black-box system to clients. Such a black-box approach not only hinders open replication, validation, and interpretation of model results, but also makes it harder for white-hat researchers to identify vulnerabilities in the MLaaS systems. Model extraction is a promising technique to address these challenges by reverse-engineering black-box models. Since training data is typically unavailable for MLaaS models, this paper focuses on the realistic version of it: data-free model extraction. We propose a data-free model extraction approach, CaBaGe, to achieve higher model extraction accuracy with a small number of queries. Our innovations include (1) a novel experience replay for focusing on difficult training samples; (2) an ensemble of generators for steadily producing diverse synthetic data; and (3) a selective filtering process for querying the victim model with harder, more balanced samples. In addition, we create a more realistic setting, for the first time, where the attacker has no knowledge of the number of classes in the victim training data, and create a solution to learn the number of classes on the fly. Our evaluation shows that CaBaGe outperforms existing techniques on seven datasets -- MNIST, FMNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet-subset, and Tiny ImageNet -- with an accuracy improvement of the extracted models by up to 43.13%. Furthermore, the number of queries required to extract a clone model matching the final accuracy of prior work is reduced by up to 75.7%.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jonathan Rosenthal",
        "Shanchao Liang",
        "Kevin Zhang",
        "Lin Tan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/30b87afb2801b52c8ee6ed381ed6e7a56ee68b70",
      "pdf_url": "",
      "publication_date": "2024-09-16",
      "keywords_matched": [
        "model extraction",
        "clone model",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "title": "Enhancing Data-Free Robustness Stealing Attack via Boundary Data Generation",
      "abstract": "With the continuous development of Machine Learning as a Service (MLaaS), model stealing has become an emerging problem in machine learning security in recent years. In model stealing, one typically obtains the soft labels of model queries and a proxy dataset as prior knowledge, but this scenario is highly idealised. How to steal models without data and hard labels is a pressing problem that needs to be solved. The current mainstream of model stealing attack methods mainly focus on stealing the accuracy of the model and overlook the robustness of the model. However, robustness is essential in security applications such as facial recognition and secure payment scenarios. Moreover, building robust models usually requires costly adversarial training and fine-tuning, making these models the primary targets for theft. To address these issues, in this paper, we propose a new data-free robustness stealing method under data-free conditions from the perspective of data generation, thereby better shaping the classification boundary data to optimise the accuracy and robustness of the models. Through testing, our method achieved clean accuracy and robust accuracy of 53.69% and 21.0%, respectively, under the more complex CIFAR-100 dataset classification. These results are only 3.06% and 3.94% different from the target model, respectively, showing a significant improvement over recent research.",
      "year": 2024,
      "venue": "2024 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics",
      "authors": [
        "Xiaoji Ma",
        "Weihao Guo",
        "Pingyuan Ge",
        "Ying Chen",
        "Qiuling Yue",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/18c9536bbc914ab9d33fd4687c4518bda0e82a0f",
      "pdf_url": "",
      "publication_date": "2024-08-19",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "title": "FP-OCS: A Fingerprint Based Ownership Detection System for Insulator Fault Detection Model",
      "abstract": "In smart grids, the robustness and reliability of the transmission system depend on the operational integrity of the insulators. The success of deep learning has facilitated the development of advanced fault detection algorithms for classifying and identifying insulator states. However, these machine learning based detection systems rely on high-quality datasets, making them potential targets for intellectual property theft, and model extraction attacks pose risks of privacy breaches and unauthorized exploitation. To address the challenge of protecting neural network ownership in this situation, we introduce a fingerprint based ownership detection system for insulator fault detection model FP-OCS. FP-OCS uses model extraction attack to generate a series of piracy models, and uses white-box access victim models to generate similarity models and universal adversarial perturbation. The system\u2019s fingerprint generation module augments the original dataset to craft distinctive model fingerprints. Subsequently, FP-OCS\u2019s encoder training module extends the fingerprint dataset using K-means methods and uses contrast learning to train the encoder network and the projection network. Upon finalization of training, FP-OCS evaluates a model\u2019s authenticity by matching its derived fingerprint against the victim model. We evaluated the effectiveness of the system using data-enhanced InsPLAD datasets. Our findings prove that FP-OCS can achieve 100% accuracy in Ownership Detection tasks with 50% similarity dividing line.",
      "year": 2024,
      "venue": "International Conference on Innovative Computing and Cloud Computing",
      "authors": [
        "Wenqian Xu",
        "Fazhong Liu",
        "Ximing Zhang",
        "Yixin Jiang",
        "Tian Dong",
        "Zhihong Liang",
        "Yiwei Yang",
        "Yan Meng",
        "Haojin Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/74fc89c1bc63d0a9e6b51fb6531528f29a0c2fd9",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
      "abstract": "Pre-trained encoders available online have been widely adopted to build downstream machine learning (ML) services, but various attacks against these encoders also post security and privacy threats toward such a downstream ML service paradigm. We unveil a new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which can extract sensitive encoder information from a targeted downstream ML service that can then be used to promote other ML attacks against the targeted service. By only providing API accesses to a targeted downstream service and a set of candidate encoders, the PEI attack can successfully infer which encoder is secretly used by the targeted service based on candidate ones. Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat. We empirically verify the effectiveness of the PEI attack on vision encoders. we first conduct PEI attacks against two downstream services (i.e., image classification and multimodal generation), and then show how PEI attacks can facilitate other ML attacks (i.e., model stealing attacks vs. image classification models and adversarial attacks vs. multimodal generative models). Our results call for new security and privacy considerations when deploying encoders in downstream services. The code is available at https://github.com/fshp971/encoder-inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shaopeng Fu",
        "Xuexue Sun",
        "Ke Qing",
        "Tianhang Zheng",
        "Di Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/69ccce44213a79c679ccde2615c5dfb1fa4f6406",
      "pdf_url": "",
      "publication_date": "2024-08-05",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "title": "Late Breaking Results: Extracting QNNs from NISQ Computers via Ensemble Learning",
      "abstract": "The recent success of Quantum Neural Networks (QNNs) prompts model extraction attacks on cloud platforms, even under black-box constraints. These attacks repeatedly query the victim QNN with malicious inputs. However, existing extraction attacks tailored for classical models yield local substitute QNNs with limited performance due to NISQ computer noise. Drawing from bagging-based ensemble learning, which uses independent weak learners to learn from noisy data, we introduce a novel QNN extraction approach. Our experimental results show this quantum ensemble learning approach improves local QNN accuracy by up to 15.09% compared to previous techniques.",
      "year": 2024,
      "venue": "Design Automation Conference",
      "authors": [
        "Zhenxiao Fu",
        "Fan Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a2d3025cee2b0e66d6e1516fc9955fd279f48eb5",
      "pdf_url": "",
      "publication_date": "2024-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "title": "Watermarking Counterfactual Explanations",
      "abstract": "Counterfactual (CF) explanations for ML model predictions provide actionable recourse recommendations to individuals adversely impacted by predicted outcomes. However, despite being preferred by end-users, CF explanations have been shown to pose significant security risks in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on the underlying proprietary ML model. To address this security challenge, we propose CFMark, a novel model-agnostic watermarking framework for detecting unauthorized model extraction attacks relying on CF explanations. CFMark involves a novel bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks using these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme. At the same time, the embedded watermark does not compromise the quality of the CF explanations. We evaluate CFMark across diverse real-world datasets, CF explanation methods, and model extraction techniques. Our empirical results demonstrate CFMark's effectiveness, achieving an F-1 score of ~0.89 in identifying unauthorized model extraction attacks using watermarked CF explanations. Importantly, this watermarking incurs only a negligible degradation in the quality of generated CF explanations (i.e., ~1.3% degradation in validity and ~1.6% in proximity). Our work establishes a critical foundation for the secure deployment of CF explanations in real-world applications.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hangzhi Guo",
        "Amulya Yadav"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/208f2c852d48f8c04e2d4cacc8c803691ac0d983",
      "pdf_url": "",
      "publication_date": "2024-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "title": "Model extraction via active learning by fusing prior and posterior knowledge from unlabeled data",
      "abstract": "As machine learning models become increasingly integrated into practical applications and are made accessible via public APIs, the risk of model extraction attacks has gained prominence. This study presents an innovative and efficient approach to model extraction attacks, aimed at reducing query costs and enhancing attack effectiveness. The method begins by leveraging a pre-trained model to identify high-confidence samples from unlabeled datasets. It then employs unsupervised contrastive learning to thoroughly dissect the structural nuances of these samples, constructing a dataset of high quality that precisely mirrors a variety of features. A mixed information confidence strategy is employed to refine the query set, effectively probing the decision boundaries of the target model. By integrating consistency regularization and pseudo-labeling techniques, reliance on authentic labels is minimized, thus improving the feature extraction capabilities and predictive precision of the surrogate models. Evaluation on four major datasets reveals that the models crafted through this method bear a close functional resemblance to the original models, with a real-world API test success rate of 62.35%, which vouches for the method\u2019s validity.",
      "year": 2024,
      "venue": "Journal of Intelligent &amp; Fuzzy Systems",
      "authors": [
        "Lijun Gao",
        "Kai Liu",
        "Wenjun Liu",
        "Jiehong Wu",
        "Xiao Jin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/696ddfebeea945fd0a0144e06135a6b151dc47cb",
      "pdf_url": "",
      "publication_date": "2024-03-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "48861a4166786313ff97a3c946c08574716aabce",
      "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
      "abstract": "With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs, employing low-frequency words as triggers and embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks, robustness against model extraction, pruning, fusion-based backdoor removal attacks, and high efficiency with 10-second embedding. Our code is available at https://github.com/edu-yinzhaoxia/Med-PLMW.",
      "year": 2024,
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "authors": [
        "Cong Kong",
        "Rui Xu",
        "Jiawei Chen",
        "Zhaoxia Yin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/48861a4166786313ff97a3c946c08574716aabce",
      "pdf_url": "",
      "publication_date": "2024-09-14",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eb9378e6c7613404e347327caf130bdcdff1d66f",
      "title": "Model Extraction Attacks on Text-to-Image Generative Adversarial Networks",
      "abstract": "Model extraction attack refers to attackers ille-gally obtaining the functionality of a victim model by querying it. Currently, attacks primarily focus on discriminative models in computer vision. However, model extraction attacks on generative models, especially tasks like generating images from text, remain underexplored. The task of generating corresponding images for text is not only captivating but also highly challenging. In this study, we are the first to comprehensively investigate the feasibility of executing model extraction attacks on Text-to-Image Generative Adversarial Networks (T2I-GANs). To provide a more nuanced understanding, we introduce the concepts of fidelity and accuracy in model extraction attacks targeting T2I-GANs. Extensive experimental validation in black-box attack scenarios demonstrates that we achieve high-fidelity and high-accuracy extraction of T2I-GAN models. We employ the CLIP model to filter queried data, resulting in a fidelity of 81 % for the substitute model. Furthermore, through subsampling techniques, we effectively filter high-quality samples that closely resemble the distribution of real datasets, thereby increasing accuracy to 87 %.",
      "year": 2024,
      "venue": "2024 IEEE Cyber Science and Technology Congress (CyberSciTech)",
      "authors": [
        "Ying Chen",
        "Weihao Guo",
        "Pingyuan Ge",
        "Xiaoji Ma",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eb9378e6c7613404e347327caf130bdcdff1d66f",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c25d2a27f1abe169d7b68078071b6698f0980469",
      "title": "Protecting Language Generation Models via Invisible Watermarking",
      "abstract": "Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as\"synonym randomization\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Xuandong Zhao",
        "Yu-Xiang Wang",
        "Lei Li"
      ],
      "citation_count": 107,
      "url": "https://www.semanticscholar.org/paper/c25d2a27f1abe169d7b68078071b6698f0980469",
      "pdf_url": "https://arxiv.org/pdf/2302.03162",
      "publication_date": "2023-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
      "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \u2018METHOD\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.Our code is available at https://github.com/yjw1029/EmbMarker.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Zhu",
        "L. Lyu",
        "Binxing Jiao",
        "Tongye Xu",
        "Guangzhong Sun",
        "Xing Xie"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/d4c3e3e3c01afed15926adf81527bf46aa491c6a",
      "pdf_url": "http://arxiv.org/pdf/2305.10036",
      "publication_date": "2023-05-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "title": "APMSA: Adversarial Perturbation Against Model Stealing Attacks",
      "abstract": "Training a Deep Learning (DL) model requires proprietary data and computing-intensive resources. To recoup their training costs, a model provider can monetize DL models through Machine Learning as a Service (MLaaS). Generally, the model is deployed at the cloud, while providing a publicly accessible Application Programming Interface (API) for paid queries to obtain benefits. However, model stealing attacks have posed security threats to this model monetizing scheme as they steal the model without paying for future extensive queries. Specifically, an adversary queries a targeted model to obtain input-output pairs and thus infer the model\u2019s internal working mechanism by reverse-engineering a substitute model, which has deprived model owner\u2019s business advantage and leaked the privacy of the model. In this work, we observe that the confidence vector or the top-1 confidence returned from the model under attack (MUA) varies in a relative large degree given different queried inputs. Therefore, rich internal information of the MUA is leaked to the attacker that facilities her reconstruction of a substitute model. We thus propose to leverage adversarial confidence perturbation to hide such varied confidence distribution given different queries, consequentially against model stealing attacks (dubbed as APMSA). In other words, the confidence vectors returned now is similar for queries from a specific category, considerably reducing information leakage of the MUA. To achieve this objective, through automated optimization, we constructively add delicate noise into per input query to make its confidence close to the decision boundary of the MUA. Generally, this process is achieved in a similar means of crafting adversarial examples but with a distinction that the hard label is preserved to be the same as the queried input. This retains the inference utility (i.e., without sacrificing the inference accuracy) for normal users but bounded the leaked confidence information to the attacker in a small constrained area (i.e., close to decision boundary). The later renders greatly deteriorated accuracy of the attacker\u2019s substitute model. As the APMSA serves as a plug-in front-end and requires no change to the MUA, it is thus generic and easy to deploy. The high efficacy of APMSA is validated through experiments on datasets of CIFAR10 and GTSRB. Given a MUA model of ResNet-18 on the CIFAR10, our defense can degrade the accuracy of the stolen model by up to 15% (rendering the stolen model useless to a large extent) with 0% accuracy drop for normal user\u2019s hard-label inference request.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Jiliang Zhang",
        "Shuang Peng",
        "Yansong Gao",
        "Zhi Zhang",
        "Q. Hong"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/796bab7acfef1076cd4e39872a34dd6c80a646fd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d032a269b465df9116f080ff9c56049bc581acb4",
      "title": "Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks",
      "abstract": "The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their \"synonyms\" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API. We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Shuai Wang",
        "Cuiyun Gao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d032a269b465df9116f080ff9c56049bc581acb4",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "title": "No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML",
      "abstract": "On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) against model weights and membership inference attack (MIA) against training data privacy. Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming of TEEs is the sharply increased latency (up to 50\u00d7). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading1 the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. However, the community lacks an in-depth understanding of the seemingly encouraging privacy guarantees offered by existing TSDP solutions during DNN inference. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding the optimal DNN partition configurations, which vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. Unlike existing approaches, TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the \"upper-bound\" security guarantees) with over 10\u00d7less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss. We make the code and artifacts publicly available on the Internet.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ziqi Zhang",
        "Chen Gong",
        "Yifeng Cai",
        "Yuanyuan Yuan",
        "Bingyan Liu",
        "Ding Li",
        "Yao Guo",
        "Xiangqun Chen"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e169ce8cc1627ff18f8fc4361f622bb31d33326b",
      "pdf_url": "https://arxiv.org/pdf/2310.07152",
      "publication_date": "2023-10-11",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68b560859078171978f2c040b1522f4e7668c38e",
      "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study",
      "abstract": "Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models. In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as \u201ccode synthesis\u201d and \u201ccode translation:\u2019 We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Zongjie Li",
        "Chaozheng Wang",
        "Pingchuan Ma",
        "Chaowei Liu",
        "Shuai Wang",
        "Daoyuan Wu",
        "Cuiyun Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/68b560859078171978f2c040b1522f4e7668c38e",
      "pdf_url": "https://arxiv.org/pdf/2303.03012",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "81cd9575100643a3463465ec19e90ee78e122f93",
      "title": "SoK: Model Inversion Attack Landscape: Taxonomy, Challenges, and Future Roadmap",
      "abstract": "A crucial module of the widely applied machine learning (ML) model is the model training phase, which involves large-scale training data, often including sensitive private data. ML models trained on these sensitive data suffer from significant privacy concerns since ML models can intentionally or unintendedly leak information about training data. Adversaries can exploit this information to perform privacy attacks, including model extraction, membership inference, and model inversion. While a model extraction attack steals and replicates a trained model functionality, and membership inference infers the data sample's inclusiveness to the training set, a model inversion attack has the goal of inferring the training data sample's sensitive attribute value or reconstructing the training sample (i.e., image/audio/text). Distinct and inconsistent characteristics of model inversion attack make this attack even more challenging and consequential, opening up model inversion attack as a more prominent and increasingly expanding research paradigm. Thereby, to flourish research in this relatively underexplored model inversion domain, we conduct the first-ever systematic literature review of the model inversion attack landscape. We characterize model inversion attacks and provide a comprehensive taxonomy based on different dimensions. We illustrate foundational perspectives emphasizing methodologies and key principles of the existing attacks and defense techniques. Finally, we discuss challenges and open issues in the existing model inversion attacks, focusing on the roadmap for future research directions.",
      "year": 2023,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "S. Dibbo"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/81cd9575100643a3463465ec19e90ee78e122f93",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "40b40e942db469663609ad6c1911cca235079434",
      "title": "D-DAE: Defense-Penetrating Model Extraction Attacks",
      "abstract": "Recent studies show that machine learning models are vulnerable to model extraction attacks, where the adversary builds a substitute model that achieves almost the same performance of a black-box victim model simply via querying the victim model. To defend against such attacks, a series of methods have been proposed to disrupt the query results before returning them to potential attackers, greatly degrading the performance of existing model extraction attacks.In this paper, we make the first attempt to develop a defense-penetrating model extraction attack framework, named D-DAE, which aims to break disruption-based defenses. The linchpins of D-DAE are the design of two modules, i.e., disruption detection and disruption recovery, which can be integrated with generic model extraction attacks. More specifically, after obtaining query results from the victim model, the disruption detection module infers the defense mechanism adopted by the defender. We design a meta-learning-based disruption detection algorithm for learning the fundamental differences between the distributions of disrupted and undisrupted query results. The algorithm features a good generalization property even if we have no access to the original training dataset of the victim model. Given the detected defense mechanism, the disruption recovery module tries to restore a clean query result from the disrupted query result with well-designed generative models. Our extensive evaluations on MNIST, FashionMNIST, CIFAR-10, GTSRB, and ImageNette datasets demonstrate that D-DAE can enhance the substitute model accuracy of the existing model extraction attacks by as much as 82.24% in the face of 4 state-of-the-art defenses and combinations of multiple defenses. We also verify the effectiveness of D-DAE in penetrating unknown defenses in real-world APIs hosted by Microsoft Azure and Face++.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yanjiao Chen",
        "Rui Guan",
        "Xueluan Gong",
        "Jianshuo Dong",
        "Meng Xue"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/40b40e942db469663609ad6c1911cca235079434",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "title": "Stealing the Decoding Algorithms of Language Models",
      "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., 0.8, 1, 4, and 40 for the four versions of GPT-3.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "A. Naseh",
        "Kalpesh Krishna",
        "Mohit Iyyer",
        "Amir Houmansadr"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fdaeb41ebb60dbe60a3193f02320e3f00f8233fd",
      "pdf_url": "https://arxiv.org/pdf/2303.04729",
      "publication_date": "2023-03-08",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "title": "Dual Student Networks for Data-Free Model Stealing",
      "abstract": "Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "James Beetham",
        "Navid Kardan",
        "A. Mian",
        "M. Shah"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
      "pdf_url": "https://arxiv.org/pdf/2309.10058",
      "publication_date": "2023-09-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1b0e8f3727f452d8ef13950ae61c631be8956306",
      "title": "Model Extraction Attacks Revisited",
      "abstract": "Model extraction (ME) attacks represent one major threat to Machine-Learning-as-a-Service (MLaaS) platforms by \"stealing\" the functionality of confidential machine-learning models through querying black-box APIs. Over seven years have passed since ME attacks were first conceptualized in the seminal work [75]. During this period, substantial advances have been made in both ME attacks and MLaaS platforms, raising the intriguing question: How has the vulnerability of MLaaS platforms to ME attacks been evolving? In this work, we conduct an in-depth study to answer this critical question. Specifically, we characterize the vulnerability of current, mainstream MLaaS platforms to ME attacks from multiple perspectives including attack strategies, learning techniques, surrogatemodel design, and benchmark tasks. Many of our findings challenge previously reported results, suggesting emerging patterns of ME vulnerability. Further, by analyzing the vulnerability of the same MLaaS platforms using historical datasets from the past four years, we retrospectively characterize the evolution of ME vulnerability over time, leading to a set of interesting findings. Finally, we make suggestions about improving the current practice of MLaaS in terms of attack robustness. Our study sheds light on the current state of ME vulnerability in the wild and points to several promising directions for future research.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Jiacheng Liang",
        "Ren Pang",
        "Changjiang Li",
        "Ting Wang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/1b0e8f3727f452d8ef13950ae61c631be8956306",
      "pdf_url": "https://arxiv.org/pdf/2312.05386",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "title": "DeepTheft: Stealing DNN Model Architectures through Power Side Channel",
      "abstract": "Deep Neural Network (DNN) models are often deployed in resource-sharing clouds as Machine Learning as a Service (MLaaS) to provide inference services. To steal model architectures that are of valuable intellectual properties, a class of attacks has been proposed via different side-channel leakage, posing a serious security challenge to MLaaS.Also targeting MLaaS, we propose a new end-to-end attack, DeepTheft, to accurately recover complex DNN model architectures on general processors via the RAPL (Running Average Power Limit)-based power side channel. While unprivileged access to the RAPL has been disabled in bare-metal OSes, we observe that the RAPL is still legitimately accessible in a platform as a service, e.g., the latest docker environment of version 20.10.18 used in this work. However, an attacker can acquire only a low sampling rate (1 KHz) of the time-series energy traces from the RAPL interface, rendering existing techniques ineffective in stealing large and deep DNN models. To this end, we design a novel and generic learning-based framework consisting of a set of meta-models, based on which DeepTheft is demonstrated to have high accuracy in recovering a large number (thousands) of models architectures from different model families including the deepest ResNet152. Particularly, DeepTheft has achieved a Levenshtein Distance Accuracy of 99.75% in recovering network structures, and a weighted average F1 score of 99.60% in recovering diverse layer-wise hyperparameters. Besides, our proposed learning framework is general to other time-series side-channel signals. To validate its generalization, another existing side channel is exploited, i.e., CPU frequency. Different from RAPL, CPU frequency is accessible to unprivileged users in bare-metal OSes. By using our generic learning framework trained against CPU frequency traces, DeepTheft has shown similarly high attack performance in stealing model architectures.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yansong Gao",
        "Huming Qiu",
        "Zhi Zhang",
        "Binghui Wang",
        "Hua Ma",
        "A. Abuadbba",
        "Minhui Xue",
        "Anmin Fu",
        "Surya Nepal"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/afbc7f9d4a4dcf0cf02ab3efd043904b361fa886",
      "pdf_url": "https://arxiv.org/pdf/2309.11894",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "steal model",
        "stealing model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b4149005980a11919731e6b4c1833d8b0af59424",
      "title": "Deep Neural Network Watermarking against Model Extraction Attack",
      "abstract": "Deep neural network (DNN) watermarking is an emerging technique to protect the intellectual property of deep learning models. At present, many DNN watermarking algorithms have been proposed to achieve provenance verification by embedding identify information into the internals or prediction behaviors of the host model. However, most methods are vulnerable to model extraction attacks, where attackers collect output labels from the model to train a surrogate or a replica. To address this issue, we present a novel DNN watermarking approach, named SSW, which constructs an adaptive trigger set progressively by optimizing over a pair of symmetric shadow models to enhance the robustness to model extraction. Precisely, we train a positive shadow model supervised by the prediction of the host model to mimic the behaviors of potential surrogate models. Additionally, a negative shadow model is normally trained to imitate irrelevant independent models. Using this pair of shadow models as a reference, we design a strategy to update the trigger samples appropriately such that they tend to persist in the host model and its stolen copies. Moreover, our method could well support two specific embedding schemes: embedding the watermark via fine-tuning or from scratch. Our extensive experimental results on popular datasets demonstrate that our SSW approach outperforms state-of-the-art methods against various model extraction attacks in whether trigger set classification accuracy based or hypothesis test based verification. The results also show that our method is robust to common model modification schemes including fine-tuning and model compression.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jingxuan Tan",
        "Nan Zhong",
        "Zhenxing Qian",
        "Xinpeng Zhang",
        "Sheng Li"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/b4149005980a11919731e6b4c1833d8b0af59424",
      "pdf_url": "",
      "publication_date": "2023-10-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "title": "QUDA: Query-Limited Data-Free Model Extraction",
      "abstract": "Model extraction attack typically refers to extracting non-public information from a black-box machine learning model. Its unauthorized nature poses significant threat to intellectual property rights of the model owners. By using the well-designed queries and the predictions returned from the victim model, the adversary is able to train a clone model from scratch to obtain similar functionality as victim model. Recently, some methods have been proposed to perform model extraction attacks without using any in-distribution data (Data-free setting). Although these methods have been shown to achieve high clone accuracy, their query budgets are typically around 10 million or even exceed 20 million in some datasets, which lead to a high cost of model stealing and can be easily defended by limiting the number of queries. To illustrate the severe threats induced by model extraction attacks with limited query budget in realistic scenarios, we propose QUDA \u2013 a novel QUey-limited DAta-free model extraction attack that incorporates GAN pre-trained by public unrelated dataset to provide weak image prior and the technique of deep reinforcement learning to make query generation strategy more efficient. Compared with the state-of-the-art data-free model extraction method, QUDA achieves better results under query-limited condition (0.1M query budget) in FMNIST and CIFAR-10 datasets, and even outperforms the baseline method in most cases when QUDA uses only 10% query budget of its. QUDA issued a warning that solely relying on the limited numbers of queries or the confidentiality of training data is not reliable to protect model\u2019s security and privacy. Potential countermeasures, such as detection-based defense approach, are also provided.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Zijun Lin",
        "Ke Xu",
        "Chengfang Fang",
        "Huadi Zheng",
        "Aneez Ahmed Jaheezuddin",
        "Jie Shi"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/06718e68bea215f2155bca2e08b70ad5d2aff62f",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "title": "Membership Inference Attacks Against Sequential Recommender Systems",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user\u2019s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user\u2019s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model\u2019s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model\u2019s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users\u2019 membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/fefdabd7bd1c0007c0ef7db5faa6486f28166c32",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
      "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query.\nWe create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Jonathan Rosenthal",
        "Eric Enouen",
        "H. Pham",
        "Lin Tan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/406d4e8d2df6f6b58e65016fea31004f781d93e7",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26150/25922",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "model extraction",
        "stealing model",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d2efd249eaffbf3675957444271ca97330156a1",
      "title": "Protecting Regression Models With Personalized Local Differential Privacy",
      "abstract": "The equation-solving model extraction attack is an intuitively simple but devastating attack to steal confidential information of regression models through a sufficient number of queries. Complete mitigation is difficult. Thus, the development of countermeasures is focused on degrading the attack effectiveness as much as possible without losing the model utilities. We investigate a novel personalized local differential privacy mechanism to defend against the attack. We obfuscate the model by adding high-dimensional Gaussian noise on model coefficients. Our solution can adaptively produce the noise to protect the model on the fly. We thoroughly evaluate the performance of our mechanisms using real-world datasets. The experiment shows that the proposed scheme outperforms the existing differential-privacy-enabled solution, i.e., 4 times more queries are required to achieve the same attack result. We also plan to publish the relevant codes to the community for further research.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xiaoguang Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/3d2efd249eaffbf3675957444271ca97330156a1",
      "pdf_url": "",
      "publication_date": "2023-03-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "title": "A Threshold Implementation-Based Neural Network Accelerator With Power and Electromagnetic Side-Channel Countermeasures",
      "abstract": "With the recent advancements in machine learning (ML) theory, a lot of energy-efficient neural network (NN) accelerators have been developed. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. This work introduces a threshold implementation (TI) masking-based NN accelerator that secures model parameters and inputs against power and electromagnetic (EM) side-channel attacks. The 0.159 mm2 demonstration in 28 nm runs at 125 MHz at 0.95 V and limits the area and energy overhead to 64% and $5.5\\times $ , respectively, while demonstrating security even greater than 2M traces. The accelerator also secures model parameters through encryption and the inputs against horizontal power analysis (HPA) attacks.",
      "year": 2023,
      "venue": "IEEE Journal of Solid-State Circuits",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4b99fb1fc7f60a16c4cc99a07d931fd79cf993e0",
      "pdf_url": "",
      "publication_date": "2023-01-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "title": "Peek into the Black-Box: Interpretable Neural Network using SAT Equations in Side-Channel Analysis",
      "abstract": "Deep neural networks (DNN) have become a significant threat to the security of cryptographic implementations with regards to side-channel analysis (SCA), as they automatically combine the leakages without any preprocessing needed, leading to a more efficient attack. However, these DNNs for SCA remain mostly black-box algorithms that are very difficult to interpret. Benamira et al. recently proposed an interpretable neural network called Truth Table Deep Convolutional Neural Network (TT-DCNN), which is both expressive and easier to interpret. In particular, a TT-DCNN has a transparent inner structure that can entirely be transformed into SAT equations after training. In this work, we analyze the SAT equations extracted from a TT-DCNN when applied in SCA context, eventually obtaining the rules and decisions that the neural networks learned when retrieving the secret key from the cryptographic primitive (i.e., exact formula). As a result, we can pinpoint the critical rules that the neural network uses to locate the exact Points of Interest (PoIs). We validate our approach first on simulated traces for higher-order masking. However, applying TT-DCNN on real traces is not straightforward. We propose a method to adapt TT-DCNN for application on real SCA traces containing thousands of sample points. Experimental validation is performed on software-based ASCADv1 and hardware-based AES_HD_ext datasets. In addition, TT-DCNN is shown to be able to learn the exact countermeasure in a best-case setting.",
      "year": 2023,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Trevor Yap",
        "Adrien Benamira",
        "S. Bhasin",
        "Thomas Peyrin"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/6915e87a10df6a0e068c043d29048bd4eed9cdc3",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/10276/9724",
      "publication_date": "2023-03-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
      "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wen",
        "Tianhao Wang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
      "pdf_url": "",
      "publication_date": "2023-10-17",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "title": "False Claims against Model Ownership Resolution",
      "abstract": "Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model. In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our core idea is that a malicious accuser can deviate (without detection) from the specified MOR process by finding (transferable) adversarial examples that successfully serve as evidence against independent suspect models. To this end, we first generalize the procedures of common MOR schemes and show that, under this generalization, defending against false claims is as challenging as preventing (transferable) adversarial examples. Via systematic empirical evaluation, we show that our false claim attacks always succeed in the MOR schemes that follow our generalization, including in a real-world model: Amazon's Rekognition API.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jian Liu",
        "Rui Zhang",
        "Sebastian Szyller",
        "Kui Ren",
        "Nirmal Asokan"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d4d199d28451b3ee9edb1eef9412d20ffee9329d",
      "pdf_url": "http://arxiv.org/pdf/2304.06607",
      "publication_date": "2023-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "title": "MirrorNet: A TEE-Friendly Framework for Secure On-Device DNN Inference",
      "abstract": "Deep neural network (DNN) models have become prevalent in edge devices for real-time inference. However, they are vulnerable to model extraction attacks and require protection. Existing defense approaches either fail to fully safeguard model confidentiality or result in significant latency issues. To overcome these challenges, this paper presents MirrorNet, which leverages Trusted Execution Environment (TEE) to enable secure on-device DNN inference. It generates a TEE-friendly implementation for any given DNN model to protect the model confidentiality, while meeting the stringent computation and storage constraints of TEE. The framework consists of two key components: the backbone model (BackboneNet), which is stored in the normal world but achieves lower inference accuracy, and the Companion Partial Monitor (CPM), a lightweight mirrored branch stored in the secure world, preserving model confidentiality. During inference, the CPM monitors the intermediate results from the BackboneNet and rectifies the classification output to achieve higher accuracy. To enhance flexibility, MirrorNet incorporates two modules: the CPM Strategy Generator, which generates various protection strategies, and the Performance Emulator, which estimates the performance of each strategy and selects the most optimal one. Extensive experiments demonstrate the effectiveness of MirrorNet in providing security guarantees while maintaining low computation latency, making MirrorNet a practical and promising solution for secure on-device DNN inference. For the evaluation, MirrorNet can achieve a 18.6% accuracy gap between authenticated and illegal use, while only introducing 0.99% hardware overhead.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Ziyu Liu",
        "Yukui Luo",
        "Shijin Duan",
        "Tong Zhou",
        "Xiaolin Xu"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/f84fb561bf1b253e0997d864c3c2ff374190c86d",
      "pdf_url": "https://arxiv.org/pdf/2311.09489",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fa9d49f32440aff7417ce46419d1073239b58b5b",
      "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
      "abstract": "Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.",
      "year": 2023,
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "authors": [
        "Qing Huang",
        "Yanbang Sun",
        "Zhenchang Xing",
        "Mingming Yu",
        "Xiwei Xu",
        "Qinghua Lu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fa9d49f32440aff7417ce46419d1073239b58b5b",
      "pdf_url": "http://arxiv.org/pdf/2301.03987",
      "publication_date": "2023-01-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "title": "SCMA: A Scattering Center Model Attack on CNN-SAR Target Recognition",
      "abstract": "Convolutional neural networks (CNNs) have been widely used in synthetic aperture radar (SAR) target recognition, which can extract feature automatically. However, due to its own structural flaws, CNNs are easy to be fooled by adversarial examples, even if they have excellent performance. In this letter, a novel attack named scattering center model attack (SCMA) is designed, and its generation process does not rely on the prior knowledge of any neural network. Therefore, we can get a stable way which can be applied to any neural network. In addition, an improved scattering center model extraction method, which is the pre-part of SCMA, can filter out the useless noise to optimize the stability of attack. In the experiment, SCMA is compared with advanced attack algorithms. From the experimental results, it is clear to find that SCMA has excellent performance in terms of transfer attack success rate. Furthermore, visualization and interpretability analysis underpin the theoretical feasibility of SCMA.",
      "year": 2023,
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "authors": [
        "Weibo Qin",
        "Bo Long",
        "Feng Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/45310a683fa761bbaa03ea9969fcf5bc7021624d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "title": "Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times",
      "abstract": "Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.",
      "year": 2023,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Haitian Zhang",
        "Guang Hua",
        "Xinya Wang",
        "Hao Jiang",
        "Wen Yang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bfb591f3ac857aa68fc63b2428c596790a96d2c3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "title": "GrOVe: Ownership Verification of Graph Neural Networks using Embeddings",
      "abstract": "Graph neural networks (GNNs) have emerged as a state-of-the-art approach to model and draw inferences from large scale graph-structured data in various application settings such as social networking. The primary goal of a GNN is to learn an embedding for each graph node in a dataset that encodes both the node features and the local graph structure around the node.Prior work has shown that GNNs are prone to model extraction attacks. Model extraction attacks and defenses have been explored extensively in other non-graph settings. While detecting or preventing model extraction appears to be difficult, deterring them via effective ownership verification techniques offer a potential defense. In non-graph settings, fingerprinting models, or the data used to build them, have shown to be a promising approach toward ownership verification.We present GrOVe, a state-of-the-art GNN model fingerprinting scheme that, given a target model and a suspect model, can reliably determine if the suspect model was trained independently of the target model or if it is a surrogate of the target model obtained via model extraction. We show that GrOVe can distinguish between surrogate and independent models even when the independent model uses the same training dataset and architecture as the original target model.Using six benchmark datasets and three model architectures, we show that GrOVe consistently achieves low falsepositive and false-negative rates. We demonstrate that GrOVe is robust against known fingerprint evasion techniques while remaining computationally efficient.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Asim Waheed",
        "Vasisht Duddu",
        "N. Asokan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/c5de53ba42dd0826dcc0844b8b61ef6a4ed944bd",
      "pdf_url": "https://arxiv.org/pdf/2304.08566",
      "publication_date": "2023-04-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
      "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4\u00d7 faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Jun Guo",
        "Xingyu Zheng",
        "Aishan Liu",
        "Siyuan Liang",
        "Yisong Xiao",
        "Yichao Wu",
        "Xianglong Liu"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
      "pdf_url": "http://arxiv.org/pdf/2308.00958",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "title": "B3: Backdoor Attacks against Black-box Machine Learning Models",
      "abstract": "Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the trigger. The reason for backdoor attacks is that resource-limited users usually download sophisticated models from model zoos or query the models from MLaaS rather than training a model from scratch, thus a malicious third party has a chance to provide a backdoored model. In general, the more precious the model provided (i.e., models trained on rare datasets), the more popular it is with users. In this article, from a malicious model provider perspective, we propose a black-box backdoor attack, named B3, where neither the rare victim model (including the model architecture, parameters, and hyperparameters) nor the training data is available to the adversary. To facilitate backdoor attacks in the black-box scenario, we design a cost-effective model extraction method that leverages a carefully constructed query dataset to steal the functionality of the victim model with a limited budget. As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted misclassification label through the neuron with the highest impact on the targeted label. Extensive experiments have been conducted on various simulated deep learning models and the commercial API of Alibaba Cloud Compute Service. We demonstrate that B3 has a high attack success rate and maintains high prediction accuracy for benign inputs. It is also shown that B3 is robust against state-of-the-art defense strategies against backdoor attacks, such as model pruning and NC.",
      "year": 2023,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Huayang Huang",
        "Qian Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "pdf_url": "",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "title": "Detection of Crucial Power Side Channel Data Leakage in Neural Networks",
      "abstract": "Neural network (NN) accelerators are now extensively utilized in a range of applications that need a high degree of security, such as driverless cars, NLP, and image recognition. Due to privacy issues and the high cost, hardware implementations contained within NN Propagators were often not accessible for general populace. Additionally with power and time data, accelerators also disclose critical data by electro-magnetic (EM) sided channels. Within this study, we demonstrate a side-channel information-based attack that can successfully steal models from large-scale NN accelerators deployed on real-world hardware. The use of these accelerators is widespread. The proposed method of attack consists of two distinct phases: 1) Using EM side-channel data to estimate networking's underlying architecture; 2) Using margin-dependent, attackers learning actively in estimating parameters, notably weights. Deducing the underlying network structure from EM sidechannel data. Inferring the underlying network structure from EM sidechannel data. Experimental findings demonstrate that the disclosed attack technique can be used to precisely retrieve the large-scale NN via the use of EM side-channel information leaking. Overall, our attack shows how critical it is to conceal electromagnetic (EM) traces for massive NN accelerators in practical settings.",
      "year": 2023,
      "venue": "International Telecommunication Networks and Applications Conference",
      "authors": [
        "A. A. Ahmed",
        "Mohammad Kamrul Hasan",
        "Nurhizam Safie Mohd Satar",
        "N. Nafi",
        "A. Aman",
        "S. Islam",
        "Saif Aamer Fadhil"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/28eaa9c2377116327199cb1eb2c9d7b93b948bb4",
      "pdf_url": "",
      "publication_date": "2023-11-29",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "title": "Practical and Efficient Model Extraction of Sentiment Analysis APIs",
      "abstract": "Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",
      "year": 2023,
      "venue": "International Conference on Software Engineering",
      "authors": [
        "Weibin Wu",
        "Jianping Zhang",
        "Victor Junqiu Wei",
        "Xixian Chen",
        "Zibin Zheng",
        "Irwin King",
        "M. Lyu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/14b4aff027ccf8fde0b19ac60b8e653c621aff30",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a3280f5d697c09946b371c8db82da514a4fa3d47",
      "title": "Efficient Nonprofiled Side-Channel Attack Using Multi-Output Classification Neural Network",
      "abstract": "Differential deep learning analysis (DDLA) is the first deep-learning-based nonprofiled side-channel attack (SCA) on embedded systems. However, DDLA requires many training processes to distinguish the correct key. In this letter, we introduce a nonprofiled SCA technique using multi-output classification to mitigate the aforementioned issue. Specifically, a multi-output multilayer perceptron and a multi-output convolutional neural network are introduced against various SCA protected schemes, such as masking, noise generation, and trace de-synchronization countermeasures. The experimental results on different power side channel datasets have clarified that our model performs the attack up to 9\u201330 times faster than DDLA in the case of masking and de-synchronization countermeasures, respectively. In addition, regarding combined masking and noise generation countermeasure, our proposed model achieves a higher success rate of at least 20% in the cases of the standard deviation equal to 1.0 and 1.5.",
      "year": 2023,
      "venue": "IEEE Embedded Systems Letters",
      "authors": [
        "Van-Phuc Hoang",
        "Ngoc-Tuan Do",
        "Van-Sang Doan"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/a3280f5d697c09946b371c8db82da514a4fa3d47",
      "pdf_url": "",
      "publication_date": "2023-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "title": "Hardware-Software Co-design for Side-Channel Protected Neural Network Inference",
      "abstract": "Physical side-channel attacks are a major threat to stealing confidential data from devices. There has been a recent surge in such attacks on edge machine learning (ML) hardware to extract the model parameters. Consequently, there has also been work, although limited, on building corresponding defenses against such attacks. Current solutions take either fully software-or fully hardware-centric approaches, which are limited in performance and flexibility, respectively. In this paper, we propose the first hardware-software co-design solution for building side-channel-protected ML hardware. Our solution targets edge devices and addresses both performance and flexibility needs. To that end, we develop a secure RISCV-based coprocessor design that can execute a neural network implemented in C/C++. Our coprocessor uses masking to execute various neural network operations like weighted summations, activation functions, and output layer computation in a sidechannel secure fashion. We extend the original RV32I instruction set with custom instructions to control the masking gadgets inside the secure coprocessor. We further use the custom instructions to implement easy-to-use APIs that are exposed to the end-user as a shared library. Finally, we demonstrate the empirical sidechannel security of the design up to 1M traces.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Avinash L. Varna",
        "Raghavan Kumar",
        "Aydin Aysu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5c39cbdd1f77e12188b8691efdfd1e635c2ca037",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
      "abstract": "Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yuanmin Tang",
        "Jing Yu",
        "Keke Gai",
        "Xiangyang Qu",
        "Yue Hu",
        "Gang Xiong",
        "Qi Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/51ffae813f187ebfc64226a2914c33f5a2f5e4dd",
      "pdf_url": "",
      "publication_date": "2023-11-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "title": "Holistic Implicit Factor Evaluation of Model Extraction Attacks",
      "abstract": "Model extraction attacks (MEAs) allow adversaries to replicate a surrogate model analogous to the target model's decision pattern. While several attacks and defenses have been studied in-depth, the underlying reasons behind our susceptibility to them often remain unclear. Analyzing these implication influence factors helps to promote secure deep learning (DL) systems, it requires studying extraction attacks in various scenarios to determine the success of different attacks and the hallmarks of DLs. However, understanding, implementing, and evaluating even a single attack requires extremely high technical effort, making it impractical to study the vast number of unique extraction attack scenarios. To this end, we present a first-of-its-kind holistic evaluation of implication factors for MEAs which relies on the attack process abstracted from state-of-the-art MEAs. Specifically, we concentrate on four perspectives. we consider the impact of the task accuracy, model architecture, and robustness of the target model on MEAs, as well as the impact of the model architecture of the surrogate model on MEAs. Our empirical evaluation includes an ablation study over sixteen model architectures and four image datasets. Surprisingly, our study shows that improving the robustness of the target model via adversarial training is more vulnerable to model extraction attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Anli Yan",
        "Hongyang Yan",
        "Li Hu",
        "Xiaozhang Liu",
        "Teng Huang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/4ae98576016b691dfda5a78d0a88d19e8ce15103",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "title": "MERCURY: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator",
      "abstract": "DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks. Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents MERCURY, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN accelerator. The key insight of MERCURY is to model the side-channel extraction process as a sequence-to-sequence problem. The adversary can leverage a time-to-digital converter (TDC) to remotely collect the power trace of the target model\u2019s inference. Then he uses a learning model to automatically recover the architecture details of the victim model from the power trace without any prior knowledge. The adversary can further use the attention mechanism to localize the leakage points that contribute most to the attack. Evaluation results indicate that MERCURY can keep the error rate of model extraction below 1%.",
      "year": 2023,
      "venue": "International Conference on Field-Programmable Technology",
      "authors": [
        "Xi-ai Yan",
        "Xiaoxuan Lou",
        "Guowen Xu",
        "Han Qiu",
        "Shangwei Guo",
        "Chip-Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c1a2f4520dfd66119a07fd3e0754e4a9aecbc78f",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/171839/2/_DR_NTU_An_Automated_Remote_Side_channel_Attack_to_FPGA_based_DNN_Accelerators.pdf",
      "publication_date": "2023-08-02",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "148adb6df70218017aba770047cacb3c9e745411",
      "title": "A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks",
      "abstract": "Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096 neurons in VGG-19, the overheads are between 2.8% and 11%.",
      "year": 2023,
      "venue": "International Conference on Cyber Security Cryptography and Machine Learning",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/148adb6df70218017aba770047cacb3c9e745411",
      "pdf_url": "http://arxiv.org/pdf/2303.18132",
      "publication_date": "2023-03-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a04f36017f7a8864118ce801029c21972c6fda8",
      "title": "DNN-Alias: Deep Neural Network Protection Against Side-Channel Attacks via Layer Balancing",
      "abstract": "Extracting the architecture of layers of a given deep neural network (DNN) through hardware-based side channels allows adversaries to steal its intellectual property and even launch powerful adversarial attacks on the target system. In this work, we propose DNN-Alias, an obfuscation method for DNNs that forces all the layers in a given network to have similar execution traces, preventing attack models from differentiating between the layers. Towards this, DNN-Alias performs various layer-obfuscation operations, e.g., layer branching, layer deepening, etc, to alter the run-time traces while maintaining the functionality. DNN-Alias deploys an evolutionary algorithm to find the best combination of obfuscation operations in terms of maximizing the security level while maintaining a user-provided latency overhead budget. We demonstrate the effectiveness of our DNN-Alias technique by obfuscating the architecture of 700 randomly generated and obfuscated DNNs running on multiple Nvidia RTX 2080 TI GPU-based machines. Our experiments show that state-of-the-art side-channel architecture stealing attacks cannot extract the original DNN accurately. Moreover, we obfuscate the architecture of various DNNs, such as the VGG-11, VGG-13, ResNet-20, and ResNet-32 networks. Training the DNNs using the standard CIFAR10 dataset, we show that our DNN-Alias maintains the functionality of the original DNNs by preserving the original inference accuracy. Further, the experiments highlight that adversarial attack on obfuscated DNNs is unsuccessful.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Mahya Morid Ahmadi",
        "Lilas Alrahis",
        "O. Sinanoglu",
        "Muhammad Shafique"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8a04f36017f7a8864118ce801029c21972c6fda8",
      "pdf_url": "http://arxiv.org/pdf/2303.06746",
      "publication_date": "2023-03-12",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "title": "Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders",
      "abstract": "Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jan Dubi'nski",
        "S. Pawlak",
        "Franziska Boenisch",
        "Tomasz Trzci'nski",
        "Adam Dziedzic"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/8ffd3bb7b0b26b33fd6f317052214e6e84dec291",
      "pdf_url": "https://arxiv.org/pdf/2310.08571",
      "publication_date": "2023-10-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b885e291f660df34d9f22777dc0678bdf8e0860d",
      "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data",
      "abstract": "We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\\sim 60-95\\%$ of true model's accuracy and uses $\\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\\sim 2-4\\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to $84-96\\%$ accuracy under membership inference attacks. Experimental results validate that Marich is query-efficient, and capable of performing task-accurate, high-fidelity, and informative model extraction.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Pratik Karmakar",
        "D. Basu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/b885e291f660df34d9f22777dc0678bdf8e0860d",
      "pdf_url": "http://arxiv.org/pdf/2302.08466",
      "publication_date": "2023-02-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3c103408ff825aad19d715edc01025a8c3fccdb4",
      "title": "EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles",
      "abstract": "Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to pruned models, and when applied across GPUs. We find that EZClone correctly predicts DNN architectures for the entire set of PyTorch vision architectures with 100% accuracy. No other work has shown this degree of architecture prediction accuracy with the same adversarial constraints or using aggregate side-channel information. Prior work has shown that, once a DNN has been successfully cloned, further attacks such as model evasion or model inversion can be accelerated significantly.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jonah O'Brien Weiss",
        "Tiago A. O. Alves",
        "S. Kundu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/3c103408ff825aad19d715edc01025a8c3fccdb4",
      "pdf_url": "http://arxiv.org/pdf/2304.03388",
      "publication_date": "2023-04-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "title": "Defense Against Model Extraction Attacks on Recommender Systems",
      "abstract": "The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.",
      "year": 2023,
      "venue": "Web Search and Data Mining",
      "authors": [
        "Sixiao Zhang",
        "Hongzhi Yin",
        "Hongxu Chen",
        "Cheng Long"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/13f08d0ed26a48bb4fa16951e2dbbd87d0ba4797",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635751",
      "publication_date": "2023-10-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "468a78d431be0d6290bb3007e6920e15761b751e",
      "title": "SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models",
      "abstract": "While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community. and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Zheng Li",
        "Ziqing Yang",
        "Xinlei He",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/468a78d431be0d6290bb3007e6920e15761b751e",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7b6db013d28e72374f301f758f432545a92b22fb",
      "title": "DivTheft: An Ensemble Model Stealing Attack by Divide-and-Conquer",
      "abstract": "Recently, model stealing attacks are widely studied but most of them are focused on stealing a single non-discrete model, e.g., neural networks. For ensemble models, these attacks are either non-executable or suffer from intolerant performance degradation due to the complex model structure (multiple sub-models) and the discreteness possessed by the sub-model (e.g., decision trees). To overcome the bottleneck, this paper proposes a divide-and-conquer strategy called DivTheft to formulate the model stealing attack to common ensemble models by combining active learning (AL). Specifically, based on the boosting learning concept, we divide a hard ensemble model stealing task into multiple simpler ones about single sub-model stealing. Then, we adopt AL to conquer the data-free sub-model stealing task. During the process, the current AL algorithm easily causes the stolen model to be biased because of ignoring the past useful memories. Thus, DivTheft involves a newly designed uncertainty sampling scheme to filter reusable samples from the previously used ones. Experiments show that compared with the prior work, DivTheft can save almost 50% queries while ensuring a competitive agreement rate to the victim model.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Zhuo Ma",
        "Xinjing Liu",
        "Yang Liu",
        "Ximeng Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7b6db013d28e72374f301f758f432545a92b22fb",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
      "abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Chengwei Dai",
        "Minxuan Lv",
        "Kun Li",
        "Wei Zhou"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "title": "Model Extraction Attacks on Split Federated Learning",
      "abstract": "Federated Learning (FL) is a popular collaborative learning scheme involving multiple clients and a server. FL focuses on protecting clients' data but turns out to be highly vulnerable to Intellectual Property (IP) threats. Since FL periodically collects and distributes the model parameters, a free-rider can download the latest model and thus steal model IP. Split Federated Learning (SFL), a recent variant of FL that supports training with resource-constrained clients, splits the model into two, giving one part of the model to clients (client-side model), and the remaining part to the server (server-side model). Thus SFL prevents model leakage by design. Moreover, by blocking prediction queries, it can be made resistant to advanced IP threats such as traditional Model Extraction (ME) attacks. While SFL is better than FL in terms of providing IP protection, it is still vulnerable. In this paper, we expose the vulnerability of SFL and show how malicious clients can launch ME attacks by querying the gradient information from the server side. We propose five variants of ME attack which differs in the gradient usage as well as in the data assumptions. We show that under practical cases, the proposed ME attacks work exceptionally well for SFL. For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90% accuracy with less than 2% accuracy degradation with VGG-11 on CIFAR-10.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jingtao Li",
        "A. S. Rakin",
        "Xing Chen",
        "Li Yang",
        "Zhezhi He",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/552d90f3ccc2879a17eb6e8f9c13f9937f6a6734",
      "pdf_url": "http://arxiv.org/pdf/2303.08581",
      "publication_date": "2023-03-13",
      "keywords_matched": [
        "model extraction",
        "steal model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "title": "AUTOLYCUS: Exploiting Explainable Artificial Intelligence (XAI) for Model Extraction Attacks against Interpretable Models",
      "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the decision-making processes of AI models. However, the data used for such explanations can pose security and privacy risks. Existing literature identifies attacks on machine learning models, including membership inference, model inversion, and model extraction attacks. These attacks target either the model or the training data, depending on the settings and parties involved. XAI tools can increase the vulnerability of model extraction attacks, which is a concern when model owners prefer black-box access, thereby keeping model parameters and architecture private. To exploit this risk, we propose AUTOLYCUS, a novel retraining (learning) based model extraction attack framework against interpretable models under black-box settings. As XAI tools, we exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley values (SHAP) to infer decision boundaries and create surrogate models that replicate the functionality of the target model. LIME and SHAP are mainly chosen for their realistic yet information-rich explanations, coupled with their extensive adoption, simplicity, and usability. We evaluate AUTOLYCUS on six machine learning datasets, measuring the accuracy and similarity of the surrogate model to the target model. The results show that AUTOLYCUS is highly effective, requiring significantly fewer queries compared to state-of-the-art attacks, while maintaining comparable accuracy and similarity. We validate its performance and transferability on multiple interpretable ML models, including decision trees, logistic regression, naive bayes, and k-nearest neighbor. Additionally, we show the resilience of AUTOLYCUS against proposed countermeasures.",
      "year": 2023,
      "venue": "Proceedings on Privacy Enhancing Technologies",
      "authors": [
        "Abdullah \u00c7aglar \u00d6ks\u00fcz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/18918a72fda197ea02671a13c49a95d6b95fc0f3",
      "pdf_url": "",
      "publication_date": "2023-02-04",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "title": "Deep-Learning Model Extraction Through Software-Based Power Side-Channel",
      "abstract": "Deep learning (DL) techniques have been increasingly applied across various applications, facing a growing number of security threats. One such threat is model extraction, an attack that steals the Intellectual Property of DL models, either by recovering the same functionality or retrieving high-fidelity models. Current model extraction methods can be categorized as learning-based or cryptanalytic, with the latter relying on model queries and computational methods to recover parameters. However, these are limited to shallow neural networks and are computationally prohibitive for deeper DL models. In this paper, we propose leveraging software-based power analysis, specifically the Intel Running Average Power Limit (RAPL) technique, for DL model extraction. RAPL allows us to measure power leakage of the most popular activation function, ReLU, through a software interface. Consequently, the ReLU branch direction can be leaked in the software power side-channel, a vulnerability common in many state-of-the-art DL frameworks. We introduce a novel methodology for model extraction Algorithm from input gradient assisted by side channel information. We implement our attack on the oneDNN framework, the most popular library on Intel processors. Compared to prior work, our model extraction, assisted by the software power side-channel, only requires 0.8% of the queries to retrieve as-layer MLP. We also successfully apply our method to a common Convolutional Neural Network (CNN) - Lenet-5. To the best of our knowledge, this is the first work that extracts CNN models with more than 5 layers based solely on queries and software.",
      "year": 2023,
      "venue": "2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)",
      "authors": [
        "Xiang Zhang",
        "A. Ding",
        "Yunsi Fei"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2b805ba343b54c03ffc637cf9d80a65d68d7ddbd",
      "pdf_url": "",
      "publication_date": "2023-10-28",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "title": "Data-Free Hard-Label Robustness Stealing Attack",
      "abstract": "The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaojian \\ Yuan",
        "Kejiang Chen",
        "Wen Huang",
        "Jie Zhang",
        "Weiming Zhang",
        "Neng H. Yu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/23faead2015ba1e36a2c0c535f987c0b36ba8534",
      "pdf_url": "",
      "publication_date": "2023-12-10",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eca3d9bca53842a53b65594762397e583901c437",
      "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models",
      "abstract": "Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.",
      "year": 2023,
      "venue": "ESORICS Workshops",
      "authors": [
        "Kevin Hector",
        "Pierre-Alain Mo\u00ebllic",
        "Mathieu Dumont",
        "J. Dutertre"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eca3d9bca53842a53b65594762397e583901c437",
      "pdf_url": "https://arxiv.org/pdf/2308.16703",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "eacb66ac30b489f704dedae7abc6e98429f95c88",
      "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
      "abstract": "As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Guangjing Wang",
        "Ce Zhou",
        "Yuanda Wang",
        "Bocheng Chen",
        "Hanqing Guo",
        "Qiben Yan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eacb66ac30b489f704dedae7abc6e98429f95c88",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "title": "Extracting Cloud-based Model with Prior Knowledge",
      "abstract": "Machine Learning-as-a-Service, a pay-as-you-go business pattern, is widely accepted by third-party users and developers. However, the open inference APIs may be utilized by malicious customers to conduct model extraction attacks, i.e., attackers can replicate a cloud-based black-box model merely via querying malicious examples. Existing model extraction attacks mainly depend on the posterior knowledge (i.e., predictions of query samples) from Oracle. Thus, they either require high query overhead to simulate the decision boundary, or suffer from generalization errors and overfitting problems due to query budget limitations. To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity with merely 1.8K queries (cost 2.16$) on the NSFW Recognition API. Also, the adversarial examples generated with our substitute model have better transferability than others, which reveals that our scheme is more conducive to downstream attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "S. Zhao",
        "Kangjie Chen",
        "Meng Hao",
        "Jian Zhang",
        "Guowen Xu",
        "Hongwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/3e4f89f6698ed4b6e5691b896c180315ab7d1d41",
      "pdf_url": "http://arxiv.org/pdf/2306.04192",
      "publication_date": "2023-06-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "title": "The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning",
      "abstract": "Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.",
      "year": 2023,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "M. Rigaki",
        "S. Garc\u00eda"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "pdf_url": "",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "title": "No Forking Way: Detecting Cloning Attacks on Intel SGX Applications",
      "abstract": "Forking attacks against TEEs like Intel SGX can be carried out either by rolling back the application to a previous state, or by cloning the application and by partitioning its inputs across the cloned instances. Current solutions to forking attacks require Trusted Third Parties (TTP) that are hard to find in real-world deployments. In the absence of a TTP, many TEE applications rely on monotonic counters to mitigate forking attacks based on rollbacks; however, they have no protection mechanism against forking attack based on cloning. In this paper, we analyze 72 SGX applications and show that approximately 20% of those are vulnerable to forking attacks based on cloning\u2014including those that rely on monotonic counters. To address this problem, we present CloneBuster, the first practical clone-detection mechanism for Intel SGX that does not rely on a TTP and, as such, can be used directly to protect existing applications. CloneBuster allows enclaves to (self-) detect whether another enclave with the same binary is running on the same platform. To do so, CloneBuster relies on a cache-based covert channel for enclaves to signal their presence to (and detect the presence of) clones on the same machine. We show that CloneBuster is robust despite a malicious OS, only incurs a marginal impact on the application performance, and adds approximately 800 LoC to the TCB. When used in conjunction with monotonic counters, CloneBuster allows applications to benefit from a comprehensive protection against forking attacks.",
      "year": 2023,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Samira Briongos",
        "Ghassan O. Karame",
        "Claudio Soriente",
        "Annika Wilde"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/21ec2007ae077dcb72f13e295cefe9ca1727b42e",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3627106.3627187",
      "publication_date": "2023-10-04",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "title": "Power2Picture: Using Generative CNNs for Input Recovery of Neural Network Accelerators through Power Side-Channels on FPGAs",
      "abstract": "Artificial neural networks pervade almost all areas of today's life, being used for both simple image classification tasks as well as highly complex decision making in mission-critical tasks. This makes artificial neural networks an attractive target for attackers to recover the model architecture or user inputs and outputs through either classical software vulnerabilities or hardware side-channel and fault attacks. With increasing complexity of the models, smaller companies now often opt for pre-trained public models, which are then used with potentially sensitive inputs, for instance, in medical applications. In this work, we present a novel remote side-channel attack methodology to steal neural network inputs using generative convolutional neural networks. After measuring voltage fluctuations using on-chip sensors, we are able to recover the original inputs to image classifiers on different FPGA platforms. Our results prove the effectiveness of our attack, as we are able to recover inputs from networks running on different devices, with different datasets, and under different operating conditions.",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Lukas Huegle",
        "M. Gotthard",
        "Vincent Meyers",
        "Jonas Krautter",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "steal neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "title": "Data-Free Model Extraction Attacks in the Context of Object Detection",
      "abstract": "A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. We find that the proposed model extraction method achieves significant results by using reasonable queries. The discovery of this object detection vulnerability will support future prospects for securing such models.",
      "year": 2023,
      "venue": "International Conference on Virtual Storytelling",
      "authors": [
        "Harshit Shah",
        "G. Aravindhan",
        "Pavan Kulkarni",
        "Yuvaraj Govidarajulu",
        "Manojkumar Somabhai Parmar"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a0a7b1aabe2f14696b15bac592b0ad5743ef0b85",
      "pdf_url": "https://arxiv.org/pdf/2308.05127",
      "publication_date": "2023-08-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "title": "Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers",
      "abstract": "Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction, the complexity of the attack is relatively low and we highlight the urgent need for practicable protections that could fit the strong memory and latency requirements of such platforms.",
      "year": 2023,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/8504a0c28bd68d820ba6e4e2102ee3e7ebf57df0",
      "pdf_url": "",
      "publication_date": "2023-11-02",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a273c40cc1f1d7096efe40d62fe28befa524245c",
      "title": "Exposing Model Theft: A Robust and Transferable Watermark for Thwarting Model Extraction Attacks",
      "abstract": "The increasing prevalence of Deep Neural Networks (DNNs) in cloud-based services has led to their widespread use through various APIs. However, recent studies reveal the susceptibility of these public APIs to model extraction attacks, where adversaries attempt to create a local duplicate of the private model using data and API-generated predictions. Existing defense methods often involve perturbing prediction distributions to hinder an attacker's training goals, inadvertently affecting API utility. In this study, we extend the concept of digital watermarking to protect DNNs' APIs. We suggest embedding a watermark into the safeguarded APIs; thus, any model attempting to copy will inherently carry the watermark, allowing the defender to verify any suspicious models. We propose a simple yet effective framework to increase watermark transferability. By requiring the model to memorize the preset watermarks in the final decision layers, we significantly enhance the transferability of watermarks. Comprehensive experiments show that our proposed framework not only successfully watermarks APIs but also maintains their utility.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Ruixiang Tang",
        "Hongye Jin",
        "Mengnan Du",
        "Curtis Wigington",
        "R. Jain",
        "Xia Hu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a273c40cc1f1d7096efe40d62fe28befa524245c",
      "pdf_url": "",
      "publication_date": "2023-10-21",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "bbd454a77f507fb292b607c6bacc941f52474009",
      "title": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps",
      "abstract": "The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the operation of a DL app. To demonstrate the effectiveness of our attack, we carry out an empirical study on 517 real-world DL apps collected from Google Play. Among 320 apps utilizing MLkit, we find that 81.56% of them can be successfully attacked. The results emphasize the importance of DL app developers being aware of and taking actions to secure on-device models from the perspective of data processing.",
      "year": 2023,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "Ye Sang",
        "Yujin Huang",
        "Shuo Huang",
        "Helei Cui"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/bbd454a77f507fb292b607c6bacc941f52474009",
      "pdf_url": "https://arxiv.org/pdf/2305.03963",
      "publication_date": "2023-05-06",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "title": "FDINet: Protecting Against DNN Model Extraction Using Feature Distortion Index",
      "abstract": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINet, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary\u2019s queries, we reveal that the feature distribution of these queries deviates from that of the model\u2019s problem domain. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINet utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINet against six state-of-the-art extraction attacks on four benchmark datasets and four popular model architectures. Empirical results demonstrate the following findings: 1) FDINet proves to be highly effective in detecting model extraction, achieving a 100% detection accuracy on DFME and DaST. 2) FDINet is highly efficient, using just 50 queries to raise an extraction alarm with an average confidence of 96.08% for GTSRB. 3) FDINet exhibits the capability to identify colluding adversaries with an accuracy exceeding 91%. Additionally, it demonstrates the ability to detect two types of adaptive attacks.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zheng Li",
        "Haiqin Weng",
        "Feng Xue",
        "Kui Ren",
        "Zhan Qin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b8d3211b4b5f636a5e2719d404b278cea80d8e1",
      "pdf_url": "",
      "publication_date": "2023-06-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "761cb683cf1bf94b878f6d7527600c2c62aee796",
      "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
      "abstract": "While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yi Xie",
        "Jie Zhang",
        "Shiqian Zhao",
        "Tianwei Zhang",
        "Xiaofeng Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/761cb683cf1bf94b878f6d7527600c2c62aee796",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "title": "ShrewdAttack: Low Cost High Accuracy Model Extraction",
      "abstract": "Machine learning as a service (MLaaS) plays an essential role in the current ecosystem. Enterprises do not need to train models by themselves separately. Instead, they can use well-trained models provided by MLaaS to support business activities. However, such an ecosystem could be threatened by model extraction attacks\u2014an attacker steals the functionality of a trained model provided by MLaaS and builds a substitute model locally. In this paper, we proposed a model extraction method with low query costs and high accuracy. In particular, we use pre-trained models and task-relevant data to decrease the size of query data. We use instance selection to reduce query samples. In addition, we divided query data into two categories, namely low-confidence data and high-confidence data, to reduce the budget and improve accuracy. We then conducted attacks on two models provided by Microsoft Azure as our experiments. The results show that our scheme achieves high accuracy at low cost, with the substitution models achieving 96.10% and 95.24% substitution while querying only 7.32% and 5.30% of their training data on the two models, respectively. This new attack approach creates additional security challenges for models deployed on cloud platforms. It raises the need for novel mitigation strategies to secure the models. In future work, generative adversarial networks and model inversion attacks can be used to generate more diverse data to be applied to the attacks.",
      "year": 2023,
      "venue": "Entropy",
      "authors": [
        "Yang Liu",
        "Ji Luo",
        "Yi Yang",
        "Xuan Wang",
        "M. Gheisari",
        "Feng Luo"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2c3f82769341bec4cfbbda760e42dbe9b16780bc",
      "pdf_url": "https://www.mdpi.com/1099-4300/25/2/282/pdf?version=1675337976",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks",
      "abstract": "Artificial Intelligence (AI) hardware accelerators have been widely adopted to enhance the efficiency of deep learning applications. However, they also raise security concerns regarding their vulnerability to power side-channel attacks (SCA). In these attacks, the adversary exploits unintended communication channels to infer sensitive information processed by the accelerator, posing significant privacy and copyright risks to the models. Advanced machine learning algorithms are further employed to facilitate the side-channel analysis and exacerbate the privacy issue of AI accelerators. Traditional defense strategies naively inject execution noise to the runtime of AI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguard FPGA-based AI accelerators and mitigate model extraction threats via power-based SCAs. The key insight of AIAShield is to leverage the prominent adversarial attack technique from the machine learning community to craft delicate noise, which can significantly obfuscate the adversary's side-channel observation while incurring minimal overhead to the execution of the protected model. At the hardware level, we design a new module based on ring oscillators to achieve fine-grained noise generation. At the algorithm level, we repurpose Neural Architecture Search to worsen the adversary's extraction results. Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA) demonstrate that AIAShield outperforms existing solutions with excellent transferability.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xiaobei Yan",
        "Chip Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "34bed407d65517ed2c8b98bab3a33da175677c59",
      "title": "A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots",
      "abstract": "Building advanced machine learning (ML) models requires expert knowledge and many trials to discover the best architecture and hyperparameter settings. Previous work demonstrates that model information can be leveraged to assist other attacks, such as membership inference, generating adversarial examples. Therefore, such information, e.g., hyperparameters, should be kept confidential. It is well known that an adversary can leverage a target ML model's output to steal the model's information. In this paper, we discover a new side channel for model information stealing attacks, i.e., models' scientific plots which are extensively used to demonstrate model performance and are easily accessible. Our attack is simple and straightforward. We leverage the shadow model training techniques to generate training data for the attack model which is essentially an image classifier. Extensive evaluation on three benchmark datasets shows that our proposed attack can effectively infer the architecture/hyperparameters of image classifiers based on convolutional neural network (CNN) given the scientific plot generated from it. We also reveal that the attack's success is mainly caused by the shape of the scientific plots, and further demonstrate that the attacks are robust in various scenarios. Given the simplicity and effectiveness of the attack method, our study indicates scientific plots indeed constitute a valid side channel for model information stealing attacks. To mitigate the attacks, we propose several defense mechanisms that can reduce the original attacks' accuracy while maintaining the plot utility. However, such defenses can still be bypassed by adaptive attacks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Boyang Zhang",
        "Xinlei He",
        "Yun Shen",
        "Tianhao Wang",
        "Yang Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/34bed407d65517ed2c8b98bab3a33da175677c59",
      "pdf_url": "http://arxiv.org/pdf/2302.11982",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "title": "A Taxonomic Survey of Model Extraction Attacks",
      "abstract": "A model extraction attack aims to clone a machine learning target model deployed in the cloud solely by querying the target in a black-box manner. Once a clone is obtained it is possible to launch further attacks with the aid of the local model. In this survey, we analyze existing approaches and present a taxonomic overview of this field based on several important aspects that affect attack efficiency and performance. We present both early works and recently explored directions. We conclude with an analysis of future directions based on recent developments in machine learning methodology.",
      "year": 2023,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Didem Gen\u00e7",
        "Mustafa \u00d6zuysal",
        "E. Tomur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2fd52a3544dc0e3a480d12af01bb978c4d1a59fc",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "title": "Scalable Scan-Chain-Based Extraction of Neural Network Models",
      "abstract": "Scan chains have greatly improved hardware testability while introducing security breaches for confidential data. Scan-chain attacks have extended their scope from cryptoprocessors to AI edge devices. The recently proposed scan-chain-based neural network (NN) model extraction attack (lCCAD 2021) made it possible to achieve fine-grained extraction and is multiple orders of magnitude more efficient both in queries and accuracy than its coarse-grained mathematical counterparts. However, both query formulation complexity and constraint solver failures increase drastically with network depth/size. We demonstrate a more powerful adversary, who is capable of improving scalability while maintaining accuracy, by relaxing high-fidelity constraints to formulate an approximate-fidelity-based layer-constrained least-squares extraction using random queries. We conduct our extraction attack on neural network inference topologies of different depths and sizes, targeting the MNIST digit recognition task. The results show that our method outperforms the scan-chain attack proposed in ICCAD 2021 by an average increase in the extracted neural network's functional accuracy of \u2248 32% and 2\u20133 orders of reduction in queries. Furthermore, we demonstrated that our attack is highly effective even in the presence of countermeasures against adversarial samples.",
      "year": 2023,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Shui Jiang",
        "S. Potluri",
        "Tsungyi Ho"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/98ba77eb485e9d10c3f56baac5ff862f59fadbec",
      "pdf_url": "",
      "publication_date": "2023-04-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
      "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model\u2019s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd\u2019s wisdom. Based on the ensemble\u2019s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV.",
      "year": 2023,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Akshit Jindal",
        "Vikram Goyal",
        "Saket Anand",
        "Chetan Arora"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
      "pdf_url": "https://arxiv.org/pdf/2311.04588",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "title": "Bits to BNNs: Reconstructing FPGA ML-IP with Joint Bitstream and Side-Channel Analysis",
      "abstract": "Energy-efficient hardware acceleration platforms for edge deployment of artificial intelligence (AI) and machine learning (ML) applications has been an ongoing research endeavor. Many efforts have focused on optimizing the algorithms and compute structures for use in resource-constrained hardware such as field-programmable gate arrays (FPGAs). Indeed, the difficult nature of crafting the best model makes the ML model itself a valuable intellectual property (IP) asset. This can be problematic, as the IP can now be exposed to an attacker through physical interfaces, enabling threats from side-channel analysis (SCA) attacks. One of the more devastating attacks is the model extraction attack, which threatens piracy and cloning of the valuable IP. While the problem of SCA-based model extraction on FPGA-deployed neural networks has been well-studied, it does not capture the full picture of what vulnerabilities may be present in those platforms. In this paper, we demonstrate how bitstream analysis can be used to obtain neural network parameters and connectivity information from block RAMs (BRAMs). We leverage the knowledge gleaned from the bitstream to mount a power SCA attack to further refine the network reconstruction effort. This is the first method that has approached the problem of ML-IP theft from the angle of FPGA bitstream analysis and suggests that further work is needed to improve security assurance for edge intelligence.",
      "year": 2023,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1a2fd1461e9160246ed51190a455916bb9d48fb3",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0fa016157057c5203793d328430a7d86ababd7cd",
      "title": "Voltage Scaling-Agnostic Counteraction of Side-Channel Neural Net Reverse Engineering via Machine Learning Compensation and Multi-Level Shuffling",
      "abstract": "This work proposes a voltage scaling-agnostic counteraction against neural network weight reverse engineering via side-channel attacks. Multi-level shuffling and machine learning-based dual power compensation are introduced. State-of-the-art protection ($\\gt200\\cdot 10^{6}$ MTD) is achieved at low power overhead (1.76$\\times $) and zero latency overhead.",
      "year": 2023,
      "venue": "2023 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)",
      "authors": [
        "Qiang Fang",
        "Longyang Lin",
        "Hui Zhang",
        "Tianqi Wang",
        "Massimo Alioto"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/0fa016157057c5203793d328430a7d86ababd7cd",
      "pdf_url": "",
      "publication_date": "2023-06-11",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "title": "RemovalNet: DNN Fingerprint Removal Attacks",
      "abstract": "With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this article, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named <sc>RemovalNet</sc>, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the <italic>fidelity</italic>, <italic>effectiveness</italic>, and <italic>efficiency</italic> of the <sc>RemovalNet</sc> against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the <sc>RemovalNet</sc> is <italic>effective</italic>. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is <inline-formula><tex-math notation=\"LaTeX\">$\\times 100$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>\u00d7</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"qin-ieq1-3315064.gif\"/></alternatives></inline-formula> times higher than that of the baseline attacks, (2) the <sc>RemovalNet</sc> is <italic>efficient</italic>. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the <sc>RemovalNet</sc> saves nearly 85% of computational resources at most, (3) the <sc>RemovalNet</sc> achieves high <italic>fidelity</italic> that the created surrogate model maintains high accuracy after the DNN fingerprint removal process.",
      "year": 2023,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Hongwei Yao",
        "Zhengguang Li",
        "Kunzhe Huang",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/32a1e6315ca47a410bdfe2577bd605cf80f134b1",
      "pdf_url": "http://arxiv.org/pdf/2308.12319",
      "publication_date": "2023-08-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "252cc0c47b76f804a2e839773a90e7a389289695",
      "title": "SNATCH: Stealing Neural Network Architecture from ML Accelerator in Intelligent Sensors",
      "abstract": "The use of Machine Learning (ML) models executing on ML Accelerators (MLA) in Intelligent sensors for feature extraction has garnered substantial interest. The Neural Network (NN) architecture implemented of MLA are intellectual property for the vendors. Along with improved power-efficiency and reduced bandwidth, the hardware based ML models embedded in the sensor also provides additional security against cyber-attacks on the ML. In this paper, we introduce an attack referred as SNATCH which uses a profiling-based side channel attack (SCA) that aims to steal the NN architecture executing on a digital MLA (Deep Learning Processing Unit (DPU) IP by Xilinx). We use electromagnetic side channel leakage from a clone device to create a profiler and then attack the victim's device to steal the NN architecture. Stealing the ML model undermines the intellectual property rights of the vendors of a sensor. Further, it also allows an adversary to mount critical Denial of Service and misuse attack.",
      "year": 2023,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Sudarshan Sharma",
        "U. Kamal",
        "Jianming Tong",
        "Tushar Krishna",
        "S. Mukhopadhyay"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/252cc0c47b76f804a2e839773a90e7a389289695",
      "pdf_url": "",
      "publication_date": "2023-10-29",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "title": "A Model Stealing Attack Against Multi-Exit Networks",
      "abstract": "Compared to traditional neural networks with a single output channel, a multi-exit network has multiple exits that allow for early outputs from the model's intermediate layers, thus significantly improving computational efficiency while maintaining similar main task accuracy. Existing model stealing attacks can only steal the model's utility while failing to capture its output strategy, i.e., a set of thresholds used to determine from which exit to output. This leads to a significant decrease in computational efficiency for the extracted model, thereby losing the advantage of multi-exit networks. In this paper, we propose the first model stealing attack against multi-exit networks to extract both the model utility and the output strategy. We employ Kernel Density Estimation to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the extracted model. Furthermore, we design a novel output strategy search algorithm to maximize the consistency between the victim model and the extracted model's output behaviors. In experiments across multiple multi-exit networks and benchmark datasets, our method always achieves accuracy and efficiency closest to the victim models.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Pan Li",
        "Peizhuo Lv",
        "Kai Chen",
        "Yuling Cai",
        "Fan Xiang",
        "Shengzhi Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1365039961e03ac532b63cf4dbd99bbe5352ec0a",
      "pdf_url": "",
      "publication_date": "2023-05-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c2a10426d91b95197f8489699026326bddb0eabc",
      "title": "MEAOD: Model Extraction Attack against Object Detectors",
      "abstract": "The widespread use of deep learning technology across various industries has made deep neural network models highly valuable and, as a result, attractive targets for potential attackers. Model extraction attacks, particularly query-based model extraction attacks, allow attackers to replicate a substitute model with comparable functionality to the victim model and present a significant threat to the confidentiality and security of MLaaS platforms. While many studies have explored threats of model extraction attacks against classification models in recent years, object detection models, which are more frequently used in real-world scenarios, have received less attention. In this paper, we investigate the challenges and feasibility of query-based model extraction attacks against object detection models and propose an effective attack method called MEAOD. It selects samples from the attacker-possessed dataset to construct an efficient query dataset using active learning and enhances the categories with insufficient objects. We additionally improve the extraction effectiveness by updating the annotations of the query dataset. According to our gray-box and black-box scenarios experiments, we achieve an extraction performance of over 70% under the given condition of a 10k query budget.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zeyu Li",
        "Chenghui Shi",
        "Yuwen Pu",
        "Xuhong Zhang",
        "Yu Li",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c2a10426d91b95197f8489699026326bddb0eabc",
      "pdf_url": "",
      "publication_date": "2023-12-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "title": "Rethink before Releasing your Model: ML Model Extraction Attack in EDA",
      "abstract": "Machine learning (ML)-based techniques for electronic design automation (EDA) have boosted the performance of modern integrated circuits (ICs). Such achievement makes ML model to be of importance for the EDA industry. In addition, ML models for EDA are widely considered having high development cost because of the time-consuming and complicated training data generation process. Thus, confidentiality protection for EDA models is a critical issue. However, an adversary could apply model extraction attacks to steal the model in the sense of achieving the comparable performance to the victim's model. As model extraction attacks have posed great threats to other application domains, e.g., computer vision and natural language process, in this paper, we study model extraction attacks for EDA models under two real-world scenarios. It is the first work that (1) introduces model extraction attacks on EDA models and (2) proposes two attack methods against the unlimited and limited query budget scenarios. Our results show that our approach can achieve competitive performance with the well-trained victim model without any performance degradation. Based on the results, we demonstrate that model extraction attacks truly threaten the EDA model privacy and hope to raise concerns about ML security issues in EDA.",
      "year": 2023,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Chen-Chia Chang",
        "Jingyu Pan",
        "Zhiyao Xie",
        "Jiangkun Hu",
        "Yiran Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7d0593c25bcd11f26e8deaa97d18da3314f7af48",
      "pdf_url": "https://doi.org/10.1145/3566097.3567896",
      "publication_date": "2023-01-16",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0005c9691c8c299476d201d0a5a3c86b49593fac",
      "title": "High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking",
      "abstract": "In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks, with an almost 100% success rate. To counter this attack, we devise an adversarial framework for the watermarking network. The framework incorporates a specially designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Huajie Chen",
        "Tianqing Zhu",
        "Chi Liu",
        "Shui Yu",
        "Wanlei Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0005c9691c8c299476d201d0a5a3c86b49593fac",
      "pdf_url": "http://arxiv.org/pdf/2302.08637",
      "publication_date": "2023-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "title": "Model Stealing Attacks and Defenses: Where Are We Now?",
      "abstract": "The success of deep learning in many application domains has been nothing short of dramatic. This has brought the spotlight onto security and privacy concerns with machine learning (ML). One such concern is the threat of model theft. I will discuss work on exploring the threat of model theft, especially in the form of \u201cmodel extraction attacks\u201d \u2014 when a model is made available to customers via an inference interface, a malicious customer can use repeated queries to this interface and use the information gained to construct a surrogate model. I will also discuss possible countermeasures, focusing on deterrence mechanisms that allow for model ownership resolution (MOR) based on watermarking or fingerprinting. In particular, I will discuss the robustness of MOR schemes. I will touch on the issue of conflicts that arise when protection mechanisms for multiple different threats need to be applied simultaneously to a given ML model, using MOR techniques as a case study. This talk is based on work done with my students and collaborators, including Buse Atli Tekgul, Jian Liu, Mika Juuti, Rui Zhang, Samuel Marchal, and Sebastian Szyller. The work was funded in part by Intel Labs in the context of the Private AI consortium.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "N. Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/72ddc601b9e6d8a0908ca49ab228fdb06ad50b79",
      "pdf_url": "",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model theft",
        "model stealing attack",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "title": "Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience",
      "abstract": "Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Janvi Thakkar",
        "Giulio Zizzo",
        "S. Maffeis"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/598acab0fdbf2a4c77e05e953498521a1a9f208f",
      "pdf_url": "",
      "publication_date": "2023-12-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "title": "Model Stealing Attack against Recommender System",
      "abstract": "Recent studies have demonstrated the vulnerability of recommender systems to data privacy attacks. However, research on the threat to model privacy in recommender systems, such as model stealing attacks, is still in its infancy. Some adversarial attacks have achieved model stealing attacks against recommender systems, to some extent, by collecting abundant training data of the target model (target data) or making a mass of queries. In this paper, we constrain the volume of available target data and queries and utilize auxiliary data, which shares the item set with the target data, to promote model stealing attacks. Although the target model treats target and auxiliary data differently, their similar behavior patterns allow them to be fused using an attention mechanism to assist attacks. Besides, we design stealing functions to effectively extract the recommendation list obtained by querying the target model. Experimental results show that the proposed methods are applicable to most recommender systems and various scenarios and exhibit excellent attack performance on multiple datasets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Rui Fan",
        "Chenwang Wu",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/378230eb80a097e2d19aaf0e91d2745c9513a25a",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "title": "Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation",
      "abstract": "Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing.",
      "year": 2023,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Vlad Hondru",
        "R. Ionescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "title": "Safe and Robust Watermark Injection with a Single OoD Image",
      "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Shuyang Yu",
        "Junyuan Hong",
        "Haobo Zhang",
        "Haotao Wang",
        "Zhangyang Wang",
        "Jiayu Zhou"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2949b03a5e1bc3cddaa0784e2a2ac4d0d0c996f7",
      "pdf_url": "https://arxiv.org/pdf/2309.01786",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8b8c962da13cbca14d510b3359b42533291ad853",
      "title": "APGP: Accuracy-Preserving Generative Perturbation for Defending Against Model Cloning Attacks",
      "abstract": "Well-trained Deep Neural Networks (DNNs) are valuable intellectual properties. Recent studies show that adversaries only with black-box query access can steal the functionality of DNNs by using knowledge distillation (KD) techniques. In this paper, we propose a novel formulation to defend against model cloning attacks. Then we implement our defense as a plug-and-play generative perturbation model, dubbed as Accuracy-Preserving Generative Perturbation (APGP). Our method is the first to effectively defend against KD-based model cloning without damaging model accuracy. Numerous experiments demonstrate the effectiveness of our defense across different datasets and DNN model cloning attacks, and the advances compared to existing methods.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Anda Cheng",
        "Jian Cheng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8b8c962da13cbca14d510b3359b42533291ad853",
      "pdf_url": "https://doi.org/10.1109/icassp49357.2023.10094956",
      "publication_date": "2023-06-04",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8f03f195bacd67b350450aa2a793e7b9861afb17",
      "title": "SparseLock: Securing Neural Network Models in Deep Learning Accelerators",
      "abstract": "Securing neural networks (NNs) against model extraction and parameter exfiltration attacks is an important problem primarily because modern NNs take a lot of time and resources to build and train. We observe that there are no countermeasures (CMs) against recently proposed attacks on sparse NNs and there is no single CM that effectively protects against all types of known attacks for both sparse as well as dense NNs. In this paper, we propose SparseLock, a comprehensive CM that protects against all types of attacks including some of the very recently proposed ones for which no CM exists as of today. We rely on a novel compression algorithm and binning strategy. Our security guarantees are based on the inherent hardness of bin packing and inverse bin packing problems. We also perform a battery of statistical and information theory based tests to successfully show that we leak very little information and side channels in our architecture are akin to random sources. In addition, we show a performance benefit of 47.13% over the nearest competing secure architecture.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Nivedita Shrivastava",
        "S. Sarangi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8f03f195bacd67b350450aa2a793e7b9861afb17",
      "pdf_url": "",
      "publication_date": "2023-11-05",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "title": "Theoretical Limits of Provable Security Against Model Extraction by Efficient Observational Defenses",
      "abstract": "Can we hope to provide provable security against model extraction attacks? As a step towards a theoretical study of this question, we unify and abstract a wide range of \u201cobservational\u201d model extraction defenses (OMEDs) - roughly, those that attempt to detect model extraction by analyzing the distribution over the adversary's queries. To accompany the abstract OMED, we define the notion of complete OMEDs - when benign clients can freely interact with the model - and sound OMEDs - when adversarial clients are caught and prevented from reverse engineering the model. Our formalism facilitates a simple argument for obtaining provable security against model extraction by complete and sound OMEDs, using (average-case) hardness assumptions for PAC-learning, in a way that abstracts current techniques in the prior literature. The main result of this work establishes a partial computational incompleteness theorem for the OMED: any efficient OMED for a machine learning model computable by a polynomial size decision tree that satisfies a basic form of completeness cannot satisfy soundness, unless the subexponential Learning Parity with Noise (LPN) assumption does not hold. To prove the incompleteness theorem, we introduce a class of model extraction attacks called natural Covert Learning attacks based on a connection to the Covert Learning model of Canetti and Karchmer (TCC '21), and show that such attacks circumvent any defense within our abstract mechanism in a black-box, nonadaptive way. As a further technical contribution, we extend the Covert Learning algorithm of Canetti and Karchmer to work over any \u201cconcise\u201d product distribution (albeit for juntas of a logarithmic number of variables rather than polynomial size decision trees), by showing that the technique of learning with a distributional inverter of Binnendyk et al. (ALT '22) remains viable in the Covert Learning setting.",
      "year": 2023,
      "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Ari Karchmer"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3b1cdd062387bccb4b8bfdd46dd61937caab5201",
      "pdf_url": "",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "title": "Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks",
      "abstract": "Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a \u00d737 faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.",
      "year": 2023,
      "venue": "International Conference on Machine Learning and Applications",
      "authors": [
        "Kacem Khaled",
        "Mouna Dhaouadi",
        "F. Magalh\u00e3es",
        "G. Nicolescu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/23aef5016a8762544e9300e4aa43ceaafa6ee244",
      "pdf_url": "",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "title": "FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems",
      "abstract": "Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious traffic, it may increase the risk of privacy leakage. This paper focuses on implementing a model stealing attack on intrusion detection systems. Existing model stealing attacks are hard to implement in practical network environments, as they either need private data of the victim dataset or frequent access to the victim model. In this paper, we propose a novel solution called Fast Model Stealing Attack (FMSA) to address the problem in the field of model stealing attacks. We also highlight the risks of using ML-NIDS in network security. First, meta-learning frameworks are introduced into the model stealing algorithm to clone the victim model in a black-box state. Then, the number of accesses to the target model is used as an optimization term, resulting in minimal queries to achieve model stealing. Finally, adversarial training is used to simulate the data distribution of the target model and achieve the recovery of privacy data. Through experiments on multiple public datasets, compared to existing state-of-the-art algorithms, FMSA reduces the number of accesses to the target model and improves the accuracy of the clone model on the test dataset to 88.9% and the similarity with the target model to 90.1%. We can demonstrate the successful execution of model stealing attacks on the ML-NIDS system even with protective measures in place to limit the number of anomalous queries.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Kaisheng Fan",
        "Weizhe Zhang",
        "Guangrui Liu",
        "Hui He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/ad26add46cae7797de1d638b84a0fa4ad1743a60",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00171-y",
      "publication_date": "2023-08-04",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "title": "NNLeak: An AI-Oriented DNN Model Extraction Attack through Multi-Stage Side Channel Analysis",
      "abstract": "Side channel analysis (SCA) attacks have become emerging threats to AI algorithms and deep neural network (DNN) models. However, most existing SCA attacks focus on extracting models deployed on embedded devices, such as microcontrollers. Accurate SCA attacks on extracting DNN models deployed on AI accelerators are largely missing, leaving researchers with an (improper) assumption that DNNs on AI accelerators may be immune to SCA attacks due to their complexity. In this paper, we propose a novel method, namely NNLeak to extract complete DNN models on FPGA-based AI accelerators. To achieve this goal, NNLeak first exploits simple power analysis (SPA) to identify model architecture. Then a multi-stage correlation power analysis (CPA) is designed to recover model weights accurately. Finally, NNLeak determines the activation functions of DNN models through an AI-oriented classifier. The efficacy of NNLeak is validated on FPGA implementations of two DNN models, including multilayer perceptron (MLP) and LeNet. Experimental results show that NNLeak can successfully extract complete DNN models within 2000 power traces.",
      "year": 2023,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Ya Gao",
        "Haocheng Ma",
        "Mingkai Yan",
        "Jiaji He",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e49363bd733bed0016e57170c31c7b3dc4dc1714",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "title": "Stolen Risks of Models with Security Properties",
      "abstract": "Verifiable robust machine learning, as a new trend of ML security defense, enforces security properties (e.g., Lipschitzness, Monotonicity) on machine learning models and achieves satisfying accuracy-security trade-off. Such security properties identify a series of evasion strategies of ML security attackers and specify logical constraints on their effects on a classifier (e.g., the classifier is monotonically increasing along some feature dimensions). However, little has been done so far to understand the side effect of those security properties on the model privacy. In this paper, we aim at better understanding the privacy impacts on security properties of robust ML models. Particularly, we report the first measurement study to investigate the model stolen risks of robust models satisfying four security properties (i.e., LocalInvariance, Lipschitzness, SmallNeighborhood, and Monotonicity). Our findings bring to light the factors that influence model stealing attacks and defense performance on models trained with security properties. In addition, to train an ML model satisfying goals in accuracy, security, and privacy, we propose a novel technique, called BoundaryFuzz, which introduces a privacy property into verifiable robust training frameworks to defend against model stealing attacks on robust models. Experimental results demonstrate the defense effectiveness of BoundaryFuzz.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yue Qin",
        "Zhuoqun Fu",
        "Chuyun Deng",
        "Xiaojing Liao",
        "Jia Zhang",
        "Haixin Duan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8e5b27857ea5ede4927d6c673ef795cb23b59071",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "title": "BarraCUDA: Edge GPUs do Leak DNN Weights",
      "abstract": "Over the last decade, applications of neural networks (NNs) have spread to various aspects of our lives. A large number of companies base their businesses on building products that use neural networks for tasks such as face recognition, machine translation, and self-driving cars. Much of the intellectual property underpinning these products is encoded in the exact parameters of the neural networks. Consequently, protecting these is of utmost priority to businesses. At the same time, many of these products need to operate under a strong threat model, in which the adversary has unfettered physical control of the product. In this work, we present BarraCUDA, a novel attack on general purpose Graphic Processing Units (GPUs) that can extract parameters of neural networks running on the popular Nvidia Jetson Nano device. BarraCUDA uses correlation electromagnetic analysis to recover parameters of real-world convolutional neural networks.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "P\u00e9ter Horv\u00e1th",
        "Lukasz Chmielewski",
        "L\u00e9o Weissbart",
        "L. Batina",
        "Y. Yarom"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9cdce5c92985967c2bd34aacdd46b1490bf8565c",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7a846254b166447bfbf28852bfde7495332e14d",
      "title": "Making Watermark Survive Model Extraction Attacks in Graph Neural Networks",
      "abstract": "Collecting graph data is costly and well-trained graph neural networks (GNNs) are viewed as intellectual property. To make better use of GNNs, they are used to provide cloud-based services. However, models on cloud-based services may be leaked under model extraction attacks. Adversaries can extract an imitation model by simply querying the GNNs on the cloud-based services. To protect GNNs, watermarks are embedded in the models. However, the watermarks can be removed by the model extraction attacks. To address this issue, we propose adding a watermark that cannot be ignored by queries from the model extraction attacks. Concretely, we add the soft nearest neighbor loss to the loss function of the watermark embedding process to merge the distributions for the normal tasks and watermarks. We also observe that the watermark brings a performance loss to GNNs and propose an optimization method to maintain the model performance. We evaluate our method on multiple real-world datasets to demonstrate the superiority of the method.",
      "year": 2023,
      "venue": "ICC 2023 - IEEE International Conference on Communications",
      "authors": [
        "Haiming Wang",
        "Zhikun Zhang",
        "Min Chen",
        "Shibo He"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7a846254b166447bfbf28852bfde7495332e14d",
      "pdf_url": "",
      "publication_date": "2023-05-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "cdc97244b92d836d759776aced5fba4f4a976e66",
      "title": "Enabling DVFS Side-Channel Attacks for Neural Network Fingerprinting in Edge Inference Services",
      "abstract": "The Inference-as-a-Service (IaaS) delivery model provides users access to pre-trained deep neural networks while safeguarding network code and weights. However, IaaS is not immune to security threats, like side-channel attacks (SCAs), that exploit unintended information leakage from the physical characteristics of the target device. Exposure to such threats grows when IaaS is deployed on distributed computing nodes at the edge. This work identifies a potential vulnerability of low-power CPUs that facilitates stealing the deep neural network architecture without physical access to the hardware or interference with the execution flow. Our approach relies on a Dynamic Voltage and Frequency Scaling (DVFS) side-channel attack, which monitors the CPU frequency state during the inference stages. Specifically, we introduce a dedicated load-testing methodology that imprints distinguishable signatures of the network on the frequency traces. A machine learning classifier is then used to infer the victim architecture. Experimental results on two commercial ARM Cortex-A CPUs, the A72 and A57, demonstrate the attack can identify the target architecture from a pool of 12 convolutional neural networks with an average accuracy of 98.7% and 92.4%",
      "year": 2023,
      "venue": "International Symposium on Low Power Electronics and Design",
      "authors": [
        "Erich Malan",
        "Valentino Peluso",
        "A. Calimera",
        "Enrico Macii"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/cdc97244b92d836d759776aced5fba4f4a976e66",
      "pdf_url": "",
      "publication_date": "2023-08-07",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "22281ae0f550dcb5b8d6c2f1768780f33f4b3e6b",
      "title": "An Analog Side-Channel Attack on a High-Speed Asynchronous SAR ADC Using Dual Neural Network Technique",
      "abstract": "SUMMARY This brief presents a side-channel attack (SCA) technique on a high-speed asynchronous successive approximation register (SAR) analog-to-digitalconverter(ADC).Theproposeddualneuralnetworkbasedonmultiplenoisewaveformsseparatelydisclosessignandabsolutevalue informationofinputsignalswhicharehiddenbythedifferentialstructureandhigh-speedasynchronousoperation.ThetargetSARADCandon-chip noisemonitorsaredesignedonasingleprototypechipforSCAdemon-stration.Fabricatedin40nm,theexperimentalresultsshowtheproposed attack on the asynchronous SAR ADC successfully restores the input data with a competitive accuracy within 300mV rms error.",
      "year": 2023,
      "venue": "IEICE transactions on electronics",
      "authors": [
        "R. Takahashi",
        "Takuji Miki",
        "M. Nagata"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/22281ae0f550dcb5b8d6c2f1768780f33f4b3e6b",
      "pdf_url": "https://www.jstage.jst.go.jp/article/transele/E106.C/10/E106.C_2022CTS0002/_pdf",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8796b36ed1a74a1c9fdafe2409c981102983653f",
      "title": "On Function-Coupled Watermarks for Deep Neural Networks",
      "abstract": "Well-performed deep neural networks (DNNs) generally require massive labeled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers can claim IP ownership by retrieving their embedded watermarks. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning, model pruning, and model extraction. In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model\u2019s performance on normal inputs. Specifically, on one hand, we sample inputs from the original training dataset and fuse them as watermark images. On the other hand, we randomly mask model weights during training to distribute the watermark information in the network. Our method can successfully defend against common watermark removal attacks, watermark ambiguity attacks, and existing widely used backdoor detection methods, outperforming existing solutions as demonstrated by evaluation results on various benchmarks. Our code is available at: https://github.com/cure-lab/Function-Coupled-Watermark.",
      "year": 2023,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Xiangyu Wen",
        "Yu Li",
        "Weizhen Jiang",
        "Qian-Lan Xu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8796b36ed1a74a1c9fdafe2409c981102983653f",
      "pdf_url": "https://doi.org/10.1109/jetcas.2024.3476386",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "title": "Data-Free Model Stealing Attack Based on Denoising Diffusion Probabilistic Model",
      "abstract": "Data-free model stealing (MS) attacks use synthetic samples to query a target model and train a substitute model to fit the target model\u2019s predictions, avoiding strong dependence on real datasets used by model developers. However, the existing data-free MS attack methods still have a big gap in generating high-quality query samples for high-precision MS attacks. In this paper, we construct the DDPM-optimized generator to generate data, in which a residual network-like structure is designed to fuse data to synthesize query samples. Our method further improves the quantity and quality of synthetic query samples, and effectively reduces the number of queries to the target model. The results show that the proposed method achieves superior performance compared to state-of-the-art methods.",
      "year": 2023,
      "venue": "2023 IEEE Smart World Congress (SWC)",
      "authors": [
        "Guofeng Gao",
        "Xiaodong Wang",
        "Zhiqiang Wei",
        "Jinghai Ai"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3166f8c8e00f2cd9dc5a903009a4f50d176d9f8a",
      "pdf_url": "",
      "publication_date": "2023-08-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "title": "Sniffer: A Novel Model Type Detection System against Machine-Learning-as-a-Service Platforms",
      "abstract": "\n Recent works explore several attacks against Machine-Learning-as-a-Service (MLaaS) platforms (e.g., the model stealing attack), allegedly posing potential real-world threats beyond viability in laboratories. However, hampered by\n model-type-sensitive\n , most of the attacks can hardly break mainstream real-world MLaaS platforms. That is, many MLaaS attacks are designed against only one certain type of model, such as tree models or neural networks. As the black-box MLaaS interface hides model type info, the attacker cannot choose a proper attack method with confidence, limiting the attack performance. In this paper, we demonstrate a system, named Sniffer, that is capable of making model-type-sensitive attacks \"great again\" in real-world applications. Specifically, Sniffer consists of four components: Generator, Querier, Probe, and Arsenal. The first two components work for preparing attack samples. Probe, as the most characteristic component in Sniffer, implements a series of self-designed algorithms to determine the type of models hidden behind the black-box MLaaS interfaces. With model type info unraveled, an optimum method can be selected from Arsenal (containing multiple attack methods) to accomplish its attack. Our demonstration shows how the audience can interact with Sniffer in a web-based interface against five mainstream MLaaS platforms.\n",
      "year": 2023,
      "venue": "Proceedings of the VLDB Endowment",
      "authors": [
        "Zhuo Ma",
        "Yilong Yang",
        "Bin Xiao",
        "Yang Liu",
        "Xinjing Liu",
        "Tong Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ecb4dff5160be86c97f27d9be1c0b74472af127b",
      "pdf_url": "",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "title": "Robot Mimicry Attack on Keystroke-Dynamics User Identification and Authentication System",
      "abstract": "Future robots will be very advanced with high flexibility and accurate control performance. They will have the ability to mimic human behaviours or even perform better, which raises the significant risk of robot attack. In this work, we study the robot mimic attack on the current keystroke-dynamic user authentication system. Specifically, we proposed a robot mimicry attack framework for keystroke-dynamics systems. We collected keyboard logging data and acoustical signal data from real users and extracted the timing pattern of keystrokes to understand victim's behaviour for robot imitation attacks. Furthermore, we develop a deep Q-Network (DQN) algorithm to control the velocity of robot which is one of the key challenges of forging the human typing timing features. We tested and evaluated our approach on the real-life robotic testbed. We presented our results considering user identification and user authentication performance. We achieved a 90.3% user identification accuracy with genuine keyboard logging data samples and 89.6% accuracy with robot-forged data samples. Furthermore, we achieved 11.1%, and 36.6% EER for user authentication performance with zero-effort attack, and robot mimicry attack, respectively.",
      "year": 2023,
      "venue": "IEEE International Conference on Robotics and Automation",
      "authors": [
        "Rongyu Yu",
        "Burak Kizilkaya",
        "Zhen Meng",
        "Emma Li",
        "Guodong Zhao",
        "M. Imran"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "pdf_url": "https://eprints.gla.ac.uk/289833/2/289833.pdf",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "title": "Interesting Near-boundary Data: Inferring Model Ownership for DNNs",
      "abstract": "Deep neural networks (DNNs) require expensive training, which is why the protection of model intellectual property (IP) is becoming more critical. Recently, model stealing has emerged frequently, and many researchers design model watermarking and fingerprinting for verifying model ownership. However, attacks such as ambiguity statements have been used to break the current defense, which poses a challenge to model ownership verification. Therefore, this paper proposes an interesting near-boundary data as evidence for obtaining model ownership and innovatively proposes to infer model ownership instead of verifying model ownership. In this paper, we propose to generate the initial near-boundary data using an algorithm that adds slight noise to generate adversarial examples. We design a generator to privatize the near-boundary data. Our main observation is that the near-boundary data exhibit results close to the classification boundary in both the source model and its derived stolen model. At the end of this work, we design many experiments to verify the effectiveness of the proposed method. The experimental results demonstrate that model ownership can be inferred with high confidence. Noting that our method does not require the training data to be private, and it is extremely costly for model stealers to reuse our method.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Zhe Sun",
        "Zongwen Yang",
        "Zhongyu Huang",
        "Yu Zhang",
        "Jianzhong Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/db8f3ab8139fdbe1a073f7b6e0d15395ab2e536f",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "title": "Adversarial machine learning in cybersecurity: Mitigating evolving threats in AI-powered defense systems",
      "abstract": "The increasing integration of artificial intelligence (AI) in cybersecurity has enhanced the ability to detect and mitigate cyber threats in real-time. However, adversarial machine learning (AML) has emerged as a significant challenge, enabling attackers to manipulate AI models and bypass security measures. This study explores the evolving landscape of AML threats and the vulnerabilities they introduce to AI-powered defense systems. The research identifies key adversarial attack techniques, including evasion, poisoning, model inversion, and model extraction, which threaten the integrity and effectiveness of AI-driven cybersecurity mechanisms. This study evaluates various mitigation strategies to address these threats, such as adversarial Training, model hardening, defensive Distillation, and hybrid AI approaches. Through experimental analysis, we assess the robustness of AI defense systems under adversarial attack and measure their effectiveness using key performance metrics, including model accuracy, false positive rates, and computational efficiency. The findings indicate that while adversarial Training improves model resilience, adaptive attack techniques continue to challenge existing defenses, necessitating continuous advancements in cybersecurity frameworks. This research highlights the need for a multi-layered security approach that integrates AI-based anomaly detection, human-AI hybrid security models, and adaptive learning techniques to counter adversarial threats effectively. Additionally, it discusses the broader implications of AML in cybersecurity, including policy considerations, ethical concerns, and future research directions. The study recommends strategies for enhancing AI-powered cyber defense systems to maintain security, reliability, and resilience against evolving adversarial threats.",
      "year": 2023,
      "venue": "World Journal of Advanced Engineering Technology and Sciences",
      "authors": [
        "Ebuka Mmaduekwe Paul",
        "Ugochukwu Mmaduekwe Stanley",
        "Joseph Darko Kessie",
        "Mukhtar Dolapo Salawudeen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/47feadf7b6d9fdc2a9f2339173e967c21554c746",
      "pdf_url": "",
      "publication_date": "2023-12-30",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "title": "SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack",
      "abstract": "Previous studies have revealed that artificial intelligence (AI) systems are vulnerable to adversarial attacks. Among them, model extraction attacks fool the target model by generating adversarial examples on a substitute model. The core of such an attack is training a substitute model as similar to the target model as possible, where the simulation process can be categorized in a data-dependent and data-free manner. Compared with the data-dependent method, the data-free one has been proven to be more practical in the real world since it trains the substitute model with synthesized data. However, the distribution of these fake data lacks diversity and cannot detect the decision boundary of the target model well, resulting in the dissatisfactory simulation effect. Besides, these data-free techniques need a vast number of queries to train the substitute model, increasing the time and computing consumption and the risk of exposure. To solve the aforementioned problems, in this paper, we propose a novel data-free model extraction method named SCME (Self-Contrastive Model Extraction), which considers both the inter- and intra-class diversity in synthesizing fake data. In addition, SCME introduces the Mixup operation to augment the fake data, which can explore the target model's decision boundary effectively and improve the simulating capacity. Extensive experiments show that the proposed method can yield diversified fake data. Moreover, our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks, the SCME outperforms SOTA methods by 11.43\\% on average for five baseline datasets.",
      "year": 2023,
      "venue": "International Conference on Neural Information Processing",
      "authors": [
        "Renyang Liu",
        "Jinhong Zhang",
        "Kwok-Yan Lam",
        "Jun Zhao",
        "Wei Zhou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/43d2aeaf2b8fbb522f50eb05de32758b3678e5ee",
      "pdf_url": "",
      "publication_date": "2023-10-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "title": "Decepticon: Attacking Secrets of Transformers",
      "abstract": "With the growing burden of training deep learning models with huge datasets, transfer learning has been widely adopted (e.g., Transformers like BERT, GPT). Transfer learning significantly reduces the time and effort of model training. However, the security impact of using shared pre-trained models has not been evaluated. In this paper, we provide in-depth characterizations of the fine-tuning process and reveal the security vulnerabilities of transfer-learned models. Then, we show a novel two-level model extraction attack; 1) identifying the pre-trained model of a victim transfer-learned model through model fingerprint collected from off-the-shelf GPUs and 2) extracting the entire weights of the victim black-box model based on the hints in the pre-trained model. The extracted model shows almost alike prediction accuracy with over 94% matching prediction outputs with the victim model. The two-level model extraction enables large model weight extraction that is considered as challenging if not impossible through significantly reduced extraction effort.",
      "year": 2023,
      "venue": "IEEE International Symposium on Workload Characterization",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Fan Yao",
        "Meng Tang",
        "Hyeran Jeon"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e1ec9a011a049115ce3dc45aeb738ad34d726f87",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "title": "Ownership Protection of Generative Adversarial Networks",
      "abstract": "Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process, and they are not robust to emerging model extraction attacks. In this paper, we propose a new ownership protection method based on the common characteristics of a target model and its stolen models. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method can achieve the best protection performance, compared to the state-of-the-art methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, different datasets, as well as adaptive attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/49b98e01423c7b857c931a9d10bec3dc5951dbe2",
      "pdf_url": "http://arxiv.org/pdf/2306.05233",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "349062d82b56fb516b0c7061940470f8d9c256a6",
      "title": "Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems",
      "abstract": "Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\\%$ gain in accuracy and up to $7.7\\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\\%$ and $4.8\\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\\%$). We implement the proposed defense in a real system with plans to open source.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Debopam Sanyal",
        "Jui-Tse Hung",
        "Manavi Agrawal",
        "Prahlad Jasti",
        "Shahab Nikkhoo",
        "S. Jha",
        "Tianhao Wang",
        "Sibin Mohan",
        "Alexey Tumanov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/349062d82b56fb516b0c7061940470f8d9c256a6",
      "pdf_url": "https://arxiv.org/pdf/2307.01292",
      "publication_date": "2023-07-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "title": "Model Stealing Attack against Graph Classification with Authenticity, Uncertainty and Diversity",
      "abstract": "Recent research demonstrates that GNNs are vulnerable to the model stealing attack, a nefarious endeavor geared towards duplicating the target model via query permissions. However, they mainly focus on node classification tasks, neglecting the potential threats entailed within the domain of graph classification tasks. Furthermore, their practicality is questionable due to unreasonable assumptions, specifically concerning the large data requirements and extensive model knowledge. To this end, we advocate following strict settings with limited real data and hard-label awareness to generate synthetic data, thereby facilitating the stealing of the target model. Specifically, following important data generation principles, we introduce three model stealing attacks to adapt to different actual scenarios: MSA-AU is inspired by active learning and emphasizes the uncertainty to enhance query value of generated samples; MSA-AD introduces diversity based on Mixup augmentation strategy to alleviate the query inefficiency issue caused by over-similar samples generated by MSA-AU; MSA-AUD combines the above two strategies to seamlessly integrate the authenticity, uncertainty, and diversity of the generated samples. Finally, extensive experiments consistently demonstrate the superiority of the proposed methods in terms of concealment, query efficiency, and stealing performance.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhihao Zhu",
        "Chenwang Wu",
        "Rui Fan",
        "Yi Yang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/82bf68c3e3aa8b2f8ccea0842e43cbe39a5b2dff",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "78c6861c798e06b5651f45594eb6c8da43708e08",
      "title": "GNMS: A novel method for model stealing based on GAN",
      "abstract": "Many well-performing models are currently deployed on the cloud to provide machine Learning as a service (MLaaS). However, these models are susceptible to Model Stealing Attacks, where attackers can access the model\u2019s functionality, parameters, and internal structure in a black-box. As a result, data-free model stealing methods have gained popularity due to their higher accuracy and not requiring real data. Previous data-free model stealing methods have mainly focused on single scenarios and limited model and dataset variations. In this paper, we introduce a novel generalized network model Stealing method (GNMS), which is suitable for both benchmark and transfer models, achieving high model stealing accuracy across various scenarios. We pre-train generative adversarial network (GAN) using publicly available datasets and efficiently steal model functionality by training a student model with the pre-trained generator and the discriminator. Adversarial samples and the generated image dataset are also used to explore the model\u2019s decision boundaries. During the training of the clone model, we train two clone models to minimize the differences with the target model further. We employ a contrastive learning approach to encourage the models to learn meaningful feature representations by distinguishing between similar and dissimilar data points, thereby enhancing the model\u2019s accuracy. We achieve a model stealing accuracy of 73.02% and 72.93% on more complex datasets CIFAR100 and Caltech101. Surpass the latest DisGUIDE by 3.55% and 2.61%.",
      "year": 2023,
      "venue": "International Conference on Advanced Cloud and Big Data",
      "authors": [
        "Moxuan Zeng",
        "Yangzhong Wang",
        "Yangming Zhang",
        "Jun Niu",
        "Yuqing Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/78c6861c798e06b5651f45594eb6c8da43708e08",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "model stealing",
        "steal model",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "title": "An attack framework for stealing black-box based on active semi-supervised learning",
      "abstract": "Neural network models are commonly used as black-box services, but they are vulnerable to model stealing attacks, where an attacker can train a substitute model with similar performance to the original model by exploiting limited information related to the target model. This can cause significant losses to the owner of the target model and pose a serious security risk. To advance our understanding of neural networks and promote the evolution of model protection mechanisms, we conducted in-depth research on neural network model stealing attacks. In this paper, we propose a black-box stealing attack framework that combines active and semi-supervised learning, even if the target black-box only provides hard-label output, an effective attack can be achieved, generating a substitute model with the same functionality as the black-box. The framework involves selectively querying the most informative samples for black-box labeling using active learning, which significantly reduces the workload of querying the black-box and enables to achieve better performance with fewer training samples. We also apply semi-supervised learning to leverage the abundance of unlabeled data and further improve model performance. We evaluated our method on various data sets and proved that the stealing ability of our method was significantly higher than 3.86%~26.64% other methods when faced with hardlabel black-box with the same number of queries, which can achieve effective black-box function stealing.",
      "year": 2023,
      "venue": "6th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2023)",
      "authors": [
        "Lijun Gao",
        "Yuting Wang",
        "Wenjun Liu",
        "Kai Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f95c674b837576be6698eda1f6ef89cf2d8c37c4",
      "pdf_url": "",
      "publication_date": "2023-08-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers",
      "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also called the online phase, the attacker observes the behavior of the unknown DNN and uses these observations to shortlist the set of final policy estimates. We provide theoretical analysis of the error between the unknown DNN and the estimated one. We also provide numerical results showing the effectiveness of the proposed algorithm.",
      "year": 2023,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Momina Sajid",
        "Yanning Shen",
        "Yasser Shoukry"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "pdf_url": "http://arxiv.org/pdf/2304.13090",
      "publication_date": "2023-04-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "title": "Semantic Awareness Model For Binary Operator Code",
      "abstract": "With the promotion of artificial intelligence in various industries, there have been some organizations and individuals using various means to attack it. Common types of attacks against models include adversarial sample attacks, data poisoning attacks, and model stealing attacks. The above attack methods require a certain understanding of the structure of the model, so it becomes a challenge to restore the category of binary operators from the model.Binary code similarity detection (BCSD) has important applications in code checking, vulnerability detection, and malicious co de analysis. Due to the lack of syntax structure information in binary operator code, it is difficult to determine the type of operator. Recent research has focused on using deep learning models to understand the semantics and structural information of binary code to achieve better results. Recent research has shown that deep learning models, especially natural language processing models, can comprehend the semantics of binary code. In this paper, we propose a method for identifying binary operators which uses a sequence-aware model and a structure-aware model to model binary operators. It combines CNN semantics and Transformer semantics for classification. The evaluation shows that our method can achieve good performance in binary operator classification.",
      "year": 2023,
      "venue": "2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)",
      "authors": [
        "Haichao Gao",
        "Liming Fang",
        "Yang Li",
        "Minghui Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/71ba257fa70f6d8cc5485c5cd8026b8935211814",
      "pdf_url": "",
      "publication_date": "2023-11-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "title": "Image Translation-Based Deniable Encryption against Model Extraction Attack",
      "abstract": "In cloud storage applications, data owners\u2019 original images are usually encrypted before being outsourced to the cloud for preserving data owners\u2019 privacy. However, in deep learning model-based image encryption methods, an adversary can conduct the model extraction attack to reveal the model parameters and thus restore the privacy information by obtaining numerous encrypted images. In this paper, we propose an image translation-based deniable encryption (ITDE) scheme to achieve encryption deniability and defend against model extraction attacks. Differing from traditional encryption methods in which encrypted images are visually meaningless, ITDE applies image translation to generate encrypted images in the form of human faces. Moreover, ITDE provides deniability for data owners to keep the encryption parameters private. To defend against model extraction attacks, the defense mechanism is introduced in our proposed ITDE to preserve deep learning models. Experimental results demonstrate the superiority of our proposed methods in terms of encryption deniability and privacy preservation.",
      "year": 2023,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Yiling Chen",
        "Yuanzhi Yao",
        "Nenghai Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/46ba62fc7b8166cc559e53d52992f2b1fde706eb",
      "pdf_url": "",
      "publication_date": "2023-10-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "title": "Proof of Spacetime as a Defensive Technique Against Model Extraction Attacks",
      "abstract": "\u2014When providing a service that utilizes a machine learning model, the countermeasures against cyber-attacks are required. The model extraction attack is one of the attacks, in which an attacker attempts to replicate the model by obtaining a large number of input-output pairs. While a defense using Proof of Work has already been proposed, an attacker can still conduct model extraction attacks by increasing their computational power. Moreover, this approach leads to unnecessary energy consumption and might not be environmentally friendly. In this paper, the defense method using Proof of Spacetime instead of Proof of Work is proposed to reduce the energy consumption. The Proof of Spacetime is a method to impose spatial and temporal costs on the users of the service. While the Proof of Work makes a user to calculate until permission is granted, the Proof of Spacetime makes a user to keep a result of calculation, so the energy consumption is reduced. Through computer simulations, it was found that systems with Proof of Spacetime, compared to those with Proof of Work, impose 0.79 times the power consumption and 1.07 times the temporal cost on the attackers, while 0.73 times and 0.64 times on the non-attackers. Therefore, the system with Proof of Spacetime can prevent model extraction attacks with lower energy consumption.",
      "year": 2023,
      "venue": "International Journal of Advanced Computer Science and Applications",
      "authors": [
        "Tatsuki Fukuda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "pdf_url": "http://thesai.org/Downloads/Volume14No6/Paper_11-Proof_of_Spacetime_as_a_Defensive_Technique_Against_Model.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "044758b8410c8bec7a542febbb8fb613d666cd1d",
      "title": "Exploring AI Attacks on Hardware Accelerated Targets",
      "abstract": "Artificial Intelligence have become integral to various applications, ranging from image recognition to natural language processing. To meet the increasing demand for real-time and low-power AI inference Hardware accelerated Embedded Systems such as Field Programmable Gate Arrays have emerged as a popular hardware platform. However, the deployment of AI models on Embedded AI Systems introduces new security concerns. AI attacks on such Embedded AI based systems pose significant risks to the integrity, confidentiality, and availability of AI applications. In our experiment, we conducted attacks such as the Model Extraction Attack and Evasion Attack for AI on Embedded Systems. These experimental results highlights the critical importance of implementing strong security measures to protect AI models running on Hardware accelerated Embedded AI Systems, ensuring their ability to withstand potential threats.",
      "year": 2023,
      "venue": "2023 IEEE 2nd International Conference on Data, Decision and Systems (ICDDS)",
      "authors": [
        "Parin Shah",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/044758b8410c8bec7a542febbb8fb613d666cd1d",
      "pdf_url": "",
      "publication_date": "2023-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "title": "Safety or Not? A Comparative Study for Deep Learning Apps on Smartphones",
      "abstract": "Recent years have witnessed an astonishing explosion in the evolution of mobile applications powered by deep learning (DL) technologies. Considering that inference of DL models in the cloud requires transferring user data to server, which is prone to the risk of user privacy leakage, many developers choose to deploy the models on local devices for executing the inference process. However, this also raises a number of other security issues, such as adversarial attacks, model stealing attacks, etc. To explore the security issues that exist in deep learning applications (DL apps), we conducted the first comprehensive comparative study of the top 200 apps in each category on Google Play. We built DLApplnspector, a vulnerability detection tool that combines dynamic and static analysis methods for dissecting apps, which helped us automate our empirical study on real-world mobile DL apps. First, we identify DL apps and extract their models by using DL Checker. Subsequently, Static Scoper is provided to detect encryption and reusability of DL models. Finally, within Dynamic Scoper, we use reverse engineering techniques on the network traffic to parse out the packets and collect side-channel information during application runtime. Our research shows that the majority of developers prefer to use open-source models, with almost 92% of models successfully parsed. This suggests that most models are unprotected. DL apps are more likely to upload user behaviour and collect private data than Non-DL apps. We provide security recommendations for developers and users to address the issues discovered.",
      "year": 2023,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Jin Au-Yeung",
        "Shanshan Wang",
        "Yuchen Liu",
        "Zhenxiang Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f8e2413f8f206b00ee6758debdf7f886d5d6fc69",
      "pdf_url": "",
      "publication_date": "2023-11-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4fb6895f62f27f200b9f9d3556257473e411ba86",
      "title": "Cloning Object Detectors with Limited Access to In-Distribution Samples",
      "abstract": "An object detector identifies and locates objects in images. Object detectors are widely deployed in practice, for example in driver assistance systems. Recently it has been shown that state-of-the-art object detectors based on neural networks can be cloned successfully if the adversary has oracle access to the detector, and if the adversary has access to either: (i) sufficiently many images drawn from the same distribution as the images of the detector's train set, also referred to as in-distribution samples, or (ii) a publicly available generative AI network capable of generating images that are close to in-distribution samples. This paper presents a new cloning attack that uses images from a publicly and freely available dataset, referred to as out-of-distribution samples, and a limited number of in-distribution samples. The new attack includes a strategy for combining in-and out-of-distribution samples during training and a calibration step to better mimic the functionality of the oracle detector. Our experiments show that CenterNet and RetinaNet object detectors trained with the Oxford-IIIT Pet, the WIDER FACE, or the Tsinghua-Tencent 100K dataset can be cloned successfully using images from the ImageNet-1K dataset supplemented with a limited number of in-distribution samples.",
      "year": 2023,
      "venue": "2023 IEEE 13th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)",
      "authors": [
        "Arne Aarts",
        "Wil Michiels",
        "Peter Roelse"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4fb6895f62f27f200b9f9d3556257473e411ba86",
      "pdf_url": "",
      "publication_date": "2023-09-03",
      "keywords_matched": [
        "cloning attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "96d3cd0854085c21595fb8efec867416af44c8b5",
      "title": "NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks",
      "abstract": "Deep neural network (DNN) models have become a critical asset of the model owner as training them requires a large amount of resource (i.e. labeled data). Therefore, many fingerprinting schemes have been proposed to safeguard the intellectual property (IP) of the model owner against model extraction and illegal redistribution. However, previous schemes adopt unnatural images as the fingerprint, such as adversarial examples and noisy images, which can be easily perceived and rejected by the adversary. In this paper, we propose NaturalFinger which generates natural fingerprint with generative adversarial networks (GANs). Besides, our proposed NaturalFinger fingerprints the decision difference areas rather than the decision boundary, which is more robust. The application of GAN not only allows us to generate more imperceptible samples, but also enables us to generate unrestricted samples to explore the decision boundary.To demonstrate the effectiveness of our fingerprint approach, we evaluate our approach against four model modification attacks including adversarial training and two model extraction attacks. Experiments show that our approach achieves 0.91 ARUC value on the FingerBench dataset (154 models), exceeding the optimal baseline (MetaV) over 17\\%.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kan Yang",
        "Kunhao Lai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96d3cd0854085c21595fb8efec867416af44c8b5",
      "pdf_url": "http://arxiv.org/pdf/2305.17868",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences",
      "abstract": "Machine-Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex Machine Learning models available for clients via, e.g., a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property such as sensitive training data, optimised hyperparameters, or learned model parameters. In some cases, adversaries can create a copy of the model with (almost) identical behaviour using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies that address isolated threats have been proposed. To arrive at a comprehensive understanding why these attacks are successful and how they could be holistically defended against, a thorough systematisation of the field of model stealing is necessary. We address this by categorising and comparing model stealing attacks, assessing their performance, and exploring corresponding defence techniques in different settings. We propose a taxonomy for attack and defence approaches and provide guidelines on how to select the right attack or defence strategy based on the goal and available resources. Finally, we analyse which defences are rendered less effective by current attack strategies.",
      "year": 2022,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Daryna Oliynyk",
        "Rudolf Mayer",
        "A. Rauber"
      ],
      "citation_count": 143,
      "url": "https://www.semanticscholar.org/paper/d405b58a8f465d5ba2e91f9541e09760904c11a8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3595292",
      "publication_date": "2022-06-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35ade8553de7259a5e8105bd20a160f045f9d112",
      "title": "Towards Data-Free Model Stealing in a Hard Label Setting",
      "abstract": "Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework11Project Page: https://sites.google.com/view/dfms-hl that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Sunandini Sanyal",
        "Sravanti Addepalli",
        "R. Venkatesh Babu"
      ],
      "citation_count": 104,
      "url": "https://www.semanticscholar.org/paper/35ade8553de7259a5e8105bd20a160f045f9d112",
      "pdf_url": "https://arxiv.org/pdf/2204.11022",
      "publication_date": "2022-04-23",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "title": "Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations",
      "abstract": "In this paper, we propose a novel and practical mechanism to enable the service provider to verify whether a suspect model is stolen from the victim model via model extraction attacks. Our key insight is that the profile of a DNN model's decision boundary can be uniquely characterized by its Universal Adversarial Perturbations (UAPs). UAPs belong to a low-dimensional subspace and piracy models' subspaces are more consistent with victim model's subspace compared with non-piracy model. Based on this, we propose a UAP fingerprinting method for DNN models and train an encoder via contrastive learning that takes fingerprints as inputs, outputs a similarity score. Extensive studies show that our framework can detect model Intellectual Property (IP) breaches with confidence > 99.99 % within only 20 fingerprints of the suspect model. It also has good generalizability across different model architectures and is robust against post-modifications on stolen models.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zirui Peng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Cheng Zhang",
        "Haojin Zhu",
        "Minhui Xue"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/7f005a1b45ff029c9b55dfcea5c83f473733cca4",
      "pdf_url": "https://arxiv.org/pdf/2202.08602",
      "publication_date": "2022-02-17",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks",
      "abstract": "Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, a recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, \\cater can effectively identify the IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Yi Zeng",
        "Lingjuan Lyu",
        "Fangzhao Wu",
        "Jiwei Li",
        "R. Jia"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/823cacd5255f3897a8d29f29a7c7cb8f978bd928",
      "pdf_url": "http://arxiv.org/pdf/2209.08773",
      "publication_date": "2022-09-19",
      "keywords_matched": [
        "imitation attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders",
      "abstract": "Self-supervised learning is an emerging machine learning (ML) paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the ML model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yang Zhang"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/a6cb46a2d7549d82abe893602b9a22b406859ebb",
      "pdf_url": "",
      "publication_date": "2022-01-27",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "pdf_url": "",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders",
      "abstract": "Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.11See our code in https://github.com/zeyangsha/Cont-Steal.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zeyang Sha",
        "Xinlei He",
        "Ning Yu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/7da6c9273a14eb8681824d0c3ee84e05366c5627",
      "pdf_url": "https://arxiv.org/pdf/2201.07513",
      "publication_date": "2022-01-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a58e101142efdd9dd653b41d504152e940096be",
      "title": "Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks",
      "abstract": "An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures. The codes are available at https://github.com/guanjiyang/SAC.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiyang Guan",
        "Jian Liang",
        "R. He"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/0a58e101142efdd9dd653b41d504152e940096be",
      "pdf_url": "https://arxiv.org/pdf/2210.15427",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "title": "Dataset Inference for Self-Supervised Models",
      "abstract": "Self-supervised models are increasingly prevalent in machine learning (ML) since they reduce the need for expensively labeled data. Because of their versatility in downstream applications, they are increasingly used as a service exposed via public APIs. At the same time, these encoder models are particularly vulnerable to model stealing attacks due to the high dimensionality of vector representations they output. Yet, encoders remain undefended: existing mitigation strategies for stealing attacks focus on supervised learning. We introduce a new dataset inference defense, which uses the private training set of the victim encoder model to attribute its ownership in the event of stealing. The intuition is that the log-likelihood of an encoder's output representations is higher on the victim's training data than on test data if it is stolen from the victim, but not if it is independently trained. We compute this log-likelihood using density estimation models. As part of our evaluation, we also propose measuring the fidelity of stolen encoders and quantifying the effectiveness of the theft detection without involving downstream tasks; instead, we leverage mutual information and distance measurements. Our extensive empirical results in the vision domain demonstrate that dataset inference is a promising direction for defending self-supervised models against model stealing.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Adam Dziedzic",
        "Haonan Duan",
        "Muhammad Ahmad Kaleem",
        "Nikita Dhawan",
        "Jonas Guan",
        "Yannis Cattan",
        "Franziska Boenisch",
        "Nicolas Papernot"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/274dbb98c63cdd282eb86b0338bdc3c5dfd9b904",
      "pdf_url": "http://arxiv.org/pdf/2209.09024",
      "publication_date": "2022-09-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f3886b3c22675da54b0d55a5bc16754e3399c979",
      "title": "DualCF: Efficient Model Extraction Attack from Counterfactual Explanations",
      "abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
      "year": 2022,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Yongjie Wang",
        "Hangwei Qian",
        "C. Miao"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/f3886b3c22675da54b0d55a5bc16754e3399c979",
      "pdf_url": "https://arxiv.org/pdf/2205.06504",
      "publication_date": "2022-05-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
      "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
      "abstract": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Adam Dziedzic",
        "Muhammad Ahmad Kaleem",
        "Y. Lu",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/2ee4127a2a6aab51a03305d8a564693181bc6424",
      "pdf_url": "",
      "publication_date": "2022-01-23",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "af72f901d7a0f2edca55ee008c8893ae93b09971",
      "title": "How to Steer Your Adversary: Targeted and Efficient Model Stealing Defenses with Gradient Redirection",
      "abstract": "Model stealing attacks present a dilemma for public machine learning APIs. To protect financial investments, companies may be forced to withhold important information about their models that could facilitate theft, including uncertainty estimates and prediction explanations. This compromise is harmful not only to users but also to external transparency. Model stealing defenses seek to resolve this dilemma by making models harder to steal while preserving utility for benign users. However, existing defenses have poor performance in practice, either requiring enormous computational overheads or severe utility trade-offs. To meet these challenges, we present a new approach to model stealing defenses called gradient redirection. At the core of our approach is a provably optimal, efficient algorithm for steering an adversary's training updates in a targeted manner. Combined with improvements to surrogate networks and a novel coordinated defense strategy, our gradient redirection defense, called GRAD${}^2$, achieves small utility trade-offs and low computational overhead, outperforming the best prior defenses. Moreover, we demonstrate how gradient redirection enables reprogramming the adversary with arbitrary behavior, which we hope will foster work on new avenues of defense.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Mantas Mazeika",
        "B. Li",
        "David A. Forsyth"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/af72f901d7a0f2edca55ee008c8893ae93b09971",
      "pdf_url": "https://arxiv.org/pdf/2206.14157",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "title": "Side-Channel Attack Analysis on In-Memory Computing Architectures",
      "abstract": "In-memory computing (IMC) systems have great potential for accelerating data-intensive tasks such as deep neural networks (DNNs). As DNN models are generally highly proprietary, the neural network architectures become valuable targets for attacks. In IMC systems, since the whole model is mapped on chip and weight memory read can be restricted, the pre-mapped DNN model acts as a \u201cblack box\u201d for users. However, the localized and stationary weight and data patterns may subject IMC systems to other attacks. In this article, we propose a side-channel attack methodology on IMC architectures. We show that it is possible to extract model architectural information from power trace measurements without any prior knowledge of the neural network. We first developed a simulation framework that can emulate the dynamic power traces of the IMC macros. We then performed side-channel leakage analysis to reverse engineer model information such as the stored layer type, layer sequence, output channel/feature size and convolution kernel size from power traces of the IMC macros. Based on the extracted information, full networks can potentially be reconstructed without any knowledge of the neural network. Finally, we discuss potential countermeasures for building IMC systems that offer resistance to these model extraction attack.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Ziyu Wang",
        "Fanruo Meng",
        "Yongmo Park",
        "J. Eshraghian",
        "Wei D. Lu"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/ecfeeca2b8ed651efe27dc45bb8a0e4901e9f756",
      "pdf_url": "https://arxiv.org/pdf/2209.02792",
      "publication_date": "2022-09-06",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "title": "On the Difficulty of Defending Self-Supervised Learning against Model Extraction",
      "abstract": "Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.",
      "year": 2022,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Adam Dziedzic",
        "Nikita Dhawan",
        "Muhammad Ahmad Kaleem",
        "Jonas Guan",
        "Nicolas Papernot"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/8bdb27ba98f457549bb7e03d6aa2d5c54a4de79e",
      "pdf_url": "http://arxiv.org/pdf/2205.07890",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "title": "Efficient Query-based Black-box Attack against Cross-modal Hashing Retrieval",
      "abstract": "Deep cross-modal hashing retrieval models inherit the vulnerability of deep neural networks. They are vulnerable to adversarial attacks, especially for the form of subtle perturbations to the inputs. Although many adversarial attack methods have been proposed to handle the robustness of hashing retrieval models, they still suffer from two problems: (1) Most of them are based on the white-box settings, which is usually unrealistic in practical application. (2) Iterative optimization for the generation of adversarial examples in them results in heavy computation. To address these problems, we propose an Efficient Query-based Black-Box Attack (EQB2A) against deep cross-modal hashing retrieval, which can efficiently generate adversarial examples for the black-box attack. Specifically, by sending a few query requests to the attacked retrieval system, the cross-modal retrieval model stealing is performed based on the neighbor relationship between the retrieved results and the query, thus obtaining the knockoffs to substitute the attacked system. A multi-modal knockoffs-driven adversarial generation is proposed to achieve efficient adversarial example generation. While the entire network training converges, EQB2A can efficiently generate adversarial examples by forward-propagation with only given benign images. Experiments show that EQB2A achieves superior attacking performance under the black-box setting.",
      "year": 2022,
      "venue": "ACM Trans. Inf. Syst.",
      "authors": [
        "Lei Zhu",
        "Tianshi Wang",
        "Jingjing Li",
        "Zheng Zhang",
        "Jialie Shen",
        "Xinhua Wang"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/a471ef4fae2c4748eac0c13c4b434e7f701c3252",
      "pdf_url": "",
      "publication_date": "2022-09-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a4c748225360d89a6b03e1f277daddb64f429bcc",
      "title": "Imitated Detectors: Stealing Knowledge of Black-box Object Detectors",
      "abstract": "Deep neural networks have shown great potential in many practical applications, yet their knowledge is at the risk of being stolen via exposed services (\\eg APIs). In contrast to the commonly-studied classification model extraction, there exist no studies on the more challenging object detection task due to the sufficiency and efficiency of problem domain data collection. In this paper, we for the first time reveal that black-box victim object detectors can be easily replicated without knowing the model structure and training data. In particular, we treat it as black-box knowledge distillation and propose a teacher-student framework named Imitated Detector to transfer the knowledge of the victim model to the imitated model. To accelerate the problem domain data construction, we extend the problem domain dataset by generating synthetic images, where we apply the text-image generation process and provide short text inputs consisting of object categories and natural scenes; to promote the feedback information, we aim to fully mine the latent knowledge of the victim model by introducing an iterative adversarial attack strategy, where we feed victim models with transferable adversarial examples making victim provide diversified predictions with more information. Extensive experiments on multiple datasets in different settings demonstrate that our approach achieves the highest model extraction accuracy and outperforms other model stealing methods by large margins in the problem domain dataset. Our codes can be found at \\urlhttps://github.com/LiangSiyuan21/Imitated-Detectors.",
      "year": 2022,
      "venue": "ACM Multimedia",
      "authors": [
        "Siyuan Liang",
        "Aishan Liu",
        "Jiawei Liang",
        "Longkang Li",
        "Yang Bai",
        "Xiaochun Cao"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/a4c748225360d89a6b03e1f277daddb64f429bcc",
      "pdf_url": "",
      "publication_date": "2022-10-10",
      "keywords_matched": [
        "model stealing",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bf87c3c380802df24628cab8f6dff90b42304f77",
      "title": "Split HE: Fast Secure Inference Combining Split Learning and Homomorphic Encryption",
      "abstract": "This work presents a novel protocol for fast secure inference of neural networks applied to computer vision applications. It focuses on improving the overall performance of the online execution by deploying a subset of the model weights in plaintext on the client's machine, in the fashion of SplitNNs. We evaluate our protocol on benchmark neural networks trained on the CIFAR-10 dataset using SEAL via TenSEAL and discuss runtime and security performances. Empirical security evaluation using Membership Inference and Model Extraction attacks showed that the protocol was more resilient under the same attacks than a similar approach also based on SplitNN. When compared to related work, we demonstrate improvements of 2.5x-10x for the inference time and 14x-290x in communication costs.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "George-Liviu Pereteanu",
        "A. Alansary",
        "Jonathan Passerat-Palmbach"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/bf87c3c380802df24628cab8f6dff90b42304f77",
      "pdf_url": "",
      "publication_date": "2022-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "51b8619bcac38d6f1880a313a4af212aac92f71c",
      "title": "MOVE: Effective and Harmless Ownership Verification via Embedded External Features",
      "abstract": "Currently, deep neural networks (DNNs) are widely adopted in different applications. Despite its commercial values, training a well-performing DNN is resource-consuming. Accordingly, the well-trained model is valuable intellectual property for its owner. However, recent studies revealed the threats of model stealing, where the adversaries can obtain a function-similar copy of the victim model, even when they can only query the model. In this paper, we propose an effective and harmless model ownership verification (MOVE) to defend against different types of model stealing simultaneously, without introducing new security risks. In general, we conduct the ownership verification by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by modifying a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. In particular, we develop our MOVE method under both glass-boxand closed-box settings and analyze its theoretical foundation to provide comprehensive model protection. Extensive experiments on benchmark datasets verify the effectiveness of our method and its resistance to potential adaptive attacks.",
      "year": 2022,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Yiming Li",
        "Linghui Zhu",
        "Xiaojun Jia",
        "Yang Bai",
        "Yong Jiang",
        "Shutao Xia",
        "Xiaochun Cao"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/51b8619bcac38d6f1880a313a4af212aac92f71c",
      "pdf_url": "",
      "publication_date": "2022-08-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "title": "Reverse Engineering Neural Network Folding with Remote FPGA Power Analysis",
      "abstract": "Specialized hardware accelerators in the form of FPGAs are widely being used for neural network implementations. By that, they also become the target of power analysis attacks that try to reverse engineer the embedded secret information, in the form of model parameters. However, most of these attacks assume rather simple implementations, not realistic frameworks. Layer folding is used in such accelerators to optimize the network under given area constraints with various degrees of parallel and sequential operations. In this paper, we show that folding does mislead existing power side-channel attacks on frameworks such as FINN. We show how we can extract the folding parameters successfully and use that information to subsequently also recover the number of neurons\u2013something not reliably possible without knowing the folding information. Following the methodologies of both profiling side-channel attacks and machine learning, our approach can extract the amount of neurons with 98% accuracy on a test device, compared to 44-79% accuracy based on related work under the same test conditions and datasets. Furthermore, we show how a classifier that is based on regression can detect previously unknown parameters, which has not been shown before. To verify our results under different environmental conditions, we test the target device in a climate chamber under various temperature ranges and still reach accuracies of at least 93%.",
      "year": 2022,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Vincent Meyers",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/89f080275bf088de3140625d6ee7c4ffa7a368b9",
      "pdf_url": "",
      "publication_date": "2022-05-15",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e8572d9722e992a770b7c00a8419dda17297a9da",
      "title": "Side-Channel Fuzzy Analysis-Based AI Model Extraction Attack With Information-Theoretic Perspective in Intelligent IoT",
      "abstract": "Accessibility to smart devices provides opportunities for side-channel attacks (SCAs) on artificial intelligent (AI) models in the intelligent Internet of Things (IoT). However, the existing literature exposes some shortcomings: 1) incapability of quantifying and analyzing the leaked information through side channels of the intelligent IoT and 2) inability to devise efficient and accurate SCA algorithms. To address these challenges, we propose a side-channel fuzzy analysis-empowered AI model extraction attack in the intelligent IoT. First, the integrated AI model extraction framework is proposed, including power trace-based structure, execution time-based metaparameters, and hierarchical weight extractions. Then, we develop the information theory-based analysis for the AI model extraction via SCA. We derive a mutual information-enabled quantification method, theoretical lower/upper bounds of information leakage, and the minimum number of attack queries to obtain accurate weights. Furthermore, a fuzzy gray correlation-based multiple-microspace parallel SCA algorithm is proposed to extract model weights in the intelligent IoT. Based on the established information-theoretic analysis model, the proposed fuzzy gray correlation-based SCA algorithm obtains high-precision AI weights. Experimental results, consisting of simulation and real-world experiments, verify that the developed analysis method with the information-theoretic perspective is feasible and demonstrate that the designed fuzzy gray correlation-based SCA algorithm is effective for AI model extraction.",
      "year": 2022,
      "venue": "IEEE transactions on fuzzy systems",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "A. Bashir",
        "Jianhua Li",
        "Jie Wu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/e8572d9722e992a770b7c00a8419dda17297a9da",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "title": "Towards explainable model extraction attacks",
      "abstract": "One key factor able to boost the applications of artificial intelligence (AI) in security\u2010sensitive domains is to leverage them responsibly, which is engaged in providing explanations for AI. To date, a plethora of explainable artificial intelligence (XAI) has been proposed to help users interpret model decisions. However, given its data\u2010driven nature, the explanation itself is potentially susceptible to a high risk of exposing privacy. In this paper, we first show that the existing XAI is vulnerable to model extraction attacks and then present an XAI\u2010aware dual\u2010task model extraction attack (DTMEA). DTMEA can attack a target model with explanation services, that is, it can extract both the classification and explanation tasks of the target model. More specifically, the substitution model extracted by DTMEA is a multitask learning architecture, consisting of a sharing layer and two task\u2010specific layers for classification and explanation. To reveal which explanation technologies are more vulnerable to expose privacy information, we conduct an empirical evaluation of four major explanation types in the benchmark data set. Experimental results show that the attack accuracy of DTMEA outperforms the predicted\u2010only method with up to 1.25%, 1.53%, 9.25%, and 7.45% in MNIST, Fashion\u2010MNIST, CIFAR\u201010, and CIFAR\u2010100, respectively. By exposing the potential threats on explanation technologies, our research offers the insights to develop effective tools that are able to trade off security\u2010sensitive relationships.",
      "year": 2022,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Anli Yan",
        "Ruitao Hou",
        "Xiaozhang Liu",
        "Hongyang Yan",
        "Teng Huang",
        "Xianmin Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb6cf8210bb8035e71557dfb45d0170d909ced1f",
      "pdf_url": "",
      "publication_date": "2022-09-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "title": "On the Effectiveness of Dataset Watermarking",
      "abstract": "In a data-driven world, datasets constitute a significant economic value. Dataset owners who spend time and money to collect and curate the data are incentivized to ensure that their datasets are not used in ways that they did not authorize. When such misuse occurs, dataset owners need technical mechanisms for demonstrating their ownership of the dataset in question. Dataset watermarking provides one approach for ownership demonstration which can, in turn, deter unauthorized use. In this paper, we investigate a recently proposed data provenance method, radioactive data, to assess if it can be used to demonstrate ownership of (image) datasets used to train machine learning (ML) models. The original paper radioactive reported that radioactive data is effective in white-box settings. We show that while this is true for large datasets with many classes, it is not as effective for datasets where the number of classes is low (\u0142eq 30) or the number of samples per class is low (\u0142eq 500). We also show that, counter-intuitively, the black-box verification technique described in radioactive is effective for all datasets used in this paper, even when white-box verification in radioactive is not. Given this observation, we show that the confidence in white-box verification can be improved by using watermarked samples directly during the verification process. We also highlight the need to assess the robustness of radioactive data if it were to be used for ownership demonstration since it is an adversarial setting unlike provenance identification. Compared to dataset watermarking, ML model watermarking has been explored more extensively in recent literature. However, most of the state-of-the-art model watermarking techniques can be defeated via model extraction robustness. We show that radioactive data can effectively survive model extraction attacks, which raises the possibility that it can be used for ML model ownership verification robust against model extraction.",
      "year": 2022,
      "venue": "IWSPA@CODASPY",
      "authors": [
        "Buse Gul",
        "Atli Tekgul",
        "N. Asokan"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/52c28b417cf9e0b11d7d49a2600459a7fa0041e4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3510548.3519376",
      "publication_date": "2022-02-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e6d21d219b1f3321ed2354c229946373d779897a",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box'' access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon. In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Shawn Shan",
        "Wen-Luan Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e6d21d219b1f3321ed2354c229946373d779897a",
      "pdf_url": "https://arxiv.org/pdf/2205.10686",
      "publication_date": "2022-05-21",
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "title": "Clairvoyance: Exploiting Far-field EM Emanations of GPU to \"See\" Your DNN Models through Obstacles at a Distance",
      "abstract": "Deep neural networks (DNNs) are becoming increasingly popular in real-world applications, and they are considered valuable assets of enterprises. In recent years, a number of model extraction attacks have been formulated that can be mounted to successfully steal proprietary DNN models. Nevertheless, previous model extraction attacks require either logical access to the target models or physical access to the victim machines, and thus are not suitable for performing model stealing in scenarios where an outside attacker is in the proximity but at a distance.In this paper, we propose a new model extraction attack named Clairvoyance that exploits certain far-field electromagnetic signals emanated from a GPU to steal DNN models at a distance of several meters away from the victim machine even with some obstacles in-between. Using Clairvoyance, an attacker can effectively deduce DNN architectures (e.g., the number of layers and their types) and layer configurations (e.g., the number of kernels, sizes of layers, and sizes of strides). We use several case studies (e.g., VGG and ResNet) to demonstrate its effectiveness.",
      "year": 2022,
      "venue": "2022 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Sisheng Liang",
        "Zihao Zhan",
        "Fan Yao",
        "Long Cheng",
        "Zhenkai Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/0bdf3349040e5b803e8ca3a9c2cacbd9c840b747",
      "pdf_url": "",
      "publication_date": "2022-05-01",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "title": "Stealthy Inference Attack on DNN via Cache-based Side-Channel Attacks",
      "abstract": "The advancement of deep neural networks (DNNs) motivates the deployment in various domains, including image classification, disease diagnoses, voice recognition, etc. Since some tasks that DNN undertakes are very sensitive, the label information is confidential and contains a commercial value or critical privacy. This paper demonstrates that DNNs also bring a new security threat, leading to the leakage of label information of input instances for the DNN models. In particular, we leverage the cache-based side-channel attack (SCA), i.e., Flush-Reload on the DNN (victim) models, to observe the execution of computation graphs, and create a database of them for building a classifier that the attacker can use to decide the label information of (unknown) input instances for victim models. Then we deploy the cache-based SCA on the same host machine with victim models and deduce the labels with the attacker's classification model to compromise the privacy and confidentiality of victim models. We explore different settings and classification techniques to achieve a high attack success rate of stealing label information from the victim models. Additionally, we consider two attacking scenarios: binary attacking identifies specific sensitive labels and others while multi-class attacking targets recognize all classes victim DNNs provide. Last, we implement the attack on both static DNN models with identical architectures for all inputs and dynamic DNN models with an adaptation of architectures for different inputs to demonstrate the vast existence of the proposed attack, including DenseNet 121, DenseNet 169, VGG 16, VGG 19, MobileNet v1, and MobileNet v2. Our experiment exhibits that MobileNet v1 is the most vulnerable one with 99% and 75.6% attacking success rates for binary and multi-class attacking scenarios, respectively.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Han Wang",
        "Syed Mahbub Hafiz",
        "Kartik Patwari",
        "Chen-Nee Chuah",
        "Zubair Shafiq",
        "H. Homayoun"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/8b2fb5e135323f8c69f11515ea3aceec86e6b66e",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "33158daa5df8197872a06e415f2f277b026d9988",
      "title": "High-Fidelity Model Extraction Attacks via Remote Power Monitors",
      "abstract": "This paper shows the first side-channel attack on neural network (NN) IPs through a remote power monitor. We demonstrate that a remote monitor implemented with time-to-digital converters can be exploited to steal the weights from a hardware implementation of NN inference. Such an attack alleviates the need to have physical access to the target device and thus expands the attack vector to multi-tenant cloud FPGA platforms. Our results quantify the effectiveness of the attack on an FPGA implementation of NN inference and compare it to an attack with physical access. We demonstrate that it is indeed possible to extract the weights using DPA with 25000 traces if the SNR is sufficient. The paper, therefore, motivates secure virtualization-to protect the confidentiality of high-valued NN model IPs in multi-tenant execution environments, platform developers need to employ strong countermeasures against physical side-channel attacks.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Anuj Dubey",
        "Emre Karabulut",
        "Amro Awad",
        "Aydin Aysu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/33158daa5df8197872a06e415f2f277b026d9988",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4c57688fde64650fe767a2ba57341520595b5b13",
      "title": "A Practical Introduction to Side-Channel Extraction of Deep Neural Network Parameters",
      "abstract": "Model extraction is a major threat for embedded deep neural network models that leverages an extended attack surface. Indeed, by physically accessing a device, an adversary may exploit side-channel leakages to extract critical information of a model (i.e., its architecture or internal parameters). Different adversarial objectives are possible including a fidelity-based scenario where the architecture and parameters are precisely extracted (model cloning). We focus this work on software implementation of deep neural networks embedded in a high-end 32-bit microcontroller (Cortex-M7) and expose several challenges related to fidelity-based parameters extraction through side-channel analysis, from the basic multiplication operation to the feed-forward connection through the layers. To precisely extract the value of parameters represented in the single-precision floating point IEEE-754 standard, we propose an iterative process that is evaluated with both simulations and traces from a Cortex-M7 target. To our knowledge, this work is the first to target such an high-end 32-bit platform. Importantly, we raise and discuss the remaining challenges for the complete extraction of a deep neural network model, more particularly the critical case of biases.",
      "year": 2022,
      "venue": "Smart Card Research and Advanced Application Conference",
      "authors": [
        "Raphael Joud",
        "Pierre-Alain Mo\u00ebllic",
        "S. Ponti\u00e9",
        "J. Rigaud"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/4c57688fde64650fe767a2ba57341520595b5b13",
      "pdf_url": "https://arxiv.org/pdf/2211.05590",
      "publication_date": "2022-11-10",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "title": "Model Stealing Defense against Exploiting Information Leak through the Interpretation of Deep Neural Nets",
      "abstract": "Model stealing techniques allow adversaries to create attack models that mimic the functionality of black-box machine learning models, querying only class membership or probability outcomes. Recently, interpretable AI is getting increasing attention, to enhance our understanding of AI models, provide additional information for diagnoses, or satisfy legal requirements. However, it has been recently reported that providing such additional information can make AI models more vulnerable to model stealing attacks. In this paper, we propose DeepDefense, the first defense mechanism that protects an AI model against model stealing attackers exploiting both class probabilities and interpretations. DeepDefense uses a misdirection model to hide the critical information of the original model against model stealing attacks, with minimal degradation on both the class probability and the interpretability of prediction output. DeepDefense is highly applicable for any model stealing scenario since it makes minimal assumptions about the model stealing adversary. In our experiments, DeepDefense shows significantly higher defense performance than the existing state-of-the-art defenses on various datasets and interpreters.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Jeonghyun Lee",
        "Sungmin Han",
        "Sangkyun Lee"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/44b068abddec3e0162670ca15ae1eeb2b247ee72",
      "pdf_url": "https://www.ijcai.org/proceedings/2022/0100.pdf",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
      "abstract": "Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Cato Pauling",
        "Michael Gimson",
        "Muhammed Qaid",
        "Ahmad Kida",
        "Basel Halak"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "pdf_url": "",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8119ab9eb8974693705bde0fa074439da40fda96",
      "title": "HDLock: exploiting privileged encoding to protect hyperdimensional computing models against IP stealing",
      "abstract": "Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC --- an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.",
      "year": 2022,
      "venue": "Design Automation Conference",
      "authors": [
        "Shijin Duan",
        "Shaolei Ren",
        "Xiaolin Xu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/8119ab9eb8974693705bde0fa074439da40fda96",
      "pdf_url": "",
      "publication_date": "2022-03-18",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5639133170aa4f4a598994518d34a9b375494876",
      "title": "A Systematic View of Model Leakage Risks in Deep Neural Network Systems",
      "abstract": "As deep neural networks (DNNs) continue to find applications in ever more domains, the exact nature of the neural network architecture becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. While prior work has explored aspects of the risk associated with model leakage, exactly which parts of the model are most sensitive and how one infers the full architecture of the DNN when nothing is known about the structure a priori are problems that have been left unexplored. In this paper we address this gap, first by presenting a schema for reasoning about model leakage holistically, and then by proposing and quantitatively evaluating DeepSniffer, a novel learning-based model extraction framework that uses no prior knowledge of the victim model. DeepSniffer is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. Taking GPU platforms as a showcase, DeepSniffer performs model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models and that the extracted models significantly improve attempts at crafting adversarial inputs. The DeepSniffer project has been released in https://github.com/xinghu7788/DeepSniffer.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Xiaobing Chen",
        "Lei Deng",
        "Yu Ji",
        "Yufei Ding",
        "Zidong Du",
        "Qi Guo",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/5639133170aa4f4a598994518d34a9b375494876",
      "pdf_url": "https://doi.org/10.1109/tc.2022.3148235",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "title": "DNNCloak: Secure DNN Models Against Memory Side-channel Based Reverse Engineering Attacks",
      "abstract": "As deep neural networks (DNN) expand their attention into various domains and the high cost of training a model, the structure of a DNN model has become a valuable intellectual property and needs to be protected. However, reversing DNN models by exploiting side-channel leakage has been demonstrated in various ways. Even if the model is encrypted and the processing hardware units are trusted, the attacker can still extract the model\u2019s structure and critical parameters through side channels, potentially posing significant commercial risks. In this paper, we begin by analyzing representative memory side-channel attacks on DNN models and identifying the primary causes of leakage. We also find that the full encryption used to protect model parameters could add extensive overhead. Based on our observations, we propose DNNCloak, a lightweight and secure framework aiming at mitigating reverse engineering attacks on common DNN architectures. DNNCloak includes a set of obfuscation schemes that increase the difficulty of reverse-engineering the DNN structure. Additionally, DNNCloak reduces the overhead of full weights encryption with an efficient matrix permutation scheme, resulting in reduced memory access time and enhanced security against retraining attacks on the model parameters. At last, we show how DNNCloak can defend DNN models from side-channel attacks effectively, with minimal performance overhead.",
      "year": 2022,
      "venue": "ICCD",
      "authors": [
        "Yuezhi Che",
        "Rujia Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a0a63d6c89b5925591da1384413d660a4a3e49a6",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "title": "Counteract Side-Channel Analysis of Neural Networks by Shuffling",
      "abstract": "Machine learning is becoming an essential part in almost every electronic device. Implementations of neural networks are mostly targeted towards computational performance or memory footprint. Nevertheless, security is also an important part in order to keep the network secret and protect the intellectual property associated to the network. Especially, since neural network implementations are demonstrated to be vulnerable to side-channel analysis, powerful and computational cheap countermeasures are in demand. In this work, we apply a shuffling countermeasure to a microcontroller implementation of a neural network to prevent side-channel analysis. The countermeasure is effective while the computational overhead is low. We investigate the extensions necessary for our countermeasure, and how shuffling increases the effort for an attack in theory. In addition, we demonstrate the increase in effort for an attacker through experiments on real side-channel measurements. Based on the mechanism of shuffling and our experimental results, we conclude that an attack on a commonly used neural network with shuffling is no longer feasible in a reasonable amount of time.",
      "year": 2022,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manuel Brosch",
        "Matthias Probst",
        "G. Sigl"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b82a66b5bcc78480131cb436d48b605ebcb891ee",
      "pdf_url": "",
      "publication_date": "2022-03-14",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ec461790ee249f9df979ac014243291e3a40794f",
      "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
      "abstract": "A variety of explanation methods have been proposed in recent years to help users gain insights into the results returned by neural networks, which are otherwise complex and opaque black-boxes. However, explanations give rise to potential side-channels that can be leveraged by an adversary for mounting attacks on the system. In particular, post-hoc explanation methods that highlight input dimensions according to their importance or relevance to the result also leak information that weakens security and privacy. In this work, we perform the first systematic characterization of the privacy and security risks arising from various popular explanation techniques. First, we propose novel explanation-guided black-box evasion attacks that lead to 10 times reduction in query count for the same success rate. We show that the adversarial advantage from explanations can be quantified as a reduction in the total variance of the estimated gradient. Second, we revisit the membership information leaked by common explanations. Contrary to observations in prior studies, via our modified attacks we show significant leakage of membership information (above 100% improvement over prior results), even in a much stricter black-box setting. Finally, we study explanation-guided model extraction attacks and demonstrate adversarial gains through a large reduction in query count.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Pengrui Quan",
        "Supriyo Chakraborty",
        "J. Jeyakumar",
        "Mani Srivastava"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/ec461790ee249f9df979ac014243291e3a40794f",
      "pdf_url": "https://arxiv.org/pdf/2206.14004",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "title": "On the Robustness of Dataset Inference",
      "abstract": "Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs. Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods. The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence. Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI (at the cost of incurring some accuracy loss) by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade. Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Sebastian Szyller",
        "Rui Zhang",
        "Jian Liu",
        "N. Asokan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/a46d1f8e5ed6182df8c864e72160372cf2a14b13",
      "pdf_url": "http://arxiv.org/pdf/2210.13631",
      "publication_date": "2022-10-24",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "title": "DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking",
      "abstract": "The functionality of a deep learning (DL) model can be stolen via model extraction where an attacker obtains a surrogate model by utilizing the responses from a prediction API of the original model. In this work, we propose a novel watermarking technique called DynaMarks to protect the intellectual property (IP) of DL models against such model extraction attacks in a black-box setting. Unlike existing approaches, DynaMarks does not alter the training process of the original model but rather embeds watermark into a surrogate model by dynamically changing the output responses from the original model prediction API based on certain secret parameters at inference runtime. The experimental outcomes on Fashion MNIST, CIFAR-10, and ImageNet datasets demonstrate the efficacy of DynaMarks scheme to watermark surrogate models while preserving the accuracies of the original models deployed in edge devices. In addition, we also perform experiments to evaluate the robustness of DynaMarks against various watermark removal strategies, thus allowing a DL model owner to reliably prove model ownership.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Chakraborty",
        "Daniel Xing",
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/53bb321ffc1864f9ed3fc689085f8bed943971a3",
      "pdf_url": "http://arxiv.org/pdf/2207.13321",
      "publication_date": "2022-07-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4908fdc1feff153269670f0f8aac837346042553",
      "title": "Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System",
      "abstract": "The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Zhendong Wang",
        "Xiaoming Zeng",
        "Xulong Tang",
        "Danfeng Zhang",
        "Xingbo Hu",
        "Yang Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4908fdc1feff153269670f0f8aac837346042553",
      "pdf_url": "http://arxiv.org/pdf/2208.13720",
      "publication_date": "2022-08-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "title": "A ThreshoId-ImpIementation-Based Neural-Network Accelerator Securing Model Parameters and Inputs Against Power Side-Channel Attacks",
      "abstract": "Neural network (NN) hardware accelerators are being widely deployed on low-power loT nodes for energy-efficient decision making. Embedded NN implementations can use locally stored proprietary models, and may operate over private inputs (e.g., health monitors with patient-specific biomedical classifiers [6]), which must not be disclosed. Side-channel attacks (SCA) are a major concern in embedded systems where physical access to the operating hardware can allow attackers to recover secret data by exploiting information leakage through power consumption, timing and electromagnetic emissions [1, 7, 8]. As shown in Fig. 34.3.1, SCA on embedded NN implementations can reveal the model parameters [9] as well as the inputs [10]. To address these concerns, we present an energy - efficient ASlC solution for protecting both the model parameters and the input data against power-based SCA.",
      "year": 2022,
      "venue": "IEEE International Solid-State Circuits Conference",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "Samuel H. Fuller",
        "A. Chandrakasan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/d1e08aa5c411179d72cfb971767f53ce8b9ce4ff",
      "pdf_url": "",
      "publication_date": "2022-02-20",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "title": "Model Extraction Attack and Defense on Deep Generative Models",
      "abstract": "The security issues of machine learning have aroused much attention and model extraction attack is one of them. The definition of model extraction attack is that an adversary can collect data through query access to a victim model and train a substitute model with it in order to steal the functionality of the target model. At present, most of the related work has focused on the research of model extraction attack against discriminative models while this paper pays attention to deep generative models. First, considering the difference of an adversary` goals, the attacks are taxonomized into two different types: accuracy extraction attack and fidelity extraction attack and the effect is evaluated by 1-NN accuracy. Attacks among three main types of deep generative models and the influence of the number of queries are also researched. Finally, this paper studies different defensive techniques to safeguard the models according to their architectures.",
      "year": 2022,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Sheng Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/8a435e15c39efebf6cad521c15fff50fcf7e0bfb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/2189/1/012024",
      "publication_date": "2022-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "title": "Protecting Deep Neural Network Intellectual Property with Architecture-Agnostic Input Obfuscation",
      "abstract": "Deep Convolutional Neural Networks (DCNNs) have revolutionized and improved many aspects of modern life. However, these models are increasingly more complex, and training them to perform at desirable levels is difficult undertaking; hence, the trained parameters represent a valuable intellectual property (IP) asset which a motivated attacker may wish to steal. To better protect the IP, we propose a method of lightweight input obfuscation that is undone prior to inference, where input data is obfuscated in order to use the model to specification. Without using the correct key and unlocking sequence, the accuracy of the classifier is reduced to a random guess, thus protecting the input/output interface and mitigating model extraction attacks which rely on such access. We evaluate the system using a VGG-16 network trained on CIFAR-10, and demonstrate that with an incorrect deobfuscation key or sequence, the classification accuracy drops to a random guess, with an inference timing overhead of 4.4% on an Nvidia-based evaluation platform. The system avoids the costs associated with retraining and has no impact on model accuracy for authorized users.",
      "year": 2022,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "pdf_url": "",
      "publication_date": "2022-06-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "eb39dda2df56270599f2a28bc6433c84c1704949",
      "title": "Extracted BERT Model Leaks More Information than You Think!",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
      "year": 2022,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xuanli He",
        "Chen Chen",
        "L. Lyu",
        "Qiongkai Xu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/eb39dda2df56270599f2a28bc6433c84c1704949",
      "pdf_url": "https://arxiv.org/pdf/2210.11735",
      "publication_date": "2022-10-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "title": "Play the Imitation Game: Model Extraction Attack against Autonomous Driving Localization",
      "abstract": "The security of the Autonomous Driving (AD) system has been gaining researchers\u2019 and public\u2019s attention recently. Given that AD companies have invested a huge amount of resources in developing their AD models, e.g., localization models, these models, especially their parameters, are important intellectual property and deserve strong protection. In this work, we examine whether the confidentiality of production-grade Multi-Sensor Fusion (MSF) models, in particular, Error-State Kalman Filter (ESKF), can be stolen from an outside adversary. We propose a new model extraction attack called TaskMaster that can infer the secret ESKF parameters under black-box assumption. In essence, TaskMaster trains a substitutional ESKF model to recover the parameters, by observing the input and output to the targeted AD system. To precisely recover the parameters, we combine a set of techniques, like gradient-based optimization, search-space reduction and multi-stage optimization. The evaluation result on real-world vehicle sensor dataset shows that TaskMaster is practical. For example, with 25 seconds AD sensor data for training, the substitutional ESKF model reaches centimeter-level accuracy, comparing with the ground-truth model.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Qifan Zhang",
        "Junjie Shen",
        "Mingtian Tan",
        "Zhe Zhou",
        "Zhou Li",
        "Qi Alfred Chen",
        "Haipeng Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/54d3e2764c3445c89fbaa9c684c5f5d03cb44254",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3564625.3567977",
      "publication_date": "2022-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f74980d18e194246618eca55faad7858fe57d08c",
      "title": "Leveraging Ferroelectric Stochasticity and In-Memory Computing for DNN IP Obfuscation",
      "abstract": "With the emergence of the Internet of Things (IoT), deep neural networks (DNNs) are widely used in different domains, such as computer vision, healthcare, social media, and defense. The hardware-level architecture of a DNN can be built using an in-memory computing-based design, which is loaded with the weights of a well-trained DNN model. However, such hardware-based DNN systems are vulnerable to model stealing attacks where an attacker reverse-engineers (REs) and extracts the weights of the DNN model. In this work, we propose an energy-efficient defense technique that combines a ferroelectric field effect transistor (FeFET)-based reconfigurable physically unclonable function (PUF) with an in-memory FeFET XNOR to thwart model stealing attacks. We leverage the inherent stochasticity in the FE domains to build a PUF that helps to corrupt the neural network\u2019s (NN) weights when an adversarial attack is detected. We showcase the efficacy of the proposed defense scheme by performing experiments on graph-NNs (GNNs), a particular type of DNN. The proposed defense scheme is a first of its kind that evaluates the security of GNNs. We investigate the effect of corrupting the weights on different layers of the GNN on the accuracy degradation of the graph classification application for two specific error models of corrupting the FeFET-based PUFs and five different bioinformatics datasets. We demonstrate that our approach successfully degrades the inference accuracy of the graph classification by corrupting any layer of the GNN after a small rewrite pulse.",
      "year": 2022,
      "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
      "authors": [
        "Likhitha Mankali",
        "N. Rangarajan",
        "Swetaki Chatterjee",
        "Shubham Kumar",
        "Y. Chauhan",
        "O. Sinanoglu",
        "Hussam Amrouch"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/f74980d18e194246618eca55faad7858fe57d08c",
      "pdf_url": "https://doi.org/10.1109/jxcdc.2022.3217043",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "title": "PUFs Physical Learning: Accelerating the Enrollment via Delay-Based Model Extraction",
      "abstract": "The introduction of Physical Unclonable Functions (PUFs) has been originally motivated by their ability to resist physical attacks, particularly in anti-counterfeiting scenarios. In these one-way functions, machine learning, cryptanalysis, and side-channel attacks are common attack vectors threatening the promised PUF's property of unclonability. These attacks often emulate a PUF by employing a large number of Challenge-Response Pairs (CRPs). Some solutions to defeat such attacks are based on a protocol, where a model of the underlying PUF primitives should be extracted during the enrollment phase. In this article, we introduce a novel physical cloning approach applicable to FPGA-based implementations, which allows extracting the PUF's unique physical characteristics with a few number of Challenge-Response Pairs (CRPs), that increases only linearly for a higher number of PUF components. Indeed, our proposed approach significantly accelerates the enrollment phase and makes complex enrollment protocols feasible. Our core idea relies on an on-chip delay sensor, which can be realized by ordinary FPGA components, measuring the unique characteristic of the PUF elements. We demonstrate the feasibility of our introduced technique by practical experiments on different FPGA platforms, cloning a couple of (complex) PUF constructions, i.e., XOR APUF, iPUF, composed of delay-based Arbiter PUFs.",
      "year": 2022,
      "venue": "IEEE Transactions on Emerging Topics in Computing",
      "authors": [
        "Anita Aghaie",
        "Maik Ender",
        "A. Moradi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2a7c3ecb7a424480687b3fd1c6f4f0b0a04b100a",
      "pdf_url": "",
      "publication_date": "2022-07-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "title": "Enhance Model Stealing Attack via Label Refining",
      "abstract": "With machine learning models being increasingly deployed, model stealing attacks have raised an increasing interest. Extracting decision-based models is a more challenging task with the information of class similarity missing. In this paper, we propose a novel and effective model stealing method as Label Refining via Feature Distance (LRFD), to re-dig the class similarity. Specifically, since the information of class similarity can be represented by the distance between samples from different classes in the feature space, we design a soft label construction module inspired by the prototype learning, and transfer the knowledge in the soft label to the substitution model. Extensive experiments conducted on four widely-used datasets consistently demonstrate that our method yields a model with significantly greater functional similarity to the victim model.",
      "year": 2022,
      "venue": "International Conference on the Software Process",
      "authors": [
        "Yixu Wang",
        "Xianming Lin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1761a6879ea8d5224aabe0e63424042dc3d901d8",
      "pdf_url": "",
      "publication_date": "2022-04-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c082d0fdc703fd06f99a93f4683416f987d335ff",
      "title": "Privacy-Preserving DNN Model Authorization against Model Theft and Feature Leakage",
      "abstract": "Today\u2019s intelligent services are built on well-trained deep neural network (DNN) models, which usually require large private datasets along with a high cost for model training. It consequently makes the model providers cherish the pre-trained DNN models and only distribute them to authorized users. However, malicious users can steal these valuable models for abuse, illegal copy and redistribution. Attackers can also extract private features from even authorized models to leak partial training datasets. They both violate privacy. Existing techniques from secure community attempt to avoid parameter leakage during model authorization but yet cannot solve privacy issues sufficiently. In this paper, we propose a privacy-preserving model authorization approach, AgAuth, to resist the aforementioned privacy threats. We devise a novel scheme called Information-Agnostic Conversion (IAC) for forwarding procedure to eliminate residual features in model parameters. Based on it, we then propose Inference-on-Ciphertext (CiFer) mechanism for DNN reasoning, which includes three stages in each forwarding. The Encrypt phase first converts the proprietary model parameters to demonstrate uniform distribution. The Forward stage per-forms forwarding function without decryption at authorized side. Specifically, this stage just computes over ciphertext. The Decrypt phase finally recovers the information-agnostic outputs to informative output tensor for real-world services. In addition, we implement a prototype and conduct extensive experiments to evaluate its performance. The qualitative and quantitative results demonstrate that our solution AgAuth is privacy-preserving to defend against model theft and feature leakage, without accuracy loss or notable performance decrease.",
      "year": 2022,
      "venue": "ICC 2022 - IEEE International Conference on Communications",
      "authors": [
        "Qiushi Li",
        "Ju Ren",
        "Yuezhi Zhou",
        "Yaoxue Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c082d0fdc703fd06f99a93f4683416f987d335ff",
      "pdf_url": "",
      "publication_date": "2022-05-16",
      "keywords_matched": [
        "model theft"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "title": "A Survey on Side-Channel-based Reverse Engineering Attacks on Deep Neural Networks",
      "abstract": "Hardware side-channels have been exploited to leak sensitive information. With the emergence of deep learning, their hardware platforms have also been scrutinized for side-channel information leakage. It has been shown that the structure, weights, and input samples of deep neural networks (DNN) can all be the victim of reverse engineering attacks that rely on side-channel information leakage. In this paper, we survey existing work on hardware side-channel-based reverse engineering attacks on DNNs as well as the countermeasures.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Yuntao Liu",
        "Michael Zuzak",
        "Daniel Xing",
        "Isaac McDaniel",
        "Priya Mittu",
        "Olsan Ozbay",
        "Abir Akib",
        "Ankur Srivastava"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1e49811d8f55244c2cd7e657ba3cebc5e5ed6dd5",
      "pdf_url": "",
      "publication_date": "2022-06-13",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "title": "Transformer-based Extraction of Deep Image Models",
      "abstract": "Model extraction attacks pose a threat to the security of ML models and to the privacy of the data used for training. Previous research has shown that such attacks can be either monetarily motivated to gain an edge over competitors or maliciously in order to mount subsequent attacks on the extracted model. In this paper, recent advances in the field of transformers are exploited to propose an attack tailored to the task of image classification that allows stealing complex convolutional neural network models without any knowledge of their architecture. The attack was performed on a range of datasets and target architectures to evaluate the robustness of the proposed attack. With only 100k queries, we were able to recover up to 99.2% of the black-box target network's accuracy on the test set. We conclude that it is possible to effectively steal complex neural networks with relatively little expertise and conventional means \u2013 even without knowledge of the target's architecture. Recently proposed defences have also been examined for their effectiveness in preventing the attack proposed in this paper.",
      "year": 2022,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Verena Battis",
        "A. Penner"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9463a4fc601ccdaa81a034f8d443718ec40330d6",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c31794c04c50ec4386110cc9efa206dce344919b",
      "title": "DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks",
      "abstract": "Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim\u2019s architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim\u2019s data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model\u2019s architecture deviates from the victim\u2019s. To accomplish this, DeepTaster generates adversarial images with perturbations, transforms them into the Fourier frequency domain, and uses these transformed images to identify the dataset used in a suspect model. The underlying premise is that adversarial images can capture the unique characteristics of DNNs built with a specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated the effectiveness of DeepTaster by assessing its detection accuracy on three datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures (ResNet18, VGG16, and DenseNet161). We conducted experiments under various attack scenarios, including transfer learning, pruning, fine-tuning, and data augmentation. Specifically, in the Multi-Architecture Attack scenario, DeepTaster was able to identify all the stolen cases across all datasets, while DeepJudge failed to detect any of the cases.",
      "year": 2022,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Seonhye Park",
        "A. Abuadbba",
        "Shuo Wang",
        "Kristen Moore",
        "Yansong Gao",
        "Hyoungshick Kim",
        "Surya Nepal"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c31794c04c50ec4386110cc9efa206dce344919b",
      "pdf_url": "",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16b149e4472f863cfee5999644e37c216900cd01",
      "title": "A Framework for Understanding Model Extraction Attack and Defense",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as-a-Service applications, where prediction services based on well-trained models are offered to users via pay-per-query. The lack of a defense mechanism can impose a high risk on the privacy of the server's model since an adversary could efficiently steal the model by querying only a few `good' data points. The interplay between a server's defense and an adversary's attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user's view and privacy from an adversary's view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the `equilibrium' between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results will be demonstrated by examples and experiments.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16b149e4472f863cfee5999644e37c216900cd01",
      "pdf_url": "https://arxiv.org/pdf/2206.11480",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
      "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chi Hong",
        "Jiyue Huang",
        "L. Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
      "pdf_url": "",
      "publication_date": "2022-01-31",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b7c49323aaa05f3732d0c43767e659c39169f724",
      "title": "Understanding Model Extraction Games",
      "abstract": "The privacy of machine learning models has become a significant concern in many emerging Machine-Learning-as- a-Service applications, where prediction services based on well- trained models are offered to users via the pay-per-query scheme. However, the lack of a defense mechanism can impose a high risk on the privacy of the server\u2019s model since an adversary could efficiently steal the model by querying only a few \u2018good\u2019 data points. The game between a server\u2019s defense and an adversary\u2019s attack inevitably leads to an arms race dilemma, as commonly seen in Adversarial Machine Learning. To study the fundamental tradeoffs between model utility from a benign user\u2019s view and privacy from an adversary\u2019s view, we develop new metrics to quantify such tradeoffs, analyze their theoretical properties, and develop an optimization problem to understand the optimal adversarial attack and defense strategies. The developed concepts and theory match the empirical findings on the \u2018equilibrium\u2019 between privacy and utility. In terms of optimization, the key ingredient that enables our results is a unified representation of the attack-defense problem as a min-max bi-level problem. The developed results are demonstrated by examples and empirical experiments.",
      "year": 2022,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "Xun Xian",
        "Min-Fong Hong",
        "Jie Ding"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b7c49323aaa05f3732d0c43767e659c39169f724",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "772c3b717c10534e58ae3cd3b1b4538ef5454b55",
      "title": "Protecting SRAM PUF from BTI Aging-based Cloning Attack",
      "abstract": "SRAM Physical Unclonable Function (PUF) is currently one of the most popular PUFs, practically adopted in IC productions, to perform security primitives like encryption. However, previous works suggest responses of an SRAM PUF may be changeable due to one of the CMOS aging effects, Bias Temperature Instability (BTI). A physical counterfeit is thereby able to be produced by using BTI to change its responses, based on those of a target PUF. To prevent the BTI-based physical cloning attack, we propose a scheme without any modifications on the current SRAM PUF circuit, which is to pre-charge a challenged cell before it is powered up, so that its response can be affected by those transistors that cannot be precisely aged in the cloning process. We also show security and reliability metrics of SRAM PUFs are not affected by the extra pre-charge phase.",
      "year": 2022,
      "venue": "Symposium on Integrated Circuits and Systems Design",
      "authors": [
        "Shengyu Duan",
        "G. Sai"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/772c3b717c10534e58ae3cd3b1b4538ef5454b55",
      "pdf_url": "",
      "publication_date": "2022-08-22",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b18ab575ec113d117bb2482243c412cceb544756",
      "title": "Data-free Defense of Black Box Models Against Adversarial Attacks",
      "abstract": "Several companies often safeguard their trained deep models (i.e. details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as \u2018black boxes' through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via a generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose \u2018wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our \u2018wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a \u2018regenerator' network with an objective to retrieve the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% against the state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender.",
      "year": 2022,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Gaurav Kumar Nayak",
        "Inder Khatri",
        "Shubham Randive",
        "Ruchit Rawal",
        "Anirban Chakraborty"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b18ab575ec113d117bb2482243c412cceb544756",
      "pdf_url": "https://arxiv.org/pdf/2211.01579",
      "publication_date": "2022-11-03",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "title": "Holistic risk assessment of inference attacks in machine learning",
      "abstract": "As machine learning expanding application, there are more and more unignorable privacy and safety issues. Especially inference attacks against Machine Learning models allow adversaries to infer sensitive information about the target model, such as training data, model parameters, etc. Inference attacks can lead to serious consequences, including violating individuals privacy, compromising the intellectual property of the owner of the machine learning model. As far as concerned, researchers have studied and analyzed in depth several types of inference attacks, albeit in isolation, but there is still a lack of a holistic rick assessment of inference attacks against machine learning models, such as their application in different scenarios, the common factors affecting the performance of these attacks and the relationship among the attacks. As a result, this paper performs a holistic risk assessment of different inference attacks against Machine Learning models. This paper focuses on three kinds of representative attacks: membership inference attack, attribute inference attack and model stealing attack. And a threat model taxonomy is established. A total of 12 target models using three model architectures, including AlexNet, ResNet18 and Simple CNN, are trained on four datasets, namely CelebA, UTKFace, STL10 and FMNIST.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yang Yang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/16af85e1e119a3d5d0e44299cd1a34d27a49568a",
      "pdf_url": "http://arxiv.org/pdf/2212.10628",
      "publication_date": "2022-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87333028a8169e957d1d74b4054ceac9b3d2116a",
      "title": "An Adversarial Learning-based Tor Malware Traffic Detection Model",
      "abstract": "Attackers often use Tor to launch cyberattacks and conduct illegal transactions, threatening cyberspace's security and people's daily lives. Existing methods for malware traffic detection on Tor can be classified as rule-based and network-based, both of which apply machine learning extensively. Tor malware traffic detection systems are often deployed in open network environments. Their machine learning systems are the first to be attacked by adversarial samples. To ensure that Tor is not abused, this paper proposes an Adversarial Learning-based Tor Malware Traffic Detection model, AL-TMTD. We generate realistic attack samples that can evade detection and use these samples to produce an augmented training set for producing hardened detectors. In such a way, we obtain a more resilient Tor malware traffic detection model that achieves adversarial robustness. We validate our proposal through an extensive experimental campaign that considers multiple machine learning algorithms and shadow models. We simulate the adversary to construct functionally approximate shadow models through black-box model extraction and generate adversarial samples to validate the adversarial robustness of our proposed AL-TMTD model. Our experimental results demonstrate that the average accuracy of AL-TMTD after the adversarial retraining is as high as 0.995 in detecting adversarial samples, which is 0.314 without the adversarial retraining, a significant improvement.",
      "year": 2022,
      "venue": "Global Communications Conference",
      "authors": [
        "Xiaoyan Hu",
        "Yishu Gao",
        "Guang Cheng",
        "Hua Wu",
        "Ruidong Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/87333028a8169e957d1d74b4054ceac9b3d2116a",
      "pdf_url": "",
      "publication_date": "2022-12-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "title": "Model Extraction Attack against Self-supervised Speech Models",
      "abstract": "Self-supervised learning (SSL) speech models generate meaningful representations of given clips and achieve incredible performance across various downstream tasks. Model extraction attack (MEA) often refers to an adversary stealing the functionality of the victim model with only query access. In this work, we study the MEA problem against SSL speech model with a small number of queries. We propose a two-stage framework to extract the model. In the first stage, SSL is conducted on the large-scale unlabeled corpus to pre-train a small speech model. Secondly, we actively sample a small portion of clips from the unlabeled corpus and query the target model with these clips to acquire their representations as labels for the small model's second-stage training. Experiment results show that our sampling methods can effectively extract the target model without knowing any information about its model architecture.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tsung-Yuan Hsu",
        "Chen-An Li",
        "Tung-Yu Wu",
        "Hung-yi Lee"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f1f33c9654649947d2dffb02c12fc36c5fd5116e",
      "pdf_url": "https://arxiv.org/pdf/2211.16044",
      "publication_date": "2022-11-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "title": "Matryoshka: Stealing Functionality of Private ML Data by Hiding Models in Model",
      "abstract": "In this paper, we present a novel insider attack called Matryoshka, which employs an irrelevant scheduled-to-publish DNN model as a carrier model for covert transmission of multiple secret models which memorize the functionality of private ML data stored in local data centers. Instead of treating the parameters of the carrier model as bit strings and applying conventional steganography, we devise a novel parameter sharing approach which exploits the learning capacity of the carrier model for information hiding. Matryoshka simultaneously achieves: (i) High Capacity -- With almost no utility loss of the carrier model, Matryoshka can hide a 26x larger secret model or 8 secret models of diverse architectures spanning different application domains in the carrier model, neither of which can be done with existing steganography techniques; (ii) Decoding Efficiency -- once downloading the published carrier model, an outside colluder can exclusively decode the hidden models from the carrier model with only several integer secrets and the knowledge of the hidden model architecture; (iii) Effectiveness -- Moreover, almost all the recovered models have similar performance as if it were trained independently on the private data; (iv) Robustness -- Information redundancy is naturally implemented to achieve resilience against common post-processing techniques on the carrier before its publishing; (v) Covertness -- A model inspector with different levels of prior knowledge could hardly differentiate a carrier model from a normal model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Xudong Pan",
        "Yifan Yan",
        "Sheng Zhang",
        "Mi Zhang",
        "Min Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d62cb1d14a12b128e2d45b22b4239dd6417780a",
      "pdf_url": "http://arxiv.org/pdf/2206.14371",
      "publication_date": "2022-06-29",
      "keywords_matched": [
        "stealing functionality"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "06992347fb403d457fb09063c528fb68a50547c3",
      "title": "Continuous Authentication against Collusion Attacks",
      "abstract": "As mobile devices become more and more popular, users gain many conveniences. It has also made smartphone makers install new software and prebuilt hardware on their products, including many kinds of sensors. With improved storage and computing power, users also become accustomed to storing and interacting with personally sensitive information. Due to convenience and efficiency, mobile devices use gait authentication widely. In recent years, protecting the information security of mobile devices has become increasingly important. It has become a hot research area because smartphones are vulnerable to theft or unauthorized access. This paper proposes a novel attack model called a collusion attack. Firstly, we study the imitation attack in the general state and its results and propose and verify the feasibility of our attack. We propose a collusion attack model and train participants with quantified action specifications. The results demonstrate that our attack increases the attacker\u2019s false match rate only using an acceleration sensor in some systems sensor. Furthermore, we propose a multi-cycle defense model based on acceleration direction changes to improve the robustness of smartphone-based gait authentication methods against such attacks. Experimental results show that our defense model can significantly reduce the attacker\u2019s success rate.",
      "year": 2022,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Pin Lyu",
        "Wandong Cai",
        "Yao Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/06992347fb403d457fb09063c528fb68a50547c3",
      "pdf_url": "https://www.mdpi.com/1424-8220/22/13/4711/pdf?version=1655896310",
      "publication_date": "2022-06-22",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "title": "Adversarial Training of Anti-Distilled Neural Network with Semantic Regulation of Class Confidence",
      "abstract": "Knowledge distillation (KD) has been identified as an effective knowledge transfer approach. By learning from the outputs of a pre-trained, over-parameterized teacher network, a compact student network can be trained efficiently to achieve superior performance. Although KD has gained substantial successes, exposure to pre-trained models usually causes potential risks of intellectual property leaks. From a model stealing attacker\u2019s perspective, one can easily mimic the model functionality via KD, resulting in huge financial loss. In this paper, we propose a novel adversarial training framework called semantic nasty teacher, which prevents the teacher model from being copied by the attacker. In specific, we disentangle the semantic relationship in the output logits when training the teacher model, which is the key to success in KD. Experiment results show that neural networks trained with our approach only sacrifices little performance while canceling out the probability of KD-based model stealing.",
      "year": 2022,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Z. Wang",
        "Chengcheng Li",
        "Husheng Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5e7c67c4e963ed3db69401d34ea5dd5a78f8da45",
      "pdf_url": "",
      "publication_date": "2022-10-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9286db7b087e6ba805d0526aeeeccfccf540069c",
      "title": "TP-NET: Training Privacy-Preserving Deep Neural Networks under Side-Channel Power Attacks",
      "abstract": "Privacy in deep learning is receiving tremendous attention with its wide applications in industry and academics. Recent studies have shown the internal structure of a deep neural network is easily inferred via side-channel power attacks in the training process. To address this pressing privacy issue, we propose TP-NET, a novel solution for training privacy-preserving deep neural networks under side-channel power attacks. The key contribution of TP-NET is the introduction of randomness into the internal structure of a deep neural network and the training process. Specifically, the workflow of TP-NET includes three steps: First, Independent Sub-network Construction, which generates multiple independent sub-networks via randomly se-lecting nodes in each hidden layer. Second, Sub-network Random Training, which randomly trains multiple sub-networks such that power traces keep random in the temporal domain. Third, Prediction, which outputs the predictions made by the most accu-rate sub-network to achieve high classification performance. The performance of TP-NET is evaluated under side-channel power attacks. The experimental results on two benchmark datasets demonstrate that TP-NET decreases the inference accuracy on the number of hidden nodes by at least 38.07% while maintaining competitive classification accuracy compared with traditional deep neural networks. Finally, a theoretical analysis shows that the power consumption of TP-NET depends on the number of sub-networks, the structure of each sub-network, and atomic operations in the training process.",
      "year": 2022,
      "venue": "International Symposium on Smart Electronic Systems",
      "authors": [
        "Hui Hu",
        "Jessa Gegax-Randazzo",
        "Clay Carper",
        "Mike Borowczak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9286db7b087e6ba805d0526aeeeccfccf540069c",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6a82176e62930e8e5bf8ba320e605404be7bf204",
      "title": "Seeds Don't Lie: An Adaptive Watermarking Framework for Computer Vision Models",
      "abstract": "In recent years, various watermarking methods were suggested to detect computer vision models obtained illegitimately from their owners, however they fail to demonstrate satisfactory robustness against model extraction attacks. In this paper, we present an adaptive framework to watermark a protected model, leveraging the unique behavior present in the model due to a unique random seed initialized during the model training. This watermark is used to detect extracted models, which have the same unique behavior, indicating an unauthorized usage of the protected model's intellectual property (IP). First, we show how an initial seed for random number generation as part of model training produces distinct characteristics in the model's decision boundaries, which are inherited by extracted models and present in their decision boundaries, but aren't present in non-extracted models trained on the same data-set with a different seed. Based on our findings, we suggest the Robust Adaptive Watermarking (RAW) Framework, which utilizes the unique behavior present in the protected and extracted models to generate a watermark key-set and verification model. We show that the framework is robust to (1) unseen model extraction attacks, and (2) extracted models which undergo a blurring method (e.g., weight pruning). We evaluate the framework's robustness against a naive attacker (unaware that the model is watermarked), and an informed attacker (who employs blurring strategies to remove watermarked behavior from an extracted model), and achieve outstanding (i.e.,>0.9) AUC values. Finally, we show that the framework is robust to model extraction attacks with different structure and/or architecture than the protected model.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Jacob Shams",
        "Ben Nassi",
        "I. Morikawa",
        "Toshiya Shimizu",
        "A. Shabtai",
        "Y. Elovici"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6a82176e62930e8e5bf8ba320e605404be7bf204",
      "pdf_url": "https://arxiv.org/pdf/2211.13644",
      "publication_date": "2022-11-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "title": "Generative Extraction of Audio Classifiers for Speaker Identification",
      "abstract": "It is perhaps no longer surprising that machine learning models, especially deep neural networks, are particularly vulnerable to attacks. One such vulnerability that has been well studied is model extraction: a phenomenon in which the attacker attempts to steal a victim's model by training a surrogate model to mimic the decision boundaries of the victim model. Previous works have demonstrated the effectiveness of such an attack and its devastating consequences, but much of this work has been done primarily for image and text processing tasks. Our work is the first attempt to perform model extraction on {\\em audio classification models}. We are motivated by an attacker whose goal is to mimic the behavior of the victim's model trained to identify a speaker. This is particularly problematic in security-sensitive domains such as biometric authentication. We find that prior model extraction techniques, where the attacker \\textit{naively} uses a proxy dataset to attack a potential victim's model, fail. We therefore propose the use of a generative model to create a sufficiently large and diverse pool of synthetic attack queries. We find that our approach is able to extract a victim's model trained on \\texttt{LibriSpeech} using queries synthesized with a proxy dataset based off of \\texttt{VoxCeleb}; we achieve a test accuracy of 84.41\\% with a budget of 3 million queries.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Tejumade Afonja",
        "Lucas Bourtoule",
        "Varun Chandrasekaran",
        "Sageev Oore",
        "Nicolas Papernot"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/fcd73d7d8fce02e55b95a5903310667367d8e64f",
      "pdf_url": "http://arxiv.org/pdf/2207.12816",
      "publication_date": "2022-07-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "title": "Verify Deep Learning Models Ownership via Preset Embedding",
      "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
      "year": 2022,
      "venue": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
      "authors": [
        "Wenxuan Yin",
        "Hai-feng Qian"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
      "pdf_url": "",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model stealing",
        "model theft",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e3bc283af5746de4ede17c123d39e02e819edcab",
      "title": "Characterizing Side-Channel Leakage of DNN Classifiers though Performance Counters",
      "abstract": "Rapid advancements in Deep Neural Networks (DNN) have led to their deployment in a wide range of com-mercial applications. DNN classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Like other applications, they have been shown to be vulnerable to side-channel information leakage. There have been several proof-of-concept attacks demon-strating the extraction of their model parameters and input data. However, no prior study has examined the possibility of using side-channels to extract the DNN classifier's decision or output. In this initial study, we aim to understand if there exists a correlation between the output class selected by a classifier and side-channel information collected while running the inference process on a CPU. Our initial evaluation shows that with the proposed approach it is possible to accurately recover the output class for model inputs via multiple side-channels: primarily power, but also branch mispredictions and cache misses.",
      "year": 2022,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Saikat Majumdar",
        "Mohammad Hossein Samavatian",
        "R. Teodorescu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e3bc283af5746de4ede17c123d39e02e819edcab",
      "pdf_url": "",
      "publication_date": "2022-06-27",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "title": "Enhancing Cybersecurity in Edge AI through Model Distillation and Quantization: A Robust and Efficient Approach",
      "abstract": "The rapid proliferation of Edge AI has introduced significant cybersecurity challenges, including adversarial attacks, model theft, and data privacy concerns. Traditional deep learning models deployed on edge devices often suffer from high computational complexity and memory requirements, making them vulnerable to exploitation. This paper explores the integration of model distillation and quantization techniques to enhance the security and efficiency of Edge AI systems. Model distillation reduces model complexity by transferring knowledge from a large, cumbersome model (teacher) to a compact, efficient one (student), thereby improving resilience against adversarial manipulations. \nQuantization further optimizes the student model by reducing bit precision, minimizing attack surfaces while maintaining performance. \nWe present a comprehensive analysis of how these techniques mitigate cybersecurity threats such as model inversion, membership inference, and evasion attacks. Additionally, we evaluate trade-offs between model accuracy, latency, and robustness in resource-constrained edge environments. Experimental results on benchmark datasets demonstrate that distilled and quantized models achieve comparable accuracy to their full-precision counterparts while significantly reducing vulnerability to cyber threats. Our findings highlight the potential of distillation and quantization as key enablers for secure, lightweight, and high-performance Edge AI deployments.",
      "year": 2022,
      "venue": "International Journal for Sciences and Technology",
      "authors": [
        "Mangesh Pujari",
        "Anshul Goel",
        "Ashwin Sharma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a9b48b2100f95ce541899fd4b7aa60d985241f2",
      "pdf_url": "",
      "publication_date": "2022-11-25",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5018c07b5581cf9398173b7311c02ad085b6349e",
      "title": "An Enhanced Adversarial Attacks Method on Power System Based on Model Extraction Algorithm",
      "abstract": "Artificial intelligence algorithms fit connections between features and problems from a data-driven perspective. Artificial intelligence algorithms perform iterative training of data through neural network, which provides a new thinking dimension for researchers. The researches on the power grid field are no longer limited to modeling and analysis through traditional physical mechanism methods. However, there are security risks on the neural network model established by artificial intelligence algorithms. Attackers apply model extraction attacks to structure a substitute model of target power system model, which supports other attack algorithms to attack the power system and ultimately affects the normal operation of power system. This paper proposes an enhancement method for adversarial attacks on power system based on model extraction algorithm and tests the promoting effect of adversarial sample attack in various scenarios.",
      "year": 2022,
      "venue": "2022 IEEE 6th Conference on Energy Internet and Energy System Integration (EI2)",
      "authors": [
        "Yucheng Ma",
        "Qi Wang",
        "Zengji Liu",
        "Chao Hong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5018c07b5581cf9398173b7311c02ad085b6349e",
      "pdf_url": "",
      "publication_date": "2022-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "title": "Revealing Secrets From Pre-trained Models",
      "abstract": "\u2014With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Trans- former models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers \ufb01ne-tune the model with their own datasets. Transfer-learning signi\ufb01cantly reduces the time and effort of training models. However, it comes at the cost of security concerns. In this paper, we show a new observation that pre-trained models and \ufb01ne-tuned models have signi\ufb01cantly high similarities in weight values. Also, we demonstrate that there exist vendor-speci\ufb01c computing patterns even for the same models. With these new \ufb01ndings, we propose a new model extraction attack that reveals the model architecture and the pre-trained model used by the black-box victim model with vendor-speci\ufb01c computing patterns and then estimates the entire model weights based on the weight value similarities between the \ufb01ne-tuned model and pre-trained model. We also show that the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning. ,",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Mujahid Al Rafi",
        "Yuan Feng",
        "Hyeran Jeon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fb6802bbd6f4f67fbaafac41ba31697712b8e525",
      "pdf_url": "http://arxiv.org/pdf/2207.09539",
      "publication_date": "2022-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "title": "Adversarial Attacks Against Network Intrusion Detection in IoT Systems",
      "abstract": "Deep learning (DL) has gained popularity in network intrusion detection, due to its strong capability of recognizing subtle differences between normal and malicious network activities. Although a variety of methods have been designed to leverage DL models for security protection, whether these systems are vulnerable to adversarial examples (AEs) is unknown. In this article, we design a novel adversarial attack against DL-based network intrusion detection systems (NIDSs) in the Internet-of-Things environment, with only black-box accesses to the DL model in such NIDS. We introduce two techniques: 1) model extraction is adopted to replicate the black-box model with a small amount of training data and 2) a saliency map is then used to disclose the impact of each packet attribute on the detection results, and the most critical features. This enables us to efficiently generate AEs using conventional methods. With these tehniques, we successfully compromise one state-of-the-art NIDS, Kitsune: the adversary only needs to modify less than 0.005% of bytes in the malicious packets to achieve an average 94.31% attack success rate.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Han Qiu",
        "Tian Dong",
        "Tianwei Zhang",
        "Jialiang Lu",
        "G. Memmi",
        "Meikang Qiu"
      ],
      "citation_count": 213,
      "url": "https://www.semanticscholar.org/paper/4df5d33673ee1f05d8bca5be3cfc1ed6c18a0745",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
      "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yugeng Liu",
        "Rui Wen",
        "Xinlei He",
        "A. Salem",
        "Zhikun Zhang",
        "M. Backes",
        "Emiliano De Cristofaro",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "citation_count": 152,
      "url": "https://www.semanticscholar.org/paper/9bb14ee43eb7607e0bf95bde3fa62882a676eb5d",
      "pdf_url": "",
      "publication_date": "2021-02-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "title": "Dataset Inference: Ownership Resolution in Machine Learning",
      "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $dataset$ $inference$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Pratyush Maini"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/b3086fbbc678a7616ac390a41945f45e0d0ab001",
      "pdf_url": "",
      "publication_date": "2021-04-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "74f6e70fd9a945b517e6495920a1267c01842bd4",
      "title": "DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories",
      "abstract": "Recent advancements in Deep Neural Networks (DNNs) have enabled widespread deployment in multiple security-sensitive domains. The need for resource-intensive training and the use of valuable domain-specific training data have made these models the top intellectual property (IP) for model owners. One of the major threats to DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. In this work, we propose an advanced model extraction framework DeepSteal that steals DNN weights remotely for the first time with the aid of a memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer-based fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailored for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate the proposed model extraction framework on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNetNGG-11). The extracted substitute model has successfully achieved more than 90% test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model. Notably, it achieves similar performance (i.e., ~1-2% test accuracy under attack) as white-box adversarial input attack (e.g., PGD/Trades).",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. S. Rakin",
        "Md Hafizul Islam Chowdhuryy",
        "Fan Yao",
        "Deliang Fan"
      ],
      "citation_count": 142,
      "url": "https://www.semanticscholar.org/paper/74f6e70fd9a945b517e6495920a1267c01842bd4",
      "pdf_url": "http://arxiv.org/pdf/2111.04625",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2569a7309142e40815cf556b6417059df9abbda8",
      "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark",
      "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "L. Lyu",
        "Fangzhao Wu",
        "Chenguang Wang"
      ],
      "citation_count": 115,
      "url": "https://www.semanticscholar.org/paper/2569a7309142e40815cf556b6417059df9abbda8",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/21321/21070",
      "publication_date": "2021-12-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
      "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "L. Lyu",
        "Qiongkai Xu",
        "Lichao Sun"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.161.pdf",
      "publication_date": "2021-03-18",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "45ea6495958f04c1f02de1741c952dac1154d4f4",
      "title": "UnSplit: Data-Oblivious Model Inversion, Model Stealing, and Label Inference Attacks against Split Learning",
      "abstract": "Training deep neural networks often forces users to work in a distributed or outsourced setting, accompanied with privacy concerns. Split learning aims to address this concern by distributing the model among a client and a server. The scheme supposedly provides privacy, since the server cannot see the clients' models and inputs. We show that this is not true via two novel attacks. (1) We show that an honest-but-curious split learning server, equipped only with the knowledge of the client neural network architecture, can recover the input samples and obtain a functionally similar model to the client model, without being detected. (2) We show that if the client keeps hidden only the output layer of the model to ''protect'' the private labels, the honest-but-curious server can infer the labels with perfect accuracy. We test our attacks using various benchmark datasets and against proposed privacy-enhancing extensions to split learning. Our results show that plaintext split learning can pose serious risks, ranging from data (input) privacy to intellectual property (model parameters), and provide no more than a false sense of security.",
      "year": 2021,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Ege Erdogan",
        "Alptekin K\u00fcp\u00e7\u00fc",
        "A. E. Cicek"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/45ea6495958f04c1f02de1741c952dac1154d4f4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3559613.3563201",
      "publication_date": "2021-08-20",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "title": "Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction",
      "abstract": "We investigate whether model extraction can be used to \u2018steal\u2019 the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.",
      "year": 2021,
      "venue": "ACM Conference on Recommender Systems",
      "authors": [
        "Zhenrui Yue",
        "Zhankui He",
        "Huimin Zeng",
        "Julian McAuley"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/864cf501b5c04bfcdc836a9cf1909a51ac1d2a99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460231.3474275",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7026ec6c12f325aec884ff802812c3c80319f668",
      "title": "Model Stealing Attacks Against Inductive Graph Neural Networks",
      "abstract": "Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary\u2019s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.1",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yun Shen",
        "Xinlei He",
        "Yufei Han",
        "Yang Zhang"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/7026ec6c12f325aec884ff802812c3c80319f668",
      "pdf_url": "https://arxiv.org/pdf/2112.08331",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "title": "Stealing Neural Network Structure Through Remote FPGA Side-Channel Analysis",
      "abstract": "Deep Neural Network (DNN) models have been extensively developed by companies for a wide range of applications. The development of a customized DNN model with great performance requires costly investments, and its structure (layers and hyper-parameters) is considered intellectual property and holds immense value. However, in this paper, we found the model secret is vulnerable when a cloud-based FPGA accelerator executes it. We demonstrate an end-to-end attack based on remote power side-channel analysis and machine-learning-based secret inference against different DNN models. The evaluation result shows that an attacker can reconstruct the layer and hyper-parameter sequence at over 90% accuracy using our method, which can significantly reduce her model development workload. We believe the threat presented by our attack is tangible, and new defense mechanisms should be developed against this threat.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Yicheng Zhang",
        "Rozhin Yasaei",
        "Hao Chen",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/03df500a29fb8717662ea5626afd9e2b22b5d14c",
      "pdf_url": "",
      "publication_date": "2021-02-17",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c3111e374ad14357172ef63e7063e0182f8030d4",
      "title": "Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models",
      "abstract": "Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.",
      "year": 2021,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jialuo Chen",
        "Jingyi Wang",
        "Tinglan Peng",
        "Youcheng Sun",
        "Peng Cheng",
        "S. Ji",
        "Xingjun Ma",
        "Bo Li",
        "D. Song"
      ],
      "citation_count": 79,
      "url": "https://www.semanticscholar.org/paper/c3111e374ad14357172ef63e7063e0182f8030d4",
      "pdf_url": "https://arxiv.org/pdf/2112.05588",
      "publication_date": "2021-12-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c3d0749f519962331d323dd3c4ec1137544d6d03",
      "title": "Fingerprinting Deep Neural Networks - a DeepFool Approach",
      "abstract": "A well-trained deep learning classifier is an expensive intellectual property of the model owner. However, recently proposed model extraction attacks and reverse engineering techniques make model theft possible and similar quality deep learning solution reproducible at a low cost. To protect the interest and revenue of the model owner, watermarking on Deep Neural Network (DNN) has been proposed. However, the extra components and computations due to the embedded watermark tend to interfere with the model training process and result in inevitable degradation in classification accuracy. In this paper, we utilize the geometry characteristics inherited in the DeepFool algorithm to extract data points near the classification boundary of the target model for ownership verification. As the fingerprint is extracted after the training process has been completed, the original achievable classification accuracy will not be compromised. This countermeasure is founded on the hypothesis that different models possess different classification boundaries determined solely by the hyperparameters of the DNN and the training it has undergone. Therefore, given a set of fingerprint data points, a pirated model or its post-processed version will produce similar prediction but another originally designed and trained DNN for the same task will produce very different prediction even if they have similar or better classification accuracy. The effectiveness of the proposed Intellectual Property (IP) protection method is validated on the CIFAR-10, CIFAR-100 and ImageNet datasets. The results show a detection rate of 100% and a false positive rate of 0% for each dataset. More importantly, the fingerprint extraction and its run time are both dataset independent. It is on average ~130\u00d7 faster than two state-of-the-art fingerprinting methods.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Si Wang",
        "Chip-Hong Chang"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/c3d0749f519962331d323dd3c4ec1137544d6d03",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/147023/2/2021021379.pdf",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5beafcd6c0222a2f45863058280873c1ef088ec4",
      "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks",
      "abstract": "Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called \"Simulator\", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online 1.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Chen Ma",
        "Li Chen",
        "Junhai Yong"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/5beafcd6c0222a2f45863058280873c1ef088ec4",
      "pdf_url": "https://arxiv.org/pdf/2009.00960",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "title": "MEGEX: Data-Free Model Extraction Attack Against Gradient-Based Explainable AI",
      "abstract": "Explainable AI encourages machine learning applications in the real world, whereas data-free model extraction attacks (DFME), in which an adversary steals a trained machine learning model by creating input queries with generative models instead of collecting training data, have attracted attention as a serious threat. In this paper, we propose MEGEX, a data-free model extraction attack against explainable AI that provides gradient-based explanations for inference results, and investigate whether the gradient-based explanations increase the vulnerability to the data-free model extraction attacks. In MEGEX, an adversary leverages explanations by Vanilla Gradient as derivative values for training a generative model. We prove that MEGEX is identical to white-box data-free knowledge distillation, whereby the adversary can train the generative model with the exact gradients. Our experiments show that the adversary in MEGEX can steal highly accurate models - 0.98\u00d7, 0.91\u00d7, and 0.96\u00d7 the victim model accuracy on SVHN, Fashion-MNIST, and CIFAR-10 datasets given 1.5M, 5M, 20M queries, respectively. In addition, we also apply sophisticated gradient-based explanations, i.e., SmoothGrad and Integrated Gradients, to MEGEX. The experimental results indicate that these explanations are potential countermeasures to MEGEX. We also found that the accuracy of the model stolen by the adversary depends on the diversity of query inputs by the generative model.",
      "year": 2021,
      "venue": "SecTL@AsiaCCS",
      "authors": [
        "T. Miura",
        "Toshiki Shibahara",
        "Naoto Yanai"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/f4847ab95c43b9d3c214d494ce3852473bf297e4",
      "pdf_url": "https://arxiv.org/pdf/2107.08909",
      "publication_date": "2021-07-19",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e32ab49974b032faeca3804685ba739f5fb09790",
      "title": "Trustworthy and Intelligent COVID-19 Diagnostic IoMT Through XR and Deep-Learning-Based Clinic Data Access",
      "abstract": "This article presents a novel extended reality (XR) and deep-learning-based Internet-of-Medical-Things (IoMT) solution for the COVID-19 telemedicine diagnostic, which systematically combines virtual reality/augmented reality (AR) remote surgical plan/rehearse hardware, customized 5G cloud computing and deep learning algorithms to provide real-time COVID-19 treatment scheme clues. Compared to existing perception therapy techniques, our new technique can significantly improve performance and security. The system collected 25 clinic data from the 347 positive and 2270 negative COVID-19 patients in the Red Zone by 5G transmission. After that, a novel auxiliary classifier generative adversarial network-based intelligent prediction algorithm is conducted to train the new COVID-19 prediction model. Furthermore, The Copycat network is employed for the model stealing and attack for the IoMT to improve the security performance. To simplify the user interface and achieve an excellent user experience, we combined the Red Zone\u2019s guiding images with the Green Zone\u2019s view through the AR navigate clue by using 5G. The XR surgical plan/rehearse framework is designed, including all COVID-19 surgical requisite details that were developed with a real-time response guaranteed. The accuracy, recall, F1-score, and area under the ROC curve (AUC) area of our new IoMT were 0.92, 0.98, 0.95, and 0.98, respectively, which outperforms the existing perception techniques with significantly higher accuracy performance. The model stealing also has excellent performance, with the AUC area of 0.90 in Copycat slightly lower than the original model. This study suggests a new framework in the COVID-19 diagnostic integration and opens the new research about the integration of XR and deep learning for IoMT implementation.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Yonghang Tai",
        "Bixuan Gao",
        "Qiong Li",
        "Zhengtao Yu",
        "Chunsheng Zhu",
        "Victor I. Chang"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/e32ab49974b032faeca3804685ba739f5fb09790",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6488907/9585129/09343340.pdf",
      "publication_date": "2021-02-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "title": "QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval",
      "abstract": "We study the query-based attack against image retrieval to evaluate its robustness against adversarial examples under the black-box setting, where the adversary only has query access to the top-k ranked unlabeled images from the database. Compared with query attacks in image classification, which produce adversaries according to the returned labels or confidence score, the challenge becomes even more prominent due to the difficulty in quantifying the attack effectiveness on the partial retrieved list. In this paper, we make the first attempt in Query-based Attack against Image Retrieval (QAIR), to completely subvert the top-k retrieval results. Specifically, a new relevance-based loss is designed to quantify the attack effects by measuring the set similarity on the top-k retrieval results before and after attacks and guide the gradient optimization. To further boost the attack efficiency, a recursive model stealing method is proposed to acquire transferable priors on the target model and generate the prior-guided gradients. Comprehensive experiments show that the proposed attack achieves a high attack success rate with few queries against the image retrieval systems under the black-box setting. The attack evaluations on the real-world visual search engine show that it successfully deceives a commercial system such as Bing Visual Search with 98% attack success rate by only 33 queries on average.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xiaodan Li",
        "Jinfeng Li",
        "Yuefeng Chen",
        "Shaokai Ye",
        "Yuan He",
        "Shuhui Wang",
        "Hang Su",
        "Hui Xue"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/446b07b8aaaaa75f0352be89bd97498ee5f69e36",
      "pdf_url": "https://arxiv.org/pdf/2103.02927",
      "publication_date": "2021-03-04",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "title": "InverseNet: Augmenting Model Extraction Attacks with Training Data Inversion",
      "abstract": "Cloud service providers, including Google, Amazon, and Alibaba, have now launched machine-learning-as-a-service (MLaaS) platforms, allowing clients to access sophisticated cloud-based machine learning models via APIs. Unfortunately, however, the commercial value of these models makes them alluring targets for theft, and their strategic position as part of the IT infrastructure of many companies makes them an enticing springboard for conducting further adversarial attacks. In this paper, we put forth a novel and effective attack strategy, dubbed InverseNet, that steals the functionality of black-box cloud-based models with only a small number of queries. The crux of the innovation is that, unlike existing model extraction attacks that rely on public datasets or adversarial samples, InverseNet constructs inversed training samples to increase the similarity between the extracted substitute model and the victim model. Further, only a small number of data samples with high confidence scores (rather than an entire dataset) are used to reconstruct the inversed dataset, which substantially reduces the attack cost. Extensive experiments conducted on three simulated victim models and Alibaba Cloud's commercially-available API demonstrate that InverseNet yields a model with significantly greater functional similarity to the victim model than the current state-of-the-art attacks at a substantially lower query budget.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Guanghao Mei",
        "Qian Wang"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/18da8b08b89646dc18d6734cac6d0222c9239cbf",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0336.pdf",
      "publication_date": "2021-08-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "title": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey",
      "abstract": "During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "M. M. Real",
        "R. Salvador"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/c1cd3ef5bd9439de1f138ab5200a2c9cecf362af",
      "pdf_url": "https://www.mdpi.com/2076-3417/11/15/6790/pdf?version=1627954373",
      "publication_date": "2021-07-23",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57d5abaa047a0401874383628c646918527d4df2",
      "title": "Monitoring-Based Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this article, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2021,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/57d5abaa047a0401874383628c646918527d4df2",
      "pdf_url": "",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c58266e1180a848310ff09143a71a4ba18e1ca86",
      "title": "S-MAPS: Scalable Mutual Authentication Protocol for Dynamic UAV Swarms",
      "abstract": "Unmanned Aerial Vehicles (UAVs) domain has seen rapid developments in recent years. UAVs have been deployed for many applications and missions like data transmission, cellular service provisioning, and computational offloading tasks etc. Yet, UAV deployment is still limited, partially owing to the security challenges it poses. UAVs are particularly vulnerable to physical capture, cloning attacks, eavesdropping, and man in the middle attacks. To address some of these security problems, this paper develops an authentication protocol for use in UAV swarms. To ensure physical security and rapid authentication, the proposed protocol uses Physical Unclonable Functions (PUFs). The protocol achieves high scalability compared to the state of the art by authenticating multiple devices at once. The proposed protocol supports dynamic topologies and multi-hop communication by using spanning tree-based traversal. It is also resistant to mobility, device tampering attack, etc., and its improvements are achieved at significantly lower communication and communication cost as compared to state-of-the-art protocols.",
      "year": 2021,
      "venue": "IEEE Transactions on Vehicular Technology",
      "authors": [
        "Gaurang Bansal",
        "B. Sikdar"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/c58266e1180a848310ff09143a71a4ba18e1ca86",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3b357acb6c807bc694372c773e507cdcef27d974",
      "title": "Leaky Nets: Recovering Embedded Neural Network Models and Inputs Through Simple Power and Timing Side-Channels\u2014Attacks and Defenses",
      "abstract": "With the recent advancements in machine learning theory, many commercial embedded microprocessors use neural network (NN) models for a variety of signal processing applications. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. But, many of these attacks involve specific assumptions, have limited applicability, or pose huge overheads to the attacker. In this work, we study the side-channel vulnerabilities of embedded NN implementations by recovering their parameters using timing-based information leakage and simple power analysis side-channel attacks. We demonstrate our attacks on popular microcontroller platforms over networks of different precisions, such as floating point, fixed point, and binary networks. We are able to successfully recover not only the model parameters but also the inputs for the above networks. Countermeasures against timing-based attacks are implemented and their overheads are analyzed.",
      "year": 2021,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Saurav Maji",
        "Utsav Banerjee",
        "A. Chandrakasan"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/3b357acb6c807bc694372c773e507cdcef27d974",
      "pdf_url": "",
      "publication_date": "2021-02-23",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack",
      "abstract": "Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \\emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
      "year": 2021,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yixu Wang",
        "Jie Li",
        "Hong Liu",
        "Yongjian Wu",
        "Rongrong Ji"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/5179302656cf46cfbcc9cce2b70d4b0c758f7d7c",
      "pdf_url": "http://arxiv.org/pdf/2105.00623",
      "publication_date": "2021-05-03",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "title": "Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define fidelity and accuracy on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of fidelity extraction and accuracy extraction, according to the adversary\u2019s goals and background knowledge. We further conduct a case study where the adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/a7f5460b2f2b1a215064e9a0669dd3d43a382af9",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3485832.3485838",
      "publication_date": "2021-12-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "title": "SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries",
      "abstract": "Given black-box access to the prediction API, model extraction attacks can steal the functionality of models deployed in the cloud. In this paper, we introduce the SEAT detector, which detects black-box model extraction attacks so that the defender can terminate malicious accounts. SEAT has a similarity encoder trained by adversarial training. Using the similarity encoder, SEAT detects accounts that make queries that indicate a model extraction attack in progress and cancels these accounts. We evaluate our defense against existing model extraction attacks and against new adaptive attacks introduced in this paper. Our results show that even against adaptive attackers, SEAT increases the cost of model extraction attacks by 3.8 times to 16 times.",
      "year": 2021,
      "venue": "AISec@CCS",
      "authors": [
        "Zhanyuan Zhang",
        "Yizheng Chen",
        "David A. Wagner"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/d167f82f1f1bec0dd173fe3173053eebddae05b8",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3474369.3486863",
      "publication_date": "2021-11-15",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "title": "Watermarking Graph Neural Networks based on Backdoor Attacks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary.This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. We also explore our watermarking mechanism against an adaptive attacker with access to partial knowledge of the watermarked data. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks.",
      "year": 2021,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Jing Xu",
        "S. Picek"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "pdf_url": "https://repository.ubn.ru.nl//bitstream/handle/2066/295585/295585.pdf",
      "publication_date": "2021-10-21",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "title": "Secure Automatic Speaker Verification (SASV) System Through sm-ALTP Features and Asymmetric Bagging",
      "abstract": "The growing number of voice-enabled devices and applications consider automatic speaker verification (ASV) a fundamental component. However, maximum outreach for ASV in critical domains e.g., financial services and health care, is not possible unless we overcome security breaches caused by voice cloning algorithms and replayed audios. Therefore, to overcome these vulnerabilities, a secure ASV (SASV) system based on the novel sign modified acoustic local ternary pattern (sm-ALTP) features and asymmetric bagging-based classifier-ensemble with enhanced attack vector is presented. The proposed audio representation approach clusters the high and low frequency components in audio frames by normally distributing frequency components against a convex function. Then, the neighborhood statistics are applied to capture the user specific vocal tract information. The proposed SASV system simultaneously verifies the bonafide speakers and detects the voice cloning attack, cloning algorithm used to synthesize cloned audio (in the defined settings), and voice-replay attacks over the ASVspoof 2019 dataset. In addition, the proposed method detects the voice replay and cloned voice replay attacks over the VSDC dataset. Both the voice cloning algorithm detection and cloned-replay attack detection are novel concepts introduced in this paper. The voice cloning algorithm detection module determines the voice cloning algorithm used to generate the fake audios. Whereas, the cloned voice replay attack detection is performed to determine the SASV behavior when audio samples are simultaneously contemplated with cloning and replay artifacts.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Muteb Aljasem",
        "Aun Irtaza",
        "Hafiz Malik",
        "Noushin Saba",
        "A. Javed",
        "K. Malik",
        "Mohammad Meharmohammadi"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/6583239ed1f9637d4eea8c5563aa13d719b712d5",
      "pdf_url": "https://doi.org/10.1109/tifs.2021.3082303",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "36654420b9a635bb860c2fc571a3fcc0a5398013",
      "title": "Nowhere to Hide: Efficiently Identifying Probabilistic Cloning Attacks in Large-Scale RFID Systems",
      "abstract": "Radio-Frequency Identification (RFID) is an emerging technology which has been widely applied in various scenarios, such as tracking, object monitoring, and social networks, etc. Cloning attacks can severely disturb the RFID systems, such as missed detection for the missing tags. Although there are some techniques with physical architecture design or complicated encryption and cryptography proposed to prevent the tags from being cloned, it is difficult to definitely avoid the cloning attack. Therefore, cloning attack detection and identification are critical for the RFID systems. Prior works rely on that each clone tag will reply to the reader when its corresponding genuine tag is queried. In this article, we consider a more general attack model, in which each clone tag replies to the reader\u2019s query with a predefined probability, i.e., attack probability. We concentrate on identifying the tags being attacked with the probability no less than a threshold $P_{t}$ with the required identification reliability $\\alpha $ . We first propose a basic protocol to Identify the Probabilistic Cloning Attacks with required identification reliability for the large-scale RFID systems called IPCA. Then we propose two enhanced protocols called MS-IPCA and S-IPCA respectively to improve the identification efficiency. We theoretically analyze the parameters of the proposed IPCA, MS-IPCA and S-IPCA protocols to maximize the identification efficiency. Finally we conduct extensive simulations to validate the effectiveness of the proposed protocols.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xin Ai",
        "Honglong Chen",
        "Kai Lin",
        "Zhibo Wang",
        "Jiguo Yu"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/36654420b9a635bb860c2fc571a3fcc0a5398013",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "title": "Preventing DNN Model IP Theft via Hardware Obfuscation",
      "abstract": "Training accurate deep learning (DL) models require large amounts of training data, significant work in labeling the data, considerable computing resources, and substantial domain expertise. In short, they are expensive to develop. Hence, protecting these models, which are valuable storehouses of intellectual properties (IP), against model stealing/cloning attacks is of paramount importance. Today\u2019s mobile processors feature Neural Processing Units (NPUs) to accelerate the execution of DL models. DL models executing on NPUs are vulnerable to hyperparameter extraction via side-channel attacks and model parameter theft via bus monitoring attacks. This paper presents a novel solution to defend against DL IP theft in NPUs during model distribution and deployment/execution via lightweight, keyed model obfuscation scheme. Unauthorized use of such models results in inaccurate classification. In addition, we present an ideal end-to-end deep learning trusted system composed of: 1) model distribution via hardware root-of-trust and public-key cryptography infrastructure (PKI) and 2) model execution via low-latency memory encryption. We demonstrate that our proposed obfuscation solution achieves IP protection objectives without requiring specialized training or sacrificing the model\u2019s accuracy. In addition, the proposed obfuscation mechanism preserves the output class distribution while degrading the model\u2019s accuracy for unauthorized parties, covering any evidence of a hacked model.",
      "year": 2021,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Brunno F. Goldstein",
        "Vinay C. Patil",
        "V. C. Ferreira",
        "A. S. Nery",
        "F. Fran\u00e7a",
        "S. Kundu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c6d4602482ca710f01d4a07e8d6638b8af59f856",
      "pdf_url": "",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model stealing",
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "645e890034126dfc08484d3509b3493e85fbd603",
      "title": "Stateful Detection of Model Extraction Attacks",
      "abstract": "Machine-Learning-as-a-Service providers expose machine learning (ML) models through application programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit these APIs to extract good approximations of such ML models, by querying them with samples of their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made by users of such a service, to detect model extraction attacks. Harnessing the latent distributions learned by a modified variational autoencoder, VarDetect robustly separates three types of attacker samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect deployed as an automated defense mechanism, the extracted substitute models are found to exhibit poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers with prior knowledge of the deployment of VarDetect, are detected by it.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Kanade",
        "S. Shevade"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/645e890034126dfc08484d3509b3493e85fbd603",
      "pdf_url": "",
      "publication_date": "2021-07-12",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "title": "NeurObfuscator: A Full-stack Obfuscation Tool to Mitigate Neural Architecture Stealing",
      "abstract": "Neural network stealing attacks have posed grave threats to neural network model deployment. Such attacks can be launched by extracting neural architecture information, such as layer sequence and dimension parameters, through leaky side-channels. To mitigate such attacks, we propose NeurObfuscator, a full-stack obfuscation tool to obfuscate the neural network architecture while preserving its functionality with very limited performance overhead. At the heart of this tool is a set of obfuscating knobs, including layer branching, layer widening, selective fusion and schedule pruning, that increase the number of operators, reduce/increase the latency, and number of cache and DRAM accesses. A genetic algorithm-based approach is adopted to orchestrate the combination of obfuscating knobs to achieve the best obfuscating effect on the layer sequence and dimension parameters so that the architecture information cannot be successfully extracted. Results on sequence obfuscation show that the proposed tool obfuscates a ResNet-18 ImageNet model to a totally different architecture (with 44 layer difference) without affecting its functionality with only 2% overall latency overhead. For dimension obfuscation, we demonstrate that an example convolution layer with 64 input and 128 output channels can be obfuscated to generate a layer with 207 input and 93 output channels with only a 2% latency overhead.",
      "year": 2021,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Jingtao Li",
        "Zhezhi He",
        "A. S. Rakin",
        "Deliang Fan",
        "C. Chakrabarti"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/12b0e9a2185229b5f2139cd5f9c9b26d34a01b70",
      "pdf_url": "https://arxiv.org/pdf/2107.09789",
      "publication_date": "2021-07-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning",
      "abstract": "Transfer learning has become a common solution to address training data scarcity in practice. It trains a specified student model by reusing or fine-tuning early layers of a well-trained teacher model that is usually publicly available. However, besides utility improvement, the transferred public knowledge also brings potential threats to model confidentiality, and even further raises other security and privacy issues. In this paper, we present the first comprehensive investigation of the teacher model exposure threat in the transfer learning context, aiming to gain a deeper insight into the tension between public knowledge and model confidentiality. To this end, we propose a teacher model fingerprinting attack to infer the origin of a student model, i.e., the teacher model it transfers from. Specifically, we propose a novel optimization-based method to carefully generate queries to probe the student model to realize our attack. Unlike existing model reverse engineering approaches, our proposed fingerprinting method neither relies on fine-grained model outputs, e.g., posteriors, nor auxiliary information of the model architecture or training dataset. We systematically evaluate the effectiveness of our proposed attack. The empirical results demonstrate that our attack can accurately identify the model origin with few probing queries. Moreover, we show that the proposed attack can serve as a stepping stone to facilitating other attacks against machine learning models, such as model stealing.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yufei Chen",
        "Chao Shen",
        "Cong Wang",
        "Yang Zhang"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/c725eaed79d84ba73caaea3ae4d0e7298d3592d1",
      "pdf_url": "",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "title": "Student Surpasses Teacher: Imitation Attack for Black-Box NLP APIs",
      "abstract": "Machine-learning-as-a-service (MLaaS) has attracted millions of users to their splendid large-scale models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we conduct unsupervised domain adaptation and multi-victim ensemble to showing that attackers could potentially surpass victims, which is beyond previous understanding of model extraction. Extensive experiments on both benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models on transferred domains. We consider our work as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.",
      "year": 2021,
      "venue": "International Conference on Computational Linguistics",
      "authors": [
        "Qiongkai Xu",
        "Xuanli He",
        "L. Lyu",
        "Lizhen Qu",
        "Gholamreza Haffari"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/975e8d7065161d3dc0020ef343aa1db2a3db5a7b",
      "pdf_url": "",
      "publication_date": "2021-08-29",
      "keywords_matched": [
        "model extraction",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "title": "Betalogger: Smartphone Sensor-based Side-channel Attack Detection and Text Inference Using Language Modeling and Dense MultiLayer Neural Network",
      "abstract": "With the recent advancement of smartphone technology in the past few years, smartphone usage has increased on a tremendous scale due to its portability and ability to perform many daily life tasks. As a result, smartphones have become one of the most valuable targets for hackers to perform cyberattacks, since the smartphone can contain individuals\u2019 sensitive data. Smartphones are embedded with highly accurate sensors. This article proposes BetaLogger, an Android-based application that highlights the issue of leaking smartphone users\u2019 privacy using smartphone hardware sensors (accelerometer, magnetometer, and gyroscope). BetaLogger efficiently infers the typed text (long or short) on a smartphone keyboard using Language Modeling and a Dense Multi-layer Neural Network (DMNN). BetaLogger is composed of two major phases: In the first phase, Text Inference Vector is given as input to the DMNN model to predict the target labels comprising the alphabet, and in the second phase, sequence generator module generate the output sequence in the shape of a continuous sentence. The outcomes demonstrate that BetaLogger generates highly accurate short and long sentences, and it effectively enhances the inference rate in comparison with conventional machine learning algorithms and state-of-the-art studies.",
      "year": 2021,
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "authors": [
        "A. R. Javed",
        "S. Rehman",
        "M. Khan",
        "M. Alazab",
        "H. Khan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/73fad5cae8101ea2c32f918f180f8cead6b75c2d",
      "pdf_url": "http://qspace.qu.edu.qa/bitstream/10576/37656/2/3460392.pdf",
      "publication_date": "2021-06-23",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "title": "Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack",
      "abstract": "Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinyi Zhang",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/aa6389843232b205bf6d494f00f2cfdeaa633cd8",
      "pdf_url": "",
      "publication_date": "2021-04-13",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "13d626a08050930fbf1cb072938d9de87e742fee",
      "title": "Extraction of Binarized Neural Network Architecture and Secret Parameters Using Side-Channel Information",
      "abstract": "In recent years, neural networks have been applied to various applications. To speed up the evaluation, a method using binarized network weights has been introduced, facilitating extremely efficient hardware implementation. Using electromagnetic (EM) side-channel analysis techniques, this study presents a framework of model extraction from practical binarized neural network (BNN) hardware. The target BNN hardware is generated and synthesized using open-source and commercial high-level synthesis tools GUINNESS and Xilinx SDSoC, respectively. With the hardware implemented on an up-to-date FPGA chip, we demonstrate how the layers can be identified from a single EM trace measured during the network evaluation, and we also demonstrate how an attacker may use side-channel attacks to recover secret weights used in the network.",
      "year": 2021,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Ville Yli-M\u00e4yry",
        "Akira Ito",
        "N. Homma",
        "S. Bhasin",
        "Dirmanto Jap"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/13d626a08050930fbf1cb072938d9de87e742fee",
      "pdf_url": "",
      "publication_date": "2021-05-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "b408a5e1a60c3afd5954613516126b6c512a7804",
      "title": "Model Extraction and Defenses on Generative Adversarial Networks",
      "abstract": "Model extraction attacks aim to duplicate a machine learning model through query access to a target model. Early studies mainly focus on discriminative models. Despite the success, model extraction attacks against generative models are less well explored. In this paper, we systematically study the feasibility of model extraction attacks against generative adversarial networks (GANs). Specifically, we first define accuracy and fidelity on model extraction attacks against GANs. Then we study model extraction attacks against GANs from the perspective of accuracy extraction and fidelity extraction, according to the adversary's goals and background knowledge. We further conduct a case study where an adversary can transfer knowledge of the extracted model which steals a state-of-the-art GAN trained with more than 3 million images to new domains to broaden the scope of applications of model extraction attacks. Finally, we propose effective defense techniques to safeguard GANs, considering a trade-off between the utility and security of GAN models.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Hailong Hu",
        "Jun Pang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/b408a5e1a60c3afd5954613516126b6c512a7804",
      "pdf_url": "",
      "publication_date": "2021-01-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "title": "Stealing Neural Network Models through the Scan Chain: A New Threat for ML Hardware",
      "abstract": "Stealing trained machine learning (ML) models is a new and growing concern due to the model's development cost. Existing work on ML model extraction either applies a mathematical attack or exploits hardware vulnerabilities such as side-channel leakage. This paper shows a new style of attack, for the first time, on ML models running on embedded devices by abusing the scan-chain infrastructure. We illustrate that having course-grained scan-chain access to non-linear layer outputs is sufficient to steal ML models. To that end, we propose a novel small-signal analysis inspired attack that applies small perturbations into the input signals, identifies the quiescent operating points and, selectively activates certain neurons. We then couple this with a Linear Constraint Satisfaction based approach to efficiently extract model parameters such as weights and biases. We conduct our attack on neural network inference topologies defined in earlier works, and we automate our attack. The results show that our attack outperforms mathematical model extraction proposed in CRYPTO 2020, USENIX 2020, and ICML 2020 by an increase in accuracy of $2^{20.7}\\times, 2^{50.7}\\times$, and $2^{33.9}\\times$, respectively, and a reduction in queries by $2^{6.5}\\times, 2^{4.6}\\times$, and $2^{14.2}\\times$, respectively.",
      "year": 2021,
      "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "S. Potluri",
        "Aydin Aysu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0b498f9f76310da0ccb0cce7ae26ee1039769b89",
      "pdf_url": "",
      "publication_date": "2021-11-01",
      "keywords_matched": [
        "model extraction",
        "extract model",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "S. Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "pdf_url": "",
      "publication_date": "2021-10-12",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "title": "Stealing Machine Learning Parameters via Side Channel Power Attacks",
      "abstract": "Side-channel attacks target system implementation statistics, such as the electricity required to run a cryptographic function. Deriving cryptographic keys, such as AES keys, has become such a simplified process that extracting sensitive information from an otherwise secure algorithm requires only a $35USD microcontroller. While cryptographic algorithms indicate the presence of sensitive data, making them a preferable target, other systems hold valuable data with significantly less protection. Due to the ubiquity and rigidity of machine learning algorithms, the ability to infer model parameters has drastic security implications. This investigation extracted information from machine learning models through the use of traditional side-channel techniques. Specifically, a side-channel power analysis was performed using a ChipWhisperer Lite to extract information from Neural Networks and Linear Regression models running on a target microcontroller. Then, time series classification tasks were performed on the resultant power traces to determine the differences between the two models and their varied hyperparameters. Three such classification tasks were tested. In the first, a neural network was differentiated from a linear regression model with 100% accuracy. In the second, two neural networks with different sized hidden layers are classified with 97.92% accuracy. In the third, two virtually identical linear regression models are compared that differ only in the initial value of one hyperparameter. These models were only classified with 67.92% accuracy. Although the accuracy decreases as the models become more alike, these results indicate that machine learning model parameters can be inferred from power-based side-channel attacks.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Shaya Wolf",
        "Hui Hu",
        "Rafer Cooley",
        "Mike Borowczak"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/6c2159b6efaab83678f98f68ec782f1ed8c8b36d",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "stealing machine learning"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "title": "Ownership Verification of DNN Architectures via Hardware Cache Side Channels",
      "abstract": "Deep Neural Networks (DNN) are gaining higher commercial values in computer vision applications, e.g., image classification, video analytics, etc. This calls for urgent demands of the intellectual property (IP) protection of DNN models. In this paper, we present a novel watermarking scheme to achieve the ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we claim the model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such watermarks can be extracted via side-channel-based model extraction techniques with high fidelity. We conduct comprehensive experiments on watermarked CNN models for image classification tasks and the experimental results show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations and adaptive attacks.",
      "year": 2021,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Xiaoxuan Lou",
        "Shangwei Guo",
        "Jiwei Li",
        "Tianwei Zhang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0e7081f0f76faabda1f9c6497fc9a83b1c051c71",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/159773/2/main.pdf",
      "publication_date": "2021-02-06",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "43c6ffef36a8acda9da688a0fca324abdb64498b",
      "title": "Model Reverse-Engineering Attack against Systolic-Array-Based DNN Accelerator Using Correlation Power Analysis",
      "abstract": "SUMMARY A model extraction attack is a security issue in deep neural networks (DNNs). Information on a trained DNN model is an attractive target for an adversary not only in terms of intellectual property but also of security. Thus, an adversary tries to reveal the sensitive information contained in the trained DNN model from machine-learning services. Previous studies on model extraction attacks assumed that the victim provides a machine-learning cloud service and the adversary accesses the service through formal queries. However, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the sensitive information contained in the implemented DNN model. We call these physical model extraction attacks model reverse-engineering (MRE) attacks to distinguish them from attacks on cloud services. Power side-channel analyses are often used in MRE attacks to reveal the internal operation from power consumption or electromagnetic leakage. Previous studies, including ours, evaluated MRE attacks against several types of DNN processors with power side-channel analyses. In this paper, information leakage from a systolic array which is used for the matrix multiplication unit in the DNN processors is evaluated. We utilized correlation power analysis (CPA) for the MRE attack and reveal weight parameters of a DNN model from the systolic array. Two types of the systolic array were implemented on \ufb01eld-programmable gate array (FPGA) to demonstrate that CPA reveals weight parameters from those systolic arrays. In addition, we applied an extended analysis approach called \u201cchain CPA\u201d for robust CPA analysis against the systolic arrays. Our experimental results indicate that an adversary can reveal trained model parameters from a DNN accelerator even if the DNN model parameters in the o \ufb00 -chip bus are protected with data encryption. Countermeasures against side-channel leaks will be important for implementing a DNN accelerator on a FPGA or application-speci\ufb01c integrated circuit (ASIC).",
      "year": 2021,
      "venue": "IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences",
      "authors": [
        "Kota Yoshida",
        "M. Shiozaki",
        "S. Okura",
        "Takaya Kubota",
        "T. Fujino"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/43c6ffef36a8acda9da688a0fca324abdb64498b",
      "pdf_url": "https://doi.org/10.1587/transfun.2020cip0024",
      "publication_date": "2021-01-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "fe139f3d17dc9460332c198be352080b964ea939",
      "title": "Yes We can: Watermarking Machine Learning Models beyond Classification",
      "abstract": "Since machine learning models have become a valuable asset for companies, watermarking techniques have been developed to protect the intellectual property of these models and prevent model theft. We observe that current watermarking frameworks solely target image classification tasks, neglecting a considerable part of machine learning techniques. In this paper, we propose to address this lack and study the watermarking process of various machine learning techniques such as machine translation, regression, binary image classification and reinforcement learning models. We adapt current definitions to each specific technique and we evaluate the main characteristics of the watermarking process, in particular the robustness of the models against a rational adversary. We show that watermarking models beyond classification is possible while preserving their overall performance. We further investigate various attacks and discuss the importance of the performance metric in the verification process and its impact on the success of the adversary.",
      "year": 2021,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "Sofiane Lounici",
        "M. Njeh",
        "Orhan Ermis",
        "Melek \u00d6nen",
        "S. Trabelsi"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/fe139f3d17dc9460332c198be352080b964ea939",
      "pdf_url": "https://hal.archives-ouvertes.fr/hal-03220793/file/publi-6532.pdf",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "title": "An Exact Poly-Time Membership-Queries Algorithm for Extraction a three-Layer ReLU Network",
      "abstract": "We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons. These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Amit Daniely",
        "Elad Granot"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5b6a6a46caac1316f268a1e6643c10b16fdaef0f",
      "pdf_url": "",
      "publication_date": "2021-05-20",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "373936d00c4a357579c4d375de0ce439e4e54d5f",
      "title": "Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs",
      "abstract": "The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Chen Chen",
        "Xuanli He",
        "Lingjuan Lyu",
        "Fangzhao Wu"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f",
      "pdf_url": "",
      "publication_date": "2021-05-23",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "title": "Power-based Attacks on Spatial DNN Accelerators",
      "abstract": "With proliferation of DNN-based applications, the confidentiality of DNN model is an important commercial goal. Spatial accelerators, which parallelize matrix/vector operations, are utilized for enhancing energy efficiency of DNN computation. Recently, model extraction attacks on simple accelerators, either with a single processing element or running a binarized network, were demonstrated using the methodology derived from differential power analysis (DPA) attack on cryptographic devices. This article investigates the vulnerability of realistic spatial accelerators using general, 8-bit, number representation. We investigate two systolic array architectures with weight-stationary dataflow: (1) a 3 \u00d7 1 array for a dot-product operation and (2) a 3 \u00d7 3 array for matrix-vector multiplication. Both are implemented on the SAKURA-G FPGA board. We show that both architectures are ultimately vulnerable. A conventional DPA succeeds fully on the 1D array, requiring 20K power measurements. However, the 2D array exhibits higher security even with 460K traces. We show that this is because the 2D array intrinsically entails multiple MACs simultaneously dependent on the same input. However, we find that a novel template-based DPA with multiple profiling phases is able to fully break the 2D array with only 40K traces. Corresponding countermeasures need to be investigated for spatial DNN accelerators.",
      "year": 2021,
      "venue": "ACM Journal on Emerging Technologies in Computing Systems",
      "authors": [
        "Ge Li",
        "Mohit Tiwari",
        "M. Orshansky"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/596aee932d0dd5ac1f634ea0a83c9e8f18bda65a",
      "pdf_url": "https://arxiv.org/pdf/2108.12579",
      "publication_date": "2021-08-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "title": "SCANet: Securing the Weights With Superparamagnetic-MTJ Crossbar Array Networks",
      "abstract": "Deep neural networks (DNNs) form a critical infrastructure supporting various systems, spanning from the iPhone neural engine to imaging satellites and drones. The design of these neural cores is often proprietary or a military secret. Nevertheless, they remain vulnerable to model replication attacks that seek to reverse engineer the network\u2019s synaptic weights. In this article, we propose SCANet (Superparamagnetic-MTJ Crossbar Array Networks), a novel defense mechanism against such model stealing attacks by utilizing the innate stochasticity in superparamagnets. When used as the synapse in DNNs, superparamagnetic magnetic tunnel junctions (s-MTJs) are shown to be significantly more secure than prior memristor-based solutions. The thermally induced telegraphic switching in the s-MTJs is robust and uncontrollable, thus thwarting the attackers from obtaining sensitive data from the network. Using a mixture of both superparamagnetic and conventional MTJs in the neural network (NN), the designer can optimize the time period between the weight updation and the power consumed by the system. Furthermore, we propose a modified NN architecture that can prevent replication attacks while minimizing power consumption. We investigate the effect of the number of layers in the deep network and the number of neurons in each layer on the sharpness of accuracy degradation when the network is under attack. We also explore the efficacy of SCANet in real-time scenarios, using a case study on object detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Dinesh Rajasekharan",
        "N. Rangarajan",
        "Satwik Patnaik",
        "O. Sinanoglu",
        "Y. Chauhan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/c092ed52816d6060929bbb40021dbfdc88ba28b7",
      "pdf_url": "",
      "publication_date": "2021-12-15",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "16d98de120fc1e55000e65a285684c4dc7a04807",
      "title": "Efficiently Learning Any One Hidden Layer ReLU Network From Queries",
      "abstract": "Model extraction attacks have renewed interest in the classic problem of learning neural networks from queries. In this work we give the first polynomial-time algorithm for learning arbitrary one hidden layer neural networks activations provided black-box access to the network. Formally, we show that if $F$ is an arbitrary one hidden layer neural network with ReLU activations, there is an algorithm with query complexity and running time that is polynomial in all parameters that outputs a network $F'$ achieving low square loss relative to $F$ with respect to the Gaussian measure. While a number of works in the security literature have proposed and empirically demonstrated the effectiveness of certain algorithms for this problem, ours is the first with fully polynomial-time guarantees of efficiency even for worst-case networks (in particular our algorithm succeeds in the overparameterized setting).",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sitan Chen",
        "Adam R. Klivans",
        "Raghu Meka"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/16d98de120fc1e55000e65a285684c4dc7a04807",
      "pdf_url": "",
      "publication_date": "2021-11-08",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "25800c4599b2e834899a180d77e093fb9282d97e",
      "title": "HODA: Hardness-Oriented Detection of Model Extraction Attacks",
      "abstract": "Model extraction attacks exploit the target model\u2019s prediction API to create a surrogate model, allowing the adversary to steal or reconnoiter the functionality of the target model in the black-box setting. Several recent studies have shown that a data-limited adversaries with no or limited access to the samples from the target model\u2019s training data distribution, can employ synthesized or semantically similar samples to conduct model extraction attacks. In this paper, we introduce the concept of hardness degree to characterize sample difficulty based on the concept of learning speed. The hardness degree of a sample depends on the epoch number at which the predicted label for that sample converges. We investigate the hardness degree of samples and demonstrate that the hardness degree histogram of a data-limited adversary\u2019s sample sequence is differs significantly from that of benign users\u2019 sample sequences. We propose Hardness-Oriented Detection Approach (HODA) to detect the sample sequences of model extraction attacks. Our results indicate that HODA can effectively detect model extraction attack sequences with a high success rate, using only 100 monitored samples. It outperforms all previously proposed methods for model extraction detection.",
      "year": 2021,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "A. M. Sadeghzadeh",
        "Amir Mohammad Sobhanian",
        "F. Dehghan",
        "R. Jalili"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/25800c4599b2e834899a180d77e093fb9282d97e",
      "pdf_url": "https://arxiv.org/pdf/2106.11424",
      "publication_date": "2021-06-21",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2c47435498b8e67873cce1cc86cfbf8397b18286",
      "title": "Timing Black-Box Attacks: Crafting Adversarial Examples through Timing Leaks against DNNs on Embedded Devices",
      "abstract": "Deep neural networks (DNNs) have been applied to various industries. In particular, DNNs on embedded devices have attracted considerable interest because they allow real-time and distributed processing on site. However, adversarial examples (AEs), which add small perturbations to the input data of DNNs to cause misclassification, are serious threats to DNNs. In this paper, a novel black-box attack is proposed to craft AEs based only on processing time, i.e., the side-channel leaks from DNNs on embedded devices. Unlike several existing black-box attacks that utilize output probability, the proposed attack exploits the relationship between the number of activated nodes and processing time without using training data, model architecture, parameters, substitute models, or output probability. The perturbations for AEs are determined by the differential processing time based on the input data of the DNNs in the proposed attack. The experimental results show that the AEs of the proposed attack effectively cause an increase in the number of activated nodes and the misclassification of one of the incorrect labels against the DNNs on a microcontroller unit. Moreover, these results indicate that the attack can evade gradient-masking and confidence reduction countermeasures, which conceal the output probability, to prevent the crafting of AEs against several black-box attacks. Finally, the countermeasures against the attack are implemented and evaluated to clarify that the implementation of an activation function with data-dependent timing leaks is the cause of the proposed attack.",
      "year": 2021,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Tsunato Nakai",
        "Daisuke Suzuki",
        "T. Fujino"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2c47435498b8e67873cce1cc86cfbf8397b18286",
      "pdf_url": "https://doi.org/10.46586/tches.v2021.i3.149-175",
      "publication_date": null,
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "title": "Side-Channel based Disassembler for AVR Micro-Controllers using Convolutional Neural Networks",
      "abstract": "Reverse engineering using Side Channel Attacks (SCA) have been known as a serious menace against embedded devices. The attacker could employ side channel data to retrieve some sensitive information from the device, security analysis, existence of a library in the device or execution of a special stream of codes. Side channel data could be gathered from the power consumption or electromagnetic radiations by the device. In this paper, we propose a disassembler to extract the instructions of the device under attack. A deep convolutional neural network is employed to make templates of the target to use it for real-time scenarios. Short Time Fourier Transform (STFT), and Mel-Frequency Cepstrum Coefficients (MFCC) are utilized as feature extractors. The proposed method consists of two different parts: 1) Hierarchical scenario and 2) Sole model. Atmel 8-bit AVR micro-controller is employed as the target device under attack. Our results indicate that, even with an experimental and low cost setup a vast number of instructions are detectable. The proposed method reaches 98.21% accuracy on the real code, outperforms state-of-the-art methods on the proposed dataset.",
      "year": 2021,
      "venue": "ISC Conference on Information Security and Cryptology",
      "authors": [
        "Pouya Narimani",
        "M. Akhaee",
        "Seyedamin Habibi"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/5df6b233b3e5eeb0f0da34158cbb53f34bb0960c",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ce9109dd491603364f6e6059d28757b52fafd5d",
      "title": "A Review of Confidentiality Threats Against Embedded Neural Network Models",
      "abstract": "Utilization of Machine Learning (ML) algorithms, especially Deep Neural Network (DNN) models, becomes a widely accepted standard in many domains more particularly IoT-based systems. DNN models reach impressive performances in several sensitive fields such as medical diagnosis, smart transport or security threat detection, and represent a valuable piece of Intellectual Property. Over the last few years, a major trend is the large-scale deployment of models in a wide variety of devices. However, this migration to embedded systems is slowed down because of the broad spectrum of attacks threatening the integrity, confidentiality and availability of embedded models. In this review, we cover the landscape of attacks targeting the confidentiality of embedded DNN models that may have a major impact on critical IoT systems, with a particular focus on model extraction and data leakage. We highlight the fact that Side-Channel Analysis (SCA) is a relatively unexplored bias by which model\u2019s confidentiality can be compromised. Input data, architecture or parameters of a model can be extracted from power or electromagnetic observations, testifying a real need from a security point of view.",
      "year": 2021,
      "venue": "World Forum on Internet of Things",
      "authors": [
        "Raphael Joud",
        "P. Moellic",
        "R\u00e9mi Bernhard",
        "J. Rigaud"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/0ce9109dd491603364f6e6059d28757b52fafd5d",
      "pdf_url": "https://cea.hal.science/cea-04176698/file/A_Review_of_Confidentiality_Threats_Against_Embedded_Neural_Network_Models.pdf",
      "publication_date": "2021-05-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "title": "Neural Network Stealing via Meltdown",
      "abstract": "Deep learning services are now deployed in various fields on top of cloud infrastructures. In such cloud environment, virtualization technology provides logically independent and isolated computing space for each tenant. However, recent studies demonstrate that by leveraging vulnerabilities of virtualization techniques and shared processor architectures in the cloud system, various side-channels can be established between cloud tenants. In this paper, we propose a novel attack scenario that can steal internal information of deep learning models by exploiting the Meltdown vulnerability in a multitenant system environment. On the basis of our experiment, the proposed attack method could extract internal information of a TensorFlow deep learning service with 92.875% accuracy and 1.325kB/s extraction speed.",
      "year": 2021,
      "venue": "International Conference on Information Networking",
      "authors": [
        "Hoyong Jeong",
        "Dohyun Ryu",
        "Junbeom Hur"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0d2e0d3c8fb930a9a577ec60152c219995cc6b10",
      "pdf_url": "",
      "publication_date": "2021-01-13",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information",
      "abstract": "Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.",
      "year": 2021,
      "venue": "International Conference on Artificial Neural Networks",
      "authors": [
        "Tommy Li",
        "Cory E. Merkel"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/0fb60c664408a7bf5b503a772449dd8ff8f57876",
      "pdf_url": "",
      "publication_date": "2021-06-15",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "title": "First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data",
      "abstract": "Model extraction attacks are a kind of attacks where an adversary obtains a machine learning model whose performance is comparable with one of the victim model through queries and their results. This paper presents a novel model extraction attack, named TEMPEST, applicable on tabular data under a practical data-free setting. Whereas model extraction is more challenging on tabular data due to normalization, TEMPEST no longer needs initial samples that previous attacks require; instead, it makes use of publicly available statistics to generate query samples. Experiments show that our attack can achieve the same level of performance as the previous attacks. Moreover, we identify that the use of mean and variance as statistics for query generation and the use of the same normalization process as the victim model can improve the performance of our attack. We also discuss a possibility whereby TEMPEST is executed in the real world through an experiment with a medical diagnosis dataset. We plan to release the source code for reproducibility and a reference to subsequent works.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Masataka Tasumi",
        "Kazuki Iwahana",
        "Naoto Yanai",
        "Katsunari Shishido",
        "Toshiya Shimizu",
        "Yuji Higuchi",
        "I. Morikawa",
        "Jun Yajima"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/6981af0aa5d7bee0c1f0c3c685824077140f6be1",
      "pdf_url": "",
      "publication_date": "2021-09-30",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "title": "Tenet: A Neural Network Model Extraction Attack in Multi-core Architecture",
      "abstract": "As neural networks (NNs) are being widely deployed in many cloud-oriented systems for safety-critical tasks, the privacy and security of NNs become significant concerns to users in the cloud platform that shares the computation infrastructure such as memory resource. In this work, we observed that the memory timing channel in the shared memory of cloud multi-core architecture poses the risk of network model information leakage. Based on the observation, we propose a learning-based method to steal the model architecture of the NNs by exploiting the memory timing channel without any high-level privilege or physical access. We first trained an end-to-end measurement network offline to learn the relation between memory timing information and NNs model architecture. Then, we performed an online attack and reconstructed the target model using the prediction from the measurement network. We evaluated the proposed attack method on a multi-core architecture simulator. The experimental results show that our learning-based attack method can reconstruct the target model with high accuracy and improve the adversarial attack success rate by 42.4%.",
      "year": 2021,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Chengsi Gao",
        "Bing Li",
        "Ying Wang",
        "Weiwei Chen",
        "Lei Zhang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73ac60e0f1fa8bbe7de599df66d2cbaef9a9b268",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3453688.3461512",
      "publication_date": "2021-06-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "44cc3c0d5a80fecd1b6285c73f79986852135162",
      "title": "ML-Stealer: Stealing Prediction Functionality of Machine Learning Models with Mere Black-Box Access",
      "abstract": "Machine Learning (ML) models are progressively deployed in many real-world applications to perform a wide range of tasks, but are exposed to the security and privacy threats which aim to infer the details and even steal the functionality of the ML models. Despite extensive attacking efforts which rely on white-box or gray-box access, how to perform attacks with black-box access continues to be elusive. Aspiring to fill this gap, we move one step further and present ML-Stealer that can steal the functionality of any type of ML models with mere black-box access. With two algorithm designs, namely, synthetic data generation and replica model construction, ML-Stealer can construct a deep neural network (DNN)-based replica model which has the similar prediction functionality to the victim ML model. ML-Stealer does not require any knowledge about the victim model, nor does it enforce the access to statistical information or samples of the victim's training data. Experiment results demonstrate that ML-Stealer can achieve the consistent prediction results with the victim model of an averaged testing accuracy of 85.6%, and up to 93.6% at best.",
      "year": 2021,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Gaoyang Liu",
        "Shijie Wang",
        "Borui Wan",
        "Zekun Wang",
        "Chen Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/44cc3c0d5a80fecd1b6285c73f79986852135162",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "title": "BODAME: Bilevel Optimization for Defense Against Model Extraction",
      "abstract": "Model extraction attacks have become serious issues for service providers using machine learning. We consider an adversarial setting to prevent model extraction under the assumption that attackers will make their best guess on the service provider's model using query accesses, and propose to build a surrogate model that significantly keeps away the predictions of the attacker's model from those of the true model. We formulate the problem as a non-convex constrained bilevel optimization problem and show that for kernel models, it can be transformed into a non-convex 1-quadratically constrained quadratic program with a polynomial-time algorithm to find the global optimum. Moreover, we give a tractable transformation and an algorithm for more complicated models that are learned by using stochastic gradient descent-based algorithms. Numerical experiments show that the surrogate model performs well compared with existing defense models when the difference between the attacker's and service provider's distributions is large. We also empirically confirm the generalization ability of the surrogate model.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Y. Mori",
        "Atsushi Nitanda",
        "A. Takeda"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2eef5df6e2347f7d83df44fe3268d4b237f0eaa5",
      "pdf_url": "",
      "publication_date": "2021-03-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "title": "A Backpropagation Extreme Learning Machine Approach to Fast Training Neural Network-Based Side-Channel Attack",
      "abstract": "This work presented new Deep learning Side-channel Attack (DL-SCA) models that are based on Extreme Learning Machine (ELM). Unlike the conventional iterative backpropagation method, ELM is a fast learning algorithm that computes the trainable weights within a single iteration. Two models (Ensemble bpELM and CAE-ebpELM) are designed to perform SCA on AES with Boolean masking and desynchronization/jittering. The best models for both attack tasks can be trained 27x faster than MLP and 5x faster than CNN respectively. Verified and validated using ASCAD dataset, our models successfully recover all 16 subkeys using approximately 3K traces in the worst case scenario.",
      "year": 2021,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Xuyang Huang",
        "M. Wong",
        "A. Do",
        "W. Goh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/33ae2ced125f5a1ebf14d6120b0049fe31e931c0",
      "pdf_url": "",
      "publication_date": "2021-12-16",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d31062435210a9ccca679114ffc0e089744ae683",
      "title": "Bandwidth Utilization Side-Channel on ML Inference Accelerators",
      "abstract": "Accelerators used for machine learning (ML) inference provide great performance benefits over CPUs. Securing confidential model in inference against off-chip side-channel attacks is critical in harnessing the performance advantage in practice. Data and memory address encryption has been recently proposed to defend against off-chip attacks. In this paper, we demonstrate that bandwidth utilization on the interface between accelerators and the weight storage can serve a side-channel for leaking confidential ML model architecture. This side channel is independent of the type of interface, leaks even in the presence of data and memory address encryption and can be monitored through performance counters or through bus contention from an on-chip unprivileged process.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sarbartha Banerjee",
        "Shijia Wei",
        "Prakash Ramrakhyani",
        "Mohit Tiwari"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d31062435210a9ccca679114ffc0e089744ae683",
      "pdf_url": "",
      "publication_date": "2021-10-14",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8283479256ea1d54d98ed00bb53caba38f2f688e",
      "title": "Side-Channel Analysis-Based Model Extraction on Intelligent CPS: An Information Theory Perspective",
      "abstract": "The intelligent cyber-physical system (CPS) has been applied in various fields, covering multiple critical infras-tructures and human daily life support areas. CPS Security is a major concern and of critical importance, especially the security of the intelligent control component. Side-channel analysis (SCA) is the common threat exploiting the weaknesses in system operation to extract information of the intelligent CPS. However, existing literature lacks the systematic theo-retical analysis of the side-channel attacks on the intelligent CPS, without the ability to quantify and measure the leaked information. To address these issues, we propose the SCA-based model extraction attack on intelligent CPS. First, we design an efficient and novel SCA-based model extraction framework, including the threat model, hierarchical attack process, and the multiple micro-space parallel search enabled weight extraction algorithm. Secondly, an information theory-empowered analy-sis model for side-channel attacks on intelligent CPS is built. We propose a mutual information-based quantification method and derive the capacity of side-channel attacks on intelligent CPS, formulating the amount of information leakage through side channels. Thirdly, we develop the theoretical bounds of the leaked information over multiple attack queries based on the data processing inequality and properties of entropy. These convergence bounds provide theoretical means to estimate the amount of information leaked. Finally, experimental evaluation, including real-world experiments, demonstrates the effective-ness of the proposed SCA-based model extraction algorithm and the information theory-based analysis method in intelligent CPS.",
      "year": 2021,
      "venue": "2021 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Qianqian Pan",
        "Jun Wu",
        "Xi Lin",
        "Jianhua Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8283479256ea1d54d98ed00bb53caba38f2f688e",
      "pdf_url": "",
      "publication_date": "2021-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models",
      "abstract": "Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.",
      "year": 2021,
      "venue": "",
      "authors": [
        "Sebastian Szyller",
        "Vasisht Duddu",
        "Tommi Grondahl",
        "Nirmal Asokan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87af159e1eaa99d17b9b6aff6781f2342e8ff385",
      "pdf_url": "",
      "publication_date": "2021-04-26",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "title": "JAXED: Reverse Engineering DNN Architectures Leveraging JIT GEMM Libraries",
      "abstract": "General matrix multiplication (GEMM) libraries on x86 architectures have recently adopted Just-in-time (JIT) based optimizations to dramatically reduce the execution time of small and medium-sized matrix multiplication. The exploitation of the latest CPU architectural extensions, such as the AVX2 and AVX-512 extensions, are the target for these optimizations. Although JIT compilers can provide impressive speedups to GEMM libraries, they expose a new attack surface through the built-in JIT code caches. These software-based caches allow an adversary to extract sensitive information through carefully designed timing attacks. The attack surface of such libraries has become more prominent due to their widespread integration into popular Machine Learning (ML) frameworks such as PyTorch and Tensorflow.In our paper, we present a novel attack strategy for JIT-compiled GEMM libraries called JAXED. We demonstrate how an adversary can exploit the GEMM library\u2019s vulnerable state management to extract confidential CNN model hyperparameters. We show that using JAXED, one can successfully extract the hyperparameters of models with fully-connected layers with an average accuracy of 92%. Further, we demonstrate our attack against the final fully connected layer of 10 popular DNN models. Finally, we perform an end-to-end attack on MobileNetV2, on both the convolution and FC layers, successfully extracting model hyperparameters.",
      "year": 2021,
      "venue": "Seed",
      "authors": [
        "Malith Jayaweera",
        "Kaustubh Shivdikar",
        "Yanzhi Wang",
        "D. Kaeli"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5416ffbbed2cec2469ff08e5d4d7c8d2810e0d15",
      "pdf_url": "",
      "publication_date": "2021-09-01",
      "keywords_matched": [
        "reverse engineering DNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "46582b5600668089180542b725a2868692bcce87",
      "title": "An extraction attack on image recognition model using VAE-kdtree model",
      "abstract": "This paper proposes a black box extraction attack model on pre-trained image classifiers to rebuild a functionally equivalent model with high similarity. Common model extraction attacks use a large number of training samples to feed the target classifier which is time-consuming with redundancy. The attack results have a high dependency on the selected training samples and the target model. The extracted model may only get part of crucial features because of inappropriate sample selection. To eliminate these uncertainties, we proposed the VAE-kdtree attack model which eliminates the high dependency between selected training samples and the target model. It can not only save redundant computation, but also extract critical boundaries more accurately in image classification. This VAE-kdtree model has shown to achieve around 90% similarity on MNIST and around 80% similarity on MNIST-Fashion with a target Convolutional Network Model and a target Support Vector Machine Model. The performance of this VAE-kdtree model could be further improved by adopting higher dimension space of the kdtree.",
      "year": 2021,
      "venue": "Other Conferences",
      "authors": [
        "Tianqi Wen",
        "Haibo Hu",
        "Huadi Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/46582b5600668089180542b725a2868692bcce87",
      "pdf_url": "",
      "publication_date": "2021-03-13",
      "keywords_matched": [
        "model extraction attack",
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "title": "Digital Watermarking System for Hard Cover Objects Against Cloning Attacks",
      "abstract": "We consider an application of digital watermark system technique for hard carriers (paper or plastic cover objects in addition). Algorithms of watermark embedding and extraction are proposed and the corresponding error probabilities of the extracted bits both for informed and blind decoders are presented. The spread spectrum signals used for embedding of watermark are optimized on their parameters. Similar experimental results are presented for data matrices carriers depending on paper sizes of data matrix copies. A protection against so-called \u201ccloning\u201d attack is elaborated where certificates are copied by attack scanner or photo camera and next printed and fixed as forges. The formulas for a missing the cloning attack and false alarm probabilities are proved. A full-scale experiment with a real scanner and printer confirms that the reliability of cloning attack detection can be provided under appropriate selection of watermark system parameters.",
      "year": 2021,
      "venue": "2021 30th Conference of Open Innovations Association FRUCT",
      "authors": [
        "V. Korzhik",
        "V. Starostin",
        "V. Yakovlev",
        "D. Flaksman",
        "Ivan Bukshin",
        "B. Izotov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1bedbcf7fe37eee6e34250cae96973841c3bb0a7",
      "pdf_url": "",
      "publication_date": "2021-10-27",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "title": "Neural Networks as a Side-Channel Countermeasure: Challenges and Opportunities",
      "abstract": "Specialized acceleration hardware for artificial deep neural network inference is available from the cloud to the edge. FPGAs in particular are heavily advertised for the acceleration of neural network-based applications. Traditionally, those applications are classification or nonlinear regression tasks with the goal to approximate an unknown function. However, they can be trained to replicate a fully known deterministic function - a classical example being the boolean XOR - as well. On the other hand, side-channel attacks remain a concern from the cloud to the edge, where attackers are often able to extract secret information through direct or indirect measurements of observables like power, voltage, electromagnetic emanation or timing. In this work, we show how an FPGA-mapped neural network implementation of the AES S-Box can improve side-channel resistance against Correlation Power Analysis (CPA) attacks. Although the implementation of a hardware-optimized algorithm such as the AES as a neural network introduces significant overhead, the generality of the representation allows to mitigate leakage in a manner agnostic to the overlying cryptographic primitive. We demonstrate the benefits of a generic representation, by optimizing an initially vulnerable neural network implementation towards side-channel resilience, through careful choice of activation function and input representation. The implementation is evaluated both against an external attacker measuring power with an oscilloscope, as well as a remote, internal adversary, who is able to capture voltage traces through FPGA-internal sensors in multi-tenant FPGAs. Our results show, how external attacks on the optimized neural network are no longer possible with up to one million traces, whereas an internal attacker is still able to recover the secret key. The latter result also exposes that in some cases measurement through internal sensors can be even more beneficial for an attacker than physical access with measurement equipment.",
      "year": 2021,
      "venue": "IEEE Computer Society Annual Symposium on VLSI",
      "authors": [
        "Jonas Krautter",
        "M. Tahoori"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e21e1386e1ece2633b8f0b82c6f685ce9eddbbf5",
      "pdf_url": "",
      "publication_date": "2021-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ef2325d326986f6e9958332100fce2f00ae65a29",
      "title": "Emerging AI Security Threats for Autonomous Cars - Case Studies",
      "abstract": "Artificial Intelligence has made a significant contribution to autonomous vehicles, from object detection to path planning. However, AI models require a large amount of sensitive training data and are usually computationally intensive to build. The commercial value of such models motivates attackers to mount various attacks. Adversaries can launch model extraction attacks for monetization purposes or step-ping-stone towards other attacks like model evasion. In specific cases, it even results in destroying brand reputation, differentiation, and value proposition. In addition, IP laws and AI-related legalities are still evolving and are not uniform across countries. We discuss model extraction attacks in detail with two use-cases and a generic kill-chain that can compromise autonomous cars. It is essential to investigate strategies to manage and mitigate the risk of model theft.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Shanthi Lekkala",
        "Tanya Motwani",
        "Manojkumar Somabhai Parmar",
        "A. Phadke"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ef2325d326986f6e9958332100fce2f00ae65a29",
      "pdf_url": "",
      "publication_date": "2021-09-10",
      "keywords_matched": [
        "model extraction",
        "model theft",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "244d8f4fc47b94efb976016acf51f3ea5852034f",
      "title": "Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract)",
      "abstract": "Graph data has been widely used to represent data from various domain, e.g., social networks, recommendation system. With great power, the GNN models, usually as valuable properties of their owners, also become attractive targets of the adversary who covets to steal them. While existing works show that simple deep neural networks can be reproduced by so-called Model Extraction Attacks, how to extract a GNN model has not been explored. In this paper, we exploit the threat of model extraction attacks against GNN models. Unlike ordinary attacks which obtain model information via only the input-output query pairs, we utilize both the node queries and the graph structure to extract the GNNs. Furthermore, we consider the stealthiness of the attack and propose to generate legitimate queries so the extraction can be applied discreetly. We implement our attack by leveraging the responses of these queries, as well as other accessible knowledge, e.g., neighbor connectives of the queried nodes. By evaluating over three real-world datasets, our attack is shown to effectively produce a surrogate model with more than 80% equivalent predictions as the target model.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bang Wu",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/244d8f4fc47b94efb976016acf51f3ea5852034f",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17959/17764",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "title": "Neural Network Model Extraction Based on Adversarial Examples",
      "abstract": "The neural network model has been applied to all walks of life. By detecting the internal information of a black-box model, the attacker can obtain potential commercial value of the model. At the same time, understanding the model structure helps the attacker customize the strategy to attack the model. We have improved a model detection method based on input and output pairs to detect the internal information of the trained neural network black-box model. On the one hand, our work proved that adversarial examples are very likely to carry architecture information of the neural network model. On the other hand, we added adversarial examples to the model pre-detection module, and verified the positive effects of adversarial examples on model detection through experiments, which improved the accuracy of the meta-model and reduced the cost of model detection.",
      "year": 2021,
      "venue": "International Conference on Advanced Information Science and System",
      "authors": [
        "Huiwen Fang",
        "Chunhua Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "pdf_url": "",
      "publication_date": "2021-11-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "title": "Entangled Watermarks as a Defense against Model Extraction",
      "abstract": "Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difficult to defend against model extraction without sacrificing significant prediction accuracy, watermarking leverages unused model capacity to have the model overfit to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer. \nWe introduce Entangled Watermarking Embeddings (EWE). Our approach encourages the model to learn common features for classifying data that is sampled from the task distribution, but also data that encodes watermarks. An adversary attempting to remove watermarks that are entangled with legitimate data is also forced to sacrifice performance on legitimate data. Experiments on MNIST, Fashion-MNIST, and Google Speech Commands validate that the defender can claim model ownership with 95% confidence after less than 10 queries to the stolen copy, at a modest cost of 1% accuracy in the defended model's performance.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hengrui Jia",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot"
      ],
      "citation_count": 261,
      "url": "https://www.semanticscholar.org/paper/abbb0fd559ade70265f4f528df094fbbd8ae2040",
      "pdf_url": "",
      "publication_date": "2020-02-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e4b1d7553020258d7e537e2cfa53865359389eac",
      "title": "Stealing Links from Graph Neural Networks",
      "abstract": "Graph data, such as social networks and chemical networks, contains a wealth of information that can help to build powerful applications. To fully unleash the power of graph data, a family of machine learning models, namely graph neural networks (GNNs), is introduced. Empirical results show that GNNs have achieved state-of-the-art performance in various tasks. \nGraph data is the key to the success of GNNs. High-quality graph is expensive to collect and often contains sensitive information, such as social relations. Various research has shown that machine learning models are vulnerable to attacks against their training data. Most of these models focus on data from the Euclidean space, such as images and texts. Meanwhile, little attention has been paid to the security and privacy risks of graph data used to train GNNs. \nIn this paper, we aim at filling the gap by proposing the first link stealing attacks against graph neural networks. Given a black-box access to a GNN model, the goal of an adversary is to infer whether there exists a link between any pair of nodes in the graph used to train the model. We propose a threat model to systematically characterize the adversary's background knowledge along three dimensions. By combination, we obtain a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these attacks. Extensive experiments over 8 real-world datasets show that our attacks are effective at inferring links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinlei He",
        "Jinyuan Jia",
        "M. Backes",
        "N. Gong",
        "Yang Zhang"
      ],
      "citation_count": 206,
      "url": "https://www.semanticscholar.org/paper/e4b1d7553020258d7e537e2cfa53865359389eac",
      "pdf_url": "",
      "publication_date": "2020-05-05",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "title": "CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples",
      "abstract": "Cloud-based Machine Learning as a Service (MLaaS) is gradually gaining acceptance as a reliable solution to various real-life scenarios. These services typically utilize Deep Neural Networks (DNNs) to perform classification and detection tasks and are accessed through Application Programming Interfaces (APIs). Unfortunately, it is possible for an adversary to steal models from cloud-based platforms, even with black-box constraints, by repeatedly querying the public prediction API with malicious inputs. In this paper, we introduce an effective and efficient black-box attack methodology that extracts largescale DNN models from cloud-based platforms with near-perfect performance. In comparison to existing attack methods, we significantly reduce the number of queries required to steal the target model by incorporating several novel algorithms, including active learning, transfer learning, and adversarial attacks. During our experimental evaluations, we validate our proposed model for conducting theft attacks on various commercialized MLaaS platforms hosted by Microsoft, Face++, IBM, Google and Clarifai. Our results demonstrate that the proposed method can easily reveal/steal large-scale DNN models from these cloud platforms. The proposed attack method can also be used to accurately evaluates the robustness of DNN based MLaaS classifiers against theft attacks.",
      "year": 2020,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Honggang Yu",
        "Kaichen Yang",
        "Teng Zhang",
        "Yun-Yun Tsai",
        "Tsung-Yi Ho",
        "Yier Jin"
      ],
      "citation_count": 183,
      "url": "https://www.semanticscholar.org/paper/4d548fd21aad60e3052455e22b7a57cc1f06e3c3",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24178",
      "publication_date": null,
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d5ffa58133940646d1339c2610cb35f27442e0d3",
      "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
      "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "A. Prakash",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 171,
      "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
      "pdf_url": "https://arxiv.org/pdf/2005.03161",
      "publication_date": "2020-05-06",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "title": "DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints",
      "abstract": "As deep neural networks (DNNs) continue their reach into a wide range of application domains, the neural network architecture of DNN models becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. Previous studies explore to leverage architecture-level events disposed in hardware platforms to extract the model architecture information. They pose the following limitations: requiring a priori knowledge of victim models, lacking in robustness and generality, or obtaining incomplete information of the victim model architecture. Our paper proposes DeepSniffer, a learning-based model extraction framework to obtain the complete model architecture information without any prior knowledge of the victim model. It is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. The basic idea of DeepSniffer is to learn the relation between extracted architectural hints (e.g., volumes of memory reads/writes obtained by side-channel or bus snooping attacks) and model internal architectures. Taking GPU platforms as a show case, DeepSniffer conducts model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models. The extracted models are directly helpful to the attempting of crafting adversarial inputs. Our experimental results show that DeepSniffer achieves a high accuracy of model extraction and thus improves the adversarial attack success rate from 14.6%$\\sim$25.5% (without network architecture knowledge) to 75.9% (with extracted network architecture). The DeepSniffer project has been released in Github.",
      "year": 2020,
      "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Shuangchen Li",
        "Lei Deng",
        "Pengfei Zuo",
        "Yu Ji",
        "Xinfeng Xie",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 153,
      "url": "https://www.semanticscholar.org/paper/109ad71af2ffce01b60852f8141ea91be6eed9e1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3373376.3378460",
      "publication_date": "2020-03-09",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "title": "Cryptanalytic Extraction of Neural Network Models",
      "abstract": "We argue that the machine learning problem of model extraction is actually a cryptanalytic problem in disguise, and should be studied as such. Given oracle access to a neural network, we introduce a differential attack that can efficiently steal the parameters of the remote model up to floating point precision. Our attack relies on the fact that ReLU neural networks are piecewise linear functions, and thus queries at the critical points reveal information about the model parameters. \nWe evaluate our attack on multiple neural network models and extract models that are 2^20 times more precise and require 100x fewer queries than prior work. For example, we extract a 100,000 parameter neural network trained on the MNIST digit recognition task with 2^21.5 queries in under an hour, such that the extracted model agrees with the oracle on all inputs up to a worst-case error of 2^-25, or a model with 4,000 parameters in 2^18.5 queries with worst-case error of 2^-40.4. Code is available at this https URL.",
      "year": 2020,
      "venue": "Annual International Cryptology Conference",
      "authors": [
        "Nicholas Carlini",
        "Matthew Jagielski",
        "Ilya Mironov"
      ],
      "citation_count": 151,
      "url": "https://www.semanticscholar.org/paper/186377b9098efc726b8f1bda7e4f8aa2ed7bafa5",
      "pdf_url": "",
      "publication_date": "2020-03-10",
      "keywords_matched": [
        "model extraction",
        "extract model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "title": "ActiveThief: Model Extraction Using Active Learning and Unannotated Public Data",
      "abstract": "Machine learning models are increasingly being deployed in practice. Machine Learning as a Service (MLaaS) providers expose such models to queries by third-party developers through application programming interfaces (APIs). Prior work has developed model extraction attacks, in which an attacker extracts an approximation of an MLaaS model by making black-box queries to it. We design ActiveThief \u2013 a model extraction framework for deep neural networks that makes use of active learning techniques and unannotated public datasets to perform model extraction. It does not expect strong domain knowledge or access to annotated data on the part of the attacker. We demonstrate that (1) it is possible to use ActiveThief to extract deep classifiers trained on a variety of datasets from image and text domains, while querying the model with as few as 10-30% of samples from public datasets, (2) the resulting model exhibits a higher transferability success rate of adversarial examples than prior work, and (3) the attack evades detection by the state-of-the-art model extraction detection method, PRADA.",
      "year": 2020,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 147,
      "url": "https://www.semanticscholar.org/paper/d8f64187b447d1b64f69f541dbbaec71bd79d205",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5432/5288",
      "publication_date": "2020-04-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3136dd4036331b4559b341560712942ca2e765e3",
      "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
      "abstract": "Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Mingfu Xue",
        "Chengxiang Yuan",
        "Heyi Wu",
        "Yushu Zhang",
        "Weiqiang Liu"
      ],
      "citation_count": 141,
      "url": "https://www.semanticscholar.org/paper/3136dd4036331b4559b341560712942ca2e765e3",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09064510.pdf",
      "publication_date": "2020-04-13",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
      "abstract": "We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.",
      "year": 2020,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Eric Wallace",
        "Mitchell Stern",
        "D. Song"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "pdf_url": "https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf",
      "publication_date": "2020-04-30",
      "keywords_matched": [
        "model stealing",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d0d8c912b20d2d081672f07b6926b620950a39c8",
      "title": "Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching Side-Channel",
      "abstract": "Machine learning has been attracting strong interests in recent years. Numerous companies have invested great efforts and resources to develop customized deep-learning models, which are their key intellectual properties. In this work, we investigate to what extent the secret of deep-learning models can be inferred by attackers. In particular, we focus on the scenario that a model developer and an adversary share the same GPU when training a Deep Neural Network (DNN) model. We exploit the GPU side-channel based on context-switching penalties. This side-channel allows us to extract the fine-grained structural secret of a DNN model, including its layer composition and hyper-parameters. Leveraging this side-channel, we developed an attack prototype named MosConS, which applies LSTM-based inference models to identify the structural secret. Our evaluation of MosConS shows the structural information can be accurately recovered. Therefore, we believe new defense mechanisms should be developed to protect training against the GPU side-channel.",
      "year": 2020,
      "venue": "Dependable Systems and Networks",
      "authors": [
        "Junyin Wei",
        "Yicheng Zhang",
        "Zhe Zhou",
        "Zhou Li",
        "M. A. Faruque"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/d0d8c912b20d2d081672f07b6926b620950a39c8",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82132f7abf204b13cc171980781f76a0d627a970",
      "title": "Towards Security Threats of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning\u2019s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches\u2019 merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",
      "year": 2020,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/82132f7abf204b13cc171980781f76a0d627a970",
      "pdf_url": "https://arxiv.org/pdf/1911.12562",
      "publication_date": "2020-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "7ea18b512074acc10d6a4025cb479955ba295f2d",
      "title": "Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realisation",
      "abstract": "Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Bang Wu",
        "Xiangwen Yang",
        "Shirui Pan",
        "Xingliang Yuan"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/7ea18b512074acc10d6a4025cb479955ba295f2d",
      "pdf_url": "",
      "publication_date": "2020-10-24",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d4a679c685de885ab98b7c1f2378e26a255de490",
      "title": "Model Extraction Attacks and Defenses on Cloud-Based Machine Learning Models",
      "abstract": "Machine learning models have achieved state-of-the-art performance in various fields, from image classification to speech recognition. However, such models are trained with a large amount of sensitive training data, and are typically computationally expensive to build. As a result, many cloud providers (e.g., Google) have launched machine-learning-as-a-service, which helps clients benefit from the sophisticated cloud-based machine learning models via accessing public APIs. Such a business paradigm significantly expedites and simplifies the development circles. Unfortunately, the commercial value of such cloud-based machine learning models motivates attackers to conduct model extraction attacks for free use or as a springboard to conduct other attacks (e.g., craft adversarial examples in black-box settings). In this article, we conduct a thorough investigation of existing approaches to model extraction attacks and defenses on cloud-based models. We classify the state-of-the-art attack schemes into two categories based on whether the attacker aims to steal the property (i.e., parameters, hyperparameters, and architecture) or the functionality of the model. We also categorize defending schemes into two groups based on whether the scheme relies on output disturbance or query observation. We not only present a detailed survey of each method, but also demonstrate the comparison of both attack and defense approaches via experiments. We highlight several future directions in both model extraction attacks and its defenses, which shed light on possible avenues for further studies.",
      "year": 2020,
      "venue": "IEEE Communications Magazine",
      "authors": [
        "Xueluan Gong",
        "Qian Wang",
        "Yanjiao Chen",
        "Wang Yang",
        "Xinye Jiang"
      ],
      "citation_count": 62,
      "url": "https://www.semanticscholar.org/paper/d4a679c685de885ab98b7c1f2378e26a255de490",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "title": "SNIFF: Reverse Engineering of Neural Networks With Fault Attacks",
      "abstract": "Neural networks have been shown to be vulnerable against fault injection attacks. These attacks change the physical behavior of the device during the computation, resulting in a change of value that is currently being computed. They can be realized by various techniques, ranging from clock/voltage glitching to application of lasers to rowhammer. Previous works have mostly explored fault attacks for output misclassification, thus affecting the reliability of neural networks. In this article, we investigate the possibility to reverse engineer neural networks with fault attacks. Sign bit flip fault attack enables the reverse engineering by changing the sign of intermediate values. We develop the first exact extraction method on deep-layer feature extractor networks that provably allows the recovery of proprietary model parameters. Our experiments with Keras library show that the precision error for the parameter recovery for the tested networks is less than $10^{-13}$ with the usage of 64-bit floats, which improves the current state of the art by six orders of magnitude.",
      "year": 2020,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "J. Breier",
        "Dirmanto Jap",
        "Xiaolu Hou",
        "S. Bhasin",
        "Yang Liu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/4eb8818dbf9e5e384741136c5ba633455bcb72a8",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/155678/2/_IEEE_TREL__Reverse_Engineering_of_Neural_Networks_with_Fault_Attacks.pdf",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "title": "Model extraction from counterfactual explanations",
      "abstract": "Post-hoc explanation techniques refer to a posteriori methods that can be used to explain how black-box machine learning models produce their outcomes. Among post-hoc explanation techniques, counterfactual explanations are becoming one of the most popular methods to achieve this objective. In particular, in addition to highlighting the most important features used by the black-box model, they provide users with actionable explanations in the form of data instances that would have received a different outcome. Nonetheless, by doing so, they also leak non-trivial information about the model itself, which raises privacy issues. In this work, we demonstrate how an adversary can leverage the information provided by counterfactual explanations to build high-fidelity and high-accuracy model extraction attacks. More precisely, our attack enables the adversary to build a faithful copy of a target model by accessing its counterfactual explanations. The empirical evaluation of the proposed attack on black-box models trained on real-world datasets demonstrates that they can achieve high-fidelity and high-accuracy extraction even under low query budgets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "U. A\u00efvodji",
        "Alexandre Bolot",
        "S. Gambs"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/983f1e19ba55aa97bd20086ca7ad5cf4e436a37a",
      "pdf_url": "",
      "publication_date": "2020-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "title": "BoMaNet: Boolean Masking of an Entire Neural Network",
      "abstract": "Recent work on stealing machine learning (ML) models from inference engines with physical side-channel attacks warrant an urgent need for effective side-channel defenses. This work proposes the first fully-masked neural network inference engine design. Masking uses secure multi-party computation to split the secrets into random shares and to decorrelate the statistical relation of secret-dependent computations to side-channels (e.g., the power draw). In this work, we construct secure hardware primitives to mask all the linear and non-linear operations in a neural network. We address the challenge of masking integer addition by converting each addition into a sequence of XOR and AND gates and by augmenting Trichina's secure Boolean masking style. We improve the traditional Trichina's AND gates by adding pipelining elements for better glitch-resistance and we architect the whole design to sustain a throughput of 1 masked addition per cycle. We implement the proposed secure inference engine on a Xilinx Spartan-6 (XC6SLX75) FPGA. The results show that masking incurs an overhead of 3.5% in latency and 5.9\u00d7 in area. Finally, we demonstrate the security of the masked design with 2M traces.",
      "year": 2020,
      "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/512c95dd0b0a04892e257ddef60af96dd7a8b8f3",
      "pdf_url": "",
      "publication_date": "2020-06-16",
      "keywords_matched": [
        "stealing machine learning",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "title": "Stealing Deep Reinforcement Learning Models for Fun and Profit",
      "abstract": "This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that our methodology can effectively recover the DRL models with high fidelity and accuracy. We also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal DRL models stealthily even they are protected by DNN watermarks. These pose a severe threat to the intellectual property and privacy protection of DRL applications.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Kangjie Chen",
        "Tianwei Zhang",
        "Xiaofei Xie",
        "Yang Liu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "pdf_url": "https://ink.library.smu.edu.sg/sis_research/7110",
      "publication_date": "2020-06-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8765853a2d7027dfe91d650462d7431552bd7315",
      "title": "ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles",
      "abstract": "Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.",
      "year": 2020,
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "authors": [
        "Xiaoyong Yuan",
        "Lei Ding",
        "Lan Zhang",
        "Xiaolin Li",
        "D. Wu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/8765853a2d7027dfe91d650462d7431552bd7315",
      "pdf_url": "https://doi.org/10.1109/tetci.2022.3147508",
      "publication_date": "2020-09-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0d43d80c40baec445bc39a8dfa4789d070786319",
      "title": "Reverse-Engineering Deep Neural Networks Using Floating-Point Timing Side-Channels",
      "abstract": "Trained Deep Neural Network (DNN) models have become valuable intellectual property. A new attack surface has emerged for DNNs: model reverse engineering. Several recent attempts have utilized various common side channels. However, recovering DNN parameters, weights and biases, remains a challenge. In this paper, we present a novel attack that utilizes a floating-point timing side channel to reverse-engineer parameters of multi-layer perceptron (MLP) models in software implementation, entirely and precisely. To the best of our knowledge, this is the first work that leverages a floating-point timing side-channel for effective DNN model recovery.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Gongye Cheng",
        "Yunsi Fei",
        "T. Wahl"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/0d43d80c40baec445bc39a8dfa4789d070786319",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "title": "GANRED: GAN-based Reverse Engineering of DNNs via Cache Side-Channel",
      "abstract": "In recent years, deep neural networks (DNN) have become an important type of intellectual property due to their high performance on various classification tasks. As a result, DNN stealing attacks have emerged. Many attack surfaces have been exploited, among which cache timing side-channel attacks are hugely problematic because they do not need physical probing or direct interaction with the victim to estimate the DNN model. However, existing cache-side-channel-based DNN reverse engineering attacks rely on analyzing the binary code of the DNN library that must be shared between the attacker and the victim in the main memory. In reality, the DNN library code is often inaccessible because 1) the code is proprietary, or 2) memory sharing has been disabled by the operating system. In our work, we propose GANRED, an attack approach based on the generative adversarial nets (GAN) framework which utilizes cache timing side-channel information to accurately recover the structure of DNNs without memory sharing or code access. The benefit of GANRED is four-fold. 1) There is no need for DNN library code analysis. 2) No shared main memory segment between the victim and the attacker is needed. 3) Our attack locates the exact structure of the victim model, unlike existing attacks which only narrow down the structure search space. 4) Our attack efficiently scales to deeper DNNs, exhibiting only linear growth in the number of layers in the victim DNN.",
      "year": 2020,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Yuntao Liu",
        "Ankur Srivastava"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/1f53fe2f4458bde77735482552dc6818ae5eb30f",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "title": "Model Reverse-Engineering Attack using Correlation Power Analysis against Systolic Array Based Neural Network Accelerator",
      "abstract": "Various deep neural network (DNN) accelerators have been proposed for artificial intelligence (AI) inference on edge devices. On the other hand, hardware security issues of the DNN accelerator have not been discussed well. Trained DNN models are important intellectual property and a valuable target for adversaries. In particular, when a DNN model is implemented on an edge device, adversaries can physically access the device and try to reveal the implemented DNN model. Therefore, the DNN execution environment on an edge device requires countermeasures such as data encryption on off-chip memory against various reverse-engineering attacks. In this paper, we reveal DNN model parameters by utilizing correlation power analysis (CPA) against a systolic array circuit that is widely used in DNN accelerator hardware. Our experimental results show that the adversary can extract trained model parameters from a DNN accelerator even if the DNN model parameters are protected with data encryption. The results suggest that countermeasures against side-channel leaks are important for implementing a DNN accelerator on FPGA or ASIC.",
      "year": 2020,
      "venue": "International Symposium on Circuits and Systems",
      "authors": [
        "Kota Yoshida",
        "Takaya Kubota",
        "S. Okura",
        "M. Shiozaki",
        "T. Fujino"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/a5991cc92fad9bbca68d8227a50dbd3eb165efe4",
      "pdf_url": "",
      "publication_date": "2020-10-01",
      "keywords_matched": [
        "power analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "729fbe386e31e36ed5108ee276a6db223176a33d",
      "title": "Information Leakage by Model Weights on Federated Learning",
      "abstract": "Federated learning aggregates data from multiple sources while protecting privacy, which makes it possible to train efficient models in real scenes. However, although federated learning uses encrypted security aggregation, its decentralised nature makes it vulnerable to malicious attackers. A deliberate attacker can subtly control one or more participants and upload malicious model parameter updates, but the aggregation server cannot detect it due to encrypted privacy protection. Based on these problems, we find a practical and novel security risk in the design of federal learning. We propose an attack for conspired malicious participants to adjust the training data strategically so that the weight of a certain dimension in the aggregation model will rise or fall with a pattern. The trend of weights or parameters in the aggregation model forms meaningful signals, which is the risk of information leakage. The leakage is exposed to other participants in this federation but only available for participants who reach an agreement with the malicious participant, i.e., the receiver must be able to understand patterns of changes in weights. The attack effect is evaluated and verified on open-source code and data sets.",
      "year": 2020,
      "venue": "PPMLP@CCS",
      "authors": [
        "Xiaoyun Xu",
        "Jingzheng Wu",
        "Mutian Yang",
        "Tianyue Luo",
        "Xu Duan",
        "Weiheng Li",
        "Yanjun Wu",
        "Bin Wu"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/729fbe386e31e36ed5108ee276a6db223176a33d",
      "pdf_url": "",
      "publication_date": "2020-11-09",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "title": "Model Extraction Attacks on Recurrent Neural Networks",
      "abstract": ": Model extraction attacks are an attack in which an adversary utilizes a query access to the target model to obtain a new model whose performance is equivalent to the target model e \ufb03 ciently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the e \ufb00 ectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks on RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful type of RNN. Speci\ufb01cally, we tackle the following problems. First, in case of a classi\ufb01cation task, such as image recognition, extraction of an RNN model without \ufb01nal outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in case of a regression task such as weather forecasting, a new attack by newly con\ufb01guring a loss function is presented. We conduct experiments on our model extraction attacks on an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted e \ufb03 ciently, especially through con\ufb01guring a loss function and a more complex architecture di \ufb00 erent from the target model.",
      "year": 2020,
      "venue": "Journal of Information Processing",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/db3f6b0581d2cc23756e3fec1076a4399e97a0d8",
      "pdf_url": "https://doi.org/10.2197/ipsjjip.28.1010",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "23a47ede9466d119448de4226aebe3193b7ce13d",
      "title": "An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its Visualization",
      "abstract": "In recent years, the convolutional neural networks (CNNs) have received a lot of interest in the side-channel community. The previous work has shown that CNNs have the potential of breaking the cryptographic algorithm protected with masking or desynchronization. Before, several CNN models have been exploited, reaching the same or even better level of performance compared to the traditional side-channel attack (SCA). In this paper, we investigate the architecture of Residual Network and build a new CNN model called attention network. To enhance the power of the attention network, we introduce an attention mechanism - Convolutional Block Attention Module (CBAM) and incorporate CBAM into the CNN architecture. CBAM points out the informative points of the input traces and makes the attention network focus on the relevant leakages of the measurements. It is able to improve the performance of the CNNs. Because the irrelevant points will introduce the extra noises and cause a worse performance of attacks. We compare our attention network with the one designed for the masking AES implementation called ASCAD network in this paper. We show that the attention network has a better performance than the ASCAD network. Finally, a new visualization method, named Class Gradient Visualization (CGV) is proposed to recognize which points of the input traces have a positive influence on the predicted result of the neural networks. In another aspect, it can explain why the attention network is superior to the ASCAD network. We validate the attention network through extensive experiments on four public datasets and demonstrate that the attention network is efficient in different AES implementations.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Minhui Jin",
        "Mengce Zheng",
        "Honggang Hu",
        "Nenghai Yu"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/23a47ede9466d119448de4226aebe3193b7ce13d",
      "pdf_url": "",
      "publication_date": "2020-09-18",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "title": "FaDec: A Fast Decision-based Attack for Adversarial Machine Learning",
      "abstract": "Due to the excessive use of cloud-based machine learning (ML) services, the smart cyber-physical systems (CPS) are increasingly becoming vulnerable to black-box attacks on their ML modules. Traditionally, the black-box attacks are either transfer attacks requiring model stealing, or score/decision-based gradient estimation attacks requiring a large number of queries. In practical scenarios, especially for cloud-based ML services and timing-constrained CPS use-cases, every query incurs a huge cost, thereby rendering state-of-the-art decision-based attacks ineffective in such settings. Towards this, we propose a novel methodology for automatically generating an extremely fast and imperceptible decision-based attack called FaDec. It follows two main steps: (1) fast estimation of the classification boundary by combining the half-interval search-based algorithm with gradient sign estimation to reduce the number of queries; and (2) adversarial noise optimization to ensure the imperceptibility. For illustration, we evaluate FaDec on the image recognition and traffic sign detection using multiple state-of-the-art DNNs trained on CIFAR-10 and the German Traffic Sign Recognition Benchmarks (GTSRB) datasets. The experimental analysis shows that the proposed FaDec attack is 16x faster compared to the state-of-the-art decision-based attacks, and generates an attack image with better imperceptibility for a much lesser number of iterations, thereby making our attack more powerful in practical scenarios. We open-sourced the complete code and results of our methodology at https://github.com/fklodhi/FaDec.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Faiq Khalid",
        "Hassan Ali",
        "M. Hanif",
        "Semeen Rehman",
        "Rehan Ahmed",
        "M. Shafique"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/e4a4a8ec2a4d538f0f4ad721d99f7617578209fc",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "85bab667c93b5983c641c03a81e8812669ba6ab8",
      "title": "Improved Hybrid Approach for Side-Channel Analysis Using Efficient Convolutional Neural Network and Dimensionality Reduction",
      "abstract": "Deep learning-based side channel attacks are burgeoning due to their better efficiency and performance, suppressing the traditional side-channel analysis. To launch the successful attack on a particular public key cryptographic (PKC) algorithm, a large number of samples per trace might need to be acquired to capture all the minor useful details from the leakage information, which increases the number of features per instance. The decreased instance-feature ratio increases the computational complexity of the deep learning-based attacks, limiting the attack efficiency. Moreover, data class imbalance can be a hindrance in accurate model training, leading to an accuracy paradox. We propose an efficient Convolutional Neural Network (CNN) based approach in which the dimensionality of the large leakage dataset is reduced, and then the data is processed using the proposed CNN based model. In the proposed model, the optimal number of convolutional blocks is used to build powerful features extractors within the cost limit. We have also analyzed and presented the impact of using the Synthetic Minority Over-sampling Technique (SMOTE) on the proposed model performance. We propose that a data-balancing step should be mandatory for analysis in the side channel attack scenario. We have also provided a performance-based comparative analysis between proposed and existing deep learning models for unprotected and protected Elliptic curve (ECC) Montgomery Power ladder implementations. The reduced network complexity, together with an improved attack efficiency, promote the proposed approach to be effectively used for side-channel attacks.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Naila Mukhtar",
        "A. Fournaris",
        "T. Khan",
        "Charis Dimopoulos",
        "Yinan Kong"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/85bab667c93b5983c641c03a81e8812669ba6ab8",
      "pdf_url": "https://doi.org/10.1109/access.2020.3029206",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "title": "Model Extraction Attacks against Recurrent Neural Networks",
      "abstract": "Model extraction attacks are a kind of attacks in which an adversary obtains a new model, whose performance is equivalent to that of a target model, via query access to the target model efficiently, i.e., fewer datasets and computational resources than those of the target model. Existing works have dealt with only simple deep neural networks (DNNs), e.g., only three layers, as targets of model extraction attacks, and hence are not aware of the effectiveness of recurrent neural networks (RNNs) in dealing with time-series data. In this work, we shed light on the threats of model extraction attacks against RNNs. We discuss whether a model with a higher accuracy can be extracted with a simple RNN from a long short-term memory (LSTM), which is a more complicated and powerful RNN. Specifically, we tackle the following problems. First, in a case of a classification problem, such as image recognition, extraction of an RNN model without final outputs from an LSTM model is presented by utilizing outputs halfway through the sequence. Next, in a case of a regression problem. such as in weather forecasting, a new attack by newly configuring a loss function is presented. We conduct experiments on our model extraction attacks against an RNN and an LSTM trained with publicly available academic datasets. We then show that a model with a higher accuracy can be extracted efficiently, especially through configuring a loss function and a more complex architecture different from the target model.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Tatsuya Takemura",
        "Naoto Yanai",
        "T. Fujiwara"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/92c6a2f7e178337fb99c695ae856b9377e8e72a5",
      "pdf_url": "",
      "publication_date": "2020-02-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "12393a8e6e7c750eae30c0c538663fa7b426f304",
      "title": "A New Privacy-Preserving Framework based on Edge-Fog-Cloud Continuum for Load Forecasting",
      "abstract": "As an essential part to intelligently fine-grained scheduling, planning and maintenance in smart grid and energy internet, short-term load forecasting makes great progress recently owing to the big data collected from smart meters and the leap forward in machine learning technologies. However, the centralized computing topology of classical electric information system, where individual electricity consumption data are frequently transmitted to the cloud center for load forecasting, tends to violate electric consumers\u2019 privacy as well as to increase the pressure on network bandwidth. To tackle the tricky issues, we propose a privacy-preserving framework based on the edge-fog-cloud continuum for smart grid. Specifically, 1) we gravitate the training of load forecasting models and forecasting workloads to distributed smart meters so that consumers\u2019 raw data are handled locally, and only the forecasting outputs that have been protected are reported to the cloud center via fog nodes; 2) we protect the local forecasting models that imply electricity features from model extraction attacks by model randomization; 3) we exploit a shuffle scheme among smart meters to protect the data ownership privacy, and utilize a re-encryption scheme to guarantee the forecasting data privacy. Finally, through comprehensive simulation and analysis, we validate our proposed privacy-preserving framework in terms of privacy protection, and computation and communication efficiency.",
      "year": 2020,
      "venue": "IEEE Wireless Communications and Networking Conference",
      "authors": [
        "S. Hou",
        "Hongjia Li",
        "Chang Yang",
        "Liming Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/12393a8e6e7c750eae30c0c538663fa7b426f304",
      "pdf_url": "",
      "publication_date": "2020-05-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "57099cf852807c652fd55274a3ce4a8c9777ef56",
      "title": "Neural Model Stealing Attack to Smart Mobile Device on Intelligent Medical Platform",
      "abstract": "To date, the Medical Internet of Things (MIoT) technology has been recognized and widely applied due to its convenience and practicality. The MIoT enables the application of machine learning to predict diseases of various kinds automatically and accurately, assisting and facilitating effective and efficient medical treatment. However, the MIoT are vulnerable to cyberattacks which have been constantly advancing. In this paper, we establish a MIoT platform and demonstrate a scenario where a trained Convolutional Neural Network (CNN) model for predicting lung cancer complicated with pulmonary embolism can be attacked. First, we use CNN to build a model to predict lung cancer complicated with pulmonary embolism and obtain high detection accuracy. Then, we build a copycat model using only a small amount of data labeled by the target network, aiming to steal the established prediction model. Experimental results prove that the stolen model can also achieve a relatively high prediction outcome, revealing that the copycat network could successfully copy the prediction performance from the target network to a large extent. This also shows that such a prediction model deployed on MIoT devices can be stolen by attackers, and effective prevention strategies are open questions for researchers.",
      "year": 2020,
      "venue": "Wireless Communications and Mobile Computing",
      "authors": [
        "Liqiang Zhang",
        "Guanjun Lin",
        "Bixuan Gao",
        "Zhibao Qin",
        "Yonghang Tai",
        "Jun Zhang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/57099cf852807c652fd55274a3ce4a8c9777ef56",
      "pdf_url": "https://downloads.hindawi.com/journals/wcmc/2020/8859489.pdf",
      "publication_date": "2020-11-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "copycat model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "title": "Learning From A Big Brother - Mimicking Neural Networks in Profiled Side-channel Analysis",
      "abstract": "Recently, deep learning has emerged as a powerful technique for side-channel attacks, capable of even breaking common countermeasures. Still, trained models are generally large, and thus, performing evaluation becomes resource-intensive. The resource requirements increase in realistic settings where traces can be noisy, and countermeasures are active. In this work, we exploit mimicking to compress the learned models. We demonstrate up to 300 times compression of a state-of-the-art CNN. The mimic shallow network can also achieve much better accuracy as compared to when trained on original data and even reach the performance of a deeper network.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Daan van der Valk",
        "Marina Kr\u010dek",
        "S. Picek",
        "S. Bhasin"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7e45bfee1d303c2823ef48f6b6bb145f85eb0671",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f9f49997c404e386610289202a7c3812c6cbfe34",
      "title": "Differentially Private Machine Learning Model against Model Extraction Attack",
      "abstract": "Machine learning model is vulnerable to model extraction attacks since the attackers can send plenty of queries to infer the hyperparameters of the machine learning model thus stealing confidential information of the learning models. Therefore, there is a urgent need to defend against such an attack. Differential privacy is a promising technique to protect the valuable information. We propose a differential privacy-based method applied in the linear neural network to obfuscate the output of the machine learning model. The security and utility issue of injecting a noise layer to the linear neural network is mathematically analyzed. The experiment results show that our proposed method can lower the attacker's extraction rate while keeping high utility.",
      "year": 2020,
      "venue": "2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
      "authors": [
        "Zelei Cheng",
        "Zuotian Li",
        "Jiwei Zhang",
        "Shuhan Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f9f49997c404e386610289202a7c3812c6cbfe34",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "title": "Stealing Your Data from Compressed Machine Learning Models",
      "abstract": "Machine learning models have been widely deployed in many real-world tasks. When a non-expert data holder wants to use a third-party machine learning service for model training, it is critical to preserve the confidentiality of the training data. In this paper, we for the first time explore the potential privacy leakage in a scenario that a malicious ML provider offers data holder customized training code including model compression which is essential in practical deployment The provider is unable to access the training process hosted by the secured third party, but could inquire models when they are released in public. As a result, adversary can extract sensitive training data with high quality even from these deeply compressed models that are tailored for resource-limited devices. Our investigation shows that existing compressions like quantization, can serve as a defense against such an attack, by degrading the model accuracy and memorized data quality simultaneously. To overcome this defense, we take an initial attempt to design a simple but stealthy quantized correlation encoding attack flow from an adversary perspective. Three integrated components-data pre-processing, layer-wise data-weight correlation regularization, data-aware quantization, are developed accordingly. Extensive experimental results show that our framework can preserve the evasiveness and effectiveness of stealing data from compressed models.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Nuo Xu",
        "Qi Liu",
        "Tao Liu",
        "Zihao Liu",
        "Xiaochen Guo",
        "Wujie Wen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/82e2e111d2a81fe8efaabec44b5d7d3c74cfa95d",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "title": "ACD: An Adaptable Approach for RFID Cloning Attack Detection",
      "abstract": "With the rapid development of the internet of things, radio frequency identification (RFID) technology plays an important role in various fields. However, RFID systems are vulnerable to cloning attacks. This is the fabrication of one or more replicas of a genuine tag, which behave exactly as a genuine tag and fool the reader to gain legal authorization, leading to potential financial loss or reputation damage. Many advanced solutions have been proposed to combat cloning attacks, but they require extra hardware resources, or they cannot detect a clone tag in time. In this article, we make a fresh attempt to counterattack tag cloning based on spatiotemporal collisions. We propose adaptable clone detection (ACD), which can intuitively and accurately display the positions of abnormal tags in real time. It uses commercial off-the-shelf (COTS) RFID devices without extra hardware resources. We evaluate its performance in practice, and the results confirm its success at detecting cloning attacks. The average accuracy can reach 98.7%, and the recall rate can reach 96%. Extensive experiments show that it can adapt to a variety of RFID application scenarios.",
      "year": 2020,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Weiqing Huang",
        "Yanfang Zhang",
        "Yue Feng"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/2551c23c2ffd1e75b22c8d9f511902854dcb4dfa",
      "pdf_url": "https://www.mdpi.com/1424-8220/20/8/2378/pdf",
      "publication_date": "2020-04-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "title": "Model Stealing Defense with Hybrid Fuzzy Models: Work-in-Progress",
      "abstract": "With increasing applications of Deep Neural Networks (DNNs) to edge computing systems, security issues have received more attentions. Particularly, model stealing attack is one of the biggest challenge to the privacy of models. To defend against model stealing attack, we propose a novel protection architecture with fuzzy models. Each fuzzy model is designed to generate wrong predictions corresponding to a particular category. In addition\u2019 we design a special voting strategy to eliminate the systemic errors, which can destroy the dark knowledge in predictions at the same time. Preliminary experiments show that our method substantially decreases the clone model's accuracy (up to 20%) without loss of inference accuracy for benign users.",
      "year": 2020,
      "venue": "International Conference on Hardware/Software Codesign and System Synthesis",
      "authors": [
        "Zicheng Gong",
        "Wei Jiang",
        "Jinyu Zhan",
        "Ziwei Song"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ebfeaa0e142b697d8fde00caa4f0b459a988e2ca",
      "pdf_url": "",
      "publication_date": "2020-09-20",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model",
        "model stealing defense"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9356a216e5ae48c0007b66687a3da47fc2bea834",
      "title": "Perturbing Inputs to Prevent Model Stealing",
      "abstract": "We show how perturbing inputs to machine learning services (ML-service) deployed in the cloud can protect against model stealing attacks. In our formulation, there is an ML-service that receives inputs from users and returns the output of the model. There is an attacker that is interested in learning the parameters of the ML-service. We use the linear and logistic regression models to illustrate how strategically adding noise to the inputs fundamentally alters the attacker\u2019s estimation problem. We show that even with infinite samples, the attacker would not be able to recover the true model parameters. We focus on characterizing the trade-off between the error in the attacker\u2019s estimate of the parameters with the error in the ML-service\u2019s output.",
      "year": 2020,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "J. Grana"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9356a216e5ae48c0007b66687a3da47fc2bea834",
      "pdf_url": "https://arxiv.org/pdf/2005.05823",
      "publication_date": "2020-05-12",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "title": "SCNet: A Neural Network for Automated Side-Channel Attack",
      "abstract": "The side-channel attack is an attack method based on the information gained about implementations of computer systems, rather than weaknesses in algorithms. Information about system characteristics such as power consumption, electromagnetic leaks and sound can be exploited by the side-channel attack to compromise the system. Much research effort has been directed towards this field. However, such an attack still requires strong skills, thus can only be performed effectively by experts. Here, we propose SCNet, which automatically performs side-channel attacks. And we also design this network combining with side-channel domain knowledge and different deep learning model to improve the performance and better to explain the result. The results show that our model achieves good performance with fewer parameters. The proposed model is a useful tool for automatically testing the robustness of computer systems.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Guanlin Li",
        "Chang Liu",
        "Han Yu",
        "Yanhong Fan",
        "Libang Zhang",
        "Zongyue Wang",
        "Meiqin Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "pdf_url": "",
      "publication_date": "2020-08-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "title": "A Low-Cost Image Encryption Method to Prevent Model Stealing of Deep Neural Network",
      "abstract": "Model stealing attack may happen by stealing useful data transmitted from embedded end to server end for an artificial intelligent systems. In this paper, we are interested in preventing model stealing of neural network for resource-constrained systems. We propose an Image Encryption based on Class Activation Map (IECAM) to encrypt information before transmitting in embedded end. According to class activation map, IECAM chooses certain key areas of the image to be encrypted with the purpose of reducing the model stealing risk of neural network. With partly encrypted information, IECAM can greatly reduce the time overheads of encryption/decryption in both embedded and server ends, especially for big size images. The experimental results demonstrate that our method can significantly reduce time overheads of encryption/decryption and the risk of model stealing compared with traditional methods.",
      "year": 2020,
      "venue": "J. Circuits Syst. Comput.",
      "authors": [
        "Wei Jiang",
        "Zicheng Gong",
        "Jinyu Zhan",
        "Zhiyuan He",
        "Weijia Pan"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/b97bc90c7106b6c44ffaa0c0ce81a67134524e89",
      "pdf_url": "",
      "publication_date": "2020-05-28",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "prevent model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "title": "Stealing Black-Box Functionality Using The Deep Neural Tree Architecture",
      "abstract": "This paper makes a substantial step towards cloning the functionality of black-box models by introducing a Machine learning (ML) architecture named Deep Neural Trees (DNTs). This new architecture can learn to separate different tasks of the black-box model, and clone its task-specific behavior. We propose to train the DNT using an active learning algorithm to obtain faster and more sample-efficient training. In contrast to prior work, we study a complex \"victim\" black-box model based solely on input-output interactions, while at the same time the attacker and the victim model may have completely different internal architectures. The attacker is a ML based algorithm whereas the victim is a generally unknown module, such as a multi-purpose digital chip, complex analog circuit, mechanical system, software logic or a hybrid of these. The trained DNT module not only can function as the attacked module, but also provides some level of explainability to the cloned model due to the tree-like nature of the proposed architecture.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Daniel Teitelman",
        "I. Naeh",
        "Shie Mannor"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "pdf_url": "",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "title": "Securing Machine Learning Architectures and Systems",
      "abstract": "Machine learning (ML), and deep learning in particular, have become a critical workload as they are becoming increasingly applied at the core of a wide range of application spaces. Computer systems, from the architecture up, have been impacted by ML in two primary directions: (1) ML is an increasingly important computing workload, with new accelerators and systems targeted to support both training and inference at scale; and (2) ML supporting computer system decisions, both during design and run times, with new machine learning based algorithms controlling systems to optimize their performance, reliability and robustness. In this paper, we will explore the intersection of security, ML and computing systems, identifying both security challenges and opportunities. Machine learning systems are vulnerable to new attacks including adversarial attacks crafted to fool a classifier to the attacker's advantage, membership inference attacks attempting to compromise the privacy of the training data, and model extraction attacks seeking to recover the hyperparameters of a (secret) model. Architecture can be a target of these attacks when supporting ML (or is supported by ML), but also provides an opportunity to develop defenses against them, which we will illustrate with three examples from our recent work. First, we show how ML based hardware malware detectors can be attacked with adversarial perturbations to the Malware and how we can develop detectors that resist these attacks. Second, we show an example of microarchitectural side channel attacks that can be used to extract the secret parameters of a neural network and potential defenses against it. Finally, we discuss how hardware and systems can be used to make ML more robust against adversarial and other attacks.",
      "year": 2020,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Shirin Haji Amin Shirazi",
        "Hoda Naghibijouybari",
        "N. Abu-Ghazaleh"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/65d434bfef6e62eb2e632f7d78f523292e0d9c0a",
      "pdf_url": "",
      "publication_date": "2020-09-07",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "title": "Special-purpose Model Extraction Attacks: Stealing Coarse Model with Fewer Queries",
      "abstract": "Model extraction (ME) attacks have been shown to cause financial losses for Machine-Learning-as-a-Service (MLaaS) providers. Attackers steal ML models on MLaaS platforms by building substitute models using queries to and responses from MLaaS platforms. The ML models targeted by attackers are called targeted models. In previous studies, researchers have assumed that attackers build substitute models that classify the same number of classes as targeted ones, which classify thousands or millions of classes to meet users' diverse expectations. We call such models general-purpose models. In fact, attackers can monetize stolen models if they accurately distinguish some classes from others. We call such models special-purpose models. For instance, a model that detects vehicles is useful for collision avoidance systems, and a model that detects wild animals is useful to drive them away from agricultural land. In this work, we investigate a threat of special-purpose ME attacks that steal special-purpose models. Our experimental results show that attackers can build an accurate special-purpose model, which achieves an 80% f-measure, with as few as 100 queries in the worst case. We discuss the difficulty in preventing the attacks with previously proposed defense methods and point out the necessity of a new defense method.",
      "year": 2020,
      "venue": "International Conference on Trust, Security and Privacy in Computing and Communications",
      "authors": [
        "Rina Okada",
        "Zen Ishikura",
        "Toshiki Shibahara",
        "Satoshi Hasegawa"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d6a0da1403eb183a24541ac5636eb7a44223b7dc",
      "pdf_url": "",
      "publication_date": "2020-12-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "steal ML model",
        "steal ML models"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "05b8495df20c2539d25448b278660e776175fccb",
      "title": "Research on Side-Channel Attack Based on the Synergy between SNR and Convolutional Neural Networks",
      "abstract": "The data encryption process in the encryption chip will be leaked by means of timing operation time, probing data collecting operation power and electromagnetic signal interception, which makes side channel attack possible. Nowadays, the research on template creation of template attacks has shifted from Gaussian distribution to the use of machine learning algorithms to create templates. With many parameters, it is difficult to find a suitable network structure. Based on many experimental studies, the experience of a convolutional neural network structure suitable for side channel analysis is summarized and proposed.",
      "year": 2020,
      "venue": "Journal of Physics: Conference Series",
      "authors": [
        "Zixin Liu",
        "Ting Zhu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/05b8495df20c2539d25448b278660e776175fccb",
      "pdf_url": "https://doi.org/10.1088/1742-6596/1575/1/012026",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "title": "Bident Structure for Neural Network Model Protection",
      "abstract": ": Deep neural networks are widely deployed in a variety of application areas to provide real-time inference services, such as mobile phones, autonomous vehicles and industrial automation. Deploying trained models in end-user devices rises high demands on protecting models against model stealing attacks. To tackle this concern, applying cryptography algorithms and using trusted execution environments have been proposed. However, both approaches cause significant overhead on inference time. With the support of trusted execution environment, we propose bident-structure networks to protect the neural networks while maintaining inference efficiency. Our main idea is inspired by the secret-sharing concept from cryptography community, where we treat the neural network as the secret to be protected. We prove the feasibility of bident-structure methods by empirical experiments on MNIST. Experimental results also demonstrate that efficiency overhead can be reduced by compressing sub-networks running in trusted execution environments.",
      "year": 2020,
      "venue": "International Conference on Information Systems Security and Privacy",
      "authors": [
        "Hsiao-Ying Lin",
        "Chengfang Fang",
        "Jie Shi"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/12bf1e90f285b23e27adaf6668d6a429ccc54887",
      "pdf_url": "https://doi.org/10.5220/0008923403770384",
      "publication_date": null,
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "title": "Monitoring-based Differential Privacy Mechanism Against Query-Flooding Parameter Duplication Attack",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this paper, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Haonan Yan",
        "Xiaoguang Li",
        "Hui Li",
        "Jiamin Li",
        "Wenhai Sun",
        "Fenghua Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/27ce5e10571e9a4e66ed0cb2eaf348582750767a",
      "pdf_url": "",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "title": "Mitigating Query-Flooding Parameter Duplication Attack on Regression Models with High-Dimensional Gaussian Mechanism",
      "abstract": "Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Differential privacy (DP) has been considered a promising technique to mitigate this attack. However, we find that the vulnerability persists when regression models are being protected by current DP solutions. We show that the adversary can launch a query-flooding parameter duplication (QPD) attack to infer the model information by repeated queries. \nTo defend against the QPD attack on logistic and linear regression models, we propose a novel High-Dimensional Gaussian (HDG) mechanism to prevent unauthorized information disclosure without interrupting the intended services. In contrast to prior work, the proposed HDG mechanism will dynamically generate the privacy budget and random noise for different queries and their results to enhance the obfuscation. Besides, for the first time, HDG enables an optimal privacy budget allocation that automatically determines the minimum amount of noise to be added per user-desired privacy level on each dimension. We comprehensively evaluate the performance of HDG using real-world datasets and shows that HDG effectively mitigates the QPD attack while satisfying the privacy requirements. We also prepare to open-source the relevant codes to the community for further research.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Xiaoguang Li",
        "Hui Li",
        "Haonan Yan",
        "Zelei Cheng",
        "Wenhai Sun",
        "Hui Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9a369aaa9f2ee96ca99df66cc6c5c6f46889b8a2",
      "pdf_url": "",
      "publication_date": "2020-02-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "title": "Adversarial Imitation Attack",
      "abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Mingyi Zhou",
        "Jing Wu",
        "Yipeng Liu",
        "Xiaolin Huang",
        "Shuaicheng Liu",
        "Xiang Zhang",
        "Ce Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/629c5459c03d7327829eb866cd3bcd2f158ad3ba",
      "pdf_url": "",
      "publication_date": "2020-03-28",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "title": "Leveraging Extracted Model Adversaries for Improved Black Box Attacks",
      "abstract": "We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY\u2014a white box attack\u2014performed on the approximate model by 25% F1, and the ADDSENT attack\u2014a black box attack\u2014by 11% F1.",
      "year": 2020,
      "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
      "authors": [
        "Naveen Jafer Nizar",
        "Ari Kobren"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "pdf_url": "https://www.aclweb.org/anthology/2020.blackboxnlp-1.6.pdf",
      "publication_date": "2020-10-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "52a222d38a8640499010d470d5589a81882bc425",
      "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
      "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input. \nTo extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights. \nWe perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alexey Kurakin",
        "Nicolas Papernot"
      ],
      "citation_count": 424,
      "url": "https://www.semanticscholar.org/paper/52a222d38a8640499010d470d5589a81882bc425",
      "pdf_url": "",
      "publication_date": "2019-09-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "title": "Model inversion attacks against collaborative inference",
      "abstract": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and datasets, to show their effectiveness and generalization. We also study the characteristics of deep learning models that make them susceptible to such inference privacy threats. This provides insights and guidelines to develop more privacy-preserving collaborative systems and algorithms.",
      "year": 2019,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Zecheng He",
        "Tianwei Zhang",
        "R. Lee"
      ],
      "citation_count": 354,
      "url": "https://www.semanticscholar.org/paper/2cb1f4688f8ba457b4fa2ed36deae5a98f4249ca",
      "pdf_url": "",
      "publication_date": "2019-12-09",
      "keywords_matched": [
        "steal model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
      "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kalpesh Krishna",
        "Gaurav Singh Tomar",
        "Ankur P. Parikh",
        "Nicolas Papernot",
        "Mohit Iyyer"
      ],
      "citation_count": 230,
      "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
      "pdf_url": "",
      "publication_date": "2019-10-27",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c40441bd48c08ab7db9443afdb3084387b06e61f",
      "title": "DAWN: Dynamic Adversarial Watermarking of Neural Networks",
      "abstract": "Training machine learning (ML) models is expensive in terms of computational power, amounts of labeled data and human expertise. Thus, ML models constitute business value for their owners. Embedding digital watermarks during model training allows a model owner to later identify their models in case of theft or misuse. However, model functionality can also be stolen via model extraction, where an adversary trains a surrogate model using results returned from a prediction API of the original model. Recent work has shown that model extraction is a realistic threat. Existing watermarking schemes are ineffective against model extraction since it is the adversary who trains the surrogate model. In this paper, we introduce DAWN (Dynamic Adversarial Watermarking of Neural Networks), the first approach to use watermarking to deter model extraction theft. Unlike prior watermarking schemes, DAWN does not impose changes to the training process but operates at the prediction API of the protected model, by dynamically changing the responses for a small subset of queries (e.g., 0.5%) from API clients. This set is a watermark that will be embedded in case a client uses its queries to train a surrogate model. We show that DAWN is resilient against two state-of-the-art model extraction attacks, effectively watermarking all extracted surrogate models, allowing model owners to reliably demonstrate ownership (with confidence greater than 1-2-64), incurring negligible loss of prediction accuracy (0.03-0.5%).",
      "year": 2019,
      "venue": "ACM Multimedia",
      "authors": [
        "Sebastian Szyller",
        "B. Atli",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 203,
      "url": "https://www.semanticscholar.org/paper/c40441bd48c08ab7db9443afdb3084387b06e61f",
      "pdf_url": "https://aaltodoc.aalto.fi/bitstreams/11ec7679-d118-4245-aad9-c8ff13a384e7/download",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "da10f79f983fd4fbd589ed7ffa68d33964841443",
      "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
      "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tribhuvanesh Orekondy",
        "B. Schiele",
        "Mario Fritz"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/da10f79f983fd4fbd589ed7ffa68d33964841443",
      "pdf_url": "",
      "publication_date": "2019-06-26",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "DNN model stealing",
        "functionality stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
      "abstract": "In Machine Learning as a Service, a provider trains a deep neural network and provides many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a \\emph{surrogate model} from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call \\emph{conferrable} adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the unremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches an AUC of 1.0 in verifying surrogates, compared to an AUC of 0.63 by previous fingerprints.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Nils Lukas",
        "Yuxuan Zhang",
        "F. Kerschbaum"
      ],
      "citation_count": 169,
      "url": "https://www.semanticscholar.org/paper/5effd428e3012061ba3d8b32f1952c2938c7ab7b",
      "pdf_url": "",
      "publication_date": "2019-12-02",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "title": "Defending Against Model Stealing Attacks With Adaptive Misinformation",
      "abstract": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose \"Adaptive Misinformation\" to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker's clone model (by up to 40%), while minimally impacting the accuracy (<0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead.",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "S. Kariyappa",
        "Moinuddin K. Qureshi"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/e6c55623cf56a70659e612eb6a741a4de7545ba7",
      "pdf_url": "https://arxiv.org/pdf/1911.07100",
      "publication_date": "2019-11-16",
      "keywords_matched": [
        "model stealing",
        "model stealing attack",
        "clone model"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "title": "Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning architectures are readily available, but obtaining the high quality labeled data for training is costly. Pre-trained models available as cloud services can be used to generate this costly labeled data, and would allow an attacker to replicate trained models, effectively stealing them. Limiting the information provided by cloud based models by omitting class probabilities has been proposed as a means of protection but significantly impacts the utility of the models. In this work, we illustrate how cloud based models can still provide useful class probability information for users, while significantly limiting the ability of an adversary to steal the model. Our defense perturbs the model's final activation layer, slightly altering the output probabilities. This forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate our defense under diverse scenarios and defense aware attacks. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or increase the number of queries required by an adversary 64 fold, all with a negligible decrease in the protected model accuracy.",
      "year": 2019,
      "venue": "2019 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 109,
      "url": "https://www.semanticscholar.org/paper/555b847ee9c39c12d5764f4e11b888b331a89cfb",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8834415/8844588/08844598.pdf",
      "publication_date": "2019-05-19",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "title": "Open DNN Box by Power Side-Channel Attack",
      "abstract": "Deep neural networks are becoming popular and important assets of many AI companies. However, recent studies indicate that they are also vulnerable to adversarial attacks. Adversarial attacks can be either white-box or black-box. The white-box attacks assume full knowledge of the models while the black-box ones assume none. In general, revealing more internal information can enable much more powerful and efficient attacks. However, in most real-world applications, the internal information of embedded AI devices is unavailable. Therefore, in this brief, we propose a side-channel information based technique to reveal the internal information of black-box models. Specifically, we have made the following contributions: (1) different from previous works, we use side-channel information to reveal internal network architecture in embedded devices; (2) we construct models for internal parameter estimation that no research has been reached yet; and (3) we validate our methods on real-world devices and applications. The experimental results show that our method can achieve 96.50% accuracy on average. Such results suggest that we should pay strong attention to the security problem of many AI devices, and further propose corresponding defensive strategies in the future.",
      "year": 2019,
      "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
      "authors": [
        "Yun Xiang",
        "Zhuangzhi Chen",
        "Zuohui Chen",
        "Zebin Fang",
        "Haiyang Hao",
        "Jinyin Chen",
        "Yi Liu",
        "Zhefu Wu",
        "Qi Xuan",
        "Xiaoniu Yang"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "pdf_url": "https://arxiv.org/pdf/1907.10406",
      "publication_date": "2019-07-21",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "title": "MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel Protection",
      "abstract": "Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. The research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored. This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the first countermeasures against these attacks by augmenting masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X FPGA board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by $ 2.8\\times$ and $2.3\\times$.",
      "year": 2019,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Anuj Dubey",
        "Rosario Cammarota",
        "Aydin Aysu"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/6854c4c8dfca1de802f2662db62deb7ba2bc10b2",
      "pdf_url": "https://arxiv.org/pdf/1910.13063",
      "publication_date": "2019-10-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "9329e150170534e99d169a345aa68aeb004279ca",
      "title": "The Attack of the Clones against Proof-of-Authority",
      "abstract": "In this paper, we explore vulnerabilities and countermeasures of the recently proposed blockchain consensus based on proof-of-authority. The proof-of-work blockchains, like Bitcoin and Ethereum, have been shown both theoretically and empirically vulnerable to double spending attacks. This is why Byzantine fault tolerant consensus algorithms have gained popularity in the blockchain context for their ability to tolerate a limited number t of attackers among n participants. We formalize the recently proposed proof-of-authority consensus algorithms that are Byzantine fault tolerant by describing the Aura and Clique protocols present in the two mainstream implementations of Ethereum. We then introduce the Cloning Attack and show how to apply it to double spend in each of these protocols with a single malicious node. Our results show that the Cloning Attack against Aura is always successful while the same attack against Clique is about twice as fast and succeeds in most cases.",
      "year": 2019,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Parinya Ekparinya",
        "Vincent Gramoli",
        "Guillaume Jourjon"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/9329e150170534e99d169a345aa68aeb004279ca",
      "pdf_url": "https://doi.org/10.14722/ndss.2020.24082",
      "publication_date": "2019-02-26",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "304ac5d62f2888e94078715413c40cf00b58ac4f",
      "title": "Efficiently Stealing your Machine Learning Models",
      "abstract": "Machine Learning as a Service (MLaaS) is a growing paradigm in the Machine Learning (ML) landscape. More and more ML models are being uploaded to the cloud and made accessible from all over the world. Creating good ML models, however, can be expensive and the used data is often sensitive. Recently, Secure Multi-Party Computation (SMPC) protocols for MLaaS have been proposed, which protect sensitive user data and ML models at the expense of substantially higher computation and communication than plaintext evaluation. In this paper, we show that for a subset of ML models used in MLaaS, namely Support Vector Machines (SVMs) and Support Vector Regression Machines (SVRs) which have found many applications to classifying multimedia data such as texts and images, it is possible for adversaries to passively extract the private models even if they are protected by SMPC, using known and newly devised model extraction attacks. We show that our attacks are not only theoretically possible but also practically feasible and cheap, which makes them lucrative to financially motivated attackers such as competitors or customers. We perform model extraction attacks on the homomorphic encryption-based protocol for privacy-preserving SVR-based indoor localization by Zhang et al. (International Workshop on Security 2016). We show that it is possible to extract a highly accurate model using only 854 queries with the estimated cost of $0.09 on the Amazon ML platform, and our attack would take only 7 minutes over the Internet. Also, we perform our model extraction attacks on SVM and SVR models trained on publicly available state-of-the-art ML datasets.",
      "year": 2019,
      "venue": "WPES@CCS",
      "authors": [
        "R. Reith",
        "T. Schneider",
        "Oleksandr Tkachenko"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/304ac5d62f2888e94078715413c40cf00b58ac4f",
      "pdf_url": "https://encrypto.de/papers/RST19.pdf",
      "publication_date": "2019-11-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "title": "A framework for the extraction of Deep Neural Networks by leveraging public data",
      "abstract": "Machine learning models trained on confidential datasets are increasingly being deployed for profit. Machine Learning as a Service (MLaaS) has made such models easily accessible to end-users. Prior work has developed model extraction attacks, in which an adversary extracts an approximation of MLaaS models by making black-box queries to it. However, none of these works is able to satisfy all the three essential criteria for practical model extraction: (1) the ability to work on deep learning models, (2) the non-requirement of domain knowledge and (3) the ability to work with a limited query budget. We design a model extraction framework that makes use of active learning and large public datasets to satisfy them. We demonstrate that it is possible to use this framework to steal deep classifiers trained on a variety of datasets from image and text domains. By querying a model via black-box access for its top prediction, our framework improves performance on an average over a uniform noise baseline by 4.70x for image tasks and 2.11x for text tasks respectively, while using only 30% (30,000 samples) of the public dataset at its disposal.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Soham Pal",
        "Yash Gupta",
        "Aditya Shukla",
        "Aditya Kanade",
        "S. Shevade",
        "V. Ganapathy"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/7dd022d67f8a1a387ca31ff6233ea7c2743c9d85",
      "pdf_url": "",
      "publication_date": "2019-05-22",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fa30abd5d9233ccde88502d5bae300e6666d3417",
      "title": "Make Some Noise. Unleashing the Power of Convolutional Neural Networks for Profiled Side-channel Analysis",
      "abstract": "Profiled side-channel analysis based on deep learning, and more precisely Convolutional Neural Networks, is a paradigm showing significant potential. The results, although scarce for now, suggest that such techniques are even able to break cryptographic implementations protected with countermeasures. In this paper, we start by proposing a new Convolutional Neural Network instance able to reach high performance for a number of considered datasets. We compare our neural network with the one designed for a particular dataset with masking countermeasure and we show that both are good designs but also that neither can be considered as a superior to the other one.Next, we address how the addition of artificial noise to the input signal can be actually beneficial to the performance of the neural network. Such noise addition is equivalent to the regularization term in the objective function. By using this technique, we are able to reduce the number of measurements needed to reveal the secret key by orders of magnitude for both neural networks. Our new convolutional neural network instance with added noise is able to break the implementation protected with the random delay countermeasure by using only 3 traces in the attack phase. To further strengthen our experimental results, we investigate the performance with a varying number of training samples, noise levels, and epochs. Our findings show that adding noise is beneficial throughout all training set sizes and epochs.",
      "year": 2019,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Jaehun Kim",
        "S. Picek",
        "Annelie Heuser",
        "S. Bhasin",
        "A. Hanjalic"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/fa30abd5d9233ccde88502d5bae300e6666d3417",
      "pdf_url": "https://tches.iacr.org/index.php/TCHES/article/download/8292/7642",
      "publication_date": "2019-05-09",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "703f6008899db58f8c2bf454f6e146efb1df38e3",
      "title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?",
      "abstract": "Recently, machine learning (ML) has introduced advanced solutions to many domains. Since ML models provide business advantage to model owners, protecting intellectual property of ML models has emerged as an important consideration. Confidentiality of ML models can be protected by exposing them to clients only via prediction APIs. However, model extraction attacks can steal the functionality of ML models using the information leaked to clients through the results returned via the API. In this work, we question whether model extraction is a serious threat to complex, real-life ML models. We evaluate the current state-of-the-art model extraction attack (Knockoff nets) against complex models. We reproduce and confirm the results in the original paper. But we also show that the performance of this attack can be limited by several factors, including ML model architecture and the granularity of API response. Furthermore, we introduce a defense based on distinguishing queries used for Knockoff nets from benign queries. Despite the limitations of the Knockoff nets, we show that a more realistic adversary can effectively steal complex ML models and evade known defenses.",
      "year": 2019,
      "venue": "Communications in Computer and Information Science",
      "authors": [
        "B. Atli",
        "Sebastian Szyller",
        "Mika Juuti",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/703f6008899db58f8c2bf454f6e146efb1df38e3",
      "pdf_url": "",
      "publication_date": "2019-10-11",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "knockoff nets",
        "knockoff net"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e36c9f2c5c4791cf504760988de33b39a013373f",
      "title": "Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints",
      "abstract": "As neural networks continue their reach into nearly every aspect of software operations, the details of those networks become an increasingly sensitive subject. Even those that deploy neural networks embedded in physical devices may wish to keep the inner working of their designs hidden -- either to protect their intellectual property or as a form of protection from adversarial inputs. The specific problem we address is how, through heavy system stack, given noisy and imperfect memory traces, one might reconstruct the neural network architecture including the set of layers employed, their connectivity, and their respective dimension sizes. Considering both the intra-layer architecture features and the inter-layer temporal association information introduced by the DNN design empirical experience, we draw upon ideas from speech recognition to solve this problem. We show that off-chip memory address traces and PCIe events provide ample information to reconstruct such neural network architectures accurately. We are the first to propose such accurate model extraction techniques and demonstrate an end-to-end attack experimentally in the context of an off-the-shelf Nvidia GPU platform with full system stack. Results show that the proposed techniques achieve a high reverse engineering accuracy and improve the one's ability to conduct targeted adversarial attack with success rate from 14.6\\%$\\sim$25.5\\% (without network architecture knowledge) to 75.9\\% (with extracted network architecture).",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Lei Deng",
        "Shuangchen Li",
        "Xinfeng Xie",
        "Yu Ji",
        "Yufei Ding",
        "Chang Liu",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/e36c9f2c5c4791cf504760988de33b39a013373f",
      "pdf_url": "",
      "publication_date": "2019-03-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "title": "High-resolution palaeovalley classification from airborne\nelectromagnetic imaging and deep neural network training using\ndigital elevation model data",
      "abstract": "Abstract. Palaeovalleys are buried ancient river valleys that often form productive aquifers, especially in the semi-arid and arid areas of Australia. Delineating their extent and hydrostratigraphy is however a challenging task in groundwater system characterization. This study developed a methodology based on the deep learning super-resolution convolutional neural network (SRCNN) approach, to convert electrical conductivity (EC) estimates from an airborne electromagnetic (AEM) survey in South Australia to a high-resolution binary palaeovalley map. The SRCNN was trained and tested with a synthetic training dataset, where valleys were generated from readily available digital elevation model (DEM) data from the AEM survey area. Electrical conductivities typical of valley sediments were generated by Archie\u2019s Law, and subsequently blurred by down-sampling and bicubic interpolation to represent noise from the AEM survey, inversion and interpolation. After a model training step, the SRCNN successfully removed such noise, and reclassified the low-resolution, unimodal but skewed EC values into a high-resolution palaeovalley index following a bimodal distribution. The latter allows distinguishing valley from non-valley pixels. Furthermore, a realistic spatial connectivity structure of the palaeovalley was predicted when compared with borehole lithology logs and valley bottom flatness indicator. Overall the methodology permitted to better constrain the three-dimensional palaeovalley geometry from AEM images that are becoming more widely available for groundwater prospecting.\n",
      "year": 2019,
      "venue": "",
      "authors": [
        "Zhenjiao Jiang",
        "D. Mallants",
        "L. Peeters",
        "Lei Gao",
        "G. Mari\u00e9thoz"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/7c2427863bf72ce1b679735ff46d6832a2e12d9b",
      "pdf_url": "https://hess.copernicus.org/articles/23/2561/2019/hess-23-2561-2019.pdf",
      "publication_date": "2019-01-21",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b82d52b95040c0682b7a8baa9254eae79b068505",
      "title": "Adversarial Model Extraction on Graph Neural Networks",
      "abstract": "Along with the advent of deep neural networks came various methods of exploitation, such as fooling the classifier or contaminating its training data. Another such attack is known as model extraction, where provided API access to some black box neural network, the adversary extracts the underlying model. This is done by querying the model in such a way that the underlying neural network provides enough information to the adversary to be reconstructed. While several works have achieved impressive results with neural network extraction in the propositional domain, this problem has not yet been considered over the relational domain, where data samples are no longer considered to be independent and identically distributed (iid). Graph Neural Networks (GNNs) are a popular deep learning framework to perform machine learning tasks over relational data. In this work, we formalize an instance of GNN extraction, present a solution with preliminary results, and discuss our assumptions and future directions.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "David DeFazio",
        "Arti Ramesh"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/b82d52b95040c0682b7a8baa9254eae79b068505",
      "pdf_url": "",
      "publication_date": "2019-12-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "d2affd606828c410d9a68c627a099101220ac8cb",
      "title": "A Spintronics Memory PUF for Resilience Against Cloning Counterfeit",
      "abstract": "With the widespread use of electronic devices in embedding or processing sensitive information, new hardware security primitives have emerged to improve the shortcomings of traditional secure data storage. One of these solutions, the physically unclonable function (PUF), which is widely used for authentication and cryptography applications, extracts the unique and unclonable value from the circuit physical properties. However, a cloning attack on memory-based PUF has been demonstrated from the circuit back-side tampering, questioning its unclonable property and opening counterfeiting vulnerability in an untrusted supply chain. Spin transfer torque-magnetic random-access memory (STT-MRAM) is a promising technology due to its nonvolatility, scalability, and CMOS compatibility, therefore envisioned to be used in many embedded secure devices. In this paper, we reveal the vulnerability of existing STT-MRAM PUF solutions to back-side attacks by modeling different levels of tampering and their impact at electrical and logical levels. We propose a tamper resilient methodology for the STT-MRAM PUF design based on its switching properties. The resilience of the proposed solution to different levels of tampering and a 100% detection are confirmed with simulation results.",
      "year": 2019,
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "authors": [
        "Samir Ben Dodo",
        "R. Bishnoi",
        "Sarath Mohanachandran Nair",
        "M. Tahoori"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/d2affd606828c410d9a68c627a099101220ac8cb",
      "pdf_url": "",
      "publication_date": "2019-11-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f45975189d1f73b313dc65401724a5e21f635b3b",
      "title": "Adversarial Exploitation of Policy Imitation",
      "abstract": "This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",
      "year": 2019,
      "venue": "AISafety@IJCAI",
      "authors": [
        "Vahid Behzadan",
        "W. Hsu"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/f45975189d1f73b313dc65401724a5e21f635b3b",
      "pdf_url": "",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "title": "Model Weight Theft With Just Noise Inputs: The Curious Case of the Petulant Attacker",
      "abstract": "This paper explores the scenarios under which an attacker can claim that 'Noise and access to the softmax layer of the model is all you need' to steal the weights of a convolutional neural network whose architecture is already known. We were able to achieve 96% test accuracy using the stolen MNIST model and 82% accuracy using the stolen KMNIST model learned using only i.i.d. Bernoulli noise inputs. We posit that this theft-susceptibility of the weights is indicative of the complexity of the dataset and propose a new metric that captures the same. The goal of this dissemination is to not just showcase how far knowing the architecture can take you in terms of model stealing, but to also draw attention to this rather idiosyncratic weight learnability aspects of CNNs spurred by i.i.d. noise input. We also disseminate some initial results obtained with using the Ising probability distribution in lieu of the i.i.d. Bernoulli distribution.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Nicholas Roberts",
        "Vinay Uday Prabhu",
        "Matthew McAteer"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/a210fd4656c3fe4a7be8770b3944a672d84c9171",
      "pdf_url": "",
      "publication_date": "2019-12-19",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "title": "Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data",
      "abstract": "As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.",
      "year": 2019,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Itay Mosafi",
        "Eli David",
        "N. Netanyahu"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/06f2af0d0d2701b7af029269eaa1c94855dd5d50",
      "pdf_url": "https://arxiv.org/pdf/1912.03959",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "title": "Poster: Recovering the Input of Neural Networks via Single Shot Side-channel Attacks",
      "abstract": "The interplay between machine learning and security is becoming more prominent. New applications using machine learning also bring new security risks. Here, we show it is possible to reverse-engineer the inputs to a neural network with only a single-shot side-channel measurement assuming the attacker knows the neural network architecture being used.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "pdf_url": "",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "title": "Towards Privacy and Security of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, \\eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fb647bbef5d0e8d202b305133d6a5c85bd9462b3",
      "pdf_url": "",
      "publication_date": "2019-11-28",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "title": "VAWS: Vulnerability Analysis of Neural Networks using Weight Sensitivity",
      "abstract": "The advancement in deep learning has taken the technology world by storm in the last decade. Although, there is enormous progress made in terms of algorithm performance, the security aspect of these algorithms has not received a lot of attention from the research community. As more industries start to adopt these algorithms the issue of security is becoming even more relevant. Security vulnerabilities in machine learning (ML), especially in deep neural networks (DNN), is becoming a concern. Various techniques have been proposed, including data manipulations and model stealing. However, most of them are focused on ML algorithms and target threat models that require access to training dataset. In this paper, we present a methodology that analyzes the DNN weight parameters under the threat model that assumes the attacker has the access to the weight memory only. This analysis is then used to develop an attack that manipulates weight parameters with respect to their sensitivity. To evaluate this attack, we implemented our methodology on a MLP trained on IRIS dataset and LeNet (DNN architecture) trained on MNIST dataset. Our experimental results demonstrate that alteration of model parameters results in subtle accuracy drop of the model. Depending on the applications such subtle changes can cause significant system malfunction or disruption, for example in vision-based industrial applications. Our results show that using our methodology a subtle accuracy drop can be achieved in a reasonable amount of time with very few parameter changes.",
      "year": 2019,
      "venue": "Midwest Symposium on Circuits and Systems",
      "authors": [
        "Muluken Hailesellasie",
        "Jacob Nelson",
        "Faiq Khalid",
        "S. R. Hasan"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/dbbd56da86525457c32b77fe87ff6b8b87fd5d78",
      "pdf_url": "",
      "publication_date": "2019-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "title": "Quantifying (Hyper) Parameter Leakage in Machine Learning",
      "abstract": "Machine Learning models are extensively used for various multimedia applications and are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks that reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Extraction attacks proposed in the literature are empirically evaluated and lack a theoretical framework to measure the information leaked under such attacks. In this work, we propose a novel model-agnostic probabilistic framework, AIRAVATA, to quantify information leakage using partial knowledge and limited evidences from model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training for stealing the target model functionality. We use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in the literature to reason about the attack efficacy. This provides a practical tool to identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model and estimate the relative threats from different attacks.",
      "year": 2019,
      "venue": "IEEE International Conference on Multimedia Big Data",
      "authors": [
        "Vasisht Duddu",
        "D. V. Rao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a8a80de402cef8a41a85e1220e24818fc010e6f8",
      "pdf_url": "https://arxiv.org/pdf/1910.14409",
      "publication_date": "2019-10-31",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3ba1d07d015d843dbfa6b6b4a8e433b3ee2caa18",
      "title": "Research on Key Protocol Technology of Safe Access and Imitation Attack for Power Industrial Control Terminal Equipment",
      "abstract": "Industrial control system is widely used in social production activities and its security is directly related to national security and social stability. In this paper, the security threats and protection status of power industry control system are analyzed. A framework of information security evaluation system for power industry control system is proposed, which consists of three parts: experimental verification environment, product detection capability and security service capability, and the construction scheme of the system is discussed. A hardware-in-the-loop simulation and verification platform is constructed to realize the simulation and verification of power industry control system tools. The vulnerability of power industry control protocol can be found in depth. The problem of ambiguous attack mechanism and lack of verification means of power industry control system is discussed, which can provide support for the research of attack and protection of power industry control system.",
      "year": 2019,
      "venue": "IOP Conference Series: Materials Science and Engineering",
      "authors": [
        "Xiaoqiang Wang",
        "Rui",
        "Zhao Xuehai Yu",
        "Jinye Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3ba1d07d015d843dbfa6b6b4a8e433b3ee2caa18",
      "pdf_url": "https://doi.org/10.1088/1757-899x/677/4/042068",
      "publication_date": "2019-12-10",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4582e2350e4822834dcf266522690722dd4430d4",
      "title": "PRADA: Protecting Against DNN Model Stealing Attacks",
      "abstract": "Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
      "year": 2018,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Mika Juuti",
        "Sebastian Szyller",
        "A. Dmitrenko",
        "Samuel Marchal",
        "N. Asokan"
      ],
      "citation_count": 481,
      "url": "https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8790377/8806708/08806737.pdf",
      "publication_date": "2018-05-07",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack",
        "model extraction attack",
        "DNN model stealing"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "title": "Reverse Engineering Convolutional Neural Networks Through Side-channel Information Leaks",
      "abstract": "A convolutional neural network (CNN) model represents a crucial piece of intellectual property in many applications. Revealing its structure or weights would leak confidential information. In this paper we present novel reverse-engineering attacks on CNNs running on a hardware accelerator, where an adversary can feed inputs to the accelerator and observe the resulting off-chip memory accesses. Our study shows that even with data encryption, the adversary can infer the underlying network structure by exploiting the memory and timing side-channels. We further identify the information leakage on the values of weights when a CNN accelerator performs dynamic zero pruning for off-chip memory accesses. Overall, this work reveals the importance of hiding off-chip memory access pattern to truly protect confidential CNN models.",
      "year": 2018,
      "venue": "Design Automation Conference",
      "authors": [
        "Weizhe Hua",
        "Zhiru Zhang",
        "G. Suh"
      ],
      "citation_count": 274,
      "url": "https://www.semanticscholar.org/paper/6c0aef543a15edc1a52fe3490281e3fe36dac486",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators",
      "abstract": "Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.",
      "year": 2018,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "Lingxiao Wei",
        "Yannan Liu",
        "Bo Luo",
        "Yu LI",
        "Qiang Xu"
      ],
      "citation_count": 220,
      "url": "https://www.semanticscholar.org/paper/669a04d8bbf8d530a8d6e1900f8b63dd906b4050",
      "pdf_url": "https://arxiv.org/pdf/1803.05847",
      "publication_date": "2018-03-05",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "title": "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
      "abstract": "In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copy them. Recent studies revealed that stateof-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.Given these facts, in this paper, we investigate if a target blackbox CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.This hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both nonproblem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.",
      "year": 2018,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Jacson Rodrigues Correia-Silva",
        "Rodrigo Berriel",
        "C. Badue",
        "A. D. Souza",
        "Thiago Oliveira-Santos"
      ],
      "citation_count": 185,
      "url": "https://www.semanticscholar.org/paper/c0be23ae7f327f9415e583aee1936b9932c9b58b",
      "pdf_url": "https://arxiv.org/pdf/1806.05476",
      "publication_date": "2018-06-14",
      "keywords_matched": [
        "copycat CNN"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "title": "Exploring Connections Between Active Learning and Model Extraction",
      "abstract": "Machine learning is being increasingly used by individuals, research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS) - cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise privacy concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramer et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Varun Chandrasekaran",
        "Kamalika Chaudhuri",
        "Irene Giacomelli",
        "S. Jha",
        "Songbai Yan"
      ],
      "citation_count": 177,
      "url": "https://www.semanticscholar.org/paper/35c863e151e47b6dbf6356e9a1abaa2a0eeab3fc",
      "pdf_url": "",
      "publication_date": "2018-11-05",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "title": "Stealing Neural Networks via Timing Side Channels",
      "abstract": "Deep learning is gaining importance in many applications. However, Neural Networks face several security and privacy threats. This is particularly significant in the scenario where Cloud infrastructures deploy a service with Neural Network model at the back end. Here, an adversary can extract the Neural Network parameters, infer the regularization hyperparameter, identify if a data point was part of the training data, and generate effective transferable adversarial examples to evade classifiers. This paper shows how a Neural Network model is susceptible to timing side channel attack. In this paper, a black box Neural Network extraction attack is proposed by exploiting the timing side channels to infer the depth of the network. Although, constructing an equivalent architecture is a complex search problem, it is shown how Reinforcement Learning with knowledge distillation can effectively reduce the search space to infer a target model. The proposed approach has been tested with VGG architectures on CIFAR10 data set. It is observed that it is possible to reconstruct substitute models with test accuracy close to the target models and the proposed approach is scalable and independent of type of Neural Network architectures.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Vasisht Duddu",
        "D. Samanta",
        "D. V. Rao",
        "V. Balas"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/f5014e34ed13191082cd20cc279ca4cc9adee84f",
      "pdf_url": "",
      "publication_date": "2018-12-31",
      "keywords_matched": [
        "neural network extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "title": "MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
      "abstract": "Machine Learning as a Service (MLaaS) is a popular and convenient way to access a trained machine learning (ML) model trough an API. However, if the user\u2019s input is sensitive, sending it to the server is not an option. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model. As a solution, we propose MLCapsule, a guarded offline deployment of MLaaS. MLCapsule executes the machine learning model locally on the user\u2019s client and therefore the data never leaves the client. Meanwhile, we show that MLCapsule is able to offer the service provider the same level of control and security of its model as the commonly used server-side execution. Beyond protecting against direct model access, we demonstrate that MLCapsule allows for implementing defenses against advanced attacks on machine learning models such as model stealing, reverse engineering and membership inference.",
      "year": 2018,
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "L. Hanzlik",
        "Yang Zhang",
        "Kathrin Grosse",
        "A. Salem",
        "Maximilian Augustin",
        "M. Backes",
        "Mario Fritz"
      ],
      "citation_count": 113,
      "url": "https://www.semanticscholar.org/paper/8589aa697cd170c793c4c729b81b6f6dfacb012c",
      "pdf_url": "https://arxiv.org/pdf/1808.00590",
      "publication_date": "2018-08-01",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "title": "Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks",
      "abstract": "Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim's deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim's entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations. Our empirical security analysis represents a step toward understanding the DNNs' vulnerability to cache side-channel attacks.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Sanghyun Hong",
        "Michael Davinroy",
        "Yigitcan Kaya",
        "S. Locke",
        "Ian Rackow",
        "Kevin Kulda",
        "Dana Dachman-Soled",
        "Tudor Dumitras"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/dcbd26094751f5064a6d9b8c7e0a347d49e0d3a5",
      "pdf_url": "",
      "publication_date": "2018-09-27",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "905ad646e5745afe6a3b02617cd8452655232c0d",
      "title": "CSI Neural Network: Using Side-channels to Recover Your Artificial Neural Network Information",
      "abstract": "Machine learning has become mainstream across industries. Numerous examples proved the validity of it for security applications. In this work, we investigate how to reverse engineer a neural network by using only power side-channel information. To this end, we consider a multilayer perceptron as the machine learning architecture of choice and assume a non-invasive and eavesdropping attacker capable of measuring only passive side-channel leakages like power consumption, electromagnetic radiation, and reaction time. \nWe conduct all experiments on real data and common neural net architectures in order to properly assess the applicability and extendability of those attacks. Practical results are shown on an ARM CORTEX-M3 microcontroller. Our experiments show that the side-channel attacker is capable of obtaining the following information: the activation functions used in the architecture, the number of layers and neurons in the layers, the number of output classes, and weights in the neural network. Thus, the attacker can effectively reverse engineer the network using side-channel information. \nNext, we show that once the attacker has the knowledge about the neural network architecture, he/she could also recover the inputs to the network with only a single-shot measurement. Finally, we discuss several mitigations one could use to thwart such attacks.",
      "year": 2018,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/905ad646e5745afe6a3b02617cd8452655232c0d",
      "pdf_url": "",
      "publication_date": "2018-10-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1e181270b4456a5ac428ca0a8029718734f74bd2",
      "title": "Defending Against Machine Learning Model Stealing Attacks Using Deceptive Perturbations",
      "abstract": "Machine learning models are vulnerable to simple model stealing attacks if the adversary can obtain output labels for chosen inputs. To protect against these attacks, it has been proposed to limit the information provided to the adversary by omitting probability scores, significantly impacting the utility of the provided service. In this work, we illustrate how a service provider can still provide useful, albeit misleading, class probability information, while significantly limiting the success of the attack. Our defense forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate several attack strategies, model architectures, and hyperparameters under varying adversarial models, and evaluate the efficacy of our defense against the strongest adversary. Finally, we quantify the amount of noise injected into the class probabilities to mesure the loss in utility, e.g., adding 1.26 nats per query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or require up to 64 times more queries while keeping the accuracy of the protected model almost intact.",
      "year": 2018,
      "venue": "",
      "authors": [
        "Taesung Lee",
        "Ben Edwards",
        "Ian Molloy",
        "D. Su"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/1e181270b4456a5ac428ca0a8029718734f74bd2",
      "pdf_url": "",
      "publication_date": "2018-05-31",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "title": "Law and Adversarial Machine Learning",
      "abstract": "When machine learning systems fail because of adversarial manipulation, how should society expect the law to respond? Through scenarios grounded in adversarial ML literature, we explore how some aspects of computer crime, copyright, and tort law interface with perturbation, poisoning, model stealing and model inversion attacks to show how some attacks are more likely to result in liability than others. We end with a call for action to ML researchers to invest in transparent benchmarks of attacks and defenses; architect ML systems with forensics in mind and finally, think more about adversarial machine learning in the context of civil liberties. The paper is targeted towards ML researchers who have no legal background.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "R. Kumar",
        "David R. O'Brien",
        "Kendra Albert",
        "Salome Vilojen"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/89378c9c0a6b363939fb54310eff70f8cbf181bc",
      "pdf_url": "",
      "publication_date": "2018-10-25",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2822dce260dc4930ccd276a9ae6392960d6b460d",
      "title": "Combating Tag Cloning with COTS RFID Devices",
      "abstract": "In RFID systems, a cloning attack is to fabricate one or more replicas of a genuine tag, so that these replicas behave exactly the same as the genuine tag and fool the reader for getting legal authorization, leading to potential financial loss or reputation damage for the corporations. These replicas are called clone tags. Although many advanced solutions have been proposed to combat cloning attack, they need to either modify the MAC- layer protocols or increase extra hardware resources, which cannot be deployed on commercial off-the-shelf (COTS) RFID devices for practical use. In this paper, we take a fresh attempt to counterattack tag cloning based on COTS RFID devices and the universal C1G2 standard, without any software redesign or hardware augment needed. The basic idea is to use the RF signal profile to characterize each tag. Since these physical-layer data are measured by the reader and susceptible to various environmental factors, they are hard to be estimated by the attackers; let alone be cloned. Even so, we assert that it is challenging to identify clone tags as the signal data from a genuine tag and its clones are all mixed together. Besides, the tag moving has a great impact on the measured RF signals. To overcome these challenges, we propose a clustering-based scheme that detects the cloning attack in the still scene and a chain- based scheme for clone detection in the dynamic scene, respectively. Extensive experiments on COTS RFID devices demonstrate that the detection accuracy of our approaches reaches 99.8% in a still case and 99.3% in a dynamic scene.",
      "year": 2018,
      "venue": "Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks",
      "authors": [
        "Xingyu Chen",
        "Jia Liu",
        "Xia Wang",
        "Xiaocong Zhang",
        "Yanyan Wang",
        "Lijun Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/2822dce260dc4930ccd276a9ae6392960d6b460d",
      "pdf_url": "",
      "publication_date": "2018-06-01",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "65d21b754a788782f05e43d6786e342ba8284cff",
      "title": "Neutralizing BLE Beacon-Based Electronic Attendance System Using Signal Imitation Attack",
      "abstract": "Many emerging location- or proximity-based applications use Bluetooth low energy (BLE) beacons thanks to the increasing popularity of the technology in mobile systems. An outstanding example is the BLE beacon-based electronic attendance system (BEAS) used in many universities today to increase the efficiency of lectures. Despite its popularity and usefulness, however, BEAS has not been thoroughly analyzed for its potential vulnerabilities. In this paper, we neutralize a university\u2019s BEAS by maliciously cheating attendance (i.e., faking attendance while the subject is not physically present at the location) in various scenarios using signal imitation attack, and investigate its possible vulnerabilities. The BEAS exploited in this paper is a commercial system actually used in a well-known university. After the exploitation experiment, we analyze the system\u2019s weaknesses and present possible counter-measures. Furthermore, additional attack methods are shown to re-counteract those possible counter-measures and to discuss the fundamental challenges, deficiencies, and suggestions in electronic attendance systems using BLE beacons.",
      "year": 2018,
      "venue": "IEEE Access",
      "authors": [
        "Moonbeom Kim",
        "Jongho Lee",
        "Jeongyeup Paek"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/65d21b754a788782f05e43d6786e342ba8284cff",
      "pdf_url": "https://doi.org/10.1109/access.2018.2884488",
      "publication_date": "2018-12-03",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "544e7fe88816924a81962eb79fbb549571ec2217",
      "title": "Killing Four Birds with one Gaussian Process: The Relation between different Test-Time Attacks",
      "abstract": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. Consequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process Classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a different norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all information when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g. model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other.",
      "year": 2018,
      "venue": "International Conference on Pattern Recognition",
      "authors": [
        "Kathrin Grosse",
        "M. Smith",
        "M. Backes"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/544e7fe88816924a81962eb79fbb549571ec2217",
      "pdf_url": "https://arxiv.org/pdf/1806.02032",
      "publication_date": "2018-06-06",
      "keywords_matched": [
        "model stealing",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "181fe8787937dbf7abf886042855be1bc6149f80",
      "title": "Model Extraction Warning in MLaaS Paradigm",
      "abstract": "Machine learning models deployed on the cloud are susceptible to several security threats including extraction attacks. Adversaries may abuse a model's prediction API to steal the model thus compromising model confidentiality, privacy of training data, and revenue from future query payments. This work introduces a model extraction monitor that quantifies the extraction status of models by continually observing the API query and response streams of users. We present two novel strategies that measure either the information gain or the coverage of the feature space spanned by user queries to estimate the learning rate of individual and colluding adversaries. Both approaches have low computational overhead and can easily be offered as services to model owners to warn them against state of the art extraction attacks. We demonstrate empirical performance results of these approaches for decision tree and neural network models using open source datasets and BigML MLaaS platform.",
      "year": 2017,
      "venue": "Asia-Pacific Computer Systems Architecture Conference",
      "authors": [
        "M. Kesarwani",
        "B. Mukhoty",
        "V. Arya",
        "S. Mehta"
      ],
      "citation_count": 154,
      "url": "https://www.semanticscholar.org/paper/181fe8787937dbf7abf886042855be1bc6149f80",
      "pdf_url": "https://arxiv.org/pdf/1711.07221",
      "publication_date": "2017-11-20",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "abstract": "Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (\"predictive analytics\") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. \nThe tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., \"steal\") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",
      "year": 2016,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Florian Tram\u00e8r",
        "Fan Zhang",
        "A. Juels",
        "M. Reiter",
        "Thomas Ristenpart"
      ],
      "citation_count": 1967,
      "url": "https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e",
      "pdf_url": "",
      "publication_date": "2016-08-10",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "stealing machine learning"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "title": "Symmetries in Quantum Key Distribution and the Connection between Optimal Attacks and Optimal Cloning",
      "abstract": "We investigate the connection between the optimal collective eavesdropping attack and the optimal cloning attack where the eavesdropper employs an optimal cloner to attack the quantum key distribution (QKD) protocol. The analysis is done in the context of the security proof in Refs. [1, 2] for discrete variable protocols in d-dimensional Hilbert spaces. We consider a scenario in which the protocols and cloners are equipped with symmetries. These symmetries are used to dene a quantum cloning scenario. We nd that, in general, it does not hold that the optimal attack is an optimal cloner. However, there are classes of protocols, where we can identify an optimal attack by an optimal cloner. We analyze protocols with 2, d and d + 1 mutually unbiased bases where d is a prime, and show that for the protocols with 2 and d + 1 MUBs the optimal attack is an optimal cloner, but for the protocols with d MUBs, it is not 1 . Finally, we give criteria to identify protocols which have dierent signal states, but the same optimal attack. Using these criteria, we present qubit protocols which have the same optimal attack as the BB84 protocol or the 6-state protocol.",
      "year": 2011,
      "venue": "",
      "authors": [
        "A. Ferenczi",
        "N. Lutkenhaus"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/e76752c9762fbd90cd4aefdbbbefa163d09ae118",
      "pdf_url": "http://arxiv.org/pdf/1112.3396",
      "publication_date": "2011-12-15",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6f83064144b659cb9576fa04d1dc48c89f1b1e99",
      "title": "Quantum Cryptography with Several Cloning Attacks",
      "abstract": "Problem statement: In a previous research, we investigated the quantum key distribution of the well known BB84 protocol with several intercept and resend attacks. In the present research, we studied the effect of many eavesdroppers cloning at tacks of the Bennett-Brassard cryptographic protocol on the quantum error and mutual informatio n between honest parties and information with sender for each eavesdropper. Approach: The quantum error and the mutual information were calculated analytically and computed for arbitrary number of cloning attacks. Our objective in this study was to know if the number of the eavesdropper s and their angle of cloning act on the safety of information. Results: It was found that the quantum error and the secure d/no secured transition depend strongly on the number of eavesdropper and their an gle of attacks. The particular cases where all eavesdroppers collaborate were also investigated. Conclusion: Furthermore, the cloning attack's quantum error is lower than the intercept and resen ds attacks one, which means that the cloning attack s is the optimal one for arbitrary number of eavesdro pper.",
      "year": 2010,
      "venue": "",
      "authors": [
        "M. Dehmani",
        "H. Ez-Zahraouy",
        "A. Benyoussef"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/6f83064144b659cb9576fa04d1dc48c89f1b1e99",
      "pdf_url": "https://doi.org/10.3844/jcssp.2010.684.688",
      "publication_date": "2010-07-31",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9ec7656ff39d07fe6fd283466aed4bc56d35b2c2",
      "title": "Photon-number-splitting versus cloning attacks in practical implementations of the Bennett-Brassard 1984 protocol for quantum cryptography",
      "abstract": "In practical quantum cryptography, the source sometimes produces multiphoton pulses, thus enabling the eavesdropper Eve to perform the powerful photon-number-splitting (PNS) attack. Recently, it was shown by Curty and Luetkenhaus [Phys. Rev. A 69, 042321 (2004)] that the PNS attack is not always the optimal attack when two photons are present: if errors are present in the correlations Alice-Bob and if Eve cannot modify Bob's detection efficiency, Eve gains a larger amount of information using another attack based on a 2{yields}3 cloning machine. In this work, we extend this analysis to all distances Alice-Bob. We identify a new incoherent 2{yields}3 cloning attack which performs better than those described before. Using it, we confirm that, in the presence of errors, Eve's better strategy uses 2{yields}3 cloning attacks instead of the PNS. However, this improvement is very small for the implementations of the Bennett-Brassard 1984 (BB84) protocol. Thus, the existence of these new attacks is conceptually interesting but basically does not change the value of the security parameters of BB84. The main results are valid both for Poissonian and sub-Poissonian sources.",
      "year": 2004,
      "venue": "",
      "authors": [
        "A. Niederberger",
        "V. Scarani",
        "N. Gisin"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/9ec7656ff39d07fe6fd283466aed4bc56d35b2c2",
      "pdf_url": "https://access.archive-ouverte.unige.ch/access/metadata/c176b0db-379c-4db9-aac3-8a97b2f2de82/download",
      "publication_date": "2004-08-19",
      "keywords_matched": [
        "cloning attack"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}