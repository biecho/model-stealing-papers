{
  "owasp_id": "ML01",
  "owasp_name": "Input Manipulation Attack",
  "total": 121,
  "updated": "2026-01-28",
  "papers": [
    {
      "paper_id": "seed_f78c1c68",
      "title": "Invisible but Detected: Physical Adversarial Shadow Attack and Defense on LiDAR Object Detection",
      "abstract": "A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN's applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.",
      "year": 2021,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Zewen Li",
        "Fan Liu",
        "Wenjie Yang",
        "Shouheng Peng",
        "Jun Zhou"
      ],
      "author_details": [
        {
          "name": "Zewen Li",
          "h_index": 8,
          "citation_count": 3915,
          "affiliations": []
        },
        {
          "name": "Fan Liu",
          "h_index": 13,
          "citation_count": 4359,
          "affiliations": []
        },
        {
          "name": "Wenjie Yang",
          "h_index": 5,
          "citation_count": 3785,
          "affiliations": []
        },
        {
          "name": "Shouheng Peng",
          "h_index": 1,
          "citation_count": 3747,
          "affiliations": []
        },
        {
          "name": "Jun Zhou",
          "h_index": 31,
          "citation_count": 6372,
          "affiliations": []
        }
      ],
      "max_h_index": 31,
      "url": "https://openalex.org/W3168997536",
      "pdf_url": "http://hdl.handle.net/10072/405164",
      "doi": "https://doi.org/10.1109/tnnls.2021.3084827",
      "citation_count": 3758,
      "influential_citation_count": 93,
      "reference_count": 223,
      "is_open_access": true,
      "publication_date": "2020-04-01",
      "tldr": "This review introduces the history of CNN, some classic and advanced CNN models are introduced, and an overview of various convolutions is provided, including those key points making them reach state-of-the-art results.",
      "fields_of_study": [
        "Computer Science",
        "Engineering",
        "Medicine"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "LiDAR",
        "shadow-attack",
        "autonomous-vehicles",
        "detection"
      ],
      "open_access_pdf": "https://research-repository.griffith.edu.au/bitstreams/b17b55ad-283a-4b00-9c07-8688d8690e1b/download"
    },
    {
      "paper_id": "seed_7166cede",
      "title": "When Translators Refuse to Translate: A Novel Attack to Speech Translation Systems",
      "abstract": "\"Prince of Networks is the first treatment of Bruno Latour specifically as a philosopher. Part One covers four key works that display Latours underrated contributions to metaphysics: Irreductions, Science in Action, We Have Never Been Modern, and Pandoras Hope. Harman contends that Latour is one of the central figures of contemporary philosophy, with a highly original ontology centered in four key concepts: actants, irreduction, translation, and alliance. In Part Two, Harman summarizes Latours most important philosophical insights, including his status as the first secular occasionalist. Working from his own object-oriented perspective, Harman also criticizes the Latourian focus on the relational character of actors at the expense of their cryptic autonomous reality. This book forms a remarkable interface between Latours Actor-Network Theory and the Speculative Realism of Harman and his confederates.\" -- Book cover.",
      "year": 2009,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hao Wu",
        "Chang Liu",
        "Jing Chen",
        "Ruiying Du",
        "Kun He",
        "Yu Zhang",
        "Cong Wu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "author_details": [
        {
          "name": "Hao Wu",
          "h_index": 1,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Chang Liu",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Jing Chen",
          "h_index": 13,
          "citation_count": 631,
          "affiliations": []
        },
        {
          "name": "Ruiying Du",
          "h_index": 24,
          "citation_count": 1848,
          "affiliations": []
        },
        {
          "name": "Kun He",
          "h_index": 19,
          "citation_count": 1068,
          "affiliations": []
        },
        {
          "name": "Yu Zhang",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Cong Wu",
          "h_index": 1,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 9,
          "citation_count": 338,
          "affiliations": []
        },
        {
          "name": "Qing Guo",
          "h_index": 4,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Jie Zhang",
          "h_index": 1,
          "citation_count": 7,
          "affiliations": []
        }
      ],
      "max_h_index": 24,
      "url": "https://openalex.org/W2165673991",
      "pdf_url": "http://libros.metabiblioteca.org/handle/001/432",
      "citation_count": 884,
      "influential_citation_count": 0,
      "reference_count": 47,
      "is_open_access": false,
      "tldr": "This work uncovers a novel security threat unique to speech translation systems, which is dubbed \"untranslation attack\", and proposes an attack model that deceives the system into outputting the source language content instead of translating it.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "audio",
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "speech-translation",
        "denial-of-service",
        "evasion"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_09f03fa8",
      "title": "SoK: Efficiency Robustness of Dynamic Deep Learning Systems",
      "abstract": "The past decade has witnessed the rapid evolution in blockchain technologies, which has attracted tremendous interests from both the research communities and industries. The blockchain network was originated from the Internet financial sector as a decentralized, immutable ledger system for transactional data ordering. Nowadays, it is envisioned as a powerful backbone/framework for decentralized data processing and data-driven self-organization in flat, open-access networks. In particular, the plausible characteristics of decentralization, immutability, and self-organization are primarily owing to the unique decentralized consensus mechanisms introduced by blockchain networks. This survey is motivated by the lack of a comprehensive literature review on the development of decentralized consensus mechanisms in blockchain networks. In this paper, we provide a systematic vision of the organization of blockchain networks. By emphasizing the unique characteristics of decentralized consensus in blockchain networks, our in-depth review of the state-of-the-art consensus protocols is focused on both the perspective of distributed consensus system design and the perspective of incentive mechanism design. From a game-theoretic point of view, we also provide a thorough review of the strategy adopted for self-organization by the individual nodes in the blockchain backbone networks. Consequently, we provide a comprehensive survey of the emerging applications of blockchain networks in a broad area of telecommunication. We highlight our special interest in how the consensus mechanisms impact these applications. Finally, we discuss several open issues in the protocol design for blockchain consensus and the related potential research directions.",
      "year": 2019,
      "venue": "IEEE Access",
      "authors": [
        "Wenbo Wang",
        "D. Hoang",
        "Peizhao Hu",
        "Zehui Xiong",
        "D. Niyato",
        "Ping Wang",
        "Yonggang Wen",
        "Dong In Kim"
      ],
      "author_details": [
        {
          "name": "Wenbo Wang",
          "h_index": 13,
          "citation_count": 1836,
          "affiliations": []
        },
        {
          "name": "D. Hoang",
          "h_index": 53,
          "citation_count": 16510,
          "affiliations": []
        },
        {
          "name": "Peizhao Hu",
          "h_index": 15,
          "citation_count": 1683,
          "affiliations": []
        },
        {
          "name": "Zehui Xiong",
          "h_index": 68,
          "citation_count": 18957,
          "affiliations": []
        },
        {
          "name": "D. Niyato",
          "h_index": 114,
          "citation_count": 57559,
          "affiliations": []
        },
        {
          "name": "Ping Wang",
          "h_index": 72,
          "citation_count": 24501,
          "affiliations": []
        },
        {
          "name": "Yonggang Wen",
          "h_index": 59,
          "citation_count": 15785,
          "affiliations": []
        },
        {
          "name": "Dong In Kim",
          "h_index": 75,
          "citation_count": 24079,
          "affiliations": []
        }
      ],
      "max_h_index": 114,
      "url": "https://openalex.org/W2905867785",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08629877.pdf",
      "doi": "https://doi.org/10.1109/access.2019.2896108",
      "citation_count": 810,
      "influential_citation_count": 70,
      "reference_count": 249,
      "is_open_access": true,
      "publication_date": "2018-05-07",
      "tldr": "This paper provides a systematic vision of the organization of the blockchain networks, a comprehensive survey of the emerging applications of blockchain networks in a broad area of telecommunication, and discusses several open issues in the protocol design for blockchain consensus.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "survey",
      "domains": [],
      "model_types": [],
      "tags": [
        "SoK",
        "efficiency-robustness",
        "dynamic-DL",
        "systematization"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08629877.pdf"
    },
    {
      "paper_id": "1904.02144",
      "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
      "abstract": "The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Jianbo Chen",
        "Michael I. Jordan"
      ],
      "author_details": [
        {
          "name": "Jianbo Chen",
          "h_index": 14,
          "citation_count": 2614,
          "affiliations": []
        },
        {
          "name": "Michael I. Jordan",
          "h_index": 187,
          "citation_count": 262447,
          "affiliations": []
        }
      ],
      "max_h_index": 187,
      "url": "https://arxiv.org/abs/1904.02144",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152788.pdf",
      "citation_count": 759,
      "influential_citation_count": 128,
      "reference_count": 50,
      "is_open_access": true,
      "publication_date": "2019-04-03",
      "tldr": "HopSkipJumpAttack is developed, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary that achieves competitive performance in attacking several widely-used defense mechanisms.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "decision-based",
        "black-box",
        "query-efficient",
        "boundary-attack"
      ],
      "open_access_pdf": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152788.pdf"
    },
    {
      "paper_id": "seed_d50d73f3",
      "title": "From Meme to Threat: On the Hateful Meme Understanding and Induced Hateful Content Generation in Open-Source Vision Language Models",
      "abstract": "Data-driven and machine learning based approaches for detecting, categorising and measuring abusive content such as hate speech and harassment have gained traction due to their scalability, robustness and increasingly high performance. Making effective detection systems for abusive content relies on having the right training datasets, reflecting a widely accepted mantra in computer science: Garbage In, Garbage Out. However, creating training datasets which are large, varied, theoretically-informed and that minimize biases is difficult, laborious and requires deep expertise. This paper systematically reviews 63 publicly available training datasets which have been created to train abusive language classifiers. It also reports on creation of a dedicated website for cataloguing abusive language data hatespeechdata.com . We discuss the challenges and opportunities of open science in this field, and argue that although more dataset sharing would bring many benefits it also poses social and ethical risks which need careful consideration. Finally, we provide evidence-based recommendations for practitioners creating new abusive content training datasets.",
      "year": 2020,
      "venue": "PLoS ONE",
      "authors": [
        "Bertie Vidgen",
        "Leon Derczynski"
      ],
      "author_details": [
        {
          "name": "Bertie Vidgen",
          "h_index": 28,
          "citation_count": 4025,
          "affiliations": []
        },
        {
          "name": "Leon Derczynski",
          "h_index": 34,
          "citation_count": 6316,
          "affiliations": [
            "IT University of Copenhagen"
          ]
        }
      ],
      "max_h_index": 34,
      "url": "https://openalex.org/W3014487746",
      "pdf_url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0243300&type=printable",
      "doi": "https://doi.org/10.1371/journal.pone.0243300",
      "citation_count": 309,
      "influential_citation_count": 12,
      "reference_count": 147,
      "is_open_access": true,
      "publication_date": "2020-04-03",
      "tldr": "This paper systematically reviews 63 publicly available training datasets which have been created to train abusive language classifiers and reports on creation of a dedicated website for cataloguing abusive language data hatespeechdata.com.",
      "fields_of_study": [
        "Computer Science",
        "Medicine"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "model_types": [
        "transformer",
        "llm"
      ],
      "tags": [
        "hateful-meme",
        "content-generation",
        "VLM-attack"
      ],
      "open_access_pdf": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0243300&type=printable"
    },
    {
      "paper_id": "seed_ea304a74",
      "title": "Transcend: Detecting Concept Drift in Malware Classification Models",
      "abstract": "Building machine learning models of malware behavior is widely accepted as a panacea towards effective malware classification. A crucial requirement for building sustainable learning models, though, is to train on a wide variety of malware samples. Unfortunately, malware evolves rapidly and it thus becomes hard\u2014if not impossible\u2014to generalize learning models to reflect future, previously-unseen behaviors. Consequently, most malware classifiers become unsustainable in the long run, becoming rapidly antiquated as malware continues to evolve. In this work, we propose Transcend, a framework to identify aging classification models in vivo during deployment, much before the machine learning model\u2019s performance starts to degrade. This is a significant departure from conventional approaches that retrain aging models retrospectively when poor performance is observed. Our approach uses a statistical comparison of samples seen during deployment with those used to train the model, thereby building metrics for prediction quality. We show how Transcend can be used to identify concept drift based on two separate case studies on Android andWindows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training.",
      "year": 2017,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Roberto Jordaney",
        "K. Sharad",
        "Santanu Kumar Dash",
        "Zhi Wang",
        "D. Papini",
        "I. Nouretdinov",
        "L. Cavallaro"
      ],
      "author_details": [
        {
          "name": "Roberto Jordaney",
          "h_index": 5,
          "citation_count": 765,
          "affiliations": []
        },
        {
          "name": "K. Sharad",
          "h_index": 6,
          "citation_count": 411,
          "affiliations": []
        },
        {
          "name": "Santanu Kumar Dash",
          "h_index": 19,
          "citation_count": 1632,
          "affiliations": []
        },
        {
          "name": "Zhi Wang",
          "h_index": 7,
          "citation_count": 546,
          "affiliations": []
        },
        {
          "name": "D. Papini",
          "h_index": 13,
          "citation_count": 784,
          "affiliations": []
        },
        {
          "name": "I. Nouretdinov",
          "h_index": 26,
          "citation_count": 2341,
          "affiliations": []
        },
        {
          "name": "L. Cavallaro",
          "h_index": 35,
          "citation_count": 5782,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://openalex.org/W2753594008",
      "citation_count": 307,
      "influential_citation_count": 28,
      "reference_count": 22,
      "is_open_access": false,
      "tldr": "This work proposes Transcend, a framework to identify aging classification models in vivo during deployment, much before the machine learning model\u2019s performance starts to degrade, a significant departure from conventional approaches that retrain aging models retrospectively when poor performance is observed.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "concept-drift",
        "malware-detection",
        "conformal-prediction"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_4cf4a072",
      "title": "Watch the Watchers! On the Security Risks of Robustness-Enhancing Diffusion Models",
      "abstract": "ABSTRACT We estimate private benefits of control in 39 countries using 393 controlling blocks sales. On average the value of control is 14 percent, but in some countries can be as low as \u22124 percent, in others as high a +65 percent. As predicted by theory, higher private benefits of control are associated with less developed capital markets, more concentrated ownership, and more privately negotiated privatizations. We also analyze what institutions are most important in curbing private benefits. We find evidence for both legal and extra\u2010legal mechanisms. In a multivariate analysis, however, media pressure and tax enforcement seem to be the dominating factors.",
      "year": 2004,
      "venue": "The Journal of Finance",
      "authors": [
        "Alexander Dyck",
        "Luigi Zingales"
      ],
      "author_details": [
        {
          "name": "Alexander Dyck",
          "h_index": 1,
          "citation_count": 301,
          "affiliations": []
        },
        {
          "name": "Luigi Zingales",
          "h_index": 72,
          "citation_count": 63824,
          "affiliations": []
        }
      ],
      "max_h_index": 72,
      "url": "https://openalex.org/W3123748565",
      "pdf_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1540-6261.2004.00642.x",
      "doi": "https://doi.org/10.1111/j.1540-6261.2004.00642.x",
      "citation_count": 302,
      "influential_citation_count": 8,
      "reference_count": 56,
      "is_open_access": true,
      "publication_date": "2004-04-01",
      "fields_of_study": [
        "Business"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "diffusion-purification",
        "security-risks"
      ],
      "open_access_pdf": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1540-6261.2004.00642.x"
    },
    {
      "paper_id": "seed_af0214b4",
      "title": "CADE: Detecting and Explaining Concept Drift Samples for Security Applications",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Limin Yang",
        "Wenbo Guo",
        "Qingying Hao",
        "A. Ciptadi",
        "Aliakbar Ahmadzadeh",
        "Xinyu Xing",
        "Gang Wang"
      ],
      "author_details": [
        {
          "name": "Limin Yang",
          "h_index": 12,
          "citation_count": 945,
          "affiliations": []
        },
        {
          "name": "Wenbo Guo",
          "h_index": 15,
          "citation_count": 966,
          "affiliations": []
        },
        {
          "name": "Qingying Hao",
          "h_index": 6,
          "citation_count": 389,
          "affiliations": []
        },
        {
          "name": "A. Ciptadi",
          "h_index": 12,
          "citation_count": 1446,
          "affiliations": []
        },
        {
          "name": "Aliakbar Ahmadzadeh",
          "h_index": 3,
          "citation_count": 386,
          "affiliations": []
        },
        {
          "name": "Xinyu Xing",
          "h_index": 34,
          "citation_count": 4548,
          "affiliations": []
        },
        {
          "name": "Gang Wang",
          "h_index": 15,
          "citation_count": 1043,
          "affiliations": []
        }
      ],
      "max_h_index": 34,
      "url": "https://www.usenix.org/system/files/sec21-yang-limin.pdf",
      "citation_count": 211,
      "influential_citation_count": 34,
      "reference_count": 75,
      "is_open_access": false,
      "tldr": "The results show that CADE can effectively detect drifting samples and provide semantically meaningful explanations and it is shown that explaining \u201cdistance\u201d is much more effective than traditional methods that focus on explaining a \u201cdecision boundary\u201d in this problem context.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "concept-drift",
        "detection",
        "explainability"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_6c2d273d",
      "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
      "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "M. Russinovich",
        "Ahmed Salem",
        "Ronen Eldan"
      ],
      "author_details": [
        {
          "name": "M. Russinovich",
          "h_index": 21,
          "citation_count": 4059,
          "affiliations": []
        },
        {
          "name": "Ahmed Salem",
          "h_index": 3,
          "citation_count": 252,
          "affiliations": []
        },
        {
          "name": "Ronen Eldan",
          "h_index": 34,
          "citation_count": 11425,
          "affiliations": []
        }
      ],
      "max_h_index": 34,
      "url": "https://openalex.org/W4393932428",
      "pdf_url": "https://arxiv.org/pdf/2404.01833",
      "doi": "https://doi.org/10.48550/arxiv.2404.01833",
      "citation_count": 206,
      "influential_citation_count": 25,
      "reference_count": 32,
      "is_open_access": false,
      "publication_date": "2024-04-02",
      "tldr": "This paper introduces a novel jailbreak attack called Crescendo, a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner and demonstrates Crescendo's ability to jailbreak multimodal models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "multi-turn",
        "crescendo",
        "jailbreak"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2302.05319",
      "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
      "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jingxuan He",
        "Martin T. Vechev"
      ],
      "author_details": [
        {
          "name": "Jingxuan He",
          "h_index": 13,
          "citation_count": 1258,
          "affiliations": []
        },
        {
          "name": "Martin T. Vechev",
          "h_index": 63,
          "citation_count": 15558,
          "affiliations": []
        }
      ],
      "max_h_index": 63,
      "url": "https://arxiv.org/abs/2302.05319",
      "citation_count": 204,
      "influential_citation_count": 31,
      "reference_count": 76,
      "is_open_access": true,
      "publication_date": "2023-02-10",
      "tldr": "This work proposes a novel learning-based approach called SVEN, which leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights, and closely matches the original LMs in functional correctness.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "code-generation",
        "security-hardening",
        "adversarial-testing"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3576915.3623175"
    },
    {
      "paper_id": "seed_9a1e7666",
      "title": "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
      "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers.   In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. Jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of LLM chatbots.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "author_details": [
        {
          "name": "Gelei Deng",
          "h_index": 21,
          "citation_count": 2874,
          "affiliations": []
        },
        {
          "name": "Yi Liu",
          "h_index": 14,
          "citation_count": 2295,
          "affiliations": []
        },
        {
          "name": "Yuekang Li",
          "h_index": 28,
          "citation_count": 4241,
          "affiliations": [
            "School of Computer Science and Engineering, University of New South Wales"
          ]
        },
        {
          "name": "Kailong Wang",
          "h_index": 9,
          "citation_count": 1684,
          "affiliations": []
        },
        {
          "name": "Ying Zhang",
          "h_index": 6,
          "citation_count": 870,
          "affiliations": []
        },
        {
          "name": "Zefeng Li",
          "h_index": 2,
          "citation_count": 385,
          "affiliations": []
        },
        {
          "name": "Haoyu Wang",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 39,
          "citation_count": 7229,
          "affiliations": []
        },
        {
          "name": "Yang Liu",
          "h_index": 18,
          "citation_count": 2052,
          "affiliations": []
        }
      ],
      "max_h_index": 39,
      "url": "https://arxiv.org/abs/2307.08715",
      "citation_count": 195,
      "influential_citation_count": 11,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2023-07-16",
      "tldr": "Jailbreaker is presented, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures, and an automatic generation method for jailbreak prompts is introduced, leveraging a fine-tuned LLM to validate the potential of automated jailbreak generation across various commercial LLM chatbots.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "automated-jailbreak",
        "chatbot-attack"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.24188"
    },
    {
      "paper_id": "seed_31676987",
      "title": "Interpretable Deep Learning under Fire",
      "abstract": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing \\imlses are highly vulnerable to adversarial manipulations. Specifically, we present ADV^2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV^2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability -- a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV^2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions.",
      "year": 2018,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyang Zhang",
        "Ningfei Wang",
        "S. Ji",
        "Hua Shen",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Xinyang Zhang",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Ningfei Wang",
          "h_index": 12,
          "citation_count": 917,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Hua Shen",
          "h_index": 12,
          "citation_count": 713,
          "affiliations": [
            "The PennState University"
          ]
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://openalex.org/W2903544706",
      "pdf_url": "https://arxiv.org/pdf/1812.00891",
      "doi": "https://doi.org/10.48550/arxiv.1812.00891",
      "citation_count": 186,
      "influential_citation_count": 23,
      "reference_count": 84,
      "is_open_access": false,
      "publication_date": "2018-12-03",
      "tldr": "This work presents ADV^2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models, and identifies the prediction-interpretation gap as one root cause of this vulnerability.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "XAI-attack",
        "interpretability",
        "explanation-manipulation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_ad39c3c9",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "abstract": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/StruQ.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "author_details": [
        {
          "name": "Sizhe Chen",
          "h_index": 6,
          "citation_count": 417,
          "affiliations": []
        },
        {
          "name": "Julien Piet",
          "h_index": 9,
          "citation_count": 486,
          "affiliations": []
        },
        {
          "name": "Chawin Sitawarin",
          "h_index": 21,
          "citation_count": 3161,
          "affiliations": []
        },
        {
          "name": "David Wagner",
          "h_index": 6,
          "citation_count": 401,
          "affiliations": []
        }
      ],
      "max_h_index": 21,
      "url": "https://openalex.org/W4391766701",
      "pdf_url": "https://arxiv.org/pdf/2402.06363",
      "doi": "https://doi.org/10.48550/arxiv.2402.06363",
      "citation_count": 173,
      "influential_citation_count": 30,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2024-02-09",
      "tldr": "This work significantly improves resistance to prompt injection attacks, with little or no impact on utility, and implements a system that supports structured queries.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "prompt-injection-defense",
        "structured-queries"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2305.12082",
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "abstract": "Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL$\\cdot$E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: \\url{https://github.com/Yuchen413/text2image_safety}.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuchen Yang",
        "Bo Hui",
        "Haolin Yuan",
        "N. Gong",
        "Yinzhi Cao"
      ],
      "author_details": [
        {
          "name": "Yuchen Yang",
          "h_index": 3,
          "citation_count": 222,
          "affiliations": []
        },
        {
          "name": "Bo Hui",
          "h_index": 7,
          "citation_count": 501,
          "affiliations": []
        },
        {
          "name": "Haolin Yuan",
          "h_index": 9,
          "citation_count": 583,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 2,
          "citation_count": 270,
          "affiliations": []
        },
        {
          "name": "Yinzhi Cao",
          "h_index": 5,
          "citation_count": 344,
          "affiliations": []
        }
      ],
      "max_h_index": 9,
      "url": "https://arxiv.org/abs/2305.12082",
      "citation_count": 155,
      "influential_citation_count": 36,
      "reference_count": 49,
      "is_open_access": true,
      "publication_date": "2023-05-20",
      "tldr": "This work proposes SneakyPrompt, the first automated attack framework to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted, and outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "jailbreak",
        "T2I",
        "safety-bypass"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2305.12082"
    },
    {
      "paper_id": "seed_371d7f7f",
      "title": "Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Milad Nasr",
        "Alireza Bahramali",
        "Amir Houmansadr"
      ],
      "author_details": [
        {
          "name": "Milad Nasr",
          "h_index": 31,
          "citation_count": 11780,
          "affiliations": []
        },
        {
          "name": "Alireza Bahramali",
          "h_index": 8,
          "citation_count": 509,
          "affiliations": []
        },
        {
          "name": "Amir Houmansadr",
          "h_index": 42,
          "citation_count": 7508,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://www.usenix.org/system/files/sec21fall-nasr.pdf",
      "citation_count": 129,
      "influential_citation_count": 17,
      "reference_count": 67,
      "is_open_access": false,
      "tldr": "For the first time, it is shown that an adversary can defeat DNN-based traf\ufb01c analysis techniques by applying adversarial perturbations on the patterns of live network traf\ufb01c.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "traffic-analysis",
        "real-time",
        "blind-perturbations"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2106.09898",
      "title": "Bad Characters: Imperceptible NLP Attacks",
      "abstract": "Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "N. Boucher",
        "Ilia Shumailov",
        "Ross Anderson",
        "Nicolas Papernot"
      ],
      "author_details": [
        {
          "name": "N. Boucher",
          "h_index": 5,
          "citation_count": 166,
          "affiliations": []
        },
        {
          "name": "Ilia Shumailov",
          "h_index": 25,
          "citation_count": 7624,
          "affiliations": [
            "Google Deepmind"
          ]
        },
        {
          "name": "Ross Anderson",
          "h_index": 15,
          "citation_count": 1638,
          "affiliations": []
        },
        {
          "name": "Nicolas Papernot",
          "h_index": 30,
          "citation_count": 5329,
          "affiliations": []
        }
      ],
      "max_h_index": 30,
      "url": "https://arxiv.org/abs/2106.09898",
      "pdf_url": "https://www.research.ed.ac.uk/en/publications/eae2499e-4a74-43b5-8926-c855c9eb0be2",
      "citation_count": 126,
      "influential_citation_count": 10,
      "reference_count": 83,
      "is_open_access": true,
      "publication_date": "2021-06-18",
      "tldr": "It is concluded that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "unicode-attack",
        "imperceptible",
        "text-adversarial"
      ],
      "open_access_pdf": "https://www.pure.ed.ac.uk/ws/files/287726702/Bad_Characters_BOUCHER_DOA01052022_AFV.pdf"
    },
    {
      "paper_id": "1705.07535",
      "title": "Evading Classifiers by Morphing in the Dark",
      "abstract": "Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
      "year": 2017,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Hung Dang",
        "Yue Huang",
        "E. Chang"
      ],
      "author_details": [
        {
          "name": "Hung Dang",
          "h_index": 12,
          "citation_count": 916,
          "affiliations": []
        },
        {
          "name": "Yue Huang",
          "h_index": 4,
          "citation_count": 157,
          "affiliations": []
        },
        {
          "name": "E. Chang",
          "h_index": 35,
          "citation_count": 5263,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://arxiv.org/abs/1705.07535",
      "pdf_url": "https://arxiv.org/pdf/1705.07535",
      "citation_count": 124,
      "influential_citation_count": 6,
      "reference_count": 42,
      "is_open_access": true,
      "publication_date": "2017-05-22",
      "tldr": "This paper presents a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available and proposes an evasion method -- EvadeHC?",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "black-box",
        "minimal-knowledge",
        "morphing",
        "evasion"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/1705.07535"
    },
    {
      "paper_id": "1905.12386",
      "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
      "abstract": "In this paper, we present a novel attack against authorship attribution of source code. We exploit that recent attribution methods rest on machine learning and thus can be deceived by adversarial examples of source code. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer. The attack is guided by Monte-Carlo tree search that enables us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack has a substantial effect on two recent attribution methods, whose accuracy drops from over 88% to 1% under attack. Furthermore, we show that our attack can imitate the coding style of developers with high accuracy and thereby induce false attributions. We conclude that current approaches for authorship attribution are inappropriate for practical application and there is a need for resilient analysis techniques.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Erwin Quiring",
        "Alwin Maier",
        "Konrad Rieck"
      ],
      "author_details": [
        {
          "name": "Erwin Quiring",
          "h_index": 9,
          "citation_count": 533,
          "affiliations": []
        },
        {
          "name": "Alwin Maier",
          "h_index": 5,
          "citation_count": 378,
          "affiliations": []
        },
        {
          "name": "Konrad Rieck",
          "h_index": 42,
          "citation_count": 11256,
          "affiliations": []
        }
      ],
      "max_h_index": 42,
      "url": "https://arxiv.org/abs/1905.12386",
      "pdf_url": "https://arxiv.org/pdf/1905.12386",
      "citation_count": 120,
      "influential_citation_count": 21,
      "reference_count": 39,
      "is_open_access": false,
      "publication_date": "2019-05-29",
      "tldr": "A novel attack against authorship attribution of source code that performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp"
      ],
      "model_types": [],
      "tags": [
        "authorship-attribution",
        "code-transformation",
        "MCTS",
        "evasion"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_377680e8",
      "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations",
      "abstract": "Research into adversarial examples (AE) has developed rapidly, yet static adversarial patches are still the main technique for conducting attacks in the real world, despite being obvious, semi-permanent and unmodifiable once deployed. In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a novel technique that allows adversaries to realize physically robust real-world AE by using a light projector. Attackers can project a specifically crafted adversarial perturbation onto a real-world object, transforming it into an AE. This allows the adversary greater control over the attack compared to adversarial patches: (i) projections can be dynamically turned on and off or modified at will, (ii) projections do not suffer from the locality constraint imposed by patches, making them harder to detect. We study the feasibility of SLAP in the self-driving scenario, targeting both object detector and traffic sign recognition tasks, focusing on the detection of stop signs. We conduct experiments in a variety of ambient light conditions, including outdoors, showing how in non-bright settings the proposed method generates AE that are extremely robust, causing misclassifications on state-of-the-art networks with up to 99% success rate for a variety of angles and distances. We also demostrate that SLAP-generated AE do not present detectable behaviours seen in adversarial patches and therefore bypass SentiNet, a physical AE detection method. We evaluate other defences including an adaptive defender using adversarial learning which is able to thwart the attack effectiveness up to 80% even in favourable attacker conditions.",
      "year": 2020,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Giulio Lovisotto",
        "H.C.M. Turner",
        "Ivo Sluganovic",
        "Martin Strohmeier",
        "I. Martinovic"
      ],
      "author_details": [
        {
          "name": "Giulio Lovisotto",
          "h_index": 13,
          "citation_count": 543,
          "affiliations": []
        },
        {
          "name": "H.C.M. Turner",
          "h_index": 6,
          "citation_count": 224,
          "affiliations": []
        },
        {
          "name": "Ivo Sluganovic",
          "h_index": 10,
          "citation_count": 436,
          "affiliations": []
        },
        {
          "name": "Martin Strohmeier",
          "h_index": 25,
          "citation_count": 2866,
          "affiliations": []
        },
        {
          "name": "I. Martinovic",
          "h_index": 41,
          "citation_count": 7280,
          "affiliations": []
        }
      ],
      "max_h_index": 41,
      "url": "https://openalex.org/W3042075786",
      "pdf_url": "https://arxiv.org/pdf/2007.04137",
      "doi": "https://doi.org/10.48550/arxiv.2007.04137",
      "citation_count": 118,
      "influential_citation_count": 14,
      "reference_count": 49,
      "is_open_access": false,
      "publication_date": "2020-07-08",
      "tldr": "Short-Lived Adversarial Perturbations (SLAP) is proposed, a novel technique that allows adversaries to realize robust, dynamic real-world AE from a distance and generates AE that are robust to different environmental conditions for several networks and lighting conditions.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "physical-attack",
        "projector",
        "short-lived",
        "real-world"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_d8fa3e95",
      "title": "Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift",
      "abstract": "Machine learning for malware classification shows encouraging results, but real deployments suffer from performance degradation as malware authors adapt their techniques to evade detection. This phenomenon, known as concept drift, occurs as new malware examples evolve and become less and less like the original training examples. One promising method to cope with concept drift is classification with rejection in which examples that are likely to be misclassified are instead quarantined until they can be expertly analyzed. We propose TRANSCENDENT, a rejection framework built on Transcend, a recently proposed strategy based on conformal prediction theory. In particular, we provide a formal treatment of Transcend, enabling us to refine conformal evaluation theory -- its underlying statistical engine -- and gain a better understanding of the theoretical reasons for its effectiveness. In the process, we develop two additional conformal evaluators that match or surpass the performance of the original while significantly decreasing the computational overhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that removes sources of experimental bias present in the original evaluation. TRANSCENDENT outperforms state-of-the-art approaches while generalizing across different malware domains and classifiers. To further assist practitioners, we determine the optimal operational settings for a TRANSCENDENT deployment and show how it can be applied to many popular learning algorithms. These insights support both old and new empirical findings, making Transcend a sound and practical solution for the first time. To this end, we release TRANSCENDENT as open source, to aid the adoption of rejection strategies by the security community.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Federico Barbero",
        "Feargus Pendlebury",
        "Fabio Pierazzi",
        "L. Cavallaro"
      ],
      "author_details": [
        {
          "name": "Federico Barbero",
          "h_index": 8,
          "citation_count": 475,
          "affiliations": []
        },
        {
          "name": "Feargus Pendlebury",
          "h_index": 10,
          "citation_count": 1134,
          "affiliations": []
        },
        {
          "name": "Fabio Pierazzi",
          "h_index": 12,
          "citation_count": 1333,
          "affiliations": []
        },
        {
          "name": "L. Cavallaro",
          "h_index": 35,
          "citation_count": 5782,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://openalex.org/W3110824252",
      "pdf_url": "https://arxiv.org/pdf/2010.03856",
      "doi": "https://doi.org/10.1109/sp46214.2022.9833659",
      "citation_count": 108,
      "influential_citation_count": 13,
      "reference_count": 60,
      "is_open_access": true,
      "publication_date": "2020-10-08",
      "tldr": "This work proposes TRANSCENDENT, a rejection framework built on Transcend, a recently proposed strategy based on conformal prediction theory that outperforms state-of-the-art approaches while generalizing across different malware domains and classifiers.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "concept-drift",
        "malware-detection",
        "retraining"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2010.03856"
    },
    {
      "paper_id": "2109.11495",
      "title": "DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications",
      "abstract": "Unsupervised Deep Learning (DL) techniques have been widely used in various security-related anomaly detection applications, owing to the great promise of being able to detect unforeseen threats and superior performance provided by Deep Neural Networks (DNN). However, the lack of interpretability creates key barriers to the adoption of DL models in practice. Unfortunately, existing interpretation approaches are proposed for supervised learning models and/or non-security domains, which are unadaptable for unsupervised DL models and fail to satisfy special requirements in security domains.   In this paper, we propose DeepAID, a general framework aiming to (1) interpret DL-based anomaly detection systems in security domains, and (2) improve the practicality of these systems based on the interpretations. We first propose a novel interpretation method for unsupervised DNNs by formulating and solving well-designed optimization problems with special constraints for security domains. Then, we provide several applications based on our Interpreter as well as a model-based extension Distiller to improve security systems by solving domain-specific problems. We apply DeepAID over three types of security-related anomaly detection systems and extensively evaluate our Interpreter with representative prior works. Experimental results show that DeepAID can provide high-quality interpretations for unsupervised DL models while meeting the special requirements of security domains. We also provide several use cases to show that DeepAID can help security operators to understand model decisions, diagnose system mistakes, give feedback to models, and reduce false positives.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Dongqi Han",
        "Zhiliang Wang",
        "Wenqi Chen",
        "Ying Zhong",
        "Su Wang",
        "Han Zhang",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "author_details": [
        {
          "name": "Dongqi Han",
          "h_index": 10,
          "citation_count": 640,
          "affiliations": []
        },
        {
          "name": "Zhiliang Wang",
          "h_index": 10,
          "citation_count": 523,
          "affiliations": []
        },
        {
          "name": "Wenqi Chen",
          "h_index": 8,
          "citation_count": 567,
          "affiliations": []
        },
        {
          "name": "Ying Zhong",
          "h_index": 8,
          "citation_count": 517,
          "affiliations": []
        },
        {
          "name": "Su Wang",
          "h_index": 10,
          "citation_count": 694,
          "affiliations": []
        },
        {
          "name": "Han Zhang",
          "h_index": 10,
          "citation_count": 576,
          "affiliations": []
        },
        {
          "name": "Jiahai Yang",
          "h_index": 16,
          "citation_count": 1155,
          "affiliations": []
        },
        {
          "name": "Xingang Shi",
          "h_index": 21,
          "citation_count": 1882,
          "affiliations": []
        },
        {
          "name": "Xia Yin",
          "h_index": 23,
          "citation_count": 2112,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://arxiv.org/abs/2109.11495",
      "citation_count": 106,
      "influential_citation_count": 11,
      "reference_count": 68,
      "is_open_access": true,
      "publication_date": "2021-09-23",
      "tldr": "DeepAID is proposed, a general framework aiming to interpret DL-based anomaly detection systems in security domains and improve the practicality of these systems based on the interpretations, and Experimental results show that DeepAID can provide high-quality interpretations for unsupervised DL models while meeting the special requirements of security domains.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "tool",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "XAI",
        "anomaly-detection",
        "interpretability"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484589"
    },
    {
      "paper_id": "seed_44804ce5",
      "title": "Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models",
      "abstract": "Since its emergence in the late 19th century, coloured identity has been pivotal to racial thinking in southern Africa. The nature of colouredness is a highly emotive and controversial issue as it embodies many of the racial antagonisms, ambiguities and derogations prevalent in the subcontinent. Throughout their existence coloured communities have had to contend with being marginal minorities stigmatised as the insalubrious by-products of miscegenation. Burdened By Race showcases recent innovative research and writing on coloured identity in southern Africa. Drawing on a wide range of disciplines and applying fresh theoretical insights, the book brings new levels of understanding to processes of coloured self-identification. It examines diverse manifestations of colouredness, using interlinking themes and case studies from South Africa, Zimbabwe, Zambia and Malawi to present analyses that challenge and overturn much of the conventional wisdom around identity in the current literature.",
      "year": 2009,
      "venue": "Directory of Open access Books (OAPEN Foundation)",
      "authors": [
        "M. Adhikari"
      ],
      "author_details": [
        {
          "name": "M. Adhikari",
          "h_index": 15,
          "citation_count": 839,
          "affiliations": []
        }
      ],
      "max_h_index": 15,
      "url": "https://openalex.org/W4211044653",
      "pdf_url": "https://www.doabooks.org/doab?func=search&query=rid:21830",
      "doi": "https://doi.org/10.26530/oapen_628130",
      "citation_count": 100,
      "influential_citation_count": 14,
      "reference_count": 66,
      "is_open_access": true,
      "publication_date": "2009-04-01",
      "tldr": "This collection breaks virgin ground by examining diverse manifestations of colouredness across the region using interlinking themes and case studies from South Africa, Zimbabwe, Zambia and Malawi to present analyses that both challenge and overturn much of the conventional wisdom around the identity in the current literature.",
      "fields_of_study": [
        "Geography"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "safety-unlearning",
        "alignment-attack"
      ],
      "open_access_pdf": "https://library.oapen.org/bitstream/20.500.12657/31443/1/628130.pdf"
    },
    {
      "paper_id": "seed_aa77bed7",
      "title": "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries",
      "abstract": "We study adversarial examples in a black-box setting where the adversary only has API access to the target model and each query is expensive. Prior work on black-box adversarial examples follows one of two main strategies: (1) transfer attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model, and (2) optimization-based attacks use queries to the target model and apply optimization techniques to search for adversarial examples. We propose hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization-based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates. We empirically demonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack strategy reduces cost and improves success rates. We also introduce a seed prioritization strategy which enables attackers to focus their resources on the most promising seeds. Combining hybrid attacks with our seed prioritization strategy enables batch attacks that can reliably find adversarial examples with only a handful of queries.",
      "year": 2019,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Fnu Suya",
        "Jianfeng Chi",
        "David Evans",
        "Yuan Tian"
      ],
      "author_details": [
        {
          "name": "Fnu Suya",
          "h_index": 8,
          "citation_count": 364,
          "affiliations": []
        },
        {
          "name": "Jianfeng Chi",
          "h_index": 15,
          "citation_count": 13084,
          "affiliations": []
        },
        {
          "name": "David Evans",
          "h_index": 49,
          "citation_count": 12330,
          "affiliations": [
            "University of Virginia",
            "MIT"
          ]
        },
        {
          "name": "Yuan Tian",
          "h_index": 15,
          "citation_count": 958,
          "affiliations": []
        }
      ],
      "max_h_index": 49,
      "url": "https://openalex.org/W2969333443",
      "pdf_url": "https://arxiv.org/pdf/1908.07000",
      "doi": "https://doi.org/10.48550/arxiv.1908.07000",
      "citation_count": 94,
      "influential_citation_count": 10,
      "reference_count": 45,
      "is_open_access": false,
      "publication_date": "2019-08-19",
      "tldr": "This work proposes hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization- based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "black-box",
        "query-efficient",
        "transfer-attack",
        "adversarial-examples"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_6697838b",
      "title": "You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks",
      "abstract": "Autonomous Vehicles (AVs) increasingly use LiDAR-based object detection systems to perceive other vehicles and pedestrians on the road. While existing attacks on LiDAR-based autonomous driving architectures focus on lowering the confidence score of AV object detection models to induce obstacle misdetection, our research discovers how to leverage laser-based spoofing techniques to selectively remove the LiDAR point cloud data of genuine obstacles at the sensor level before being used as input to the AV perception. The ablation of this critical LiDAR information causes autonomous driving obstacle detectors to fail to identify and locate obstacles and, consequently, induces AVs to make dangerous automatic driving decisions. In this paper, we present a method invisible to the human eye that hides objects and deceives autonomous vehicles' obstacle detectors by exploiting inherent automatic transformation and filtering processes of LiDAR sensor data integrated with autonomous driving frameworks. We call such attacks Physical Removal Attacks (PRA), and we demonstrate their effectiveness against three popular AV obstacle detectors (Apollo, Autoware, PointPillars), and we achieve 45\u00b0 attack capability. We evaluate the attack impact on three fusion models (Frustum-ConvNet, AVOD, and Integrated-Semantic Level Fusion) and the consequences on the driving decision using LGSVL, an industry-grade simulator. In our moving vehicle scenarios, we achieve a 92.7% success rate removing 90\\% of a target obstacle's cloud points. Finally, we demonstrate the attack's success against two popular defenses against spoofing and object hiding attacks and discuss two enhanced defense strategies to mitigate our attack.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yulong Cao",
        "S. Bhupathiraju",
        "Pirouz Naghavi",
        "Takeshi Sugawara",
        "Z. Mao",
        "Sara Rampazzi"
      ],
      "author_details": [
        {
          "name": "Yulong Cao",
          "h_index": 17,
          "citation_count": 2148,
          "affiliations": []
        },
        {
          "name": "S. Bhupathiraju",
          "h_index": 1,
          "citation_count": 93,
          "affiliations": []
        },
        {
          "name": "Pirouz Naghavi",
          "h_index": 23,
          "citation_count": 7707,
          "affiliations": []
        },
        {
          "name": "Takeshi Sugawara",
          "h_index": 6,
          "citation_count": 180,
          "affiliations": []
        },
        {
          "name": "Z. Mao",
          "h_index": 15,
          "citation_count": 1538,
          "affiliations": []
        },
        {
          "name": "Sara Rampazzi",
          "h_index": 7,
          "citation_count": 219,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://openalex.org/W4306887028",
      "pdf_url": "https://arxiv.org/pdf/2210.09482",
      "doi": "https://doi.org/10.48550/arxiv.2210.09482",
      "citation_count": 93,
      "influential_citation_count": 13,
      "reference_count": 65,
      "is_open_access": true,
      "publication_date": "2022-10-18",
      "tldr": "This research discovers how to leverage laser-based spoofing techniques to selectively remove the LiDAR point cloud data of genuine obstacles at the sensor level before being used as input to the AV perception.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "LiDAR",
        "autonomous-vehicles",
        "spoofing",
        "removal-attack"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2210.09482"
    },
    {
      "paper_id": "2302.09491",
      "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
      "abstract": "Adversarial attacks are valuable for evaluating the robustness of deep learning models. Existing attacks are primarily conducted on the visible light spectrum (e.g., pixel-wise texture perturbation). However, attacks targeting texture-free X-ray images remain underexplored, despite the widespread application of X-ray imaging in safety-critical scenarios such as the X-ray detection of prohibited items. In this paper, we take the first step toward the study of adversarial attacks targeted at X-ray prohibited item detection, and reveal the serious threats posed by such attacks in this safety-critical scenario. Specifically, we posit that successful physical adversarial attacks in this scenario should be specially designed to circumvent the challenges posed by color/texture fading and complex overlapping. To this end, we propose X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage. To resolve the issues associated with color/texture fading, we develop a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes, using the gradients of a surrogate model rather than directly generating adversarial textures. To place the printed 3D adversarial objects in luggage with complex overlapped instances, we design a policy-based reinforcement learning strategy to find locations eliciting strong attack performance in worst-case scenarios whereby the prohibited items are heavily occluded by other items. To verify the effectiveness of the proposed X-Adv, we conduct extensive experiments in both the digital and the physical world (employing a commercial X-ray security inspection system for the latter case). Furthermore, we present the physical-world X-ray adversarial attack dataset XAD.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Aishan Liu",
        "Jun Guo",
        "Jiakai Wang",
        "Siyuan Liang",
        "Renshuai Tao",
        "Wenbo Zhou",
        "Cong Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "author_details": [
        {
          "name": "Aishan Liu",
          "h_index": 27,
          "citation_count": 2738,
          "affiliations": []
        },
        {
          "name": "Jun Guo",
          "h_index": 6,
          "citation_count": 207,
          "affiliations": []
        },
        {
          "name": "Jiakai Wang",
          "h_index": 19,
          "citation_count": 1467,
          "affiliations": []
        },
        {
          "name": "Siyuan Liang",
          "h_index": 23,
          "citation_count": 1870,
          "affiliations": []
        },
        {
          "name": "Renshuai Tao",
          "h_index": 10,
          "citation_count": 734,
          "affiliations": []
        },
        {
          "name": "Wenbo Zhou",
          "h_index": 22,
          "citation_count": 3199,
          "affiliations": []
        },
        {
          "name": "Cong Liu",
          "h_index": 14,
          "citation_count": 908,
          "affiliations": []
        },
        {
          "name": "Xianglong Liu",
          "h_index": 63,
          "citation_count": 12634,
          "affiliations": []
        },
        {
          "name": "Dacheng Tao",
          "h_index": 34,
          "citation_count": 4561,
          "affiliations": []
        }
      ],
      "max_h_index": 63,
      "url": "https://arxiv.org/abs/2302.09491",
      "citation_count": 90,
      "influential_citation_count": 2,
      "reference_count": 54,
      "is_open_access": true,
      "publication_date": "2023-02-19",
      "tldr": "This paper proposes X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage, and develops a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "X-ray",
        "physical-attack",
        "prohibited-items",
        "security-screening"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2302.09491"
    },
    {
      "paper_id": "seed_851c5dcb",
      "title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples",
      "abstract": "There has been a recent surge in adversarial attacks on deep learning based automatic speech recognition (ASR) systems. These attacks pose new challenges to deep learning security and have raised significant concerns in deploying ASR systems in safety-critical applications. In this work, we introduce WaveGuard: a framework for detecting adversarial inputs that are crafted to attack ASR systems. Our framework incorporates audio transformation functions and analyses the ASR transcriptions of the original and transformed audio to detect adversarial inputs. We demonstrate that our defense framework is able to reliably detect adversarial examples constructed by four recent audio adversarial attacks, with a variety of audio transformation functions. With careful regard for best practices in defense evaluations, we analyze our proposed defense and its strength to withstand adaptive and robust attacks in the audio domain. We empirically demonstrate that audio transformations that recover audio from perceptually informed representations can lead to a strong defense that is robust against an adaptive adversary even in a complete white-box setting. Furthermore, WaveGuard can be used out-of-the box and integrated directly with any ASR model to efficiently detect audio adversarial examples, without the need for model retraining.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Shehzeen Samarah Hussain",
        "Paarth Neekhara",
        "S. Dubnov",
        "Julian McAuley",
        "F. Koushanfar"
      ],
      "author_details": [
        {
          "name": "Shehzeen Samarah Hussain",
          "h_index": 15,
          "citation_count": 856,
          "affiliations": [
            "University of California, San Diego"
          ]
        },
        {
          "name": "Paarth Neekhara",
          "h_index": 18,
          "citation_count": 1071,
          "affiliations": []
        },
        {
          "name": "S. Dubnov",
          "h_index": 36,
          "citation_count": 5100,
          "affiliations": []
        },
        {
          "name": "Julian McAuley",
          "h_index": 72,
          "citation_count": 33102,
          "affiliations": [
            "UC San Diego"
          ]
        },
        {
          "name": "F. Koushanfar",
          "h_index": 68,
          "citation_count": 28916,
          "affiliations": []
        }
      ],
      "max_h_index": 72,
      "url": "https://openalex.org/W3135197931",
      "pdf_url": "https://arxiv.org/pdf/2103.03344",
      "doi": "https://doi.org/10.48550/arxiv.2103.03344",
      "citation_count": 83,
      "influential_citation_count": 12,
      "reference_count": 58,
      "is_open_access": false,
      "publication_date": "2021-03-04",
      "tldr": "WaveGuard is introduced: a framework for detecting adversarial inputs that are crafted to attack ASR systems and empirically demonstrates that audio transformations that recover audio from perceptually informed representations can lead to a strong defense that is robust against an adaptive adversary even in a complete white-box setting.",
      "fields_of_study": [
        "Computer Science",
        "Engineering"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "audio"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "ASR-defense",
        "audio-adversarial",
        "detection"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_41f18639",
      "title": "That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency",
      "abstract": "Smart cities are being developed worldwide with the use of technology to improve the quality of life of citizens and enhance their safety. Video surveillance is a key component of smart city infrastructure, as it involves the installation of cameras at strategic locations throughout the city for monitoring public spaces and providing real-time surveillance footage to law enforcement and other city representatives. Video surveillance systems have evolved rapidly in recent years, and are now integrated with advanced technologies like deep learning, blockchain, edge computing, and cloud computing. This study provides a comprehensive overview of video surveillance systems in smart cities, as well as the functions and challenges of those systems. The aim of this paper is to highlight the importance of video surveillance systems in smart cities and to provide insights into how they could be used to enhance safety, security, and the overall quality of life for citizens.",
      "year": 2023,
      "venue": "Electronics",
      "authors": [
        "Yanjinlkham Myagmar-Ochir",
        "Wooseong Kim"
      ],
      "author_details": [
        {
          "name": "Yanjinlkham Myagmar-Ochir",
          "h_index": 1,
          "citation_count": 82,
          "affiliations": []
        },
        {
          "name": "Wooseong Kim",
          "h_index": 19,
          "citation_count": 1184,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://openalex.org/W4386141924",
      "pdf_url": "https://www.mdpi.com/2079-9292/12/17/3567/pdf?version=1692796872",
      "doi": "https://doi.org/10.3390/electronics12173567",
      "citation_count": 82,
      "influential_citation_count": 2,
      "reference_count": 169,
      "is_open_access": true,
      "publication_date": "2023-08-23",
      "tldr": "This study provides a comprehensive overview of video surveillance systems in smart cities, as well as the functions and challenges of those systems.",
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "autonomous-systems",
        "attack-detection",
        "spatiotemporal",
        "smart-city"
      ],
      "open_access_pdf": "https://www.mdpi.com/2079-9292/12/17/3567/pdf?version=1692796872"
    },
    {
      "paper_id": "seed_1e31a394",
      "title": "AIRS: Explanation for Deep Reinforcement Learning based Security Applications",
      "abstract": "This perspective paper is based on several sessions by the members of the Round Table AI at FIRM 1 , with input from a number of external and international speakers. Its particular focus lies on the management of the model risk of productive models in banks and other financial institutions. The models in view range from simple rules-based approaches to Artificial Intelligence (AI) or Machine learning (ML) models with a high level of sophistication. The typical applications of those models are related to predictions and decision making around the value chain of credit risk (including accounting side under IFRS9 or related national GAAP approaches), insurance risk or other financial risk types. We expect more models of higher complexity in the space of anti-money laundering, fraud detection and transaction monitoring as well as a rise of AI/ML models as alternatives to current methods in solving some of the more intricate stochastic differential equations needed for the pricing and/or valuation of derivatives. The same type of model is also successful in areas unrelated to risk management, such as sales optimization, customer lifetime value considerations, robo-advisory, and other fields of applications. The paper refers to recent related publications from central banks, financial supervisors and regulators as well as other relevant sources and working groups. It aims to give practical advice for establishing a risk-based governance and testing framework for the mentioned model types and discusses the use of recent technologies, approaches, and platforms to support the establishment of responsible, trustworthy, explainable, auditable, and manageable AI/ML in production. In view of the recent EU publication on AI, also referred to as the EU Artificial Intelligence Act (AIA), we also see a certain added value for this paper as an instigator of further thinking outside of the financial services sector, in particular where \u201cHigh Risk\u201d models according to the mentioned EU consultation are concerned.",
      "year": 2022,
      "venue": "Frontiers in Artificial Intelligence",
      "authors": [
        "Sebastian G. Fritz-Morgenthal",
        "Bernhard Hein",
        "Jochen Papenbrock"
      ],
      "author_details": [
        {
          "name": "Sebastian G. Fritz-Morgenthal",
          "h_index": 4,
          "citation_count": 125,
          "affiliations": []
        },
        {
          "name": "Bernhard Hein",
          "h_index": 2,
          "citation_count": 90,
          "affiliations": []
        },
        {
          "name": "Jochen Papenbrock",
          "h_index": 10,
          "citation_count": 745,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://openalex.org/W4225496733",
      "pdf_url": "https://www.frontiersin.org/articles/10.3389/frai.2022.779799/pdf",
      "doi": "https://doi.org/10.3389/frai.2022.779799",
      "citation_count": 79,
      "influential_citation_count": 2,
      "reference_count": 27,
      "is_open_access": true,
      "publication_date": "2021-06-25",
      "tldr": "This perspective paper is based on several sessions by the members of the Round Table AI at FIRM1 and discusses the use of recent technologies, approaches, and platforms to support the establishment of responsible, trustworthy, explainable, auditable, and manageable AI/ML in production.",
      "fields_of_study": [
        "Medicine",
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "XAI",
        "DRL-explanation",
        "security-applications"
      ],
      "open_access_pdf": "https://www.frontiersin.org/articles/10.3389/frai.2022.779799/pdf"
    },
    {
      "paper_id": "2209.03463",
      "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
      "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Waiman Si",
        "M. Backes",
        "Jeremy Blackburn",
        "Emiliano De Cristofaro",
        "G. Stringhini",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Waiman Si",
          "h_index": 6,
          "citation_count": 176,
          "affiliations": []
        },
        {
          "name": "M. Backes",
          "h_index": 71,
          "citation_count": 19959,
          "affiliations": []
        },
        {
          "name": "Jeremy Blackburn",
          "h_index": 46,
          "citation_count": 8415,
          "affiliations": []
        },
        {
          "name": "Emiliano De Cristofaro",
          "h_index": 18,
          "citation_count": 1109,
          "affiliations": []
        },
        {
          "name": "G. Stringhini",
          "h_index": 53,
          "citation_count": 10769,
          "affiliations": []
        },
        {
          "name": "Savvas Zannettou",
          "h_index": 39,
          "citation_count": 5735,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 6,
          "citation_count": 286,
          "affiliations": []
        }
      ],
      "max_h_index": 71,
      "url": "https://arxiv.org/abs/2209.03463",
      "citation_count": 79,
      "influential_citation_count": 9,
      "reference_count": 78,
      "is_open_access": true,
      "publication_date": "2022-09-07",
      "tldr": "It is shown that publicly available chatbots are prone to providing toxic responses when fed toxic queries, and three defense mechanisms against ToxicBuddy are evaluated, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "chatbot-attack",
        "toxic-behavior",
        "trigger"
      ],
      "open_access_pdf": "https://repository.tudelft.nl/file/File_0fbdcaf5-5e41-48f9-8cdb-7e982d87b5cf"
    },
    {
      "paper_id": "seed_c40e42d5",
      "title": "Adversarial Training for Raw-Binary Malware Classifiers",
      "abstract": "Learning on execution behaviour, i.e., sequences of API calls, is proven to be effective in malware detection. In this paper, we present CruParamer, a deep neural network based malware detection approach for Windows platform that performs learning on sequences of parameter-augmented APIs. It first employs rule-based and clustering-based classification to assess the sensitivity of a parameter to malicious behaviour, and further labels the API following the run-time parameters with varying degrees of sensitivities. Then, it encodes the APIs by concatenating the native embedding and the sensitive embedding of labelled APIs, for characterizing the relationship between successive labelled APIs and their correspondence in terms of security semantics. Finally, it feeds the sequences of API embedding into the deep neural network for training a binary classifier to detect malware. In addition to presenting the design, we have implemented CruParamer and evaluated it on two datasets. The results demonstrate that CruParamer outperforms na\u00efve models when taking raw APIs as input, proving the effectiveness of CruParamer. Moreover, we have evaluated the impact of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">mimicry</i> and adversarial attacks on our model, and the results verify the robustness of CruParamer.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Xiaohui Chen",
        "Zhiyu Hao",
        "Lun Li",
        "Lei Cui",
        "Yiran Zhu",
        "Zhenquan Ding",
        "Yongji Liu"
      ],
      "author_details": [
        {
          "name": "Xiaohui Chen",
          "h_index": 3,
          "citation_count": 359,
          "affiliations": []
        },
        {
          "name": "Zhiyu Hao",
          "h_index": 10,
          "citation_count": 494,
          "affiliations": []
        },
        {
          "name": "Lun Li",
          "h_index": 8,
          "citation_count": 313,
          "affiliations": []
        },
        {
          "name": "Lei Cui",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Yiran Zhu",
          "h_index": 2,
          "citation_count": 106,
          "affiliations": []
        },
        {
          "name": "Zhenquan Ding",
          "h_index": 7,
          "citation_count": 230,
          "affiliations": []
        },
        {
          "name": "Yongji Liu",
          "h_index": 5,
          "citation_count": 191,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://openalex.org/W4212847133",
      "doi": "https://doi.org/10.1109/tifs.2022.3152360",
      "citation_count": 77,
      "influential_citation_count": 5,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "malware-detection",
        "adversarial-training",
        "raw-binary"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5a511665",
      "title": "Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation",
      "abstract": "Concept drift is one of the most frustrating challenges for learning-based security applications built on the closeworld assumption of identical distribution between training and deployment.Anomaly detection, one of the most important tasks in security domains, is instead immune to the drift of abnormal behavior due to the training without any abnormal data (known as zero-positive), which however comes at the cost of more severe impacts when normality shifts.However, existing studies mainly focus on concept drift of abnormal behaviour and/or supervised learning, leaving the normality shift for zero-positive anomaly detection largely unexplored.In this work, we are the first to explore the normality shift for deep learning-based anomaly detection in security applications, and propose OWAD, a general framework to detect, explain, and adapt to normality shift in practice.In particular, OWAD outperforms prior work by detecting shift in an unsupervised fashion, reducing the overhead of manual labeling, and providing better adaptation performance through distribution-level tackling.We demonstrate the effectiveness of OWAD through several realistic experiments on three security-related anomaly detection applications with long-term practical data.Results show that OWAD can provide better adaptation performance of normality shift with less labeling overhead.We provide case studies to analyze the normality shift and provide operational recommendations for security applications.We also conduct an initial real-world deployment on a SCADA security system.1 Normality shift intuitively refers to the change of distribution of normal data (detailed definition is in \u00a7II-C).In this paper, we interchangeably use terms \"drift\" and \"shift\".We tend to use \"normality shift\" as a whole term.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Dongqi Han",
        "Zhiliang Wang",
        "Wenqi Chen",
        "Kai Wang",
        "Rui Yu",
        "Su Wang",
        "Han Zhang",
        "Zhihua Wang",
        "Minghui Jin",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "author_details": [
        {
          "name": "Dongqi Han",
          "h_index": 10,
          "citation_count": 640,
          "affiliations": []
        },
        {
          "name": "Zhiliang Wang",
          "h_index": 10,
          "citation_count": 523,
          "affiliations": []
        },
        {
          "name": "Wenqi Chen",
          "h_index": 8,
          "citation_count": 567,
          "affiliations": []
        },
        {
          "name": "Kai Wang",
          "h_index": 5,
          "citation_count": 181,
          "affiliations": []
        },
        {
          "name": "Rui Yu",
          "h_index": 1,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Su Wang",
          "h_index": 10,
          "citation_count": 694,
          "affiliations": []
        },
        {
          "name": "Han Zhang",
          "h_index": 10,
          "citation_count": 576,
          "affiliations": []
        },
        {
          "name": "Zhihua Wang",
          "h_index": 3,
          "citation_count": 109,
          "affiliations": []
        },
        {
          "name": "Minghui Jin",
          "h_index": 3,
          "citation_count": 228,
          "affiliations": []
        },
        {
          "name": "Jiahai Yang",
          "h_index": 16,
          "citation_count": 1155,
          "affiliations": []
        },
        {
          "name": "Xingang Shi",
          "h_index": 21,
          "citation_count": 1882,
          "affiliations": []
        },
        {
          "name": "Xia Yin",
          "h_index": 23,
          "citation_count": 2112,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://openalex.org/W4324007053",
      "doi": "https://doi.org/10.14722/ndss.2023.24830",
      "citation_count": 76,
      "influential_citation_count": 8,
      "reference_count": 85,
      "is_open_access": true,
      "tldr": "Concept drift is one of the most frustrating challenges for learning-based security applications built on the close-world assumption of identical distribution between training and deployment, but existing studies mainly focus on concept drift of abnormal behaviour and or supervised learning, leaving the normality shift for zero-positive anomaly detection largely unexplored.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "concept-drift",
        "anomaly-detection",
        "open-world"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2023.24830"
    },
    {
      "paper_id": "2102.00918",
      "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
      "abstract": "Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Alireza Bahramali",
        "Milad Nasr",
        "Amir Houmansadr",
        "D. Goeckel",
        "D. Towsley"
      ],
      "author_details": [
        {
          "name": "Alireza Bahramali",
          "h_index": 8,
          "citation_count": 509,
          "affiliations": []
        },
        {
          "name": "Milad Nasr",
          "h_index": 31,
          "citation_count": 11780,
          "affiliations": []
        },
        {
          "name": "Amir Houmansadr",
          "h_index": 42,
          "citation_count": 7508,
          "affiliations": []
        },
        {
          "name": "D. Goeckel",
          "h_index": 46,
          "citation_count": 8847,
          "affiliations": []
        },
        {
          "name": "D. Towsley",
          "h_index": 121,
          "citation_count": 62096,
          "affiliations": []
        }
      ],
      "max_h_index": 121,
      "url": "https://arxiv.org/abs/2102.00918",
      "citation_count": 74,
      "influential_citation_count": 7,
      "reference_count": 51,
      "is_open_access": true,
      "publication_date": "2021-02-01",
      "tldr": "The results demonstrate that even when employing well-considered defenses, DNN-based wireless communication systems are vulnerable to adversarial attacks and call into question the employment of DNNs for a number of tasks in robust wireless communication.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "wireless",
        "UAP",
        "robust-attack",
        "GAN"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484777"
    },
    {
      "paper_id": "seed_5c2b6503",
      "title": "Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs",
      "abstract": "Natural language prompts serve as an essential interface between users and Large Language Models (LLMs) like GPT-3.5 and GPT-4, which are employed by ChatGPT to produce outputs across various tasks. However, prompts crafted with malicious intent, known as jailbreak prompts, can circumvent the restrictions of LLMs, posing a significant threat to systems integrated with these models. Despite their critical importance, there is a lack of systematic analysis and comprehensive understanding of jailbreak prompts. Our paper aims to address this gap by exploring key research questions to enhance the robustness of LLM systems: 1) What common patterns are present in jailbreak prompts? 2) How effectively can these prompts bypass the restrictions of LLMs? 3) With the evolution of LLMs, how does the effectiveness of jailbreak prompts change? To address our research questions, we embarked on an empirical study targeting the LLMs underpinning ChatGPT, one of today\u2019s most advanced chatbots. Our methodology involved categorizing 78 jailbreak prompts into 10 distinct patterns, further organized into three jailbreak strategy types, and examining their distribution.We assessed the effectiveness of these prompts on GPT-3.5 and GPT-4, using a set of 3,120 questions across 8 scenarios deemed prohibited by OpenAI. Additionally, our study tracked the performance of these prompts over a 3-month period, observing the evolutionary response of ChatGPT to such inputs. Our findings offer a comprehensive view of jailbreak prompts, elucidating their taxonomy, effectiveness, and temporal dynamics. Notably, we discovered that GPT-3.5 and GPT-4 could still generate inappropriate content in response to malicious prompts without the need for jailbreaking. This underscores the critical need for effective prompt management within LLM systems and provides valuable insights and data to spur further research in LLM testing and jailbreak prevention.",
      "year": 2024,
      "venue": "SEA4DQ@SIGSOFT FSE",
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Zhengzi Xu",
        "Yuekang Li",
        "Yaowen Zheng",
        "Ying Zhang",
        "Lida Zhao",
        "Tianwei Zhang",
        "Kailong Wang"
      ],
      "author_details": [
        {
          "name": "Yi Liu",
          "h_index": 9,
          "citation_count": 426,
          "affiliations": []
        },
        {
          "name": "Gelei Deng",
          "h_index": 21,
          "citation_count": 2874,
          "affiliations": []
        },
        {
          "name": "Zhengzi Xu",
          "h_index": 22,
          "citation_count": 2143,
          "affiliations": []
        },
        {
          "name": "Yuekang Li",
          "h_index": 28,
          "citation_count": 4241,
          "affiliations": [
            "School of Computer Science and Engineering, University of New South Wales"
          ]
        },
        {
          "name": "Yaowen Zheng",
          "h_index": 6,
          "citation_count": 182,
          "affiliations": []
        },
        {
          "name": "Ying Zhang",
          "h_index": 2,
          "citation_count": 70,
          "affiliations": []
        },
        {
          "name": "Lida Zhao",
          "h_index": 7,
          "citation_count": 805,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 5,
          "citation_count": 203,
          "affiliations": []
        },
        {
          "name": "Kailong Wang",
          "h_index": 6,
          "citation_count": 391,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://openalex.org/W4400484590",
      "pdf_url": "https://vtechworks.lib.vt.edu/bitstreams/f6d649f9-6b40-4a05-ad19-dcbfbc6e9ff1/download",
      "doi": "https://doi.org/10.1145/3663530.3665021",
      "citation_count": 67,
      "influential_citation_count": 1,
      "reference_count": 26,
      "is_open_access": false,
      "publication_date": "2024-07-15",
      "tldr": "It was discovered that GPT-3.5 and GPT-4 could still generate inappropriate content in response to malicious prompts without the need for jailbreaking, underscores the critical need for effective prompt management within LLM systems and provides valuable insights and data to spur further research in LLM testing and jailbreak prevention.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book"
      ],
      "paper_type": "benchmark",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "task-level",
        "jailbreak-benchmark"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_52fb0e9c",
      "title": "Structural Attack against Graph Based Android Malware Detection",
      "abstract": "Malware detection techniques achieve great success with deeper insight into the semantics of malware. Among existing detection techniques, function call graph (FCG) based methods achieve promising performance due to their prominent representations of malware's functionalities. Meanwhile, recent adversarial attacks not only perturb feature vectors to deceive classifiers (i.e., feature-space attacks) but also investigate how to generate real evasive malware (i.e., problem-space attacks). However, existing problem-space attacks are limited due to their inconsistent transformations between feature space and problem space.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Kaifa Zhao",
        "Hao Zhou",
        "Yulin Zhu",
        "Xian Zhan",
        "Kai Zhou",
        "Jianfeng Li",
        "Le Yu",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "author_details": [
        {
          "name": "Kaifa Zhao",
          "h_index": 11,
          "citation_count": 534,
          "affiliations": []
        },
        {
          "name": "Hao Zhou",
          "h_index": 22,
          "citation_count": 2920,
          "affiliations": [
            "Tsinghua University"
          ]
        },
        {
          "name": "Yulin Zhu",
          "h_index": 8,
          "citation_count": 259,
          "affiliations": []
        },
        {
          "name": "Xian Zhan",
          "h_index": 13,
          "citation_count": 599,
          "affiliations": []
        },
        {
          "name": "Kai Zhou",
          "h_index": 10,
          "citation_count": 337,
          "affiliations": []
        },
        {
          "name": "Jianfeng Li",
          "h_index": 14,
          "citation_count": 481,
          "affiliations": []
        },
        {
          "name": "Le Yu",
          "h_index": 18,
          "citation_count": 1242,
          "affiliations": []
        },
        {
          "name": "Wei Yuan",
          "h_index": 11,
          "citation_count": 559,
          "affiliations": []
        },
        {
          "name": "Xiapu Luo",
          "h_index": 59,
          "citation_count": 13207,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W3212677680",
      "doi": "https://doi.org/10.1145/3460120.3485387",
      "citation_count": 63,
      "influential_citation_count": 10,
      "reference_count": 72,
      "is_open_access": false,
      "publication_date": "2021-11-12",
      "tldr": "This paper proposes the first structural attack against graph-based Android malware detection techniques, which addresses the inverse-transformation problem between feature-space attacks and problem space attacks, and designs a Heuristic optimization model integrated with Reinforcement learning framework to optimize this structural ATtack.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "malware-detection",
        "FCG-attack",
        "Android",
        "problem-space"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2302.04332",
      "title": "Continuous Learning for Android Malware Detection",
      "abstract": "Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.   In this paper, we propose new methods to combat the concept drift problem of Android malware classifiers. Since machine learning technique needs to be continuously deployed, we use active learning: we select new samples for analysts to label, and then add the labeled samples to the training set to retrain the classifier. Our key idea is, similarity-based uncertainty is more robust against concept drift. Therefore, we combine contrastive learning with active learning. We propose a new hierarchical contrastive learning scheme, and a new sample selection technique to continuously train the Android malware classifier. Our evaluation shows that this leads to significant improvements, compared to previously published methods for active learning. Our approach reduces the false negative rate from 14% (for the best baseline) to 9%, while also reducing the false positive rate (from 0.86% to 0.48%). Also, our approach maintains more consistent performance across a seven-year time period than past methods.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yizheng Chen",
        "Zhoujie Ding",
        "David A. Wagner"
      ],
      "author_details": [
        {
          "name": "Yizheng Chen",
          "h_index": 15,
          "citation_count": 1152,
          "affiliations": []
        },
        {
          "name": "Zhoujie Ding",
          "h_index": 3,
          "citation_count": 324,
          "affiliations": []
        },
        {
          "name": "David A. Wagner",
          "h_index": 16,
          "citation_count": 1480,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://arxiv.org/abs/2302.04332",
      "citation_count": 63,
      "influential_citation_count": 21,
      "reference_count": 60,
      "is_open_access": true,
      "publication_date": "2023-02-08",
      "tldr": "This paper proposes a new hierarchical contrastive learning scheme, and a new sample selection technique to continuously train the Android malware classifier, and shows that this approach maintains more consistent performance across a seven-year time period than past methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "concept-drift",
        "continuous-learning",
        "malware-detection",
        "Android"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2302.04332"
    },
    {
      "paper_id": "seed_fd1628f4",
      "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
      "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query, ostensibly because it lacks relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. Our method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not employ an auxiliary LLM. We evaluate jamming attacks on several embeddings and LLMs and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Avital Shafran",
        "R. Schuster",
        "Vitaly Shmatikov"
      ],
      "author_details": [
        {
          "name": "Avital Shafran",
          "h_index": 5,
          "citation_count": 112,
          "affiliations": []
        },
        {
          "name": "R. Schuster",
          "h_index": 14,
          "citation_count": 2354,
          "affiliations": []
        },
        {
          "name": "Vitaly Shmatikov",
          "h_index": 7,
          "citation_count": 594,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://openalex.org/W4399553840",
      "pdf_url": "https://arxiv.org/pdf/2406.05870",
      "doi": "https://doi.org/10.48550/arxiv.2406.05870",
      "citation_count": 63,
      "influential_citation_count": 17,
      "reference_count": 69,
      "is_open_access": false,
      "publication_date": "2024-06-09",
      "tldr": "This work describes and measures the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization that does not rely on instruction injection, does not require the adversary to know the embedding or LLM used by the target RAG system, and does not employ an auxiliary LLM.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "RAG-attack",
        "DoS",
        "blocker-documents"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2410.05451",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "Chuan Guo"
      ],
      "author_details": [
        {
          "name": "Sizhe Chen",
          "h_index": 6,
          "citation_count": 417,
          "affiliations": []
        },
        {
          "name": "Arman Zharmagambetov",
          "h_index": 14,
          "citation_count": 579,
          "affiliations": [
            "University of California, Merced"
          ]
        },
        {
          "name": "Saeed Mahloujifar",
          "h_index": 3,
          "citation_count": 114,
          "affiliations": []
        },
        {
          "name": "Kamalika Chaudhuri",
          "h_index": 9,
          "citation_count": 471,
          "affiliations": []
        },
        {
          "name": "Chuan Guo",
          "h_index": 5,
          "citation_count": 262,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://arxiv.org/abs/2410.05451",
      "citation_count": 63,
      "influential_citation_count": 15,
      "reference_count": 82,
      "is_open_access": false,
      "publication_date": "2024-10-07",
      "tldr": "This work proposes a new defense called SecAlign based on the technique of preference optimization that provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "prompt-injection-defense",
        "preference-optimization"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2002.12398",
      "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
      "abstract": "As machine learning (ML) systems become pervasive, safeguarding their security is critical. However, recently it has been demonstrated that motivated adversaries are able to mislead ML systems by perturbing test data using semantic transformations. While there exists a rich body of research providing provable robustness guarantees for ML models against $\\ell_p$ norm bounded adversarial perturbations, guarantees against semantic perturbations remain largely underexplored. In this paper, we provide TSS -- a unified framework for certifying ML robustness against general adversarial semantic transformations. First, depending on the properties of each transformation, we divide common transformations into two categories, namely resolvable (e.g., Gaussian blur) and differentially resolvable (e.g., rotation) transformations. For the former, we propose transformation-specific randomized smoothing strategies and obtain strong robustness certification. The latter category covers transformations that involve interpolation errors, and we propose a novel approach based on stratified sampling to certify the robustness. Our framework TSS leverages these certification strategies and combines with consistency-enhanced training to provide rigorous certification of robustness. We conduct extensive experiments on over ten types of challenging semantic transformations and show that TSS significantly outperforms the state of the art. Moreover, to the best of our knowledge, TSS is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset. For instance, our framework achieves 30.4% certified robust accuracy against rotation attack (within $\\pm 30^\\circ$) on ImageNet. Moreover, to consider a broader range of transformations, we show TSS is also robust against adaptive attacks and unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Linyi Li",
        "Maurice Weber",
        "Xiaojun Xu",
        "Luka Rimanic",
        "B. Kailkhura",
        "Tao Xie",
        "Ce Zhang",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Linyi Li",
          "h_index": 15,
          "citation_count": 958,
          "affiliations": []
        },
        {
          "name": "Maurice Weber",
          "h_index": 5,
          "citation_count": 133,
          "affiliations": []
        },
        {
          "name": "Xiaojun Xu",
          "h_index": 15,
          "citation_count": 2405,
          "affiliations": []
        },
        {
          "name": "Luka Rimanic",
          "h_index": 10,
          "citation_count": 401,
          "affiliations": []
        },
        {
          "name": "B. Kailkhura",
          "h_index": 42,
          "citation_count": 5580,
          "affiliations": []
        },
        {
          "name": "Tao Xie",
          "h_index": 11,
          "citation_count": 561,
          "affiliations": []
        },
        {
          "name": "Ce Zhang",
          "h_index": 14,
          "citation_count": 692,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 69,
          "citation_count": 27430,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://arxiv.org/abs/2002.12398",
      "citation_count": 61,
      "influential_citation_count": 8,
      "reference_count": 70,
      "is_open_access": true,
      "publication_date": "2020-02-27",
      "tldr": "This paper provides TSS-a unified framework for certifying ML robustness against general adversarial semantic transformations and is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "semantic-transformations",
        "smoothing"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2002.12398"
    },
    {
      "paper_id": "2108.09513",
      "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.   In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiaming Mu",
        "Binghui Wang",
        "Qi Li",
        "Kun Sun",
        "Mingwei Xu",
        "Zhuotao Liu"
      ],
      "author_details": [
        {
          "name": "Jiaming Mu",
          "h_index": 4,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "Binghui Wang",
          "h_index": 26,
          "citation_count": 3350,
          "affiliations": [
            "Iowa State University"
          ]
        },
        {
          "name": "Qi Li",
          "h_index": 33,
          "citation_count": 3518,
          "affiliations": []
        },
        {
          "name": "Kun Sun",
          "h_index": 35,
          "citation_count": 4188,
          "affiliations": []
        },
        {
          "name": "Mingwei Xu",
          "h_index": 34,
          "citation_count": 4702,
          "affiliations": []
        },
        {
          "name": "Zhuotao Liu",
          "h_index": 13,
          "citation_count": 594,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://arxiv.org/abs/2108.09513",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484796",
      "citation_count": 47,
      "influential_citation_count": 4,
      "reference_count": 65,
      "is_open_access": true,
      "publication_date": "2021-08-21",
      "tldr": "This work conducts a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure and designs a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "hard-label",
        "black-box",
        "graph-classification"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484796"
    },
    {
      "paper_id": "seed_c668d459",
      "title": "EarArray: Defending against DolphinAttack via Acoustic Attenuation",
      "abstract": "DolphinAttacks (i.e., inaudible voice commands) modulate audible voices over ultrasounds to inject malicious commands silently into voice assistants and manipulate controlled systems (e.g., doors or smart speakers).Eliminating DolphinAttacks is challenging if ever possible since it requires to modify the microphone hardware.In this paper, we design EarArray, a lightweight method that can not only detect such attacks but also identify the direction of attackers without requiring any extra hardware or hardware modification.Essentially, inaudible voice commands are modulated on ultrasounds that inherently attenuate faster than the one of audible sounds.By inspecting the command sound signals via the built-in multiple microphones on smart devices, EarArray is able to estimate the attenuation rate and thus detect the attacks.We propose a model of the propagation of audible sounds and ultrasounds from the sound source to a voice assistant, e.g., a smart speaker, and illustrate the underlying principle and its feasibility.We implemented EarArray using two specially-designed microphone arrays and our experiments show that EarArray can detect inaudible voice commands with an accuracy of 99% and recognize the direction of the attackers with an accuracy of 97.89%.",
      "year": 2021,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Guoming Zhang",
        "Xiaoyu Ji",
        "Xinfeng Li",
        "Gang Qu",
        "Wenyuan Xu"
      ],
      "author_details": [
        {
          "name": "Guoming Zhang",
          "h_index": 29,
          "citation_count": 3470,
          "affiliations": []
        },
        {
          "name": "Xiaoyu Ji",
          "h_index": 25,
          "citation_count": 2806,
          "affiliations": []
        },
        {
          "name": "Xinfeng Li",
          "h_index": 10,
          "citation_count": 259,
          "affiliations": []
        },
        {
          "name": "Gang Qu",
          "h_index": 4,
          "citation_count": 216,
          "affiliations": []
        },
        {
          "name": "Wenyuan Xu",
          "h_index": 26,
          "citation_count": 3023,
          "affiliations": []
        }
      ],
      "max_h_index": 29,
      "url": "https://openalex.org/W3138076532",
      "pdf_url": "https://doi.org/10.14722/ndss.2021.24551",
      "doi": "https://doi.org/10.14722/ndss.2021.24551",
      "citation_count": 46,
      "influential_citation_count": 8,
      "reference_count": 36,
      "is_open_access": true,
      "tldr": "EarArray is a lightweight method that can not only detect inaudible voice commands but also identify the direction of attackers without requiring any extra hardware or hardware modi\ufb01cation.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "audio"
      ],
      "model_types": [],
      "tags": [
        "ultrasonic-defense",
        "voice-assistant",
        "DolphinAttack"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2021.24551"
    },
    {
      "paper_id": "seed_ca879b84",
      "title": "SpecPatch: Human-in-the-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition",
      "abstract": "The rapid development of deep neural networks and generative AI has catalyzed growth in realistic speech synthesis. While this technology has great potential to improve lives, it also leads to the emergence of ''DeepFake'' where synthesized speech can be misused to deceive humans and machines for nefarious purposes. In response to this evolving threat, there has been a significant amount of interest in mitigating this threat by DeepFake detection.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhiyuan Yu",
        "Shixuan Zhai",
        "Ning Zhang"
      ],
      "author_details": [
        {
          "name": "Zhiyuan Yu",
          "h_index": 13,
          "citation_count": 549,
          "affiliations": [
            "Washington University in St. Louis"
          ]
        },
        {
          "name": "Shixuan Zhai",
          "h_index": 3,
          "citation_count": 131,
          "affiliations": []
        },
        {
          "name": "Ning Zhang",
          "h_index": 5,
          "citation_count": 130,
          "affiliations": [
            "Washington University in St. Louis"
          ]
        }
      ],
      "max_h_index": 13,
      "url": "https://openalex.org/W4388856757",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3576915.3623209",
      "doi": "https://doi.org/10.1145/3576915.3623209",
      "citation_count": 46,
      "influential_citation_count": 7,
      "reference_count": 75,
      "is_open_access": true,
      "publication_date": "2023-11-15",
      "tldr": "This work proposes to take the preventative approach and introduce AntiFake, a defense mechanism that relies on adversarial examples to prevent unauthorized speech synthesis and evaluates the efficacy of the proposed system against state-of-the-art synthesizers.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "spectrogram-attack",
        "human-in-loop",
        "ASR-attack",
        "physical"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3576915.3623209"
    },
    {
      "paper_id": "seed_1ba3949d",
      "title": "SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models",
      "abstract": "Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinfeng Li",
        "Yuchen Yang",
        "Jiangyi Deng",
        "Chen Yan",
        "Yanjiao Chen",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "author_details": [
        {
          "name": "Xinfeng Li",
          "h_index": 10,
          "citation_count": 259,
          "affiliations": []
        },
        {
          "name": "Yuchen Yang",
          "h_index": 2,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Jiangyi Deng",
          "h_index": 7,
          "citation_count": 153,
          "affiliations": []
        },
        {
          "name": "Chen Yan",
          "h_index": 17,
          "citation_count": 1581,
          "affiliations": []
        },
        {
          "name": "Yanjiao Chen",
          "h_index": 5,
          "citation_count": 100,
          "affiliations": []
        },
        {
          "name": "Xiaoyu Ji",
          "h_index": 25,
          "citation_count": 2806,
          "affiliations": []
        },
        {
          "name": "Wenyuan Xu",
          "h_index": 4,
          "citation_count": 87,
          "affiliations": []
        }
      ],
      "max_h_index": 25,
      "url": "https://arxiv.org/abs/2404.06666",
      "citation_count": 46,
      "influential_citation_count": 10,
      "reference_count": 52,
      "is_open_access": true,
      "publication_date": "2024-04-10",
      "tldr": "SafeGen is presented, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner that outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "NSFW-mitigation",
        "T2I-defense"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670295"
    },
    {
      "paper_id": "seed_8bf018c0",
      "title": "Transferable Multimodal Attack on Vision-Language Pre-training Models",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Haodi Wang",
        "Kai Dong",
        "Zhilei Zhu",
        "Haotong Qin",
        "Aishan Liu",
        "Xiaolin Fang",
        "Jiakai Wang",
        "Xianglong Liu"
      ],
      "author_details": [
        {
          "name": "Haodi Wang",
          "h_index": 1,
          "citation_count": 43,
          "affiliations": []
        },
        {
          "name": "Kai Dong",
          "h_index": 1,
          "citation_count": 43,
          "affiliations": []
        },
        {
          "name": "Zhilei Zhu",
          "h_index": 2,
          "citation_count": 53,
          "affiliations": []
        },
        {
          "name": "Haotong Qin",
          "h_index": 24,
          "citation_count": 2491,
          "affiliations": [
            "Beihang University"
          ]
        },
        {
          "name": "Aishan Liu",
          "h_index": 27,
          "citation_count": 2738,
          "affiliations": []
        },
        {
          "name": "Xiaolin Fang",
          "h_index": 1,
          "citation_count": 43,
          "affiliations": []
        },
        {
          "name": "Jiakai Wang",
          "h_index": 19,
          "citation_count": 1467,
          "affiliations": []
        },
        {
          "name": "Xianglong Liu",
          "h_index": 15,
          "citation_count": 620,
          "affiliations": []
        }
      ],
      "max_h_index": 27,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg",
      "citation_count": 43,
      "influential_citation_count": 9,
      "reference_count": 80,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "VLP-attack",
        "transferable",
        "multimodal"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_56b12302",
      "title": "Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",
      "abstract": "Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianyu Du",
        "S. Ji",
        "Lujia Shen",
        "Yao Zhang",
        "Jinfeng Li",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "R. Beyah",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Tianyu Du",
          "h_index": 16,
          "citation_count": 1624,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Lujia Shen",
          "h_index": 5,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "Yao Zhang",
          "h_index": 6,
          "citation_count": 163,
          "affiliations": []
        },
        {
          "name": "Jinfeng Li",
          "h_index": 10,
          "citation_count": 1458,
          "affiliations": [
            "Alibaba Group",
            "Zhejiang University"
          ]
        },
        {
          "name": "Jie Shi",
          "h_index": 13,
          "citation_count": 662,
          "affiliations": []
        },
        {
          "name": "Chengfang Fang",
          "h_index": 14,
          "citation_count": 674,
          "affiliations": []
        },
        {
          "name": "Jianwei Yin",
          "h_index": 7,
          "citation_count": 379,
          "affiliations": []
        },
        {
          "name": "R. Beyah",
          "h_index": 39,
          "citation_count": 5488,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://openalex.org/W3214321642",
      "doi": "https://doi.org/10.1145/3460120.3484538",
      "citation_count": 38,
      "influential_citation_count": 1,
      "reference_count": 54,
      "is_open_access": false,
      "publication_date": "2021-11-12",
      "tldr": "Cert-RNN enables a range of practical applications including evaluating the provable effectiveness for various defenses, improving the robustness of RNNs, and identifying sensitive words, which helps build more robust and interpretable deep learning systems.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "rnn"
      ],
      "tags": [
        "certified-robustness",
        "RNN",
        "verification"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2202.01811",
      "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
      "abstract": "Object detectors, which are widely deployed in security-critical systems such as autonomous vehicles, have been found vulnerable to patch hiding attacks. An attacker can use a single physically-realizable adversarial patch to make the object detector miss the detection of victim objects and undermine the functionality of object detection applications. In this paper, we propose ObjectSeeker for certifiably robust object detection against patch hiding attacks. The key insight in ObjectSeeker is patch-agnostic masking: we aim to mask out the entire adversarial patch without knowing the shape, size, and location of the patch. This masking operation neutralizes the adversarial effect and allows any vanilla object detector to safely detect objects on the masked images. Remarkably, we can evaluate ObjectSeeker's robustness in a certifiable manner: we develop a certification procedure to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness. Our experiments demonstrate a significant (~10%-40% absolute and ~2-6x relative) improvement in certifiable robustness over the prior work, as well as high clean performance (~1% drop compared with undefended models).",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Chong Xiang",
        "Alexander Valtchanov",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "author_details": [
        {
          "name": "Chong Xiang",
          "h_index": 14,
          "citation_count": 1341,
          "affiliations": []
        },
        {
          "name": "Alexander Valtchanov",
          "h_index": 1,
          "citation_count": 37,
          "affiliations": []
        },
        {
          "name": "Saeed Mahloujifar",
          "h_index": 14,
          "citation_count": 972,
          "affiliations": []
        },
        {
          "name": "Prateek Mittal",
          "h_index": 59,
          "citation_count": 21589,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://arxiv.org/abs/2202.01811",
      "citation_count": 37,
      "influential_citation_count": 8,
      "reference_count": 90,
      "is_open_access": true,
      "publication_date": "2022-02-03",
      "tldr": "A certification procedure is developed to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "object-detection",
        "patch-hiding"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2202.01811"
    },
    {
      "paper_id": "2309.14122",
      "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
      "abstract": "Advanced text-to-image models such as DALL$\\cdot$E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zhongjie Ba",
        "Jieming Zhong",
        "Jiachen Lei",
        "Pengyu Cheng",
        "Qinglong Wang",
        "Zhan Qin",
        "Zhibo Wang",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Zhongjie Ba",
          "h_index": 18,
          "citation_count": 1419,
          "affiliations": []
        },
        {
          "name": "Jieming Zhong",
          "h_index": 2,
          "citation_count": 91,
          "affiliations": []
        },
        {
          "name": "Jiachen Lei",
          "h_index": 3,
          "citation_count": 47,
          "affiliations": []
        },
        {
          "name": "Pengyu Cheng",
          "h_index": 30,
          "citation_count": 3249,
          "affiliations": [
            "Duke University"
          ]
        },
        {
          "name": "Qinglong Wang",
          "h_index": 5,
          "citation_count": 119,
          "affiliations": []
        },
        {
          "name": "Zhan Qin",
          "h_index": 37,
          "citation_count": 5380,
          "affiliations": []
        },
        {
          "name": "Zhibo Wang",
          "h_index": 41,
          "citation_count": 6001,
          "affiliations": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Kui Ren",
          "h_index": 14,
          "citation_count": 655,
          "affiliations": []
        }
      ],
      "max_h_index": 41,
      "url": "https://arxiv.org/abs/2309.14122",
      "citation_count": 35,
      "influential_citation_count": 6,
      "reference_count": 42,
      "is_open_access": true,
      "publication_date": "2023-09-25",
      "tldr": "This work successfully devise and exhibit the first prompt attacks on Midjourney, producing abundant photorealistic NSFW images and reveals the fundamental principles of such prompt attacks and strategically substitute high-risk sections within a suspect prompt to evade closed-source safety measures.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "safety-bypass",
        "substitution",
        "T2I-attack"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690346"
    },
    {
      "paper_id": "2208.12897",
      "title": "ATTRITION: Attacking Static Hardware Trojan Detection Techniques Using Reinforcement Learning",
      "abstract": "Stealthy hardware Trojans (HTs) inserted during the fabrication of integrated circuits can bypass the security of critical infrastructures. Although researchers have proposed many techniques to detect HTs, several limitations exist, including: (i) a low success rate, (ii) high algorithmic complexity, and (iii) a large number of test patterns. Furthermore, the most pertinent drawback of prior detection techniques stems from an incorrect evaluation methodology, i.e., they assume that an adversary inserts HTs randomly. Such inappropriate adversarial assumptions enable detection techniques to claim high HT detection accuracy, leading to a \"false sense of security.\" Unfortunately, to the best of our knowledge, despite more than a decade of research on detecting HTs inserted during fabrication, there have been no concerted efforts to perform a systematic evaluation of HT detection techniques.   In this paper, we play the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL). ATTRITION evades eight detection techniques across two HT detection categories, showcasing its agnostic behavior. ATTRITION achieves average attack success rates of $47\\times$ and $211\\times$ compared to randomly inserted HTs against state-of-the-art HT detection techniques. We demonstrate ATTRITION's ability to evade detection techniques by evaluating designs ranging from the widely-used academic suites to larger designs such as the open-source MIPS and mor1kx processors to AES and a GPS module. Additionally, we showcase the impact of ATTRITION-generated HTs through two case studies (privilege escalation and kill switch) on the mor1kx processor. We envision that our work, along with our released HT benchmarks and models, fosters the development of better HT detection techniques.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Vasudev Gohil",
        "Hao Guo",
        "Satwik Patnaik",
        "Jeyavijayan Rajendran"
      ],
      "author_details": [
        {
          "name": "Vasudev Gohil",
          "h_index": 8,
          "citation_count": 250,
          "affiliations": []
        },
        {
          "name": "Hao Guo",
          "h_index": 5,
          "citation_count": 90,
          "affiliations": []
        },
        {
          "name": "Satwik Patnaik",
          "h_index": 21,
          "citation_count": 1045,
          "affiliations": []
        },
        {
          "name": "Jeyavijayan Rajendran",
          "h_index": 25,
          "citation_count": 2959,
          "affiliations": []
        }
      ],
      "max_h_index": 25,
      "url": "https://arxiv.org/abs/2208.12897",
      "citation_count": 34,
      "influential_citation_count": 6,
      "reference_count": 80,
      "is_open_access": true,
      "publication_date": "2022-08-26",
      "tldr": "This work plays the role of a realistic adversary and question the efficacy of HT detection techniques by developing an automated, scalable, and practical attack framework, ATTRITION, using reinforcement learning (RL), and demonstrates its ability in evading detection techniques.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "hardware-trojan",
        "RL-attack",
        "evasion",
        "detection-bypass"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2208.12897"
    },
    {
      "paper_id": "2209.09577",
      "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
      "abstract": "Famous for its superior performance, deep learning (DL) has been popularly used within many applications, which also at the same time attracts various threats to the models. One primary threat is from adversarial attacks. Researchers have intensively studied this threat for several years and proposed dozens of approaches to create adversarial examples (AEs). But most of the approaches are only evaluated on limited models and datasets (e.g., MNIST, CIFAR-10). Thus, the effectiveness of attacking real-world DL models is not quite clear. In this paper, we perform the first systematic study of adversarial attacks on real-world DNN models and provide a real-world model dataset named RWM. Particularly, we design a suite of approaches to adapt current AE generation algorithms to the diverse real-world DL models, including automatically extracting DL models from Android apps, capturing the inputs and outputs of the DL models in apps, generating AEs and validating them by observing the apps' execution. For black-box DL models, we design a semantic-based approach to build suitable datasets and use them for training substitute models when performing transfer-based attacks. After analyzing 245 DL models collected from 62,583 real-world apps, we have a unique opportunity to understand the gap between real-world DL models and contemporary AE generation algorithms. To our surprise, the current AE generation algorithms can only directly attack 6.53% of the models. Benefiting from our approach, the success rate upgrades to 47.35%.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Zizhuang Deng",
        "Kai Chen",
        "Guozhu Meng",
        "Xiaodong Zhang",
        "Ke Xu",
        "Yao Cheng"
      ],
      "author_details": [
        {
          "name": "Zizhuang Deng",
          "h_index": 4,
          "citation_count": 238,
          "affiliations": []
        },
        {
          "name": "Kai Chen",
          "h_index": 7,
          "citation_count": 305,
          "affiliations": []
        },
        {
          "name": "Guozhu Meng",
          "h_index": 11,
          "citation_count": 536,
          "affiliations": []
        },
        {
          "name": "Xiaodong Zhang",
          "h_index": 3,
          "citation_count": 43,
          "affiliations": []
        },
        {
          "name": "Ke Xu",
          "h_index": 14,
          "citation_count": 972,
          "affiliations": []
        },
        {
          "name": "Yao Cheng",
          "h_index": 5,
          "citation_count": 136,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://arxiv.org/abs/2209.09577",
      "citation_count": 34,
      "influential_citation_count": 5,
      "reference_count": 81,
      "is_open_access": true,
      "publication_date": "2022-09-20",
      "tldr": "This paper designs a suite of approaches to adapt current AE generation algorithms to the diverse real-world DL models, including automatically extracting DL models from Android apps, capturing the inputs and outputs of the DL models in apps, generating AEs and validating them by observing the apps' execution.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "Android",
        "real-world-threats",
        "mobile-DL"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3559388"
    },
    {
      "paper_id": "2105.11363",
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "abstract": "Many recent works have proposed methods to train classifiers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classifier. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness.   In this paper, we present a framework and tools for training classifiers that satisfy global robustness properties. We define new notions of global robustness that are more suitable for security classifiers. We design a novel booster-fixer training framework to enforce global robustness properties. We structure our classifier as an ensemble of logic rules and design a new verifier to verify the properties. In our training algorithm, the booster increases the classifier's capacity, and the fixer enforces verified global robustness properties following counterexample guided inductive synthesis.   We show that we can train classifiers to satisfy different global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classifier's performance. For example, we train a Twitter spam account classifier to satisfy five global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn't satisfy any property.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Yue Qin",
        "Xiaojing Liao",
        "S. Jana",
        "David A. Wagner"
      ],
      "author_details": [
        {
          "name": "Yizheng Chen",
          "h_index": 15,
          "citation_count": 1152,
          "affiliations": []
        },
        {
          "name": "Shiqi Wang",
          "h_index": 18,
          "citation_count": 2927,
          "affiliations": [
            "Amazon"
          ]
        },
        {
          "name": "Yue Qin",
          "h_index": 11,
          "citation_count": 289,
          "affiliations": []
        },
        {
          "name": "Xiaojing Liao",
          "h_index": 26,
          "citation_count": 1659,
          "affiliations": []
        },
        {
          "name": "S. Jana",
          "h_index": 47,
          "citation_count": 12126,
          "affiliations": []
        },
        {
          "name": "David A. Wagner",
          "h_index": 16,
          "citation_count": 1480,
          "affiliations": []
        }
      ],
      "max_h_index": 47,
      "url": "https://arxiv.org/abs/2105.11363",
      "citation_count": 34,
      "influential_citation_count": 3,
      "reference_count": 101,
      "is_open_access": true,
      "publication_date": "2021-05-24",
      "tldr": "This paper designs a novel booster-fixer training framework to enforce global robustness properties, and presents a framework and tools for training classifiers that satisfy global robusts properties.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "global-robustness",
        "certified",
        "security-classifiers"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484776"
    },
    {
      "paper_id": "2307.16630",
      "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
      "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xinyu Zhang",
        "Hanbin Hong",
        "Yuan Hong",
        "Peng Huang",
        "Binghui Wang",
        "Zhongjie Ba",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Xinyu Zhang",
          "h_index": 6,
          "citation_count": 252,
          "affiliations": []
        },
        {
          "name": "Hanbin Hong",
          "h_index": 6,
          "citation_count": 185,
          "affiliations": []
        },
        {
          "name": "Yuan Hong",
          "h_index": 7,
          "citation_count": 258,
          "affiliations": []
        },
        {
          "name": "Peng Huang",
          "h_index": 3,
          "citation_count": 60,
          "affiliations": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Binghui Wang",
          "h_index": 26,
          "citation_count": 3350,
          "affiliations": [
            "Iowa State University"
          ]
        },
        {
          "name": "Zhongjie Ba",
          "h_index": 18,
          "citation_count": 1419,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 19,
          "citation_count": 1201,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://arxiv.org/abs/2307.16630",
      "citation_count": 34,
      "influential_citation_count": 2,
      "reference_count": 87,
      "is_open_access": true,
      "publication_date": "2023-07-31",
      "tldr": "This paper proposes Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing and provides the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "certified-robustness",
        "text-adversarial",
        "synonym-substitution"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2307.16630"
    },
    {
      "paper_id": "2009.09663",
      "title": "DeepDyve: Dynamic Verification for Deep Neural Networks",
      "abstract": "Deep neural networks (DNNs) have become one of the enabling technologies in many safety-critical applications, e.g., autonomous driving and medical image analysis. DNN systems, however, suffer from various kinds of threats, such as adversarial example attacks and fault injection attacks. While there are many defense methods proposed against maliciously crafted inputs, solutions against faults presented in the DNN system itself (e.g., parameters and calculations) are far less explored. In this paper, we develop a novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification. The key to enabling such lightweight checking is that the smaller neural network only needs to produce approximate results for the initial task without sacrificing fault coverage much. We develop efficient and effective architecture and task exploration techniques to achieve optimized risk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve can reduce 90% of the risks at around 10% overhead.",
      "year": 2020,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yu Li",
        "Min Li",
        "Bo Luo",
        "Ye Tian",
        "Qiang Xu"
      ],
      "author_details": [
        {
          "name": "Yu Li",
          "h_index": 9,
          "citation_count": 201,
          "affiliations": []
        },
        {
          "name": "Min Li",
          "h_index": 10,
          "citation_count": 271,
          "affiliations": []
        },
        {
          "name": "Bo Luo",
          "h_index": 9,
          "citation_count": 799,
          "affiliations": []
        },
        {
          "name": "Ye Tian",
          "h_index": 14,
          "citation_count": 836,
          "affiliations": []
        },
        {
          "name": "Qiang Xu",
          "h_index": 19,
          "citation_count": 5130,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://arxiv.org/abs/2009.09663",
      "citation_count": 34,
      "influential_citation_count": 2,
      "reference_count": 49,
      "is_open_access": true,
      "publication_date": "2020-09-21",
      "tldr": "A novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification, which can reduce 90% of the risks at around 10% overhead.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "runtime-verification",
        "adversarial-defense",
        "safety"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2009.09663"
    },
    {
      "paper_id": "seed_1dff79a0",
      "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
      "abstract": "Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xunguang Wang",
        "Daoyuan Wu",
        "Zhenlan Ji",
        "Zongjie Li",
        "Pingchuan Ma",
        "Shuaibao Wang",
        "Yingjiu Li",
        "Yang Liu",
        "Ning Liu",
        "Juergen Rahmel"
      ],
      "author_details": [
        {
          "name": "Xunguang Wang",
          "h_index": 3,
          "citation_count": 62,
          "affiliations": []
        },
        {
          "name": "Daoyuan Wu",
          "h_index": 11,
          "citation_count": 570,
          "affiliations": []
        },
        {
          "name": "Zhenlan Ji",
          "h_index": 9,
          "citation_count": 190,
          "affiliations": [
            "HKUST"
          ]
        },
        {
          "name": "Zongjie Li",
          "h_index": 17,
          "citation_count": 864,
          "affiliations": []
        },
        {
          "name": "Pingchuan Ma",
          "h_index": 18,
          "citation_count": 951,
          "affiliations": [
            "Hong Kong University of Science and Technology"
          ]
        },
        {
          "name": "Shuaibao Wang",
          "h_index": 2,
          "citation_count": 45,
          "affiliations": []
        },
        {
          "name": "Yingjiu Li",
          "h_index": 1,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Yang Liu",
          "h_index": 8,
          "citation_count": 426,
          "affiliations": []
        },
        {
          "name": "Ning Liu",
          "h_index": 3,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Juergen Rahmel",
          "h_index": 2,
          "citation_count": 37,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://openalex.org/W4399554837",
      "pdf_url": "https://arxiv.org/pdf/2406.05498",
      "doi": "https://doi.org/10.48550/arxiv.2406.05498",
      "citation_count": 34,
      "influential_citation_count": 4,
      "reference_count": 109,
      "is_open_access": false,
      "publication_date": "2024-06-08",
      "tldr": "This paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance to concurrently protect the target LLM instance in the normal stack and collaborate with it for checkpoint-based access control.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "self-defense",
        "practical",
        "jailbreak-defense"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2303.08509",
      "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
      "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting \"never-executed\" function calls into malware code. The main challenges are two-fold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent.   To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges.   Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Heng Li",
        "Zhang Cheng",
        "Bang Wu",
        "Liheng Yuan",
        "Cuiying Gao",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "author_details": [
        {
          "name": "Heng Li",
          "h_index": 11,
          "citation_count": 502,
          "affiliations": []
        },
        {
          "name": "Zhang Cheng",
          "h_index": 2,
          "citation_count": 55,
          "affiliations": []
        },
        {
          "name": "Bang Wu",
          "h_index": 11,
          "citation_count": 509,
          "affiliations": []
        },
        {
          "name": "Liheng Yuan",
          "h_index": 4,
          "citation_count": 121,
          "affiliations": []
        },
        {
          "name": "Cuiying Gao",
          "h_index": 6,
          "citation_count": 304,
          "affiliations": []
        },
        {
          "name": "Wei Yuan",
          "h_index": 11,
          "citation_count": 559,
          "affiliations": []
        },
        {
          "name": "Xiapu Luo",
          "h_index": 3,
          "citation_count": 45,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://arxiv.org/abs/2303.08509",
      "pdf_url": "https://arxiv.org/pdf/2303.08509",
      "citation_count": 33,
      "influential_citation_count": 7,
      "reference_count": 74,
      "is_open_access": true,
      "publication_date": "2023-03-15",
      "tldr": "A novel black-box AE attack towards the FCG based malware detection system, called BagAmmo, which achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [],
      "tags": [
        "malware-detection",
        "FCG",
        "black-box",
        "Android"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2303.08509"
    },
    {
      "paper_id": "2105.08619",
      "title": "On the Robustness of Domain Constraints",
      "abstract": "Machine learning is vulnerable to adversarial examples-inputs designed to cause models to perform poorly. However, it is unclear if adversarial examples represent realistic inputs in the modeled domains. Diverse domains such as networks and phishing have domain constraints-complex relationships between features that an adversary must satisfy for an attack to be realized (in addition to any adversary-specific goals). In this paper, we explore how domain constraints limit adversarial capabilities and how adversaries can adapt their strategies to create realistic (constraint-compliant) examples. In this, we develop techniques to learn domain constraints from data, and show how the learned constraints can be integrated into the adversarial crafting process. We evaluate the efficacy of our approach in network intrusion and phishing datasets and find: (1) up to 82% of adversarial examples produced by state-of-the-art crafting algorithms violate domain constraints, (2) domain constraints are robust to adversarial examples; enforcing constraints yields an increase in model accuracy by up to 34%. We observe not only that adversaries must alter inputs to satisfy domain constraints, but that these constraints make the generation of valid adversarial examples far more challenging.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Yohan Beugin",
        "Mike Weisman",
        "P. Mcdaniel"
      ],
      "author_details": [
        {
          "name": "Ryan Sheatsley",
          "h_index": 9,
          "citation_count": 759,
          "affiliations": []
        },
        {
          "name": "Blaine Hoak",
          "h_index": 4,
          "citation_count": 80,
          "affiliations": []
        },
        {
          "name": "Eric Pauley",
          "h_index": 7,
          "citation_count": 246,
          "affiliations": []
        },
        {
          "name": "Yohan Beugin",
          "h_index": 5,
          "citation_count": 107,
          "affiliations": []
        },
        {
          "name": "Mike Weisman",
          "h_index": 5,
          "citation_count": 131,
          "affiliations": []
        },
        {
          "name": "P. Mcdaniel",
          "h_index": 75,
          "citation_count": 44128,
          "affiliations": []
        }
      ],
      "max_h_index": 75,
      "url": "https://arxiv.org/abs/2105.08619",
      "citation_count": 32,
      "influential_citation_count": 3,
      "reference_count": 76,
      "is_open_access": true,
      "publication_date": "2021-05-18",
      "tldr": "This paper develops techniques to learn domain constraints from data, and shows how the learned constraints can be integrated into the adversarial crafting process and evaluates the efficacy of the approach in network intrusion and phishing datasets.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "domain-constraints",
        "realistic-attacks",
        "feature-constraints"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3460120.3484570"
    },
    {
      "paper_id": "seed_2a9592ec",
      "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
      "abstract": "Diffusion models have been leveraged to perform adversarial purification and thus provide both empirical and certified robustness for a standard model. On the other hand, different robustly trained smoothed models have been studied to improve the certified robustness. Thus, it raises a natural question: Can diffusion model be used to achieve improved certified robustness on those robustly trained smoothed models? In this work, we first theoretically show that recovered instances by diffusion models are in the bounded neighborhood of the original instance with high probability; and the \"one-shot\" denoising diffusion probabilistic models (DDPM) can approximate the mean of the generated distribution of a continuous-time diffusion model, which approximates the original instance under mild conditions. Inspired by our analysis, we propose a certifiably robust pipeline DiffSmooth, which first performs adversarial purification via diffusion models and then maps the purified instances to a common region via a simple yet effective local smoothing strategy. We conduct extensive experiments on different datasets and show that DiffSmooth achieves SOTA-certified robustness compared with eight baselines. For instance, DiffSmooth improves the SOTA-certified accuracy from $36.0\\%$ to $53.0\\%$ under $\\ell_2$ radius $1.5$ on ImageNet. The code is available at [https://github.com/javyduck/DiffSmooth].",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jiawei Zhang",
        "Zhongzhu Chen",
        "Huan Zhang",
        "Chaowei Xiao",
        "Bo Li"
      ],
      "author_details": [
        {
          "name": "Jiawei Zhang",
          "h_index": 4,
          "citation_count": 293,
          "affiliations": []
        },
        {
          "name": "Zhongzhu Chen",
          "h_index": 6,
          "citation_count": 166,
          "affiliations": []
        },
        {
          "name": "Huan Zhang",
          "h_index": 34,
          "citation_count": 11377,
          "affiliations": [
            "UCLA",
            "UC Davis"
          ]
        },
        {
          "name": "Chaowei Xiao",
          "h_index": 43,
          "citation_count": 12462,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 69,
          "citation_count": 27430,
          "affiliations": []
        }
      ],
      "max_h_index": 69,
      "url": "https://openalex.org/W4386272826",
      "pdf_url": "https://arxiv.org/pdf/2308.14333",
      "doi": "https://doi.org/10.48550/arxiv.2308.14333",
      "citation_count": 32,
      "influential_citation_count": 2,
      "reference_count": 63,
      "is_open_access": true,
      "publication_date": "2023-08-28",
      "tldr": "This work theoretically shows that recovered instances by diffusion models are in the bounded neighborhood of the original instance with high probability; and the\"one-shot\"denoising diffusion probabilistic models (DDPM) can approximate the mean of the generated distribution of a continuous-time diffusion model, which approximates the original instances under mild conditions.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "diffusion",
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "diffusion-purification",
        "smoothing"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2308.14333"
    },
    {
      "paper_id": "seed_75866050",
      "title": "Physical Hijacking Attacks against Object Trackers",
      "abstract": "Modern autonomous systems rely on both object detection and object tracking in their visual perception pipelines. Although many recent works have attacked the object detection component of autonomous vehicles, these attacks do not work on full pipelines that integrate object tracking to enhance the object detector\u2019s accuracy. Meanwhile, existing attacks against object tracking either lack real-world applicability or do not work against a powerful class of object trackers, Siamese trackers. In this paper, we present AttrackZone, a new physically-realizable tracker hijacking attack against Siamese trackers that systematically determines valid regions in an environment that can be used for physical perturbations. AttrackZone exploits the heatmap generation process of Siamese Region Proposal Networks in order to take control of an object\u2019s bounding box, resulting in physical consequences including vehicle collisions and masked intrusion of pedestrians into unauthorized areas. Evaluations in both the digital and physical domain show that AttrackZone achieves its attack goals 92% of the time, requiring only 0.3-3 seconds on average.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Raymond Muller",
        "Yanmao Man",
        "Z. Berkay Celik",
        "Ming Li",
        "Ryan M. Gerdes"
      ],
      "author_details": [
        {
          "name": "Raymond Muller",
          "h_index": 5,
          "citation_count": 126,
          "affiliations": []
        },
        {
          "name": "Yanmao Man",
          "h_index": 10,
          "citation_count": 389,
          "affiliations": []
        },
        {
          "name": "Z. Berkay Celik",
          "h_index": 17,
          "citation_count": 1069,
          "affiliations": [
            "Purdue University"
          ]
        },
        {
          "name": "Ming Li",
          "h_index": 15,
          "citation_count": 1003,
          "affiliations": []
        },
        {
          "name": "Ryan M. Gerdes",
          "h_index": 22,
          "citation_count": 1595,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://openalex.org/W4308411153",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3548606.3559390",
      "doi": "https://doi.org/10.1145/3548606.3559390",
      "citation_count": 30,
      "influential_citation_count": 5,
      "reference_count": 56,
      "is_open_access": true,
      "publication_date": "2022-11-07",
      "tldr": "AttrackZone is presented, a new physically-realizable tracker hijacking attack against Siamese trackers that systematically determines valid regions in an environment that can be used for physical perturbations.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "physical-attack",
        "object-tracking",
        "autonomous-systems"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3548606.3559390"
    },
    {
      "paper_id": "2107.04284",
      "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
      "abstract": "Widely deployed deep neural network (DNN) models have been proven to be vulnerable to adversarial perturbations in many applications (e.g., image, audio and text classifications). To date, there are only a few adversarial perturbations proposed to deviate the DNN models in video recognition systems by simply injecting 2D perturbations into video frames. However, such attacks may overly perturb the videos without learning the spatio-temporal features (across temporal frames), which are commonly extracted by DNN models for video recognition. To our best knowledge, we propose the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems. U3D has many advantages, such as (1) as the transfer-based attack, U3D can universally attack multiple DNN models for video recognition without accessing to the target DNN model; (2) the high transferability of U3D makes such universal black-box attack easy-to-launch, which can be further enhanced by integrating queries over the target model when necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the existing state-of-the-art defense schemes; (5) U3D can be efficiently generated with a few pre-learned parameters, and then immediately injected to attack real-time DNN-based video recognition systems. We have conducted extensive experiments to evaluate U3D on multiple DNN models and three large-scale video datasets. The experimental results demonstrate its superiority and practicality.",
      "year": 2022,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Shangyu Xie",
        "Han Wang",
        "Yu Kong",
        "Yuan Hong"
      ],
      "author_details": [
        {
          "name": "Shangyu Xie",
          "h_index": 10,
          "citation_count": 267,
          "affiliations": []
        },
        {
          "name": "Han Wang",
          "h_index": 7,
          "citation_count": 337,
          "affiliations": []
        },
        {
          "name": "Yu Kong",
          "h_index": 25,
          "citation_count": 7249,
          "affiliations": []
        },
        {
          "name": "Yuan Hong",
          "h_index": 11,
          "citation_count": 376,
          "affiliations": []
        }
      ],
      "max_h_index": 25,
      "url": "https://arxiv.org/abs/2107.04284",
      "citation_count": 30,
      "influential_citation_count": 4,
      "reference_count": 96,
      "is_open_access": true,
      "publication_date": "2021-07-09",
      "tldr": "This work proposes the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems and demonstrates its superiority and practicality.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "video-attack",
        "3D-perturbations",
        "black-box",
        "universal"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2107.04284"
    },
    {
      "paper_id": "2203.16000",
      "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
      "abstract": "Video classification systems are vulnerable to adversarial attacks, which can create severe security problems in video verification. Current black-box attacks need a large number of queries to succeed, resulting in high computational overhead in the process of attack. On the other hand, attacks with restricted perturbations are ineffective against defenses such as denoising or adversarial training. In this paper, we focus on unrestricted perturbations and propose StyleFool, a black-box video adversarial attack via style transfer to fool the video classification system. StyleFool first utilizes color theme proximity to select the best style image, which helps avoid unnatural details in the stylized videos. Meanwhile, the target class confidence is additionally considered in targeted attacks to influence the output distribution of the classifier by moving the stylized video closer to or even across the decision boundary. A gradient-free method is then employed to further optimize the adversarial perturbations. We carry out extensive experiments to evaluate StyleFool on two standard datasets, UCF-101 and HMDB-51. The experimental results demonstrate that StyleFool outperforms the state-of-the-art adversarial attacks in terms of both the number of queries and the robustness against existing defenses. Moreover, 50% of the stylized videos in untargeted attacks do not need any query since they can already fool the video classification model. Furthermore, we evaluate the indistinguishability through a user study to show that the adversarial samples of StyleFool look imperceptible to human eyes, despite unrestricted perturbations.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yu Cao",
        "Xi Xiao",
        "Ruoxi Sun",
        "Derui Wang",
        "Minhui Xue",
        "Sheng Wen"
      ],
      "author_details": [
        {
          "name": "Yu Cao",
          "h_index": 100,
          "citation_count": 50949,
          "affiliations": []
        },
        {
          "name": "Xi Xiao",
          "h_index": 2,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Ruoxi Sun",
          "h_index": 14,
          "citation_count": 836,
          "affiliations": []
        },
        {
          "name": "Derui Wang",
          "h_index": 9,
          "citation_count": 1617,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 32,
          "citation_count": 4765,
          "affiliations": []
        },
        {
          "name": "Sheng Wen",
          "h_index": 13,
          "citation_count": 899,
          "affiliations": []
        }
      ],
      "max_h_index": 100,
      "url": "https://arxiv.org/abs/2203.16000",
      "citation_count": 29,
      "influential_citation_count": 2,
      "reference_count": 92,
      "is_open_access": true,
      "publication_date": "2022-03-30",
      "tldr": "StyleFool is proposed, a black-box video adversarial attack via style transfer to fool the video classification system and outperforms the state-of-the-art adversarial attacks in terms of both the number of queries and the robustness against existing defenses.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "video-attack",
        "style-transfer",
        "black-box",
        "unrestricted"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2203.16000"
    },
    {
      "paper_id": "2401.03582",
      "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
      "abstract": "All vehicles must follow the rules that govern traffic behavior, regardless of whether the vehicles are human-driven or Connected Autonomous Vehicles (CAVs). Road signs indicate locally active rules, such as speed limits and requirements to yield or stop. Recent research has demonstrated attacks, such as adding stickers or projected colored patches to signs, that cause CAV misinterpretation, resulting in potential safety issues. Humans can see and potentially defend against these attacks. But humans can not detect what they can not observe. We have developed an effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is designed to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification. In this work, we formulate the threat model and requirements for an ILR-based traffic sign perception attack to succeed. We evaluate the effectiveness of the ILR attack with real-world experiments against two major traffic sign recognition architectures on four IR-sensitive cameras. Our black-box optimization methodology allows the attack to achieve up to a 100% attack success rate in indoor, static scenarios and a >80.5% attack success rate in our outdoor, moving vehicle scenarios. We find the latest state-of-the-art certifiable defense is ineffective against ILR attacks as it mis-certifies >33.5% of cases. To address this, we propose a detection strategy based on the physical properties of IR laser reflections which can detect 96% of ILR attacks.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Takami Sato",
        "Sri Hrushikesh Varma Bhupathiraju",
        "Michael Clifford",
        "Takeshi Sugawara",
        "Qi Alfred Chen",
        "Sara Rampazzi"
      ],
      "author_details": [
        {
          "name": "Takami Sato",
          "h_index": 14,
          "citation_count": 603,
          "affiliations": []
        },
        {
          "name": "Sri Hrushikesh Varma Bhupathiraju",
          "h_index": 5,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Michael Clifford",
          "h_index": 3,
          "citation_count": 44,
          "affiliations": []
        },
        {
          "name": "Takeshi Sugawara",
          "h_index": 6,
          "citation_count": 180,
          "affiliations": []
        },
        {
          "name": "Qi Alfred Chen",
          "h_index": 4,
          "citation_count": 63,
          "affiliations": []
        },
        {
          "name": "Sara Rampazzi",
          "h_index": 7,
          "citation_count": 219,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://arxiv.org/abs/2401.03582",
      "citation_count": 29,
      "influential_citation_count": 3,
      "reference_count": 93,
      "is_open_access": false,
      "publication_date": "2024-01-07",
      "tldr": "An effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILR) to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification is developed.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "traffic-sign",
        "infrared",
        "autonomous-vehicles",
        "physical"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_546bce6d",
      "title": "You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks",
      "abstract": "Image data have been extensively used in Deep Neural Network (DNN) tasks in various scenarios, e.g., autonomous driving and medical image analysis, which incurs significant privacy concerns.Existing privacy protection techniques are unable to efficiently protect such data.For example, Differential Privacy (DP) that is an emerging technique protects data with strong privacy guarantee cannot effectively protect visual features of exposed image dataset.In this paper, we propose a novel privacy-preserving framework VisualMixer that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises.VisualMixer utilizes a new privacy metric called Visual Feature Entropy (VFE) to effectively quantify the visual features of an image from both biological and machine vision aspects.In VisualMixer, we devise a task-agnostic image obfuscation method to protect the visual privacy of data for DNN training and inference.For each image, it determines regions for pixel shuffling in the image and the sizes of these regions according to the desired VFE.It shuffles pixels both in the spatial domain and in the chromatic channel space in the regions without injecting noises so that it can prevent visual features from being discerned and recognized, while incurring negligible accuracy loss.Extensive experiments on real-world datasets demonstrate that VisualMixer can effectively preserve the visual privacy with negligible accuracy loss, i.e., at average 2.35 percentage points of model accuracy loss, and almost no performance degradation on model training.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Qiushi Li",
        "Yan Zhang",
        "Ju Ren",
        "Qi Li",
        "Yaoxue Zhang"
      ],
      "author_details": [
        {
          "name": "Qiushi Li",
          "h_index": 5,
          "citation_count": 93,
          "affiliations": []
        },
        {
          "name": "Yan Zhang",
          "h_index": 3,
          "citation_count": 69,
          "affiliations": []
        },
        {
          "name": "Ju Ren",
          "h_index": 5,
          "citation_count": 87,
          "affiliations": []
        },
        {
          "name": "Qi Li",
          "h_index": 2,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Yaoxue Zhang",
          "h_index": 10,
          "citation_count": 370,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://openalex.org/W4391724770",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.241361",
      "doi": "https://doi.org/10.14722/ndss.2024.241361",
      "citation_count": 29,
      "influential_citation_count": 0,
      "reference_count": 55,
      "is_open_access": true,
      "publication_date": "2024-04-05",
      "tldr": "A novel privacy-preserving framework VisualMixer is proposed that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises, and can effectively preserve the visual privacy with negligible accuracy loss.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "visual-privacy",
        "privacy-preserving"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.241361"
    },
    {
      "paper_id": "seed_6945ef9b",
      "title": "Dompteur: Taming Audio Adversarial Examples",
      "abstract": "Adversarial examples seem to be inevitable. These specifically crafted inputs allow attackers to arbitrarily manipulate machine learning systems. Even worse, they often seem harmless to human observers. In our digital society, this poses a significant threat. For example, Automatic Speech Recognition (ASR) systems, which serve as hands-free interfaces to many kinds of systems, can be attacked with inputs incomprehensible for human listeners. The research community has unsuccessfully tried several approaches to tackle this problem. In this paper we propose a different perspective: We accept the presence of adversarial examples against ASR systems, but we require them to be perceivable by human listeners. By applying the principles of psychoacoustics, we can remove semantically irrelevant information from the ASR input and train a model that resembles human perception more closely. We implement our idea in a tool named DOMPTEUR and demonstrate that our augmented system, in contrast to an unmodified baseline, successfully focuses on perceptible ranges of the input signal. This change forces adversarial examples into the audible range, while using minimal computational overhead and preserving benign performance. To evaluate our approach, we construct an adaptive attacker that actively tries to avoid our augmentations and demonstrate that adversarial examples from this attacker remain clearly perceivable. Finally, we substantiate our claims by performing a hearing test with crowd-sourced human listeners.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Thorsten Eisenhofer",
        "Lea Sch\u00f6nherr",
        "J. Frank",
        "Lars Speckemeier",
        "D. Kolossa",
        "Thorsten Holz"
      ],
      "author_details": [
        {
          "name": "Thorsten Eisenhofer",
          "h_index": 10,
          "citation_count": 1052,
          "affiliations": []
        },
        {
          "name": "Lea Sch\u00f6nherr",
          "h_index": 12,
          "citation_count": 1453,
          "affiliations": []
        },
        {
          "name": "J. Frank",
          "h_index": 8,
          "citation_count": 1223,
          "affiliations": []
        },
        {
          "name": "Lars Speckemeier",
          "h_index": 4,
          "citation_count": 54,
          "affiliations": []
        },
        {
          "name": "D. Kolossa",
          "h_index": 26,
          "citation_count": 3137,
          "affiliations": []
        },
        {
          "name": "Thorsten Holz",
          "h_index": 71,
          "citation_count": 19806,
          "affiliations": [
            "CISPA Helmholtz Center for Information Security"
          ]
        }
      ],
      "max_h_index": 71,
      "url": "https://openalex.org/W3127994681",
      "pdf_url": "https://arxiv.org/pdf/2102.05431",
      "doi": "https://doi.org/10.48550/arxiv.2102.05431",
      "citation_count": 27,
      "influential_citation_count": 2,
      "reference_count": 81,
      "is_open_access": false,
      "publication_date": "2021-02-10",
      "tldr": "This paper accepts the presence of adversarial examples against ASR systems, but it requires them to be perceivable by human listeners and applies the principles of psychoacoustics to remove semantically irrelevant information from the ASR input and train a model that resembles human perception more closely.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "audio"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "ASR-defense",
        "psychoacoustic",
        "adversarial-mitigation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_ac5526aa",
      "title": "SoK: Explainable Machine Learning in Adversarial Environments",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Maximilian Noppel",
        "Christian Wressnegger"
      ],
      "author_details": [
        {
          "name": "Maximilian Noppel",
          "h_index": 4,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Christian Wressnegger",
          "h_index": 19,
          "citation_count": 1625,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "citation_count": 27,
      "influential_citation_count": 1,
      "reference_count": 205,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "survey",
      "domains": [],
      "model_types": [],
      "tags": [
        "SoK",
        "XAI",
        "adversarial-environments",
        "systematization"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_7b93c37c",
      "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models",
      "abstract": "Today's proliferation of powerful facial recognition systems poses a real threat to personal privacy. As Clearview.ai demonstrated, anyone can canvas the Internet for data and train highly accurate facial recognition models of individuals without their knowledge. We need tools to protect ourselves from potential misuses of unauthorized facial recognition systems. Unfortunately, no practical or effective solutions exist. In this paper, we propose Fawkes, a system that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (we call them \"cloaks\") to their own photos before releasing them. When used to train facial recognition models, these \"cloaked\" images produce functional models that consistently cause normal images of the user to be misidentified. We experimentally demonstrate that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are \"leaked\" to the tracker and used for training, Fawkes can still maintain an 80+% protection success rate. We achieve 100% success in experiments against today's state-of-the-art facial recognition services. Finally, we show that Fawkes is robust against a variety of countermeasures that try to detect or disrupt image cloaks.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Shawn Shan",
        "Emily Wenger",
        "Jiayun Zhang",
        "Huiying Li",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "author_details": [
        {
          "name": "Shawn Shan",
          "h_index": 18,
          "citation_count": 2943,
          "affiliations": [
            "University of Chicago"
          ]
        },
        {
          "name": "Emily Wenger",
          "h_index": 15,
          "citation_count": 1220,
          "affiliations": []
        },
        {
          "name": "Jiayun Zhang",
          "h_index": 8,
          "citation_count": 507,
          "affiliations": []
        },
        {
          "name": "Huiying Li",
          "h_index": 11,
          "citation_count": 2691,
          "affiliations": []
        },
        {
          "name": "Haitao Zheng",
          "h_index": 59,
          "citation_count": 14282,
          "affiliations": []
        },
        {
          "name": "Ben Y. Zhao",
          "h_index": 67,
          "citation_count": 25558,
          "affiliations": []
        }
      ],
      "max_h_index": 67,
      "url": "https://openalex.org/W3036952806",
      "pdf_url": "https://arxiv.org/pdf/2002.08327",
      "doi": "https://doi.org/10.48550/arxiv.2002.08327",
      "citation_count": 26,
      "influential_citation_count": 4,
      "reference_count": 71,
      "is_open_access": false,
      "publication_date": "2020-02-19",
      "tldr": "Fawkes is a system that allow individuals to inoculate themselves against unauthorized facial recognition models by helping users adding imperceptible pixel-level changes to their own photos before publishing them online.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "face-cloaking",
        "privacy-protection"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_ccccfa59",
      "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
      "abstract": "Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats. Some defenses have been proposed to detect or mitigate jailbreaks, but they are unable to withstand the test of time due to an insufficient understanding of jailbreak mechanisms. In this work, we investigate the mechanisms behind jailbreaks based on the Linear Representation Hypothesis (LRH), which states that neural networks encode high-level concepts as subspaces in their hidden representations. We define the toxic semantics in harmful and jailbreak prompts as toxic concepts and describe the semantics in jailbreak prompts that manipulate LLMs to comply with unsafe requests as jailbreak concepts. Through concept extraction and analysis, we reveal that LLMs can recognize the toxic concepts in both harmful and jailbreak prompts. However, unlike harmful prompts, jailbreak prompts activate the jailbreak concepts and alter the LLM output from rejection to compliance. Building on our analysis, we propose a comprehensive jailbreak defense framework, JBShield, consisting of two key components: jailbreak detection JBShield-D and mitigation JBShield-M. JBShield-D identifies jailbreak prompts by determining whether the input activates both toxic and jailbreak concepts. When a jailbreak prompt is detected, JBShield-M adjusts the hidden representations of the target LLM by enhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs produce safe content. Extensive experiments demonstrate the superior performance of JBShield, achieving an average detection accuracy of 0.95 and reducing the average attack success rate of various jailbreak attacks to 2% from 61% across distinct LLMs.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Shenyi Zhang",
        "Yuchen Zhai",
        "Keyan Guo",
        "Hongxin Hu",
        "Shengnan Guo",
        "Zheng Fang",
        "Lingchen Zhao",
        "Chao Shen",
        "Cong Wang",
        "Qian Wang"
      ],
      "author_details": [
        {
          "name": "Shenyi Zhang",
          "h_index": 4,
          "citation_count": 138,
          "affiliations": [
            "Wuhan University"
          ]
        },
        {
          "name": "Yuchen Zhai",
          "h_index": 1,
          "citation_count": 25,
          "affiliations": []
        },
        {
          "name": "Keyan Guo",
          "h_index": 5,
          "citation_count": 110,
          "affiliations": [
            "University at Buffalo"
          ]
        },
        {
          "name": "Hongxin Hu",
          "h_index": 6,
          "citation_count": 126,
          "affiliations": []
        },
        {
          "name": "Shengnan Guo",
          "h_index": 1,
          "citation_count": 25,
          "affiliations": []
        },
        {
          "name": "Zheng Fang",
          "h_index": 3,
          "citation_count": 49,
          "affiliations": []
        },
        {
          "name": "Lingchen Zhao",
          "h_index": 14,
          "citation_count": 896,
          "affiliations": []
        },
        {
          "name": "Chao Shen",
          "h_index": 1,
          "citation_count": 26,
          "affiliations": []
        },
        {
          "name": "Cong Wang",
          "h_index": 1,
          "citation_count": 28,
          "affiliations": []
        },
        {
          "name": "Qian Wang",
          "h_index": 1,
          "citation_count": 26,
          "affiliations": []
        }
      ],
      "max_h_index": 14,
      "url": "https://openalex.org/W4407425033",
      "pdf_url": "https://arxiv.org/pdf/2502.07557",
      "doi": "https://doi.org/10.48550/arxiv.2502.07557",
      "citation_count": 25,
      "influential_citation_count": 3,
      "reference_count": 68,
      "is_open_access": false,
      "publication_date": "2025-02-11",
      "tldr": "This work investigates the mechanisms behind jailbreaks based on the Linear Representation Hypothesis, which states that neural networks encode high-level concepts as subspaces in their hidden representations and proposes a comprehensive jailbreak defense framework, JBShield, consisting of two key components: jailbreak detection JBShield-D and mitigation JBShield-M.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "concept-analysis",
        "jailbreak-defense"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_060979ef",
      "title": "Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries",
      "abstract": "Recent advances in Large Language Models (LLMs) have led to impressive alignment where models learn to distinguish harmful from harmless queries through supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In this paper, we reveal a subtle yet impactful weakness in these aligned models. We find that simply appending multiple end of sequence (eos) tokens can cause a phenomenon we call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space. Building on this observation, we propose a straightforward method to BOOST jailbreak attacks by appending eos tokens. Our systematic evaluation shows that this strategy significantly increases the attack success rate across 8 representative jailbreak techniques and 16 open-source LLMs, ranging from 2B to 72B parameters. Moreover, we develop a novel probing mechanism for commercial APIs and discover that major providers such as OpenAI, Anthropic, and Qwen do not filter eos tokens, making them similarly vulnerable. These findings highlight a hidden yet critical blind spot in existing alignment and content filtering approaches. We call for heightened attention to eos tokens' unintended influence on model behaviors, particularly in production systems. Our work not only calls for an input-filtering based defense, but also points to new defenses that make refusal boundaries more robust and generalizable, as well as fundamental alignment techniques that can defend against context segmentation attacks.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiahao Yu",
        "Haozheng Luo",
        "Jerry Yao-Chieh Hu",
        "Wenbo Guo",
        "Han Liu",
        "Xinyu Xing"
      ],
      "author_details": [
        {
          "name": "Jiahao Yu",
          "h_index": 11,
          "citation_count": 927,
          "affiliations": [
            "Northwestern University"
          ]
        },
        {
          "name": "Haozheng Luo",
          "h_index": 7,
          "citation_count": 198,
          "affiliations": [
            "Northwestern University"
          ]
        },
        {
          "name": "Jerry Yao-Chieh Hu",
          "h_index": 14,
          "citation_count": 547,
          "affiliations": []
        },
        {
          "name": "Wenbo Guo",
          "h_index": 15,
          "citation_count": 966,
          "affiliations": []
        },
        {
          "name": "Han Liu",
          "h_index": 3,
          "citation_count": 40,
          "affiliations": []
        },
        {
          "name": "Xinyu Xing",
          "h_index": 10,
          "citation_count": 797,
          "affiliations": []
        }
      ],
      "max_h_index": 15,
      "url": "https://openalex.org/W4399317489",
      "pdf_url": "https://arxiv.org/pdf/2405.20653",
      "doi": "https://doi.org/10.48550/arxiv.2405.20653",
      "citation_count": 24,
      "influential_citation_count": 0,
      "reference_count": 61,
      "is_open_access": false,
      "publication_date": "2024-05-31",
      "tldr": "It is found that simply appending multiple end of sequence (eos) tokens can cause a phenomenon the authors call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space.",
      "fields_of_study": [
        "Computer Science"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "alignment-weakness",
        "refusal-bypass"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2309.01866",
      "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "abstract": "The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Ping He",
        "Yifan Xia",
        "Xuhong Zhang",
        "Shouling Ji"
      ],
      "author_details": [
        {
          "name": "Ping He",
          "h_index": 3,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Yifan Xia",
          "h_index": 4,
          "citation_count": 84,
          "affiliations": []
        },
        {
          "name": "Xuhong Zhang",
          "h_index": 18,
          "citation_count": 940,
          "affiliations": []
        },
        {
          "name": "Shouling Ji",
          "h_index": 10,
          "citation_count": 237,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://arxiv.org/abs/2309.01866",
      "citation_count": 23,
      "influential_citation_count": 3,
      "reference_count": 74,
      "is_open_access": true,
      "publication_date": "2023-09-05",
      "tldr": "AdvDroidZero is introduced, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting that is effective against various mainstream ML- based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [],
      "tags": [
        "malware-detection",
        "query-based",
        "zero-knowledge",
        "Android"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2309.01866"
    },
    {
      "paper_id": "2202.03277",
      "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
      "abstract": "While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealistic attacks. We shed light on the patterns that discriminate which unrealistic examples can be used for effective hardening. We release our code, datasets and models to support future research in exploring how to reduce the gap between unrealistic and realistic adversarial attacks.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Salijona Dyrmishi",
        "Salah Ghamizi",
        "Thibault Simonetto",
        "Y. L. Traon",
        "Maxime Cordy"
      ],
      "author_details": [
        {
          "name": "Salijona Dyrmishi",
          "h_index": 5,
          "citation_count": 138,
          "affiliations": []
        },
        {
          "name": "Salah Ghamizi",
          "h_index": 6,
          "citation_count": 134,
          "affiliations": []
        },
        {
          "name": "Thibault Simonetto",
          "h_index": 4,
          "citation_count": 79,
          "affiliations": []
        },
        {
          "name": "Y. L. Traon",
          "h_index": 66,
          "citation_count": 18665,
          "affiliations": []
        },
        {
          "name": "Maxime Cordy",
          "h_index": 20,
          "citation_count": 1112,
          "affiliations": []
        }
      ],
      "max_h_index": 66,
      "url": "https://arxiv.org/abs/2202.03277",
      "citation_count": 23,
      "influential_citation_count": 1,
      "reference_count": 46,
      "is_open_access": true,
      "publication_date": "2022-02-07",
      "tldr": "A study on three real-world use cases and seven datasets to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples reveals discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "realistic-attacks",
        "adversarial-hardening",
        "robustness-evaluation"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2202.03277"
    },
    {
      "paper_id": "seed_4a39fd44",
      "title": "BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems",
      "abstract": "Deep learning (DL) performs well in many traffic analysis tasks.Nevertheless, the vulnerability of deep learning weakens the real-world performance of these traffic analyzers (e.g., suffering from evasion attack).Many studies in recent years focused on robustness certification for DL-based models.But existing methods perform far from perfectly in the traffic analysis domain.In this paper, we try to match three attributes of DL-based traffic analysis systems at the same time: (1) highly heterogeneous features, (2) varied model designs, (3) adversarial operating environments.Therefore, we propose BARS, a general robustness certification framework for DL-based traffic analysis systems based on boundary-adaptive randomized smoothing.To obtain tighter robustness guarantee, BARS uses optimized smoothing noise converging on the classification boundary.We firstly propose the Distribution Transformer for generating optimized smoothing noise.Then to optimize the smoothing noise, we propose some special distribution functions and two gradient based searching algorithms for noise shape and noise scale.We implement and evaluate BARS in three practical DL-based traffic analysis systems.Experiment results show that BARS can achieve tighter robustness guarantee than baseline methods.Furthermore, we illustrate the practicability of BARS through five application cases (e.g., quantitatively evaluating robustness).",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Kai Wang",
        "Zhiliang Wang",
        "Dongqi Han",
        "Wenqi Chen",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "author_details": [
        {
          "name": "Kai Wang",
          "h_index": 5,
          "citation_count": 181,
          "affiliations": []
        },
        {
          "name": "Zhiliang Wang",
          "h_index": 10,
          "citation_count": 523,
          "affiliations": []
        },
        {
          "name": "Dongqi Han",
          "h_index": 10,
          "citation_count": 640,
          "affiliations": []
        },
        {
          "name": "Wenqi Chen",
          "h_index": 8,
          "citation_count": 567,
          "affiliations": []
        },
        {
          "name": "Jiahai Yang",
          "h_index": 16,
          "citation_count": 1155,
          "affiliations": []
        },
        {
          "name": "Xingang Shi",
          "h_index": 21,
          "citation_count": 1882,
          "affiliations": []
        },
        {
          "name": "Xia Yin",
          "h_index": 23,
          "citation_count": 2112,
          "affiliations": []
        }
      ],
      "max_h_index": 23,
      "url": "https://openalex.org/W4324007187",
      "doi": "https://doi.org/10.14722/ndss.2023.24508",
      "citation_count": 21,
      "influential_citation_count": 3,
      "reference_count": 80,
      "is_open_access": true,
      "tldr": "This paper proposes BARS, a general robustness certification framework for DL-based traffic analysis systems based on boundary-adaptive randomized smoothing, and shows that BARS can achieve tighter robustness guarantee than baseline methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "traffic-analysis",
        "local-certification"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2023.24508"
    },
    {
      "paper_id": "seed_47c10bec",
      "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
      "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Hanna Kim",
        "Minkyoo Song",
        "S. Na",
        "Seungwon Shin",
        "Kimin Lee"
      ],
      "author_details": [
        {
          "name": "Hanna Kim",
          "h_index": 3,
          "citation_count": 26,
          "affiliations": []
        },
        {
          "name": "Minkyoo Song",
          "h_index": 4,
          "citation_count": 40,
          "affiliations": []
        },
        {
          "name": "S. Na",
          "h_index": 6,
          "citation_count": 171,
          "affiliations": []
        },
        {
          "name": "Seungwon Shin",
          "h_index": 4,
          "citation_count": 51,
          "affiliations": []
        },
        {
          "name": "Kimin Lee",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://openalex.org/W4403996051",
      "pdf_url": "https://arxiv.org/pdf/2410.14569",
      "doi": "https://doi.org/10.48550/arxiv.2410.14569",
      "citation_count": 19,
      "influential_citation_count": 0,
      "reference_count": 84,
      "is_open_access": false,
      "publication_date": "2024-10-18",
      "tldr": "This work aims to understand how potent LLM agents can be when directed to conduct cyberattacks, how cyberattacks are enhanced by web-based tools, and how affordable and easy it becomes to launch cyberattacks using LLM agents.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "web-enabled",
        "agentic-LLM",
        "emerging-threat"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_63696f9d",
      "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
      "abstract": "The proliferation of automated face recognition in the commercial and government sectors has caused significant privacy concerns for individuals. One approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems: Face obfuscation systems generate imperceptibly perturbed images that cause face recognition systems to misidentify the user. Perturbed faces are generated on metric embedding networks, which are known to be unfair in the context of face recognition. A question of demographic fairness naturally follows: are there demographic disparities in face obfuscation system performance? We answer this question with an analytical and empirical exploration of recent face obfuscation systems. Metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic. We show how this clustering behavior leads to reduced face obfuscation utility for faces in minority groups. An intuitive analytical model yields insight into these phenomena.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Harrison Rosenberg",
        "Brian Tang",
        "Kassem Fawaz",
        "S. Jha"
      ],
      "author_details": [
        {
          "name": "Harrison Rosenberg",
          "h_index": 4,
          "citation_count": 48,
          "affiliations": []
        },
        {
          "name": "Brian Tang",
          "h_index": 7,
          "citation_count": 94,
          "affiliations": []
        },
        {
          "name": "Kassem Fawaz",
          "h_index": 23,
          "citation_count": 2444,
          "affiliations": []
        },
        {
          "name": "S. Jha",
          "h_index": 86,
          "citation_count": 41377,
          "affiliations": []
        }
      ],
      "max_h_index": 86,
      "url": "https://openalex.org/W3188750497",
      "pdf_url": "https://arxiv.org/pdf/2108.02707",
      "doi": "https://doi.org/10.48550/arxiv.2108.02707",
      "citation_count": 18,
      "influential_citation_count": 2,
      "reference_count": 86,
      "is_open_access": false,
      "publication_date": "2021-08-05",
      "tldr": "It is shown how metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic, which leads to reduced face obfuscation utility for faces in minority groups.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "face-obfuscation",
        "fairness"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_26ed0105",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "S. Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Ren Pang",
          "h_index": 11,
          "citation_count": 588,
          "affiliations": []
        },
        {
          "name": "Zhaohan Xi",
          "h_index": 9,
          "citation_count": 459,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Xiapu Luo",
          "h_index": 59,
          "citation_count": 13207,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W3206584998",
      "pdf_url": "https://arxiv.org/pdf/2110.06018",
      "doi": "https://doi.org/10.48550/arxiv.2110.06018",
      "citation_count": 16,
      "influential_citation_count": 1,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2021-10-12",
      "tldr": "Through an extensive empirical study of 10 popular NAS methods, it is shown that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "AutoML",
        "NAS",
        "security-risks"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2308.05362",
      "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis",
      "abstract": "Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yiling He",
        "Jian Lou",
        "Zhan Qin",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Yiling He",
          "h_index": 6,
          "citation_count": 199,
          "affiliations": []
        },
        {
          "name": "Jian Lou",
          "h_index": 8,
          "citation_count": 233,
          "affiliations": []
        },
        {
          "name": "Zhan Qin",
          "h_index": 5,
          "citation_count": 168,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 19,
          "citation_count": 1201,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://arxiv.org/abs/2308.05362",
      "citation_count": 16,
      "influential_citation_count": 0,
      "reference_count": 91,
      "is_open_access": true,
      "publication_date": "2023-08-10",
      "tldr": "FINER is proposed, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations and it is demonstrated that FINER outperforms a state-of-the-art tool in facilitating malware analysis.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "tool",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "feature-attribution",
        "security-analysis",
        "XAI"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2308.05362"
    },
    {
      "paper_id": "seed_062923da",
      "title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Peng Cheng",
        "Yuwei Wang",
        "Peng Huang",
        "Zhongjie Ba",
        "Xiaodong Lin",
        "Feng Lin",
        "Liwang Lu",
        "Kui Ren"
      ],
      "author_details": [
        {
          "name": "Peng Cheng",
          "h_index": 2,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Yuwei Wang",
          "h_index": 2,
          "citation_count": 17,
          "affiliations": []
        },
        {
          "name": "Peng Huang",
          "h_index": 3,
          "citation_count": 60,
          "affiliations": [
            "Zhejiang University"
          ]
        },
        {
          "name": "Zhongjie Ba",
          "h_index": 18,
          "citation_count": 1419,
          "affiliations": []
        },
        {
          "name": "Xiaodong Lin",
          "h_index": 1,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Feng Lin",
          "h_index": 8,
          "citation_count": 280,
          "affiliations": []
        },
        {
          "name": "Liwang Lu",
          "h_index": 11,
          "citation_count": 395,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 6,
          "citation_count": 169,
          "affiliations": []
        }
      ],
      "max_h_index": 18,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a056/1RjEav0Daa4",
      "citation_count": 15,
      "influential_citation_count": 0,
      "reference_count": 52,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "tldr": "This paper proposes ALIF, the first black-box adversarial linguistic feature-based attack pipeline that leverages the reciprocal process of text-to-speech and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides.",
      "fields_of_study": [
        "Computer Science",
        "Engineering"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "black-box",
        "linguistic-features",
        "low-cost",
        "ASR-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_cd05f758",
      "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
      "abstract": "With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassified. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a significant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher's robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model $\\sim 1.8\\times$ faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model $\\sim 18\\times$ faster than standard adversarial training.",
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Pratik Vaishnavi",
        "Kevin Eykholt",
        "Amir Rahmati"
      ],
      "author_details": [
        {
          "name": "Pratik Vaishnavi",
          "h_index": 7,
          "citation_count": 133,
          "affiliations": [
            "Stony Brook University"
          ]
        },
        {
          "name": "Kevin Eykholt",
          "h_index": 10,
          "citation_count": 3794,
          "affiliations": []
        },
        {
          "name": "Amir Rahmati",
          "h_index": 22,
          "citation_count": 5351,
          "affiliations": [
            "Stony Brook University"
          ]
        }
      ],
      "max_h_index": 22,
      "url": "https://openalex.org/W4221166176",
      "pdf_url": "https://arxiv.org/pdf/2202.09994",
      "doi": "https://doi.org/10.48550/arxiv.2202.09994",
      "citation_count": 15,
      "influential_citation_count": 1,
      "reference_count": 28,
      "is_open_access": false,
      "publication_date": "2022-02-21",
      "tldr": "Inspired by student-teacher learning, this paper proposes Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "robustness-transfer",
        "representation-matching",
        "adversarial-training"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_caf294a5",
      "title": "Adversarial Policy Training against Deep Reinforcement Learning",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xian Wu",
        "Wenbo Guo",
        "Hua Wei",
        "Xinyu Xing"
      ],
      "author_details": [
        {
          "name": "Xian Wu",
          "h_index": 4,
          "citation_count": 52,
          "affiliations": []
        },
        {
          "name": "Wenbo Guo",
          "h_index": 15,
          "citation_count": 966,
          "affiliations": []
        },
        {
          "name": "Hua Wei",
          "h_index": 2,
          "citation_count": 20,
          "affiliations": []
        },
        {
          "name": "Xinyu Xing",
          "h_index": 10,
          "citation_count": 797,
          "affiliations": []
        }
      ],
      "max_h_index": 15,
      "url": "https://www.usenix.org/system/files/sec21summer_wu-xian.pdf",
      "citation_count": 14,
      "influential_citation_count": 1,
      "reference_count": 56,
      "is_open_access": false,
      "tldr": "This work shows existing adversarial attacks against reinforcement learning either work in an impractical setting or perform less effectively when being launched in a twoagent competitive game, and proposes a new method to train adversarial agents.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "adversarial-policy",
        "DRL-attack",
        "multi-agent"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_f24b7a6e",
      "title": "Attributions for ML-based ICS Anomaly Detection: From Theory to Practice",
      "abstract": "Industrial Control Systems (ICS) govern critical infrastructure like power plants and water treatment plants.ICS can be attacked through manipulations of its sensor or actuator values, causing physical harm.A promising technique for detecting such attacks is machine-learning-based anomaly detection, but it does not identify which sensor or actuator was manipulated and makes it difficult for ICS operators to diagnose the anomaly's root cause.Prior work has proposed using attribution methods to identify what features caused an ICS anomaly-detection model to raise an alarm, but it is unclear how well these attribution methods work in practice.In this paper, we compare state-of-the-art attribution methods for the ICS domain with real attacks from multiple datasets.We find that attribution methods for ICS anomaly detection do not perform as well as suggested in prior work and identify two main reasons.First, anomaly detectors often detect attacks either immediately or significantly after the attack start; we find that attributions computed at these detection points are inaccurate.Second, attribution accuracy varies greatly across attack properties, and attribution methods struggle with attacks on categorical-valued actuators.Despite these challenges, we find that ensembles of attributions can compensate for weaknesses in individual attribution methods.Towards practical use of attributions for ICS anomaly detection, we provide recommendations for researchers and practitioners, such as the need to evaluate attributions with diverse datasets and the potential for attributions in non-real-time workflows.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Clement Fung",
        "Eric Zeng",
        "Lujo Bauer"
      ],
      "author_details": [
        {
          "name": "Clement Fung",
          "h_index": 11,
          "citation_count": 1689,
          "affiliations": []
        },
        {
          "name": "Eric Zeng",
          "h_index": 2,
          "citation_count": 20,
          "affiliations": []
        },
        {
          "name": "Lujo Bauer",
          "h_index": 3,
          "citation_count": 104,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://openalex.org/W4391725261",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.23216",
      "doi": "https://doi.org/10.14722/ndss.2024.23216",
      "citation_count": 14,
      "influential_citation_count": 2,
      "reference_count": 75,
      "is_open_access": true,
      "tldr": "This paper compares state-of-the-art attribution methods for the ICS domain with real attacks from multiple datasets and finds that ensembles of attributions can compensate for weaknesses in individual attribution methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "XAI",
        "ICS",
        "anomaly-detection",
        "attributions"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.23216"
    },
    {
      "paper_id": "2402.03741",
      "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
      "abstract": "Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Oubo Ma",
        "Yuwen Pu",
        "L. Du",
        "Yang Dai",
        "Ruo Wang",
        "Xiaolei Liu",
        "Yingcai Wu",
        "Shouling Ji"
      ],
      "author_details": [
        {
          "name": "Oubo Ma",
          "h_index": 3,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "Yuwen Pu",
          "h_index": 9,
          "citation_count": 197,
          "affiliations": []
        },
        {
          "name": "L. Du",
          "h_index": 8,
          "citation_count": 252,
          "affiliations": []
        },
        {
          "name": "Yang Dai",
          "h_index": 3,
          "citation_count": 25,
          "affiliations": []
        },
        {
          "name": "Ruo Wang",
          "h_index": 1,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Xiaolei Liu",
          "h_index": 6,
          "citation_count": 77,
          "affiliations": []
        },
        {
          "name": "Yingcai Wu",
          "h_index": 3,
          "citation_count": 67,
          "affiliations": []
        },
        {
          "name": "Shouling Ji",
          "h_index": 3,
          "citation_count": 35,
          "affiliations": []
        }
      ],
      "max_h_index": 9,
      "url": "https://arxiv.org/abs/2402.03741",
      "citation_count": 13,
      "influential_citation_count": 0,
      "reference_count": 69,
      "is_open_access": true,
      "publication_date": "2024-02-06",
      "tldr": "This study unveils the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments, and proposes a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "MARL",
        "adversarial-policy",
        "partial-observation"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670293"
    },
    {
      "paper_id": "seed_5fd92817",
      "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
      "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN's latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Shengwei An",
        "Y. Yao",
        "Qiuling Xu",
        "Shiqing Ma",
        "Guanhong Tao",
        "Siyuan Cheng",
        "Kaiyuan Zhang",
        "Yingqi Liu",
        "Guangyu Shen",
        "Ian Kelk",
        "Xiangyu Zhang"
      ],
      "author_details": [
        {
          "name": "Shengwei An",
          "h_index": 17,
          "citation_count": 1032,
          "affiliations": []
        },
        {
          "name": "Y. Yao",
          "h_index": 59,
          "citation_count": 14965,
          "affiliations": []
        },
        {
          "name": "Qiuling Xu",
          "h_index": 13,
          "citation_count": 753,
          "affiliations": []
        },
        {
          "name": "Shiqing Ma",
          "h_index": 43,
          "citation_count": 8258,
          "affiliations": []
        },
        {
          "name": "Guanhong Tao",
          "h_index": 27,
          "citation_count": 2840,
          "affiliations": []
        },
        {
          "name": "Siyuan Cheng",
          "h_index": 21,
          "citation_count": 1806,
          "affiliations": []
        },
        {
          "name": "Kaiyuan Zhang",
          "h_index": 11,
          "citation_count": 477,
          "affiliations": []
        },
        {
          "name": "Yingqi Liu",
          "h_index": 13,
          "citation_count": 1832,
          "affiliations": []
        },
        {
          "name": "Guangyu Shen",
          "h_index": 20,
          "citation_count": 1240,
          "affiliations": []
        },
        {
          "name": "Ian Kelk",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Xiangyu Zhang",
          "h_index": 16,
          "citation_count": 944,
          "affiliations": []
        }
      ],
      "max_h_index": 59,
      "url": "https://openalex.org/W4384948696",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179360",
      "citation_count": 13,
      "influential_citation_count": 1,
      "reference_count": 48,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "face-recognition",
        "impersonation",
        "physical-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2408.07728",
      "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
      "abstract": "We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Peiran Wang",
        "Qiyu Li",
        "Longxuan Yu",
        "Ziyao Wang",
        "Ang Li",
        "Haojian Jin"
      ],
      "author_details": [
        {
          "name": "Peiran Wang",
          "h_index": 2,
          "citation_count": 23,
          "affiliations": []
        },
        {
          "name": "Qiyu Li",
          "h_index": 4,
          "citation_count": 45,
          "affiliations": []
        },
        {
          "name": "Longxuan Yu",
          "h_index": 1,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Ziyao Wang",
          "h_index": 6,
          "citation_count": 233,
          "affiliations": []
        },
        {
          "name": "Ang Li",
          "h_index": 6,
          "citation_count": 214,
          "affiliations": []
        },
        {
          "name": "Haojian Jin",
          "h_index": 4,
          "citation_count": 43,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://arxiv.org/abs/2408.07728",
      "citation_count": 13,
      "influential_citation_count": 2,
      "reference_count": 86,
      "is_open_access": true,
      "publication_date": "2024-08-14",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "policy-based",
        "content-moderation",
        "T2I-defense"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690327"
    },
    {
      "paper_id": "seed_07a7bc21",
      "title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
      "abstract": "Face Verification Systems (FVSes) are more and more deployed by real-world mobile applications (apps) to verify a human's claimed identity. One popular type of FVSes is called cross-side FVS (XFVS), which splits the FVS functionality into two sides: one at a mobile phone to take pictures or videos and the other at a trusted server for verification. Prior works have studied the security of XFVSes from the machine learning perspective, i.e., whether the learning models used by XFVSes are robust to adversarial attacks. However, the security of other parts of XFVSes, especially the design and implementation of the verification procedure used by XFVSes, is not well understood.In this paper, we conduct the first measurement study on the security of real-world XFVSes used by popular mobile apps from a system perspective. More specifically, we design and implement a semi-automated system, called XFVSChecker, to detect XFVSes in mobile apps and then inspect their compliance with four security properties. Our evaluation reveals that most of existing XFVS apps, including those with billions of downloads, are vulnerable to at least one of four types of attacks. These attacks require only easily available attack prerequisites, such as one photo of the victim, to pose significant security risks, including complete account takeover, identity fraud and financial loss. Our findings result in 14 Chinese National Vulnerability Database (CNVD) IDs and one of them, particularly CNVD-2021-86899, is awarded the most valuable vulnerability in 2021 among all the reported vulnerabilities to CNVD.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xiaohan Zhang",
        "Haoqi Ye",
        "Ziqi Huang",
        "Xiao Ye",
        "Yinzhi Cao",
        "Yuan Zhang",
        "Min Yang"
      ],
      "author_details": [
        {
          "name": "Xiaohan Zhang",
          "h_index": 10,
          "citation_count": 542,
          "affiliations": []
        },
        {
          "name": "Haoqi Ye",
          "h_index": 1,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Ziqi Huang",
          "h_index": 1,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Xiao Ye",
          "h_index": 1,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Yinzhi Cao",
          "h_index": 27,
          "citation_count": 4673,
          "affiliations": []
        },
        {
          "name": "Yuan Zhang",
          "h_index": 10,
          "citation_count": 315,
          "affiliations": []
        },
        {
          "name": "Min Yang",
          "h_index": 7,
          "citation_count": 118,
          "affiliations": []
        }
      ],
      "max_h_index": 27,
      "url": "https://openalex.org/W4385187425",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179474",
      "citation_count": 12,
      "influential_citation_count": 0,
      "reference_count": 89,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "face-verification",
        "mobile-security",
        "system-analysis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_fa39728a",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "abstract": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xueluan Gong",
        "Mingzhe Li",
        "Yilin Zhang",
        "Fengyuan Ran",
        "Chen Chen",
        "Yanjiao Chen",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "author_details": [
        {
          "name": "Xueluan Gong",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Mingzhe Li",
          "h_index": 1,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Yilin Zhang",
          "h_index": 1,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Fengyuan Ran",
          "h_index": 1,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Chen Chen",
          "h_index": 3,
          "citation_count": 84,
          "affiliations": []
        },
        {
          "name": "Yanjiao Chen",
          "h_index": 3,
          "citation_count": 29,
          "affiliations": []
        },
        {
          "name": "Qian Wang",
          "h_index": 3,
          "citation_count": 24,
          "affiliations": []
        },
        {
          "name": "Kwok-Yan Lam",
          "h_index": 3,
          "citation_count": 50,
          "affiliations": []
        }
      ],
      "max_h_index": 3,
      "url": "https://openalex.org/W4403780392",
      "pdf_url": "https://arxiv.org/pdf/2409.14866",
      "doi": "https://doi.org/10.48550/arxiv.2409.14866",
      "citation_count": 12,
      "influential_citation_count": 1,
      "reference_count": 60,
      "is_open_access": false,
      "publication_date": "2024-09-23",
      "tldr": "This paper introduces a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs and demonstrates transferability and is robust to state-of-the-art defenses.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "fuzzing",
        "stealthy-jailbreak"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2308.08505",
      "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
      "abstract": "Deploying machine learning (ML) models in the wild is challenging as it suffers from distribution shifts, where the model trained on an original domain cannot generalize well to unforeseen diverse transfer domains. To address this challenge, several test-time adaptation (TTA) methods have been proposed to improve the generalization ability of the target pre-trained models under test data to cope with the shifted distribution. The success of TTA can be credited to the continuous fine-tuning of the target model according to the distributional hint from the test samples during test time. Despite being powerful, it also opens a new attack surface, i.e., test-time poisoning attacks, which are substantially different from previous poisoning attacks that occur during the training time of ML models (i.e., adversaries cannot intervene in the training process). In this paper, we perform the first test-time poisoning attack against four mainstream TTA methods, including TTT, DUA, TENT, and RPL. Concretely, we generate poisoned samples based on the surrogate models and feed them to the target TTA models. Experimental results show that the TTA methods are generally vulnerable to test-time poisoning attacks. For instance, the adversary can feed as few as 10 poisoned samples to degrade the performance of the target model from 76.20% to 41.83%. Our results demonstrate that TTA algorithms lacking a rigorous security assessment are unsuitable for deployment in real-life scenarios. As such, we advocate for the integration of defenses against test-time poisoning attacks into the design of TTA methods.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Tianshuo Cong",
        "Xinlei He",
        "Yun Shen",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Tianshuo Cong",
          "h_index": 10,
          "citation_count": 673,
          "affiliations": []
        },
        {
          "name": "Xinlei He",
          "h_index": 20,
          "citation_count": 1612,
          "affiliations": []
        },
        {
          "name": "Yun Shen",
          "h_index": 16,
          "citation_count": 1497,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 82,
          "citation_count": 35162,
          "affiliations": []
        }
      ],
      "max_h_index": 82,
      "url": "https://arxiv.org/abs/2308.08505",
      "citation_count": 10,
      "influential_citation_count": 2,
      "reference_count": 72,
      "is_open_access": true,
      "publication_date": "2023-08-16",
      "tldr": "The first test-time poisoning attack against four mainstream TTA methods, including TTT, DUA, TENT, and RPL is performed, demonstrating that TTA algorithms lacking a rigorous security assessment are unsuitable for deployment in real-life scenarios.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "test-time-adaptation",
        "inference-attack",
        "distribution-shift"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2308.08505"
    },
    {
      "paper_id": "2311.17400",
      "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
      "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Lujia Shen",
        "Yuwen Pu",
        "Shouling Ji",
        "Changjiang Li",
        "Xuhong Zhang",
        "Chunpeng Ge",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Lujia Shen",
          "h_index": 5,
          "citation_count": 223,
          "affiliations": []
        },
        {
          "name": "Yuwen Pu",
          "h_index": 9,
          "citation_count": 197,
          "affiliations": []
        },
        {
          "name": "Shouling Ji",
          "h_index": 10,
          "citation_count": 237,
          "affiliations": []
        },
        {
          "name": "Changjiang Li",
          "h_index": 11,
          "citation_count": 362,
          "affiliations": [
            "Stony Brook"
          ]
        },
        {
          "name": "Xuhong Zhang",
          "h_index": 7,
          "citation_count": 142,
          "affiliations": []
        },
        {
          "name": "Chunpeng Ge",
          "h_index": 2,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 6,
          "citation_count": 123,
          "affiliations": []
        }
      ],
      "max_h_index": 11,
      "url": "https://arxiv.org/abs/2311.17400",
      "citation_count": 10,
      "influential_citation_count": 1,
      "reference_count": 84,
      "is_open_access": true,
      "publication_date": "2023-11-29",
      "tldr": "This paper proposes a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks, and demonstrates that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm",
        "transformer"
      ],
      "tags": [
        "dynamic-attention",
        "robustness"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.24115"
    },
    {
      "paper_id": "seed_a3e33bfd",
      "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
      "abstract": "Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Kevin Eykholt",
        "Taesung Lee",
        "D. Schales",
        "Jiyong Jang",
        "Ian Molloy",
        "Masha Zorin"
      ],
      "author_details": [
        {
          "name": "Kevin Eykholt",
          "h_index": 10,
          "citation_count": 3794,
          "affiliations": []
        },
        {
          "name": "Taesung Lee",
          "h_index": 11,
          "citation_count": 1423,
          "affiliations": []
        },
        {
          "name": "D. Schales",
          "h_index": 9,
          "citation_count": 497,
          "affiliations": []
        },
        {
          "name": "Jiyong Jang",
          "h_index": 16,
          "citation_count": 2092,
          "affiliations": []
        },
        {
          "name": "Ian Molloy",
          "h_index": 28,
          "citation_count": 4981,
          "affiliations": []
        },
        {
          "name": "Masha Zorin",
          "h_index": 2,
          "citation_count": 564,
          "affiliations": []
        }
      ],
      "max_h_index": 28,
      "url": "https://openalex.org/W4385964791",
      "pdf_url": "https://arxiv.org/pdf/2308.01840",
      "doi": "https://doi.org/10.48550/arxiv.2308.01840",
      "citation_count": 9,
      "influential_citation_count": 0,
      "reference_count": 50,
      "is_open_access": true,
      "publication_date": "2023-08-03",
      "tldr": "This work proposes a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain and discovers a sequence of transformations that result in a semantically correct and functional adversarial input.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "tool",
      "domains": [],
      "model_types": [],
      "tags": [
        "evaluation-toolkit",
        "evasion",
        "robustness-testing"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2308.01840"
    },
    {
      "paper_id": "seed_00817b50",
      "title": "DepthFake: Spoofing 3D Face Authentication with a 2D Photo",
      "abstract": "Face authentication has been widely used in access control, and the latest 3D face authentication systems employ 3D liveness detection techniques to cope with the photo replay attacks, whereby an attacker uses a 2D photo to bypass the authentication. In this paper, we analyze the security of 3D liveness detection systems that utilize structured light depth cameras and discover a new attack surface against 3D face authentication systems. We propose DepthFake attacks that can spoof a 3D face authentication using only one single 2D photo. To achieve this goal, DepthFake first estimates the 3D depth information of a target victim's face from his 2D photo. Then, DepthFake projects the carefully-crafted scatter patterns embedded with the face depth information, in order to empower the 2D photo with 3D authentication properties. We overcome a collection of practical challenges, e.g., depth estimation errors from 2D photos, depth images forgery based on structured light, the alignment of the RGB image and depth images for a face, and implemented DepthFake in laboratory setups. We validated DepthFake on 3 commercial face authentication systems (i.e., Tencent Cloud, Baidu Cloud, and 3DiVi) and one commercial access control device. The results over 50 users demonstrate that DepthFake achieves an overall Depth attack success rate of 79.4% and RGB-D attack success rate of 59.4% in the real world.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Zhihao Wu",
        "Yushi Cheng",
        "Jiahui Yang",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "author_details": [
        {
          "name": "Zhihao Wu",
          "h_index": 2,
          "citation_count": 18,
          "affiliations": []
        },
        {
          "name": "Yushi Cheng",
          "h_index": 14,
          "citation_count": 734,
          "affiliations": []
        },
        {
          "name": "Jiahui Yang",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Xiaoyu Ji",
          "h_index": 25,
          "citation_count": 2806,
          "affiliations": []
        },
        {
          "name": "Wenyuan Xu",
          "h_index": 26,
          "citation_count": 3023,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://openalex.org/W4384948616",
      "doi": "https://doi.org/10.1109/sp46215.2023.10179429",
      "citation_count": 9,
      "influential_citation_count": 0,
      "reference_count": 60,
      "is_open_access": false,
      "publication_date": "2023-05-01",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "face-authentication",
        "3D-spoofing",
        "liveness-bypass"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_9082ba7d",
      "title": "AutoDA: Automated Decision-based Iterative Adversarial Attacks",
      "abstract": "In the rapidly evolving field of machine learning, adversarial attacks\\npresent a significant challenge to model robustness and security.\\nDecision-based attacks, which only require feedback on the decision of a model\\nrather than detailed probabilities or scores, are particularly insidious and\\ndifficult to defend against. This work introduces L-AutoDA (Large Language\\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\\nleveraging the generative capabilities of Large Language Models (LLMs) to\\nautomate the design of these attacks. By iteratively interacting with LLMs in\\nan evolutionary framework, L-AutoDA automatically designs competitive attack\\nalgorithms efficiently without much human effort. We demonstrate the efficacy\\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\\nmethods in both success rate and computational efficiency. Our findings\\nunderscore the potential of language models as tools for adversarial attack\\ngeneration and highlight new avenues for the development of robust AI systems.\\n",
      "year": 2024,
      "venue": "GECCO Companion",
      "authors": [
        "Ping Guo",
        "Fei Liu",
        "Xi Lin",
        "Qingchuan Zhao",
        "Qingfu Zhang"
      ],
      "author_details": [
        {
          "name": "Ping Guo",
          "h_index": 5,
          "citation_count": 85,
          "affiliations": []
        },
        {
          "name": "Fei Liu",
          "h_index": 14,
          "citation_count": 790,
          "affiliations": []
        },
        {
          "name": "Xi Lin",
          "h_index": 19,
          "citation_count": 1658,
          "affiliations": [
            "City University of Hong Kong"
          ]
        },
        {
          "name": "Qingchuan Zhao",
          "h_index": 3,
          "citation_count": 18,
          "affiliations": []
        },
        {
          "name": "Qingfu Zhang",
          "h_index": 8,
          "citation_count": 247,
          "affiliations": []
        }
      ],
      "max_h_index": 19,
      "url": "https://openalex.org/W4391418171",
      "pdf_url": "https://arxiv.org/pdf/2401.15335",
      "doi": "https://doi.org/10.1145/3638530.3664121",
      "citation_count": 8,
      "influential_citation_count": 1,
      "reference_count": 42,
      "is_open_access": false,
      "publication_date": "2024-01-27",
      "tldr": "L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks) is presented, an innovative methodology that harnesses the generative capabilities of large language models to streamline the creation of such attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "decision-based",
        "automated",
        "program-synthesis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2301.02905",
      "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
      "abstract": "Encoder as a service is an emerging cloud service. Specifically, a service provider first pre-trains an encoder (i.e., a general-purpose feature extractor) via either supervised learning or self-supervised learning and then deploys it as a cloud service API. A client queries the cloud service API to obtain feature vectors for its training/testing inputs when training/testing its classifier (called downstream classifier). A downstream classifier is vulnerable to adversarial examples, which are testing inputs with carefully crafted perturbation that the downstream classifier misclassifies. Therefore, in safety and security critical applications, a client aims to build a robust downstream classifier and certify its robustness guarantees against adversarial examples.   What APIs should the cloud service provide, such that a client can use any certification method to certify the robustness of its downstream classifier against adversarial examples while minimizing the number of queries to the APIs? How can a service provider pre-train an encoder such that clients can build more certifiably robust downstream classifiers? We aim to answer the two questions in this work. For the first question, we show that the cloud service only needs to provide two APIs, which we carefully design, to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the APIs. For the second question, we show that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.",
      "year": 2023,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Wenjie Qu",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "author_details": [
        {
          "name": "Wenjie Qu",
          "h_index": 6,
          "citation_count": 379,
          "affiliations": []
        },
        {
          "name": "Jinyuan Jia",
          "h_index": 21,
          "citation_count": 3203,
          "affiliations": []
        },
        {
          "name": "N. Gong",
          "h_index": 52,
          "citation_count": 11238,
          "affiliations": []
        }
      ],
      "max_h_index": 52,
      "url": "https://arxiv.org/abs/2301.02905",
      "citation_count": 7,
      "influential_citation_count": 2,
      "reference_count": 45,
      "is_open_access": true,
      "publication_date": "2023-01-07",
      "tldr": "This work shows that the cloud service only needs to provide two APIs to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the APIs, and shows that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "encoder-service",
        "cloud-ML",
        "downstream-robustness"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2301.02905"
    },
    {
      "paper_id": "seed_54d774be",
      "title": "Robustifying ML-powered Network Classifiers with PANTS",
      "abstract": "Multiple network management tasks, from resource allocation to intrusion detection, rely on some form of ML-based network traffic classification (MNC). Despite their potential, MNCs are vulnerable to adversarial inputs, which can lead to outages, poor decision-making, and security violations, among other issues. The goal of this paper is to help network operators assess and enhance the robustness of their MNC against adversarial inputs. The most critical step for this is generating inputs that can fool the MNC while being realizable under various threat models. Compared to other ML models, finding adversarial inputs against MNCs is more challenging due to the existence of non-differentiable components e.g., traffic engineering and the need to constrain inputs to preserve semantics and ensure reliability. These factors prevent the direct use of well-established gradient-based methods developed in adversarial ML (AML). To address these challenges, we introduce PANTS, a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs. We also embed PANTS into an iterative adversarial training process that enhances the robustness of MNCs against adversarial inputs. PANTS is 70% and 2x more likely in median to find adversarial inputs against target MNCs compared to state-of-the-art baselines, namely Amoeba and BAP. PANTS improves the robustness of the target MNCs by 52.7% (even against attackers outside of what is considered during robustification) without sacrificing their accuracy.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Minhao Jin",
        "Maria Apostolaki"
      ],
      "author_details": [
        {
          "name": "Minhao Jin",
          "h_index": 1,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "Maria Apostolaki",
          "h_index": 3,
          "citation_count": 29,
          "affiliations": []
        }
      ],
      "max_h_index": 3,
      "url": "https://openalex.org/W4403594037",
      "pdf_url": "https://arxiv.org/pdf/2409.04691",
      "doi": "https://doi.org/10.48550/arxiv.2409.04691",
      "citation_count": 6,
      "influential_citation_count": 0,
      "reference_count": 61,
      "is_open_access": false,
      "publication_date": "2024-09-07",
      "tldr": "PANTS is a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs and improves the robustness of MNCs against adversarial inputs without sacrificing their accuracy.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "network-classification",
        "robustness",
        "traffic-analysis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_f81baf3b",
      "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
      "abstract": "Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism that automatically rejects such harmful prompts. However, they can be bypassed through LLM jailbreaks. Current jailbreaks often require significant manual effort, high computational costs, or result in excessive model modifications that may degrade regular utility. We introduce TwinBreak, an innovative safety alignment removal method. Building on the idea that the safety mechanism operates like an embedded backdoor, TwinBreak identifies and prunes parameters responsible for this functionality. By focusing on the most relevant model layers, TwinBreak performs fine-grained analysis of parameters essential to model utility and safety. TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters. We present the TwinPrompt dataset containing 100 such twin prompts. Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success rates with minimal computational requirements across 16 LLMs from five vendors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "T. Krau\u00df",
        "Hamid Dashtbani",
        "Alexandra Dmitrienko"
      ],
      "author_details": [
        {
          "name": "T. Krau\u00df",
          "h_index": 6,
          "citation_count": 132,
          "affiliations": []
        },
        {
          "name": "Hamid Dashtbani",
          "h_index": 2,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "Alexandra Dmitrienko",
          "h_index": 5,
          "citation_count": 107,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://openalex.org/W4417133177",
      "pdf_url": "https://arxiv.org/pdf/2506.07596",
      "doi": "https://doi.org/10.48550/arxiv.2506.07596",
      "citation_count": 6,
      "influential_citation_count": 0,
      "reference_count": 60,
      "is_open_access": false,
      "publication_date": "2025-06-09",
      "tldr": "TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters, and is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "twin-prompts",
        "jailbreak"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_769bf67c",
      "title": "Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search",
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly querying a URL will increase the number of main risk-inclusive responses, while querying with natural language will slightly mitigate such risk. Compared to traditional search engines, AIPSEs outperform in both utility and safety. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector. Our evaluation shows that our defense can effectively reduce the risk, with only a minor cost of reducing available information by approximately 10.7%. Our research highlights the urgent need for robust safety measures in AIPSEs.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Zeren Luo",
        "Zifan Peng",
        "Yule Liu",
        "Zhen Sun",
        "Mingchen Li",
        "Jingyi Zheng",
        "Xinlei He"
      ],
      "author_details": [
        {
          "name": "Zeren Luo",
          "h_index": 3,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Zifan Peng",
          "h_index": 3,
          "citation_count": 33,
          "affiliations": []
        },
        {
          "name": "Yule Liu",
          "h_index": 6,
          "citation_count": 309,
          "affiliations": [
            "HKUST(GZ)"
          ]
        },
        {
          "name": "Zhen Sun",
          "h_index": 4,
          "citation_count": 73,
          "affiliations": []
        },
        {
          "name": "Mingchen Li",
          "h_index": 3,
          "citation_count": 31,
          "affiliations": [
            "University of North Texas"
          ]
        },
        {
          "name": "Jingyi Zheng",
          "h_index": 4,
          "citation_count": 52,
          "affiliations": []
        },
        {
          "name": "Xinlei He",
          "h_index": 5,
          "citation_count": 83,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://openalex.org/W4407310131",
      "pdf_url": "https://arxiv.org/pdf/2502.04951",
      "doi": "https://doi.org/10.48550/arxiv.2502.04951",
      "citation_count": 6,
      "influential_citation_count": 1,
      "reference_count": 68,
      "is_open_access": false,
      "publication_date": "2025-02-07",
      "tldr": "This study conducts the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types, and develops an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "AI-search",
        "safety-risks",
        "AIPSE"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_74186653",
      "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
      "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yining Wang",
        "Mi Zhang",
        "Junjie Sun",
        "Chenyue Wang",
        "Min Yang",
        "Hui Xue",
        "Jialing Tao",
        "Ranjie Duan",
        "Jiexi Liu"
      ],
      "author_details": [
        {
          "name": "Yining Wang",
          "h_index": 3,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Mi Zhang",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Junjie Sun",
          "h_index": 3,
          "citation_count": 26,
          "affiliations": []
        },
        {
          "name": "Chenyue Wang",
          "h_index": 2,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Min Yang",
          "h_index": 4,
          "citation_count": 39,
          "affiliations": []
        },
        {
          "name": "Hui Xue",
          "h_index": 4,
          "citation_count": 48,
          "affiliations": []
        },
        {
          "name": "Jialing Tao",
          "h_index": 4,
          "citation_count": 75,
          "affiliations": []
        },
        {
          "name": "Ranjie Duan",
          "h_index": 10,
          "citation_count": 949,
          "affiliations": []
        },
        {
          "name": "Jiexi Liu",
          "h_index": 4,
          "citation_count": 31,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://openalex.org/W4406880263",
      "pdf_url": "https://arxiv.org/pdf/2501.15269",
      "doi": "https://doi.org/10.48550/arxiv.2501.15269",
      "citation_count": 6,
      "influential_citation_count": 1,
      "reference_count": 86,
      "is_open_access": false,
      "publication_date": "2025-01-25",
      "tldr": "This work proposes a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "multimodal",
        "llm",
        "vision"
      ],
      "model_types": [
        "llm",
        "transformer"
      ],
      "tags": [
        "hallucination-attack",
        "attention-sink",
        "MLLM"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_835f9fe4",
      "title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Chaoxiang He",
        "Bin B. Zhu",
        "Xiaojing Ma",
        "Hai Jin",
        "Shengshan Hu"
      ],
      "author_details": [
        {
          "name": "Chaoxiang He",
          "h_index": 4,
          "citation_count": 96,
          "affiliations": []
        },
        {
          "name": "Bin B. Zhu",
          "h_index": 17,
          "citation_count": 1150,
          "affiliations": [
            "Microsoft Research Asia, Microsoft AI, Microsoft Corporation"
          ]
        },
        {
          "name": "Xiaojing Ma",
          "h_index": 12,
          "citation_count": 553,
          "affiliations": []
        },
        {
          "name": "Hai Jin",
          "h_index": 14,
          "citation_count": 713,
          "affiliations": []
        },
        {
          "name": "Shengshan Hu",
          "h_index": 26,
          "citation_count": 1978,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://openalex.org/W3213785680",
      "doi": "https://doi.org/10.1145/3460120.3485378",
      "citation_count": 5,
      "influential_citation_count": 0,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2021-11-12",
      "tldr": "This paper proposes a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA), which circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "defense-bypass",
        "trapdoor-evasion",
        "adversarial-examples"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_4d703ac5",
      "title": "Make a Feint to the East While Attacking in the West: Blinding LLM-Based Code Auditors with Flashboom Attacks",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Xiao Li",
        "Yue Li",
        "Hao Wu",
        "Yue Zhang",
        "Kaidi Xu",
        "Xiuzhen Cheng",
        "Sheng Zhong",
        "Fengyuan Xu"
      ],
      "author_details": [
        {
          "name": "Xiao Li",
          "h_index": 4,
          "citation_count": 36,
          "affiliations": []
        },
        {
          "name": "Yue Li",
          "h_index": 3,
          "citation_count": 32,
          "affiliations": []
        },
        {
          "name": "Hao Wu",
          "h_index": 2,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Yue Zhang",
          "h_index": 6,
          "citation_count": 268,
          "affiliations": []
        },
        {
          "name": "Kaidi Xu",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Xiuzhen Cheng",
          "h_index": 4,
          "citation_count": 47,
          "affiliations": []
        },
        {
          "name": "Sheng Zhong",
          "h_index": 7,
          "citation_count": 125,
          "affiliations": []
        },
        {
          "name": "Fengyuan Xu",
          "h_index": 4,
          "citation_count": 178,
          "affiliations": []
        }
      ],
      "max_h_index": 7,
      "url": "https://ieeexplore.ieee.org/document/11023369",
      "citation_count": 5,
      "influential_citation_count": 1,
      "reference_count": 65,
      "is_open_access": false,
      "publication_date": "2025-05-12",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Review"
      ],
      "paper_type": "attack",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "code-auditing",
        "LLM-attack",
        "evasion"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_de1f4a5e",
      "title": "VisionGuard: Secure and Robust Visual Perception of Autonomous Vehicles in Practice",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xingshuo Han",
        "Haozhao Wang",
        "Kangqiao Zhao",
        "Gelei Deng",
        "Yuan Xu",
        "Hangcheng Liu",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "author_details": [
        {
          "name": "Xingshuo Han",
          "h_index": 11,
          "citation_count": 381,
          "affiliations": []
        },
        {
          "name": "Haozhao Wang",
          "h_index": 21,
          "citation_count": 1610,
          "affiliations": []
        },
        {
          "name": "Kangqiao Zhao",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Gelei Deng",
          "h_index": 21,
          "citation_count": 2874,
          "affiliations": []
        },
        {
          "name": "Yuan Xu",
          "h_index": 9,
          "citation_count": 508,
          "affiliations": []
        },
        {
          "name": "Hangcheng Liu",
          "h_index": 10,
          "citation_count": 413,
          "affiliations": []
        },
        {
          "name": "Han Qiu",
          "h_index": 10,
          "citation_count": 315,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 5,
          "citation_count": 121,
          "affiliations": []
        }
      ],
      "max_h_index": 21,
      "url": "hhttps://tianweiz07.github.io/Papers/24-ccs1.pdf",
      "citation_count": 5,
      "influential_citation_count": 1,
      "reference_count": 72,
      "is_open_access": true,
      "publication_date": "2024-12-02",
      "tldr": "The key of VisionGuard is to leverage the spatiotemporal inconsistency property of PAEs to detect anomalies and it predicts the motion states from historical ones and compares them with the current driving states to identify any motion inconsistency caused by physical attacks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "autonomous-vehicles",
        "visual-perception",
        "practical-defense"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670296"
    },
    {
      "paper_id": "seed_815d0a56",
      "title": "Cost-Aware Robust Tree Ensembles for Security Applications",
      "abstract": "There are various costs for attackers to manipulate the features of security classifiers. The costs are asymmetric across features and to the directions of changes, which cannot be precisely captured by existing cost models based on $L_p$-norm robustness. In this paper, we utilize such domain knowledge to increase the attack cost of evading classifiers, specifically, tree ensemble models that are widely used by security tasks. We propose a new cost modeling method to capture the feature manipulation cost as constraint, and then we integrate the cost-driven constraint into the node construction process to train robust tree ensembles. During the training process, we use the constraint to find data points that are likely to be perturbed given the feature manipulation cost, and we use a new robust training algorithm to optimize the quality of the trees. Our cost-aware training method can be applied to different types of tree ensembles, including gradient boosted decision trees and random forest models. Using Twitter spam detection as the case study, our evaluation results show that we can increase the attack cost by 10.6X compared to the baseline. Moreover, our robust training method using cost-driven constraint can achieve higher accuracy, lower false positive rate, and stronger cost-aware robustness than the state-of-the-art training method using $L_\\infty$-norm cost model. Our code is available at https://github.com/surrealyz/growtrees.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Weifan Jiang",
        "Asaf Cidon",
        "S. Jana"
      ],
      "author_details": [
        {
          "name": "Yizheng Chen",
          "h_index": 15,
          "citation_count": 1152,
          "affiliations": []
        },
        {
          "name": "Shiqi Wang",
          "h_index": 5,
          "citation_count": 360,
          "affiliations": []
        },
        {
          "name": "Weifan Jiang",
          "h_index": 5,
          "citation_count": 152,
          "affiliations": []
        },
        {
          "name": "Asaf Cidon",
          "h_index": 24,
          "citation_count": 2142,
          "affiliations": []
        },
        {
          "name": "S. Jana",
          "h_index": 47,
          "citation_count": 12126,
          "affiliations": []
        }
      ],
      "max_h_index": 47,
      "url": "https://openalex.org/W3026606204",
      "pdf_url": "https://arxiv.org/pdf/1912.01149",
      "doi": "https://doi.org/10.48550/arxiv.1912.01149",
      "citation_count": 5,
      "influential_citation_count": 2,
      "reference_count": 69,
      "is_open_access": false,
      "publication_date": "2019-12-03",
      "tldr": "A new algorithm to train robust tree ensembles based on greedy heuristic to find better solutions to the minimization problem than previous work and proposes attack-cost-driven constraints for the robust training process to make the robustness increase meaningful in security applications.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [
        "tree",
        "ensemble"
      ],
      "tags": [
        "cost-aware",
        "domain-knowledge",
        "robustness"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2309.11005",
      "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
      "abstract": "Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\\%$ more samples at noise scale $\u03c3= 1$, with greater relative improvements observed as the difficulty of the predictive task increases.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. C. Cullen",
        "Paul Montague",
        "Shijie Liu",
        "S. Erfani",
        "Benjamin I. P. Rubinstein"
      ],
      "author_details": [
        {
          "name": "A. C. Cullen",
          "h_index": 7,
          "citation_count": 117,
          "affiliations": []
        },
        {
          "name": "Paul Montague",
          "h_index": 18,
          "citation_count": 1415,
          "affiliations": [
            "Defence Science and Technology Group Australia",
            "University of Cambridge",
            "University of Adelaide",
            "University of California"
          ]
        },
        {
          "name": "Shijie Liu",
          "h_index": 4,
          "citation_count": 40,
          "affiliations": []
        },
        {
          "name": "S. Erfani",
          "h_index": 28,
          "citation_count": 5553,
          "affiliations": []
        },
        {
          "name": "Benjamin I. P. Rubinstein",
          "h_index": 34,
          "citation_count": 4799,
          "affiliations": [
            "School of Computing and Information Systems, University of Melbourne"
          ]
        }
      ],
      "max_h_index": 34,
      "url": "https://arxiv.org/abs/2309.11005",
      "citation_count": 5,
      "influential_citation_count": 0,
      "reference_count": 35,
      "is_open_access": true,
      "publication_date": "2023-09-20",
      "tldr": "By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "certified-robustness",
        "evaluation-metrics",
        "disaggregation"
      ],
      "open_access_pdf": "http://arxiv.org/pdf/2309.11005"
    },
    {
      "paper_id": "seed_bba06dbc",
      "title": "Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Mahmoud Nazzal",
        "Issa M. Khalil",
        "Abdallah Khreishah",
        "Nhathai Phan",
        "Yao Ma"
      ],
      "author_details": [
        {
          "name": "Mahmoud Nazzal",
          "h_index": 12,
          "citation_count": 369,
          "affiliations": []
        },
        {
          "name": "Issa M. Khalil",
          "h_index": 31,
          "citation_count": 5995,
          "affiliations": []
        },
        {
          "name": "Abdallah Khreishah",
          "h_index": 35,
          "citation_count": 6663,
          "affiliations": []
        },
        {
          "name": "Nhathai Phan",
          "h_index": 19,
          "citation_count": 1370,
          "affiliations": []
        },
        {
          "name": "Yao Ma",
          "h_index": 7,
          "citation_count": 109,
          "affiliations": []
        }
      ],
      "max_h_index": 35,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a006/1RjE9LaYR0c",
      "citation_count": 5,
      "influential_citation_count": 1,
      "reference_count": 79,
      "is_open_access": true,
      "publication_date": "2023-08-22",
      "tldr": "An inference-time, multi-instance adversarial attack, dubbed MintA, against GNN-based MDD, which optimizes node perturbations to enhance the evasiveness of a node and its neighborhood and formulate an optimization problem that satisfies the attack objectives of MintA and devise an approximate solution for it.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "malicious-domain",
        "multi-instance",
        "black-box"
      ],
      "open_access_pdf": "https://arxiv.org/pdf/2308.11754"
    },
    {
      "paper_id": "seed_0079375d",
      "title": "Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense",
      "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadA's effectiveness in enhancing the safety capabilities of LLMs after activation approximations.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jiawen Zhang",
        "Kejia Chen",
        "Lipeng He",
        "Jian Lou",
        "Dan Li",
        "Zunlei Feng",
        "Min-Gyoo Song",
        "Jian Liu",
        "Kui Ren",
        "Xiaohu Yang"
      ],
      "author_details": [
        {
          "name": "Jiawen Zhang",
          "h_index": 2,
          "citation_count": 10,
          "affiliations": []
        },
        {
          "name": "Kejia Chen",
          "h_index": 3,
          "citation_count": 77,
          "affiliations": []
        },
        {
          "name": "Lipeng He",
          "h_index": 4,
          "citation_count": 96,
          "affiliations": [
            "University of Waterloo"
          ]
        },
        {
          "name": "Jian Lou",
          "h_index": 1,
          "citation_count": 7,
          "affiliations": []
        },
        {
          "name": "Dan Li",
          "h_index": 2,
          "citation_count": 10,
          "affiliations": []
        },
        {
          "name": "Zunlei Feng",
          "h_index": 20,
          "citation_count": 2451,
          "affiliations": []
        },
        {
          "name": "Min-Gyoo Song",
          "h_index": 11,
          "citation_count": 351,
          "affiliations": []
        },
        {
          "name": "Jian Liu",
          "h_index": 3,
          "citation_count": 79,
          "affiliations": []
        },
        {
          "name": "Kui Ren",
          "h_index": 5,
          "citation_count": 183,
          "affiliations": []
        },
        {
          "name": "Xiaohu Yang",
          "h_index": 2,
          "citation_count": 41,
          "affiliations": []
        }
      ],
      "max_h_index": 20,
      "url": "https://openalex.org/W4407124169",
      "pdf_url": "https://arxiv.org/pdf/2502.00840",
      "doi": "https://doi.org/10.48550/arxiv.2502.00840",
      "citation_count": 5,
      "influential_citation_count": 0,
      "reference_count": 86,
      "is_open_access": false,
      "publication_date": "2025-02-02",
      "tldr": "This work proposes QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations, and spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "llm",
        "nlp"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "activation-approximation",
        "safety-vulnerability"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2509.07764",
      "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
      "abstract": "Large Language Models (LLMs) have been increasingly integrated into computer-use agents, which can autonomously operate tools on a user's computer to accomplish complex tasks. However, due to the inherently unstable and unpredictable nature of LLM outputs, they may issue unintended tool commands or incorrect inputs, leading to potentially harmful operations. Unlike traditional security risks stemming from insecure user prompts, tool execution results from LLM-driven decisions introduce new and unique security challenges. These vulnerabilities span across all components of a computer-use agent. To mitigate these risks, we propose AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer. AgentSentinel intercepts all sensitive operations within agent-related services and halts execution until a comprehensive security audit is completed. Our security auditing mechanism introduces a novel inspection process that correlates the current task context with system traces generated during task execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a benchmark consisting of 60 diverse attack scenarios across six attack categories. The benchmark demonstrates a 87% average attack success rate on four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Haitao Hu",
        "Peng Chen",
        "Yanpeng Zhao",
        "Yuqi Chen"
      ],
      "author_details": [
        {
          "name": "Haitao Hu",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Peng Chen",
          "h_index": 1,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Yanpeng Zhao",
          "h_index": 8,
          "citation_count": 518,
          "affiliations": [
            "NA"
          ]
        },
        {
          "name": "Yuqi Chen",
          "h_index": 3,
          "citation_count": 19,
          "affiliations": []
        }
      ],
      "max_h_index": 8,
      "url": "https://arxiv.org/abs/2509.07764",
      "citation_count": 5,
      "influential_citation_count": 0,
      "reference_count": 44,
      "is_open_access": false,
      "publication_date": "2025-09-09",
      "tldr": "This work proposes AgentSentinel, an end-to-end, real-time defense framework designed to mitigate potential security threats on a user's computer, and shows that AgentSentinel achieves an average defense success rate of 79.6%, significantly outperforming all baseline defenses.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "llm"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "computer-use-agent",
        "real-time-defense"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_46951381",
      "title": "Self-interpreting Adversarial Images",
      "abstract": "We introduce a new type of indirect, cross-modal injection attacks against visual language models that enable creation of self-interpreting images. These images contain hidden \"meta-instructions\" that control how models answer users' questions about the image and steer models' outputs to express an adversary-chosen style, sentiment, or point of view. Self-interpreting images act as soft prompts, conditioning the model to satisfy the adversary's (meta-)objective while still producing answers based on the image's visual content. Meta-instructions are thus a stronger form of prompt injection. Adversarial images look natural and the model's answers are coherent and plausible, yet they also follow the adversary-chosen interpretation, e.g., political spin, or even objectives that are not achievable with explicit text instructions. We evaluate the efficacy of self-interpreting images for a variety of models, interpretations, and user prompts. We describe how these attacks could cause harm by enabling creation of self-interpreting content that carries spam, misinformation, or spin. Finally, we discuss defenses.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tingwei Zhang",
        "Collin Zhang",
        "John X. Morris",
        "Eugene Bagdasarian",
        "Vitaly Shmatikov"
      ],
      "author_details": [
        {
          "name": "Tingwei Zhang",
          "h_index": 4,
          "citation_count": 44,
          "affiliations": []
        },
        {
          "name": "Collin Zhang",
          "h_index": 4,
          "citation_count": 87,
          "affiliations": []
        },
        {
          "name": "John X. Morris",
          "h_index": 7,
          "citation_count": 362,
          "affiliations": []
        },
        {
          "name": "Eugene Bagdasarian",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Vitaly Shmatikov",
          "h_index": 7,
          "citation_count": 594,
          "affiliations": []
        }
      ],
      "max_h_index": 7,
      "url": "https://openalex.org/W4400667173",
      "pdf_url": "https://arxiv.org/pdf/2407.08970",
      "doi": "https://doi.org/10.48550/arxiv.2407.08970",
      "citation_count": 4,
      "influential_citation_count": 0,
      "reference_count": 48,
      "is_open_access": false,
      "publication_date": "2024-07-12",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision",
        "llm"
      ],
      "model_types": [
        "transformer",
        "llm"
      ],
      "tags": [
        "VLM-attack",
        "cross-modal",
        "prompt-injection",
        "meta-instructions"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2309.05679",
      "title": "Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing",
      "abstract": "While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \\ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jinwen He",
        "Kai Chen",
        "Guozhu Meng",
        "Jiangshan Zhang",
        "Congyi Li"
      ],
      "author_details": [
        {
          "name": "Jinwen He",
          "h_index": 4,
          "citation_count": 166,
          "affiliations": [
            "Institute of Information Engineering, Chinese Academy of Sciences"
          ]
        },
        {
          "name": "Kai Chen",
          "h_index": 7,
          "citation_count": 305,
          "affiliations": []
        },
        {
          "name": "Guozhu Meng",
          "h_index": 22,
          "citation_count": 2234,
          "affiliations": [
            "Institute of Information Engineering"
          ]
        },
        {
          "name": "Jiangshan Zhang",
          "h_index": 3,
          "citation_count": 148,
          "affiliations": []
        },
        {
          "name": "Congyi Li",
          "h_index": 3,
          "citation_count": 21,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://arxiv.org/abs/2309.05679",
      "citation_count": 4,
      "influential_citation_count": 0,
      "reference_count": 77,
      "is_open_access": true,
      "publication_date": "2023-09-09",
      "tldr": "This paper evaluates the faithfulness of explanation methods and finds that traditional tests on faithfulness encounter the random dominance problem, i.e., the random selection performs the best, especially for complex data.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "XAI-evaluation",
        "faithfulness",
        "explanation-testing"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3576915.3616605"
    },
    {
      "paper_id": "seed_c267b2cc",
      "title": "Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems",
      "abstract": "The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users' speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Weifei Jin",
        "Yuxin Cao",
        "Junjie Su",
        "Derui Wang",
        "Yedi Zhang",
        "Minhui Xue",
        "Jie Hao",
        "Jin Song Dong",
        "Yixian Yang"
      ],
      "author_details": [
        {
          "name": "Weifei Jin",
          "h_index": 2,
          "citation_count": 9,
          "affiliations": []
        },
        {
          "name": "Yuxin Cao",
          "h_index": 4,
          "citation_count": 69,
          "affiliations": []
        },
        {
          "name": "Junjie Su",
          "h_index": 3,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "Derui Wang",
          "h_index": 5,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Yedi Zhang",
          "h_index": 3,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 4,
          "citation_count": 66,
          "affiliations": []
        },
        {
          "name": "Jie Hao",
          "h_index": 3,
          "citation_count": 23,
          "affiliations": []
        },
        {
          "name": "Jin Song Dong",
          "h_index": 3,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Yixian Yang",
          "h_index": 3,
          "citation_count": 14,
          "affiliations": []
        }
      ],
      "max_h_index": 5,
      "url": "https://openalex.org/W4417229822",
      "pdf_url": "https://arxiv.org/pdf/2504.00858",
      "doi": "https://doi.org/10.48550/arxiv.2504.00858",
      "citation_count": 4,
      "influential_citation_count": 0,
      "reference_count": 62,
      "is_open_access": false,
      "publication_date": "2025-04-01",
      "tldr": "Comprehensive evaluation on four commercial ASR APIs demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "audio"
      ],
      "model_types": [],
      "tags": [
        "speech-privacy",
        "adversarial-protection",
        "ASR"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5442d202",
      "title": "Understanding and Benchmarking the Commonality of Adversarial Examples",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Ruiwen He",
        "Yushi Cheng",
        "Junning Ze",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "author_details": [
        {
          "name": "Ruiwen He",
          "h_index": 3,
          "citation_count": 31,
          "affiliations": []
        },
        {
          "name": "Yushi Cheng",
          "h_index": 14,
          "citation_count": 734,
          "affiliations": []
        },
        {
          "name": "Junning Ze",
          "h_index": 3,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Xiaoyu Ji",
          "h_index": 25,
          "citation_count": 2806,
          "affiliations": []
        },
        {
          "name": "Wenyuan Xu",
          "h_index": 26,
          "citation_count": 3023,
          "affiliations": []
        }
      ],
      "max_h_index": 26,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a111/1Ub23jYBBHa",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 81,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "adversarial-commonality",
        "benchmark",
        "transferability"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_2b933416",
      "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
      "abstract": "Recent research efforts on adversarial machine learning (ML) have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This article makes three major contributions. Firstly, we propose a general formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the by-product of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. Secondly, building on our general formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations in terms of semantics and artifacts. We have tested our approach on a dataset with 150K Android apps from 2016 and 2018 which show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Thirdly, we explore the effectiveness of adversarial training as a possible approach to enforce robustness against adversarial samples, evaluating its effectiveness on the considered machine learning models under different scenarios. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial instance.",
      "year": 2019,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Jacopo Cortellazzi",
        "Erwin Quiring",
        "Daniel Arp",
        "Feargus Pendlebury",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "author_details": [
        {
          "name": "Jacopo Cortellazzi",
          "h_index": 4,
          "citation_count": 397,
          "affiliations": []
        },
        {
          "name": "Erwin Quiring",
          "h_index": 9,
          "citation_count": 533,
          "affiliations": []
        },
        {
          "name": "Daniel Arp",
          "h_index": 4,
          "citation_count": 38,
          "affiliations": []
        },
        {
          "name": "Feargus Pendlebury",
          "h_index": 10,
          "citation_count": 1134,
          "affiliations": []
        },
        {
          "name": "Fabio Pierazzi",
          "h_index": 12,
          "citation_count": 1333,
          "affiliations": []
        },
        {
          "name": "Lorenzo Cavallaro",
          "h_index": 7,
          "citation_count": 190,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://arxiv.org/abs/1911.02142",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152781.pdf",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 88,
      "is_open_access": false,
      "publication_date": "2019-11-05",
      "tldr": "A general formalization for adversarial ML evasion attacks in the problem-space is proposed, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility, and the concept of side-effect features as the by-product of the inverse feature-mapping problem is introduced.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [],
      "model_types": [],
      "tags": [
        "problem-space",
        "formalization",
        "real-evasive-objects"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_8eb535d6",
      "title": "AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification",
      "abstract": "Graph neural networks (GNNs) achieve the state-of-the-art on graph-relevant tasks such as node and graph classification. However, recent works show GNNs are vulnerable to adversarial perturbations include the perturbation on edges, nodes, and node features, the three components forming a graph. Empirical defenses against such attacks are soon broken by adaptive ones. While certified defenses offer robustness guarantees, they face several limitations: 1) almost all restrict the adversary's capability to only one type of perturbation, which is impractical; 2) all are designed for a particular GNN task, which limits their applicability; and 3) the robustness guarantees of all methods except one are not 100% accurate. We address all these limitations by developing AGNNCert, the first certified defense for GNNs against arbitrary (edge, node, and node feature) perturbations with deterministic robustness guarantees, and applicable to the two most common node and graph classification tasks. AGNNCert also encompass existing certified defenses as special cases. Extensive evaluations on multiple benchmark node/graph classification datasets and two real-world graph datasets, and multiple GNNs validate the effectiveness of AGNNCert to provably defend against arbitrary perturbations. AGNNCert also shows its superiority over the state-of-the-art certified defenses against the individual edge perturbation and node perturbation.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jiate Li",
        "Binghui Wang"
      ],
      "author_details": [
        {
          "name": "Jiate Li",
          "h_index": 3,
          "citation_count": 27,
          "affiliations": []
        },
        {
          "name": "Binghui Wang",
          "h_index": 6,
          "citation_count": 227,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://openalex.org/W4407124074",
      "pdf_url": "https://arxiv.org/pdf/2502.00765",
      "doi": "https://doi.org/10.48550/arxiv.2502.00765",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 80,
      "is_open_access": false,
      "publication_date": "2025-02-02",
      "tldr": "AGNNCert is developed, the first certified defense for GNNs against arbitrary perturbations with deterministic robustness guarantees, and applicable to the two most common node and graph classification tasks.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "graph"
      ],
      "model_types": [
        "gnn"
      ],
      "tags": [
        "certified-robustness",
        "GNN-defense",
        "deterministic"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_b13259c3",
      "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
      "abstract": "Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Zhisheng Zhang",
        "Derui Wang",
        "Qianyi Yang",
        "Pengyang Huang",
        "Junhan Pu",
        "Yuxin Cao",
        "Kai Ye",
        "Jie Hao",
        "Yixian Yang"
      ],
      "author_details": [
        {
          "name": "Zhisheng Zhang",
          "h_index": 3,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Derui Wang",
          "h_index": 5,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Qianyi Yang",
          "h_index": 2,
          "citation_count": 13,
          "affiliations": []
        },
        {
          "name": "Pengyang Huang",
          "h_index": 3,
          "citation_count": 19,
          "affiliations": []
        },
        {
          "name": "Junhan Pu",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Yuxin Cao",
          "h_index": 4,
          "citation_count": 69,
          "affiliations": []
        },
        {
          "name": "Kai Ye",
          "h_index": 3,
          "citation_count": 18,
          "affiliations": []
        },
        {
          "name": "Jie Hao",
          "h_index": 3,
          "citation_count": 23,
          "affiliations": []
        },
        {
          "name": "Yixian Yang",
          "h_index": 3,
          "citation_count": 14,
          "affiliations": []
        }
      ],
      "max_h_index": 5,
      "url": "https://openalex.org/W4415158376",
      "pdf_url": "https://arxiv.org/pdf/2504.09839",
      "doi": "https://doi.org/10.48550/arxiv.2504.09839",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 68,
      "is_open_access": false,
      "publication_date": "2025-04-14",
      "tldr": "This work proposes a defensive framework, SafeSpeech, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "audio"
      ],
      "model_types": [],
      "tags": [
        "voice-protection",
        "anti-cloning",
        "adversarial-perturbation"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_5f7dbceb",
      "title": "Cloak, Honey, Trap: Proactive Defenses Against LLM Agents",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Daniel Ayzenshteyn",
        "Roy Weiss",
        "Yisroel Mirsky"
      ],
      "author_details": [
        {
          "name": "Daniel Ayzenshteyn",
          "h_index": 3,
          "citation_count": 29,
          "affiliations": []
        },
        {
          "name": "Roy Weiss",
          "h_index": 3,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Yisroel Mirsky",
          "h_index": 22,
          "citation_count": 4807,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-ayzenshteyn.pdf",
      "citation_count": 3,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "llm"
      ],
      "model_types": [
        "llm"
      ],
      "tags": [
        "LLM-agent-defense",
        "proactive",
        "honeypot"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "1911.01559",
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Ren Pang",
        "Hua Shen",
        "Xinyang Zhang",
        "S. Ji",
        "Yevgeniy Vorobeychik",
        "Xiaopu Luo",
        "Ting Wang"
      ],
      "author_details": [
        {
          "name": "Ren Pang",
          "h_index": 11,
          "citation_count": 588,
          "affiliations": []
        },
        {
          "name": "Hua Shen",
          "h_index": 12,
          "citation_count": 713,
          "affiliations": [
            "The PennState University"
          ]
        },
        {
          "name": "Xinyang Zhang",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "S. Ji",
          "h_index": 51,
          "citation_count": 9646,
          "affiliations": []
        },
        {
          "name": "Yevgeniy Vorobeychik",
          "h_index": 43,
          "citation_count": 6610,
          "affiliations": []
        },
        {
          "name": "Xiaopu Luo",
          "h_index": 2,
          "citation_count": 76,
          "affiliations": []
        },
        {
          "name": "Ting Wang",
          "h_index": 27,
          "citation_count": 3358,
          "affiliations": []
        }
      ],
      "max_h_index": 51,
      "url": "https://arxiv.org/abs/1911.01559",
      "pdf_url": "https://arxiv.org/pdf/1911.01559",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 49,
      "is_open_access": false,
      "publication_date": "2019-11-05",
      "tldr": "This paper develops a new attack model that integrates both adversarial inputs and backdoored models, and reveals that there exists an intricate \"mutual reinforcement\" effect between the two attack vectors.",
      "fields_of_study": [
        "Computer Science",
        "Mathematics"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "adversarial-examples",
        "data-poisoning",
        "unified-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_2f948b9d",
      "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
      "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training. Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Derui Wang",
        "Kristen Moore",
        "Diksha Goel",
        "Minjune Kim",
        "Gang Li",
        "Yang Li",
        "Robin Doss",
        "Minhui Xue",
        "Bo Li",
        "Seyit Ahmet Camtepe",
        "Liming Zhu"
      ],
      "author_details": [
        {
          "name": "Derui Wang",
          "h_index": 2,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Kristen Moore",
          "h_index": 2,
          "citation_count": 15,
          "affiliations": []
        },
        {
          "name": "Diksha Goel",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Minjune Kim",
          "h_index": 2,
          "citation_count": 17,
          "affiliations": []
        },
        {
          "name": "Gang Li",
          "h_index": 8,
          "citation_count": 336,
          "affiliations": []
        },
        {
          "name": "Yang Li",
          "h_index": 3,
          "citation_count": 25,
          "affiliations": []
        },
        {
          "name": "Robin Doss",
          "h_index": 2,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Minhui Xue",
          "h_index": 4,
          "citation_count": 66,
          "affiliations": []
        },
        {
          "name": "Bo Li",
          "h_index": 2,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Seyit Ahmet Camtepe",
          "h_index": 43,
          "citation_count": 8643,
          "affiliations": [
            "CSIRO Data61"
          ]
        },
        {
          "name": "Liming Zhu",
          "h_index": 2,
          "citation_count": 10,
          "affiliations": []
        }
      ],
      "max_h_index": 43,
      "url": "https://openalex.org/W4406975609",
      "pdf_url": "https://arxiv.org/pdf/2501.17667",
      "doi": "https://doi.org/10.48550/arxiv.2501.17667",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 71,
      "is_open_access": false,
      "publication_date": "2025-01-29",
      "tldr": "A novel paradigm dubbed CAMP is introduced, designed to enhance DRL policies, achieving better utility without compromising provable robustness, and introduces \\textit{policy imitation} as a novel technique to stabilize training.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [
        "reinforcement-learning"
      ],
      "model_types": [],
      "tags": [
        "certified-robustness",
        "DRL-defense",
        "provable"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_2a3c0aa2",
      "title": "Pryde: A Modular Generalizable Workflow for Uncovering Evasion Attacks Against Stateful Firewall Deployments",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Soo-Jin Moon",
        "Milind Srivastava",
        "Yves Bieri",
        "Ruben Martins",
        "Vyas Sekar"
      ],
      "author_details": [
        {
          "name": "Soo-Jin Moon",
          "h_index": 5,
          "citation_count": 205,
          "affiliations": []
        },
        {
          "name": "Milind Srivastava",
          "h_index": 2,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Yves Bieri",
          "h_index": 2,
          "citation_count": 35,
          "affiliations": []
        },
        {
          "name": "Ruben Martins",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Vyas Sekar",
          "h_index": 3,
          "citation_count": 25,
          "affiliations": []
        }
      ],
      "max_h_index": 5,
      "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a144/1Ub242nYFoY",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 67,
      "is_open_access": false,
      "publication_date": "2024-05-19",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "tool",
      "domains": [],
      "model_types": [],
      "tags": [
        "firewall-evasion",
        "workflow",
        "network-security"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_2500586a",
      "title": "Swallow: A Transfer-Robust Website Fingerprinting Attack via Consistent Feature Learning",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Meng Shen",
        "Jinhe Wu",
        "Junyu Ai",
        "Qi Li",
        "Chenchen Ren",
        "Ke Xu",
        "Liehuang Zhu"
      ],
      "author_details": [
        {
          "name": "Meng Shen",
          "h_index": 7,
          "citation_count": 226,
          "affiliations": []
        },
        {
          "name": "Jinhe Wu",
          "h_index": 2,
          "citation_count": 33,
          "affiliations": []
        },
        {
          "name": "Junyu Ai",
          "h_index": 1,
          "citation_count": 3,
          "affiliations": []
        },
        {
          "name": "Qi Li",
          "h_index": 10,
          "citation_count": 582,
          "affiliations": []
        },
        {
          "name": "Chenchen Ren",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Ke Xu",
          "h_index": 4,
          "citation_count": 98,
          "affiliations": []
        },
        {
          "name": "Liehuang Zhu",
          "h_index": 40,
          "citation_count": 6229,
          "affiliations": []
        }
      ],
      "max_h_index": 40,
      "url": "https://ccs25files.zoolab.org/main/ccsfa/TaZ6VzOa/3719027.3744795.pdf",
      "citation_count": 2,
      "influential_citation_count": 0,
      "reference_count": 33,
      "is_open_access": false,
      "publication_date": "2025-11-19",
      "tldr": "This paper proposes Swallow, a transfer-robust WF attack that can quickly transfer to new network conditions while maintaining robustness against various WF defenses, and proposes a novel trace representation named Consistent Interaction Feature (CIF), which aligns traffic distributions across different network conditions to capture consistent features.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "attack",
      "domains": [],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "website-fingerprinting",
        "transfer-robust",
        "traffic-analysis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2311.07780",
      "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
      "abstract": "Audio adversarial examples (AEs) have posed significant security challenges to real-world speaker recognition systems. Most black-box attacks still require certain information from the speaker recognition model to be effective (e.g., keeping probing and requiring the knowledge of similarity scores). This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model. Although it is not feasible for an attacker to succeed with completely zero knowledge, we assume that the attacker only knows a short (or a few seconds) speech sample of a target speaker. Without any probing to gain further knowledge about the target model, we propose a new mechanism, called parrot training, to generate AEs against the target model. Motivated by recent advancements in voice conversion (VC), we propose to use the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech. Then, we use these parrot speech samples to train a parrot-trained(PT) surrogate model for the attacker. Under a joint transferability and perception framework, we investigate different ways to generate AEs on the PT model (called PT-AEs) to ensure the PT-AEs can be generated with high transferability to a black-box target model with good human perceptual quality. Real-world experiments show that the resultant PT-AEs achieve the attack success rates of 45.8% - 80.8% against the open-source models in the digital-line scenario and 47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon Echo, and Google Home, in the over-the-air scenario.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "author_details": [
        {
          "name": "Rui Duan",
          "h_index": 4,
          "citation_count": 230,
          "affiliations": []
        },
        {
          "name": "Zhe Qu",
          "h_index": 9,
          "citation_count": 624,
          "affiliations": []
        },
        {
          "name": "Leah Ding",
          "h_index": 2,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "Yao Liu",
          "h_index": 2,
          "citation_count": 12,
          "affiliations": []
        },
        {
          "name": "Zhuo Lu",
          "h_index": 4,
          "citation_count": 34,
          "affiliations": []
        }
      ],
      "max_h_index": 9,
      "url": "https://arxiv.org/abs/2311.07780",
      "pdf_url": "https://arxiv.org/pdf/2311.07780",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "publication_date": "2023-11-13",
      "tldr": "This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model by using the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech.",
      "fields_of_study": [
        "Computer Science",
        "Engineering"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "audio"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "speaker-recognition",
        "black-box",
        "practical-attack"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_1f7c2241",
      "title": "From Threat to Trust: Exploiting Attention Mechanisms for Attacks and Defenses in Cooperative Perception",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Chenyi Wang",
        "Raymond Muller",
        "Ruoyu Song",
        "J. Monteuuis",
        "Jonathan Petit",
        "Yanmao Man",
        "Ryan M. Gerdes",
        "Z. B. Celik",
        "Ming Li"
      ],
      "author_details": [
        {
          "name": "Chenyi Wang",
          "h_index": 1,
          "citation_count": 4,
          "affiliations": []
        },
        {
          "name": "Raymond Muller",
          "h_index": 5,
          "citation_count": 126,
          "affiliations": []
        },
        {
          "name": "Ruoyu Song",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "J. Monteuuis",
          "h_index": 8,
          "citation_count": 202,
          "affiliations": []
        },
        {
          "name": "Jonathan Petit",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Yanmao Man",
          "h_index": 10,
          "citation_count": 389,
          "affiliations": []
        },
        {
          "name": "Ryan M. Gerdes",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Z. B. Celik",
          "h_index": 3,
          "citation_count": 73,
          "affiliations": []
        },
        {
          "name": "Ming Li",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        }
      ],
      "max_h_index": 10,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-wang-chenyi.pdf",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "attention-mechanism",
        "cooperative-perception",
        "V2X"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_96a4775d",
      "title": "Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tianyu Qi",
        "Lei Xue",
        "Yufeng Zhan",
        "Xiaobo Ma"
      ],
      "author_details": [
        {
          "name": "Tianyu Qi",
          "h_index": 2,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Lei Xue",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Yufeng Zhan",
          "h_index": 4,
          "citation_count": 61,
          "affiliations": []
        },
        {
          "name": "Xiaobo Ma",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        }
      ],
      "max_h_index": 4,
      "url": "https://arxiv.org/html/2506.05402v1",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 77,
      "is_open_access": false,
      "publication_date": "2025-11-19",
      "tldr": "Sylva is proposed, a personalized collaborative adversarial training framework designed to deliver customized defense models for each client through a two-phase process that can achieve up to 50\u00d7 improvements in communication efficiency compared to state-of-the-art algorithms.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn",
        "transformer"
      ],
      "tags": [
        "personalized-defense",
        "fine-tuning",
        "pretrained-models"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2506.17162",
      "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
      "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability.   To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Side Liu",
        "Jiang Ming",
        "Guodong Zhou",
        "Xinyi Liu",
        "Jianming Fu",
        "Guojun Peng"
      ],
      "author_details": [
        {
          "name": "Side Liu",
          "h_index": 2,
          "citation_count": 24,
          "affiliations": []
        },
        {
          "name": "Jiang Ming",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Guodong Zhou",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Xinyi Liu",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Jianming Fu",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Guojun Peng",
          "h_index": 12,
          "citation_count": 733,
          "affiliations": []
        }
      ],
      "max_h_index": 12,
      "url": "https://arxiv.org/abs/2506.17162",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 50,
      "is_open_access": false,
      "publication_date": "2025-06-20",
      "tldr": "This work introduces the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which it extracts semantic features using a pretrained language model, and constructs an Object Reference Graph to capture structural features, drawing inspiration from program analysis.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "PDF-malware",
        "adversarial-robustness",
        "IR-analysis"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "2510.05173",
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
      "year": 2025,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Peigui Qi",
        "Kunsheng Tang",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Neng H. Yu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "author_details": [
        {
          "name": "Peigui Qi",
          "h_index": 1,
          "citation_count": 10,
          "affiliations": []
        },
        {
          "name": "Kunsheng Tang",
          "h_index": 3,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Wenbo Zhou",
          "h_index": 22,
          "citation_count": 3199,
          "affiliations": []
        },
        {
          "name": "Weiming Zhang",
          "h_index": 3,
          "citation_count": 30,
          "affiliations": []
        },
        {
          "name": "Neng H. Yu",
          "h_index": 19,
          "citation_count": 1601,
          "affiliations": []
        },
        {
          "name": "Tianwei Zhang",
          "h_index": 9,
          "citation_count": 338,
          "affiliations": []
        },
        {
          "name": "Qing Guo",
          "h_index": 4,
          "citation_count": 41,
          "affiliations": []
        },
        {
          "name": "Jie Zhang",
          "h_index": 15,
          "citation_count": 1156,
          "affiliations": []
        }
      ],
      "max_h_index": 22,
      "url": "https://arxiv.org/abs/2510.05173",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 48,
      "is_open_access": false,
      "publication_date": "2025-10-05",
      "tldr": "This work introduces SafeGuider, a two-step framework designed for robust safety control without compromising generation quality, which combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm and demonstrates exceptional effectiveness in minimizing attack success rates.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Book",
        "Conference"
      ],
      "paper_type": "defense",
      "domains": [
        "generative",
        "vision"
      ],
      "model_types": [
        "diffusion"
      ],
      "tags": [
        "adversarial-prompt-defense",
        "content-safety",
        "T2I"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_097119fe",
      "title": "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities",
      "abstract": "Vision-language models (VLMs) are increasingly applied to identify unsafe or inappropriate images due to their internal ethical standards and powerful reasoning abilities. However, it is still unclear whether they can recognize various unsafe concepts when presented in different modalities, such as text and images. To address this, we first compile the UnsafeConcepts dataset, featuring 75 unsafe concepts, i.e., ``Swastika,'' ``Sexual Harassment,'' and ``Assaults,'' along with associated 1.5K images. We then conduct a systematic evaluation of VLMs' perception (concept recognition) and alignment (ethical reasoning) capabilities. We assess eight popular VLMs and find that, although most VLMs accurately perceive unsafe concepts, they sometimes mistakenly classify these concepts as safe. We also identify a consistent modality gap among open-source VLMs in distinguishing between visual and textual unsafe concepts. To bridge this gap, we introduce a simplified reinforcement learning (RL)-based approach using proximal policy optimization (PPO) to strengthen the ability to identify unsafe concepts from images. Our approach uses reward scores based directly on VLM responses, bypassing the need for collecting human-annotated preference data to train a new reward model. Experimental results show that our approach effectively enhances VLM alignment on images while preserving general capabilities. It outperforms baselines such as supervised fine-tuning (SFT) and direct preference optimization (DPO). We hope our dataset, evaluation findings, and proposed alignment solution contribute to the community's efforts in advancing safe VLMs.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Y. Qu",
        "Michael Backes",
        "Yang Zhang"
      ],
      "author_details": [
        {
          "name": "Y. Qu",
          "h_index": 6,
          "citation_count": 326,
          "affiliations": []
        },
        {
          "name": "Michael Backes",
          "h_index": 16,
          "citation_count": 877,
          "affiliations": []
        },
        {
          "name": "Yang Zhang",
          "h_index": 6,
          "citation_count": 286,
          "affiliations": []
        }
      ],
      "max_h_index": 16,
      "url": "https://openalex.org/W4414944889",
      "pdf_url": "https://arxiv.org/pdf/2507.11155",
      "doi": "https://doi.org/10.48550/arxiv.2507.11155",
      "citation_count": 1,
      "influential_citation_count": 0,
      "reference_count": 59,
      "is_open_access": false,
      "publication_date": "2025-07-15",
      "tldr": "A simplified reinforcement learning (RL)-based approach using proximal policy optimization (PPO) to strengthen the ability to identify unsafe concepts from images and effectively enhances VLM alignment on images while preserving general capabilities.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "empirical",
      "domains": [
        "multimodal",
        "vision",
        "nlp"
      ],
      "model_types": [
        "transformer"
      ],
      "tags": [
        "VLM-safety",
        "cross-modal",
        "unsafe-concepts"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_e5270083",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "author_details": [
        {
          "name": "Wenlong Meng",
          "h_index": 4,
          "citation_count": 65,
          "affiliations": []
        },
        {
          "name": "Shuguo Fan",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Chengkun Wei",
          "h_index": 9,
          "citation_count": 269,
          "affiliations": []
        },
        {
          "name": "Min Chen",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Yuwei Li",
          "h_index": 1,
          "citation_count": 9,
          "affiliations": []
        },
        {
          "name": "Yuanchao Zhang",
          "h_index": 1,
          "citation_count": 1,
          "affiliations": []
        },
        {
          "name": "Zhikun Zhang",
          "h_index": 4,
          "citation_count": 47,
          "affiliations": []
        },
        {
          "name": "Wenzhi Chen",
          "h_index": 8,
          "citation_count": 165,
          "affiliations": []
        }
      ],
      "max_h_index": 9,
      "url": "https://openalex.org/W4417255476",
      "pdf_url": "https://arxiv.org/pdf/2506.08188",
      "doi": "https://doi.org/10.48550/arxiv.2506.08188",
      "citation_count": 1,
      "influential_citation_count": 1,
      "reference_count": 108,
      "is_open_access": false,
      "publication_date": "2025-06-09",
      "tldr": "This paper introduces GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors, and introduces a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "attack",
      "domains": [
        "nlp",
        "llm"
      ],
      "model_types": [
        "llm",
        "transformer"
      ],
      "tags": [
        "detector-evasion",
        "gradient-based"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_02261740",
      "title": "Avara: A Uniform Evaluation System for Perceptibility Analysis Against Adversarial Object Evasion Attacks",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinyao Ma",
        "Chaoqi Zhang",
        "Huadi Zhu",
        "L. Camp",
        "Ming Li",
        "Xiaojing Liao"
      ],
      "author_details": [
        {
          "name": "Xinyao Ma",
          "h_index": 2,
          "citation_count": 8,
          "affiliations": []
        },
        {
          "name": "Chaoqi Zhang",
          "h_index": 2,
          "citation_count": 11,
          "affiliations": []
        },
        {
          "name": "Huadi Zhu",
          "h_index": 6,
          "citation_count": 178,
          "affiliations": []
        },
        {
          "name": "L. Camp",
          "h_index": 2,
          "citation_count": 21,
          "affiliations": []
        },
        {
          "name": "Ming Li",
          "h_index": 6,
          "citation_count": 248,
          "affiliations": []
        },
        {
          "name": "Xiaojing Liao",
          "h_index": 3,
          "citation_count": 20,
          "affiliations": []
        }
      ],
      "max_h_index": 6,
      "url": "https://drive.google.com/file/d/16qfqZpOED2W3wXmGibdDOIK5ctboend7/view",
      "citation_count": 0,
      "influential_citation_count": 0,
      "reference_count": 63,
      "is_open_access": true,
      "publication_date": "2024-12-02",
      "tldr": "Avara is proposed, the first unified evaluation platform for assessing human drivers' perceptibility to adversarial attacks in AD contexts, and identifies an intriguing discovery that the current imperceptibility metrics for adversarial attacks fail to accurately reflect the autonomous vehicle driver's perceptibility.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "Book",
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "benchmark",
      "domains": [
        "vision"
      ],
      "model_types": [
        "cnn"
      ],
      "tags": [
        "evaluation-system",
        "perceptibility",
        "object-detection"
      ],
      "open_access_pdf": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670291"
    },
    {
      "paper_id": "seed_3ec0a9e7",
      "title": "CertTA: Certified Robustness Made Practical for Learning-Based Traffic Analysis",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Jinzhu Yan",
        "Zhuotao Liu",
        "Yuyang Xie",
        "Shiyu Liang",
        "Lin Liu",
        "Ke Xu"
      ],
      "author_details": [
        {
          "name": "Jinzhu Yan",
          "h_index": 1,
          "citation_count": 37,
          "affiliations": []
        },
        {
          "name": "Zhuotao Liu",
          "h_index": 2,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Yuyang Xie",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        },
        {
          "name": "Shiyu Liang",
          "h_index": 1,
          "citation_count": 2,
          "affiliations": []
        },
        {
          "name": "Lin Liu",
          "h_index": 2,
          "citation_count": 5,
          "affiliations": []
        },
        {
          "name": "Ke Xu",
          "h_index": 0,
          "citation_count": 0,
          "affiliations": []
        }
      ],
      "max_h_index": 2,
      "url": "https://www.usenix.org/system/files/usenixsecurity25-yan-jinzhu.pdf",
      "citation_count": 0,
      "influential_citation_count": 0,
      "reference_count": 0,
      "is_open_access": false,
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle"
      ],
      "paper_type": "defense",
      "domains": [],
      "model_types": [],
      "tags": [
        "certified-robustness",
        "traffic-analysis",
        "practical"
      ],
      "open_access_pdf": ""
    },
    {
      "paper_id": "seed_b8ef1770",
      "title": "Experimental Analyses of the Physical Surveillance Risks in Client-Side Content Scanning",
      "abstract": "Content scanning systems employ perceptual hashing algorithms to scan user content for illicit material, such as child pornography or terrorist recruitment flyers.Perceptual hashing algorithms help determine whether two images are visually similar while preserving the privacy of the input images.Several efforts from industry and academia propose scanning on client devices such as smartphones due to the impending rollout of end-to-end encryption that will make server-side scanning difficult.These proposals have met with strong criticism because of the potential for the technology to be misused for censorship.However, the risks of this technology in the context of surveillance are not well understood.Our work informs this conversation by experimentally characterizing the potential for one type of misuse -attackers manipulating the content scanning system to perform physical surveillance on target locations.Our contributions are threefold: (1) we offer a definition of physical surveillance in the context of client-side image scanning systems; (2) we experimentally characterize this risk and create a surveillance algorithm that achieves physical surveillance rates more than 30% by poisoning 0.2% of the perceptual hash database; (3) we experimentally study the trade-off between the robustness of client-side image scanning systems and surveillance, showing that more robust detection of illicit material leads to an increased potential for physical surveillance in most settings.",
      "year": 2024,
      "venue": "Network and Distributed System Security Symposium",
      "authors": [
        "Ashish Hooda",
        "Andrey Labunets",
        "Tadayoshi Kohno",
        "Earlence Fernandes"
      ],
      "author_details": [
        {
          "name": "Ashish Hooda",
          "h_index": 7,
          "citation_count": 242,
          "affiliations": [
            "University of Wisconsin Madison"
          ]
        },
        {
          "name": "Andrey Labunets",
          "h_index": 2,
          "citation_count": 14,
          "affiliations": []
        },
        {
          "name": "Tadayoshi Kohno",
          "h_index": 7,
          "citation_count": 191,
          "affiliations": []
        },
        {
          "name": "Earlence Fernandes",
          "h_index": 4,
          "citation_count": 92,
          "affiliations": []
        }
      ],
      "max_h_index": 7,
      "url": "https://openalex.org/W4391725331",
      "pdf_url": "https://doi.org/10.14722/ndss.2024.241401",
      "doi": "https://doi.org/10.14722/ndss.2024.241401",
      "citation_count": 0,
      "influential_citation_count": 0,
      "reference_count": 54,
      "is_open_access": true,
      "tldr": "A definition of physical surveillance is offered in the context of client-side image scanning systems and the trade-off between the robustness of client-side image scanning systems and surveillance is studied, showing that more robust detection of illicit material leads to an increased potential for physical surveillance in most settings.",
      "fields_of_study": [
        "Computer Science"
      ],
      "publication_types": [
        "JournalArticle",
        "Conference"
      ],
      "paper_type": "empirical",
      "domains": [
        "vision"
      ],
      "model_types": [],
      "tags": [
        "perceptual-hashing",
        "content-scanning",
        "surveillance-risk"
      ],
      "open_access_pdf": "https://doi.org/10.14722/ndss.2024.241401"
    }
  ]
}