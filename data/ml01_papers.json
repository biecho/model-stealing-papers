{
  "updated": "2026-01-06",
  "total": 263,
  "owasp_id": "ML01",
  "owasp_name": "Input Manipulation Attack",
  "description": "Adversarial attacks on machine learning model inputs. This includes\n        adversarial examples, evasion attacks, perturbation attacks, and any\n        technique that manipulates input data to cause misclassification or\n        incorrect model behavior. Covers both white-box and black-box adversarial\n        attacks, robustness evaluation, and input perturbation methods.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "368109e6ad7e652b2310ab79cc2d9857f647550a",
      "title": "AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models",
      "abstract": "Previous work on adversarial examples typically involves a fixed norm perturbation budget, which fails to capture the way humans perceive perturbations. Recent work has shifted towards natural unrestricted adversarial examples (UAEs) that breaks \u2113p perturbation bounds but nonetheless remain semantically plausible. Current methods use GAN or VAE to generate UAEs by perturbing latent codes. However, this leads to loss of high-level information, resulting in low-quality and unnatural UAEs. In light of this, we propose AdvDiffuser, a new method for synthesizing natural UAEs using diffusion models. It can generate UAEs from scratch or conditionally based on reference images. To generate natural UAEs, we perturb predicted images to steer their latent code towards the adversarial sample space of a particular classifier. We also propose adversarial inpainting based on class activation mapping to retain the salient regions of the image while perturbing less important areas. On CIFAR-10, CelebA and ImageNet, we demonstrate that it can defeat the most robust models on the RobustBench leaderboard with near 100% success rates. Furthermore, The synthesized UAEs are not only more natural but also stronger compared to the current state-of-the-art attacks. Specifically, compared with GA-attack, the UAEs generated with AdvDiffuser exhibit 6\u00d7 smaller LPIPS perturbations, 2 ~ 3\u00d7 smaller FID scores and 0.28 higher in SSIM metrics, making them perceptually stealthier. Finally, adversarial training with AdvDiffuser further improves the model robustness against attacks with unseen threat models.1",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Xinquan Chen",
        "Xitong Gao",
        "Juanjuan Zhao",
        "Kejiang Ye",
        "Chengjie Xu"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/368109e6ad7e652b2310ab79cc2d9857f647550a",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90d6adb9eda4641f60c4de0579946ff0f7481690",
      "title": "Code Difference Guided Adversarial Example Generation for Deep Code Models",
      "abstract": "Adversarial examples are important to test and enhance the robustness of deep code models. As source code is discrete and has to strictly stick to complex grammar and semantics constraints, the adversarial example generation techniques in other domains are hardly applicable. Moreover, the adversarial example generation techniques specific to deep code models still suffer from unsatisfactory effectiveness due to the enormous ingredient search space. In this work, we propose a novel adversarial example generation technique (i.e., CODA) for testing deep code models. Its key idea is to use code differences between the target input (i.e., a given code snippet as the model input) and reference inputs (i.e., the inputs that have small code differences but different prediction results with the target input) to guide the generation of adversarial examples. It considers both structure differences and identifier differences to preserve the original semantics. Hence, the ingredient search space can be largely reduced as the one constituted by the two kinds of code differences, and thus the testing process can be improved by designing and guiding corresponding equivalent structure transformations and identifier renaming transformations. Our experiments on 15 deep code models demonstrate the effective-ness and efficiency of CODA, the naturalness of its generated examples, and its capability of enhancing model robustness after adversarial fine-tuning. For example, CODA reveals 88.05 % and 72.51 % more faults in models than the state-of-the-art techniques (i.e., CARROT and ALERT) on average, respectively.",
      "year": 2023,
      "venue": "International Conference on Automated Software Engineering",
      "authors": [
        "Zhao Tian",
        "Junjie Chen",
        "Zhi Jin"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/90d6adb9eda4641f60c4de0579946ff0f7481690",
      "pdf_url": "http://arxiv.org/pdf/2301.02412",
      "publication_date": "2023-01-06",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b110fce160468eb179b6c43ea27e098757a56dd",
      "title": "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
      "abstract": "We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) \u201cfool\u201d pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.",
      "year": 2018,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Mohit Iyyer",
        "J. Wieting",
        "Kevin Gimpel",
        "Luke Zettlemoyer"
      ],
      "citation_count": 762,
      "url": "https://www.semanticscholar.org/paper/2b110fce160468eb179b6c43ea27e098757a56dd",
      "pdf_url": "https://www.aclweb.org/anthology/N18-1170.pdf",
      "publication_date": "2018-04-17",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1935668faf67ed68c6320d45144785ce4256fdf0",
      "title": "Learning Universal Adversarial Perturbation by Adversarial Example",
      "abstract": "Deep learning models have shown to be susceptible to universal adversarial perturbation (UAP), which has aroused wide concerns in the community. Compared with the conventional adversarial attacks that generate adversarial samples at the instance level, UAP can fool the target model for different instances with only a single perturbation, enabling us to evaluate the robustness of the model from a more effective and accurate perspective. The existing universal attack methods fail to exploit the differences and connections between the instance and universal levels to produce dominant perturbations. To address this challenge, we propose a new universal attack method that unifies instance-specific and universal attacks from a feature perspective to generate a more dominant UAP. Specifically, we reformulate the UAP generation task as a minimax optimization problem and then utilize the instance-specific attack method to solve the minimization problem thereby obtaining better training data for generating UAP. At the same time, we also introduce a consistency regularizer to explore the relationship between training data, thus further improving the dominance of the generated UAP. Furthermore, our method is generic with no additional assumptions about the training data and hence can be applied to both data-dependent (supervised) and data-independent (unsupervised) manners. Extensive experiments demonstrate that the proposed method improves the performance by a significant margin over the existing methods in both data-dependent and data-independent settings. Code is available at https://github.com/lisenxd/AT-UAP.",
      "year": 2022,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Maosen Li",
        "Yanhua Yang",
        "Kun-Juan Wei",
        "Xu Yang",
        "Heng Huang"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/1935668faf67ed68c6320d45144785ce4256fdf0",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/20023/19782",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "02134d1dca5244fe53bfccc173d1411889654f44",
      "title": "Joint Adversarial Example and False Data Injection Attacks for State Estimation in Power Systems",
      "abstract": "Although state estimation using a bad data detector (BDD) is a key procedure employed in power systems, the detector is vulnerable to false data injection attacks (FDIAs). Substantial deep learning methods have been proposed to detect such attacks. However, deep neural networks are susceptible to adversarial attacks or adversarial examples, where slight changes in inputs may lead to sharp changes in the corresponding outputs in even well-trained networks. This article introduces the joint adversarial example and FDIAs (AFDIAs) to explore various attack scenarios for state estimation in power systems. Considering that perturbations added directly to measurements are likely to be detected by BDDs, our proposed method of adding perturbations to state variables can guarantee that the attack is stealthy to BDDs. Then, malicious data that are stealthy to both BDDs and deep learning-based detectors can be generated. Theoretical and experimental results show that our proposed state-perturbation-based AFDIA method (S-AFDIA) can carry out attacks stealthy to both conventional BDDs and deep learning-based detectors, while our proposed measurement-perturbation-based adversarial FDIA method (M-AFDIA) succeeds if only deep learning-based detectors are used. The comparative experiments show that our proposed methods provide better performance than state-of-the-art methods. Besides, the ultimate effect of attacks can also be optimized using the proposed joint attack methods.",
      "year": 2021,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Jiwei Tian",
        "Buhong Wang",
        "Zhen Wang",
        "Kunrui Cao",
        "Jing Li",
        "M. Ozay"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/02134d1dca5244fe53bfccc173d1411889654f44",
      "pdf_url": "",
      "publication_date": "2021-11-19",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "899bb5f4ad958834eb8bd79a4c9336645cc39d99",
      "title": "Adversarial Example Detection Using Latent Neighborhood Graph",
      "abstract": "Detection of adversarial examples with high accuracy is critical for the security of deployed deep neural network-based models. We present the first graph-based adversarial detection method that constructs a Latent Neighborhood Graph (LNG) around an input example to determine if the input example is adversarial. Given an input example, selected reference adversarial and benign examples (represented as LNG nodes in Figure 1) are used to capture the local manifold in the vicinity of the input example. The LNG node connectivity parameters are optimized jointly with the parameters of a graph attention network in an end-to-end manner to determine the optimal graph topology for adversarial example detection. The graph attention network is used to determine if the LNG is derived from an adversarial or benign input example. Experimental evaluations on CIFAR-10, STL-10, and ImageNet datasets, using six adversarial attack methods, demonstrate that the proposed method outperforms state-of-the-art adversarial detection methods in white-box and gray-box settings. The proposed method is able to successfully detect adversarial examples crafted with small perturbations using unseen attacks.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Ahmed A. Abusnaina",
        "Yuhang Wu",
        "S. S. Arora",
        "Yizhen Wang",
        "Fei Wang",
        "Hao Yang",
        "David A. Mohaisen"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/899bb5f4ad958834eb8bd79a4c9336645cc39d99",
      "pdf_url": "",
      "publication_date": "2021-10-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
      "title": "Adversarial examples in the physical world",
      "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
      "year": 2016,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Alexey Kurakin",
        "I. Goodfellow",
        "Samy Bengio"
      ],
      "citation_count": 6412,
      "url": "https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
      "pdf_url": "https://arxiv.org/pdf/1607.02533",
      "publication_date": "2016-07-08",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13a5aedf89c0e6c10b18350e4b228708f22a6605",
      "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent",
      "abstract": "Evading adversarial example detection defenses requires finding adversarial examples that must simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. We find that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. We introduce Orthogonal Projected Gradient Descent, an improved attack technique to generate adversarial examples that avoids this problem by orthogonalizing the gradients when running standard gradient-based attacks. We use our technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Oliver Bryniarski",
        "Nabeel Hingun",
        "Pedro Pachuca",
        "Vincent Wang",
        "Nicholas Carlini"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/13a5aedf89c0e6c10b18350e4b228708f22a6605",
      "pdf_url": "",
      "publication_date": "2021-06-28",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "63ed798f1847d8a952cbeb75d3b286f16c144914",
      "title": "Understanding Measures of Uncertainty for Adversarial Example Detection",
      "abstract": "Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive en- tropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for MC dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using MNIST to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.",
      "year": 2018,
      "venue": "Conference on Uncertainty in Artificial Intelligence",
      "authors": [
        "Lewis Smith",
        "Y. Gal"
      ],
      "citation_count": 394,
      "url": "https://www.semanticscholar.org/paper/63ed798f1847d8a952cbeb75d3b286f16c144914",
      "pdf_url": "",
      "publication_date": "2018-03-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fb866c16d67098ba9cc6194e6b2ac250b47b8a0e",
      "title": "Selective Audio Adversarial Example in Evasion Attack on Speech Recognition System",
      "abstract": "Deep neural networks (DNNs) are widely used for image recognition, speech recognition, and other pattern analysis tasks. Despite the success of DNNs, these systems can be exploited by what is termed adversarial examples. An adversarial example, in which a small distortion is added to the input data, can be designed to be misclassified by the DNN while remaining undetected by humans or other systems. Such adversarial examples have been studied mainly in the image domain. Recently, however, studies on adversarial examples have been expanding into the voice domain. For example, when an adversarial example is applied to enemy wiretapping devices (victim classifiers) in a military environment, the enemy device will misinterpret the intended message. In such scenarios, it is necessary that friendly wiretapping devices (protected classifiers) should not be deceived. Therefore, the selective adversarial example concept can be useful in mixed situations, defined as situations in which there is both a classifier to be protected and a classifier to be attacked. In this paper, we propose a selective audio adversarial example with minimum distortion that will be misclassified as the target phrase by a victim classifier but correctly classified as the original phrase by a protected classifier. To generate such examples, a transformation is carried out to minimize the probability of incorrect classification by the protected classifier and that of correct classification by the victim classifier. We conducted experiments targeting the state-of-the-art DeepSpeech voice recognition model using Mozilla Common Voice datasets and the Tensorflow library. They showed that the proposed method can generate a selective audio adversarial example with a 91.67% attack success rate and 85.67% protected classifier accuracy.",
      "year": 2020,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Hyun Kwon",
        "Hyun Kwon",
        "H. Yoon",
        "D. Choi"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/fb866c16d67098ba9cc6194e6b2ac250b47b8a0e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18063ed998c99bfef92fad8418610b97f863d878",
      "title": "Motivating the Rules of the Game for Adversarial Example Research",
      "abstract": "Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "J. Gilmer",
        "Ryan P. Adams",
        "I. Goodfellow",
        "David G. Andersen",
        "George E. Dahl"
      ],
      "citation_count": 236,
      "url": "https://www.semanticscholar.org/paper/18063ed998c99bfef92fad8418610b97f863d878",
      "pdf_url": "",
      "publication_date": "2018-07-18",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c8c8482b98e7ace68a6cf0d857a6c16fba9f9a15",
      "title": "Adversarial example detection based on saliency map features",
      "abstract": null,
      "year": 2021,
      "venue": "Applied intelligence (Boston)",
      "authors": [
        "Shen Wang",
        "Yuxin Gong"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/c8c8482b98e7ace68a6cf0d857a6c16fba9f9a15",
      "pdf_url": "",
      "publication_date": "2021-09-06",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "88dda624fd265accf54612788414ca9674bfeede",
      "title": "Towards Multiple Black-boxes Attack via Adversarial Example Generation Network",
      "abstract": "The current research on adversarial attacks aims at a single model while the research on attacking multiple models simultaneously is still challenging. In this paper, we propose a novel black-box attack method, referred to as MBbA, which can attack multiple black-boxes at the same time. By encoding input image and its target category into an associated space, each decoder seeks the appropriate attack areas from the image through the designed loss functions, and then generates effective adversarial examples. This process realizes end-to-end adversarial example generation without involving substitute models for the black-box scenario. On the other hand, adopting the adversarial examples generated by MBbA for adversarial training, the robustness of the attacked models are greatly improved. More importantly, those adversarial examples can achieve satisfactory attack performance, even if these black-box models are trained with the adversarial examples generated by other black-box attack methods, which show good transferability. Finally, extensive experiments show that compared with other state-of-the-art methods: (1) MBbA takes the least time to obtain the most effective attack effects in multi-black-box attack scenario. Furthermore, MBbA achieves the highest attack success rates in a single black-box attack scenario; (2) the adversarial examples generated by MBbA can effectively improve the robustness of the attacked models and exhibit good transferability.",
      "year": 2021,
      "venue": "ACM Multimedia",
      "authors": [
        "Mingxing Duan",
        "Kenli Li",
        "Lingxi Xie",
        "Qi Tian",
        "Bin Xiao"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/88dda624fd265accf54612788414ca9674bfeede",
      "pdf_url": "",
      "publication_date": "2021-10-17",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "516afc7d5a427f0b53ca459fe2624a1aae2ee00d",
      "title": "Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks",
      "abstract": "\n \n Machine learning systems based on deep neural networks, being able to produce state-of-the-art results on various perception tasks, have gained mainstream adoption in many applications. However, they are shown to be vulnerable to adversarial example attack, which generates malicious output by adding slight perturbations to the input. Previous adversarial example crafting methods, however, use simple metrics to evaluate the distances between the original examples and the adversarial ones, which could be easily detected by human eyes. In addition, these attacks are often not robust due to the inevitable noises and deviation in the physical world. In this work, we present a new adversarial example attack crafting method, which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example. Experimental results demonstrate the efficacy of the proposed technique.\n \n",
      "year": 2018,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Bo Luo",
        "Yannan Liu",
        "Lingxiao Wei",
        "Q. Xu"
      ],
      "citation_count": 149,
      "url": "https://www.semanticscholar.org/paper/516afc7d5a427f0b53ca459fe2624a1aae2ee00d",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/11499/11358",
      "publication_date": "2018-01-15",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9fa4eb87cbaa395852738e96fae2ad41d04b26e5",
      "title": "Targeted Speech Adversarial Example Generation With Generative Adversarial Network",
      "abstract": "Although neural network-based speech recognition models have enjoyed significant success in many acoustic systems, they are susceptible to be attacked by the adversarial examples. In this work, we make first step towards using generative adversarial network (GAN) for constructing the targeted speech adversarial examples. Specifically, we integrate the target speech recognition network with GAN framework, which can then be formulated as a three-party game. The generator in GAN aims at generating perturbation that could make the target network misclassified to a specific target, while simultaneously fooling the discriminator treating the adversarial example as a beguine one. The discriminator is to distinguish the crafted examples from the geniue samples. The classification error of the target network is back-propagated via gradient flow to the generator for updating. The target network is responsible for back-propagating the classification error via gradients to the generator for updating, but the target network itself is freezed. With the carefully designed network architecture, loss function and training strategy, we successfully train a generator that could generate the adversarial perturbation for a given speech clip and a target label. Experiential results show that the generated adversarial examples could effectively fool the state-of-the-art speech classification networks, while attaining an acceptable auditory perception quality. In addition, our proposed method runs much faster than the prevalent optimization-based schemes. To facilitate reproducible research, codes, models and data are publicly available at https://github.com/winterwindwang/SpeechAdvGan.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Donghua Wang",
        "Li Dong",
        "Rangding Wang",
        "Diqun Yan",
        "Jie Wang"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/9fa4eb87cbaa395852738e96fae2ad41d04b26e5",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09129727.pdf",
      "publication_date": null,
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5e495dd75c20f939b07021d52abc75748a556d95",
      "title": "Classification score approach for detecting adversarial example in deep neural network",
      "abstract": "Deep neural networks (DNNs) provide superior performance on machine learning tasks such as image recognition, speech recognition, pattern analysis, and intrusion detection. However, an adversarial example, created by adding a little noise to an original sample, can cause misclassification by a DNN. This is a serious threat to the DNN because the added noise is not detected by the human eye. For example, if an attacker modifies a right-turn sign so that it misleads to the left, autonomous vehicles with the DNN will incorrectly classify the modified sign as pointing to the left, but a person will correctly classify the modified sign as pointing to the right. Studies are under way to defend against such adversarial examples. The existing method of defense against adversarial examples requires an additional process such as changing the classifier or modifying input data. In this paper, we propose a new method for detecting adversarial examples that does not invoke any additional process. The proposed scheme can detect adversarial examples by using a pattern feature of the classification scores of adversarial examples. We used MNIST and CIFAR10 as experimental datasets and Tensorflow as a machine learning library. The experimental results show that the proposed method can detect adversarial examples with success rates: 99.05% and 99.9% for the untargeted and targeted cases in MNIST, respectively, and 94.7% and 95.8% for the untargeted and targeted cases in CIFAR10, respectively.",
      "year": 2020,
      "venue": "Multimedia tools and applications",
      "authors": [
        "Hyun Kwon",
        "Yongchul Kim",
        "H. Yoon",
        "D. Choi"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/5e495dd75c20f939b07021d52abc75748a556d95",
      "pdf_url": "https://link.springer.com/content/pdf/10.1007/s11042-020-09167-z.pdf",
      "publication_date": "2020-11-21",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e3b0a5a95d7ccb1d1b2325f20af9c9e61021a229",
      "title": "On the Effectiveness of Adversarial Training in Defending against Adversarial Example Attacks for Image Classification",
      "abstract": "State-of-the-art neural network models are actively used in various fields, but it is well-known that they are vulnerable to adversarial example attacks. Throughout the efforts to make the models robust against adversarial example attacks, it has been found to be a very difficult task. While many defense approaches were shown to be not effective, adversarial training remains as one of the promising methods. In adversarial training, the training data are augmented by \u201cadversarial\u201d samples generated using an attack algorithm. If the attacker uses a similar attack algorithm to generate adversarial examples, the adversarially trained network can be quite robust to the attack. However, there are numerous ways of creating adversarial examples, and the defender does not know what algorithm the attacker may use. A natural question is: Can we use adversarial training to train a model robust to multiple types of attack? Previous work have shown that, when a network is trained with adversarial examples generated from multiple attack methods, the network is still vulnerable to white-box attacks where the attacker has complete access to the model parameters. In this paper, we study this question in the context of black-box attacks, which can be a more realistic assumption for practical applications. Experiments with the MNIST dataset show that adversarially training a network with an attack method helps defending against that particular attack method, but has limited effect for other attack methods. In addition, even if the defender trains a network with multiple types of adversarial examples and the attacker attacks with one of the methods, the network could lose accuracy to the attack if the attacker uses a different data augmentation strategy on the target network. These results show that it is very difficult to make a robust network using adversarial training, even for black-box settings where the attacker has restricted information on the target network.",
      "year": 2020,
      "venue": "Applied Sciences",
      "authors": [
        "Sanglee Park",
        "Jungmin So"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/e3b0a5a95d7ccb1d1b2325f20af9c9e61021a229",
      "pdf_url": "https://www.mdpi.com/2076-3417/10/22/8079/pdf?version=1605445320",
      "publication_date": "2020-11-14",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4e755130f1f48d32610b431154aee7335b88e1c3",
      "title": "Robust Audio Adversarial Example for a Physical Attack",
      "abstract": "We propose a method to generate audio adversarial examples that can attack a state-of-the-art speech recognition model in the physical world.\u00a0Previous work assumes that generated adversarial examples are directly fed to the recognition model, and is not able to perform such a physical attack because of reverberation and noise from playback environments.\u00a0In contrast, our method obtains robust adversarial examples by simulating transformations caused by playback or recording in the physical world and incorporating the transformations into the generation process.\u00a0Evaluation and a listening experiment demonstrated that our adversarial examples are able to attack without being noticed by humans.\u00a0This result suggests that audio adversarial examples generated by the proposed method may become a real threat.",
      "year": 2018,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Hiromu Yakura",
        "Jun Sakuma"
      ],
      "citation_count": 201,
      "url": "https://www.semanticscholar.org/paper/4e755130f1f48d32610b431154aee7335b88e1c3",
      "pdf_url": "https://www.ijcai.org/proceedings/2019/0741.pdf",
      "publication_date": "2018-10-28",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4eda7387222780e4cb874f0bbed2fb795c42efb8",
      "title": "POSTER: Detecting Audio Adversarial Example through Audio Modification",
      "abstract": "Deep neural networks (DNNs) perform well in the fields of image recognition, speech recognition, pattern analysis, and intrusion detection. However, DNNs are vulnerable to adversarial examples that add a small amount of noise to the original samples. These adversarial examples have mainly been studied in the field of images, but their effect on the audio field is currently of great interest. For example, adding small distortion that is difficult to identify by humans to the original sample can create audio adversarial examples that allow humans to hear without errors, but only to misunderstand the machine. Therefore, a defense method against audio adversarial examples is needed because it is a threat in this audio field. In this paper, we propose a method to detect audio adversarial examples. The key point of this method is to add a new low level distortion using audio modification, so that the classification result of the adversarial example changes sensitively. On the other hand, the original sample has little change in the classification result for low level distortion. Using this feature, we propose a method to detect audio adversarial examples. To verify the proposed method, we used the Mozilla Common Voice dataset and the DeepSpeech model as the target model. Based on the experimental results, it was found that the accuracy of the adversarial example decreased to 6.21% at approximately 12 dB. It can detect the audio adversarial example compared to the initial audio sample.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Hyun Kwon",
        "H. Yoon",
        "Ki-Woong Park"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/4eda7387222780e4cb874f0bbed2fb795c42efb8",
      "pdf_url": "",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "06b98537324dbf11c7de2040e519b4d110f5d622",
      "title": "On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses",
      "abstract": "Neural networks are known to be vulnerable to adversarial examples. In this note, we evaluate the two white-box defenses that appeared at CVPR 2018 and find they are ineffective: when applying existing techniques, we can reduce the accuracy of the defended models to 0%.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "Anish Athalye",
        "Nicholas Carlini"
      ],
      "citation_count": 173,
      "url": "https://www.semanticscholar.org/paper/06b98537324dbf11c7de2040e519b4d110f5d622",
      "pdf_url": "",
      "publication_date": "2018-04-10",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af7eb7c0fe179888e6079545e06491ffb73f0405",
      "title": "Adversarial Example Detection by Classification for Deep Speech Recognition",
      "abstract": "Machine Learning systems are vulnerable to adversarial attacks and will highly likely produce incorrect outputs under these attacks. There are white-box and black-box attacks regarding to adversary\u2019s access level to the victim learning algorithm. To defend the learning systems from these attacks, existing methods in the speech domain focus on modifying input signals and testing the behaviours of speech recognizers. We, however, formulate the defense as a classification problem and present a strategy for systematically generating adversarial example datasets: one for white-box attacks and one for black-box attacks, containing both adversarial and normal examples. The white-box attack is a gradient-based method on Baidu DeepSpeech with the Mozilla Common Voice database while the black-box attack is a gradient-free method on a deep model-based keyword spotting system with the Google Speech Command dataset. The generated datasets are used to train a proposed Convolutional Neural Network (CNN), together with cepstral features, to detect adversarial examples. Experimental results show that, it is possible to accurately distinct between adversarial and normal examples for known attacks, in both single-condition and multi-condition training settings, while the performance degrades dramatically for unknown attacks. The adversarial datasets and the source code are made publicly available.",
      "year": 2019,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Saeid Samizade",
        "Zheng-Hua Tan",
        "Chao Shen",
        "X. Guan"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/af7eb7c0fe179888e6079545e06491ffb73f0405",
      "pdf_url": "https://arxiv.org/pdf/1910.10013.pdf",
      "publication_date": "2019-10-22",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95239030dcb8d9213b77cf04b03f2e14a900e08b",
      "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification",
      "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper, we consider the adversarial detection problem under the robust optimization framework. We partition the input space into subspaces and train adversarial robust subspace detectors using asymmetrical adversarial training (AAT). The integration of the classifier and detectors presents a detection mechanism that provides a performance guarantee to the adversary it considered. We demonstrate that AAT promotes the learning of class-conditional distributions, which further gives rise to generative detection/classification approaches that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Xuwang Yin",
        "Soheil Kolouri",
        "G. Rohde"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/95239030dcb8d9213b77cf04b03f2e14a900e08b",
      "pdf_url": "",
      "publication_date": "2019-05-27",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4d8b87760bd35ac26e18354c6f4ff06cfdc36ddc",
      "title": "A survey of practical adversarial example attacks",
      "abstract": "Adversarial examples revealed the weakness of machine learning techniques in terms of robustness, which moreover inspired adversaries to make use of the weakness to attack systems employing machine learning. Existing researches covered the methodologies of adversarial example generation, the root reason of the existence of adversarial examples, and some defense schemes. However practical attack against real world systems did not appear until recent, mainly because of the difficulty in injecting a artificially generated example into the model behind the hosting system without breaking the integrity. Recent case study works against face recognition systems and road sign recognition systems finally abridged the gap between theoretical adversarial example generation methodologies and practical attack schemes against real systems. To guide future research in defending adversarial examples in the real world, we formalize the threat model for practical attacks with adversarial examples, and also analyze the restrictions and key procedures for launching real world adversarial example attacks.",
      "year": 2018,
      "venue": "Cybersecur.",
      "authors": [
        "Lu Sun",
        "Mingtian Tan",
        "Zhe Zhou"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/4d8b87760bd35ac26e18354c6f4ff06cfdc36ddc",
      "pdf_url": "https://cybersecurity.springeropen.com/track/pdf/10.1186/s42400-018-0012-9",
      "publication_date": "2018-09-06",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9108ac69fe4ceb756e54ec490ed9abb86e40f8ba",
      "title": "Multi-Targeted Adversarial Example in Evasion Attack on Deep Neural Network",
      "abstract": "Deep neural networks (DNNs) are widely used for image recognition, speech recognition, pattern analysis, and intrusion detection. Recently, the adversarial example attack, in which the input data are only slightly modified, although not an issue for human interpretation, is a serious threat to a DNN as an attack as it causes the machine to misinterpret the data. The adversarial example attack has been receiving considerable attention owing to its potential threat to machine learning. It is divided into two categories: targeted adversarial example and untargeted adversarial example. The untargeted adversarial example happens when machines misclassify an object into an incorrect class. In contrast, the targeted adversarial example attack causes machines to misinterpret the image as the attacker\u2019s desired class. Thus, the latter is a more elaborate and powerful attack than the former. The existing targeted adversarial example is a single targeted attack that allows only one class to be recognized. However, in some cases, a multi-targeted adversarial example can be useful for an attacker to make multiple models recognize a single original image as different classes. For example, an attacker can use a single road sign generated by a multi-targeted adversarial example scheme to make model A recognize it as a stop sign and model B recognize it as a left turn, whereas a human might recognize it as a right turn. Therefore, in this paper, we propose a multi-targeted adversarial example that attacks multiple models within each target class with a single modified image. To produce such examples, we carried out a transformation to maximize the probability of different target classes by multiple models. We used the MNIST datasets and TensorFlow library for our experiment. The experimental results showed that the proposed scheme for generating a multi-targeted adversarial example achieved a 100% attack success rate.",
      "year": 2018,
      "venue": "IEEE Access",
      "authors": [
        "Hyun Kwon",
        "Yongchul Kim",
        "Ki-Woong Park",
        "H. Yoon",
        "D. Choi"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/9108ac69fe4ceb756e54ec490ed9abb86e40f8ba",
      "pdf_url": "https://doi.org/10.1109/access.2018.2866197",
      "publication_date": "2018-08-20",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ff8c2c2d486e9de56aae115ce75a37587dd8aa2",
      "title": "Adversarial Example Detection and Classification With Asymmetrical Adversarial Training",
      "abstract": null,
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Xuwang Yin",
        "Soheil Kolouri",
        "G. Rohde"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/7ff8c2c2d486e9de56aae115ce75a37587dd8aa2",
      "pdf_url": "",
      "publication_date": "2019-05-27",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "74ae9bff0f3ddc8b2ae19bcd27fe94687e19210c",
      "title": "Restricted Evasion Attack: Generation of Restricted-Area Adversarial Example",
      "abstract": "Deep neural networks (DNNs) show superior performance in image and speech recognition. However, adversarial examples created by adding a little noise to an original sample can lead to misclassification by a DNN. Conventional studies on adversarial examples have focused on ways of causing misclassification by a DNN by modulating the entire image. However, in some cases, a restricted adversarial example may be required in which only certain parts of the image are modified rather than the entire image and that results in misclassification by the DNN. For example, when the placement of a road sign has already been completed, an attack may be required that will change only a specific part of the sign, such as by placing a sticker on it, to cause misidentification of the entire image. As another example, an attack may be required that causes a DNN to misinterpret images according to a minimal modulation of the outside border of the image. In this paper, we propose a new restricted adversarial example that modifies only a restricted area to cause misclassification by a DNN while minimizing distortion from the original sample. It can also select the size of the restricted area. We used the CIFAR10 and ImageNet datasets to evaluate the performance. We measured the attack success rate and distortion of the restricted adversarial example while adjusting the size, shape, and position of the restricted area. The results show that the proposed scheme generates restricted adversarial examples with a 100% attack success rate in a restricted area of the whole image (approximately 14% for CIFAR10 and 1.07% for ImageNet) while minimizing the distortion distance.",
      "year": 2019,
      "venue": "IEEE Access",
      "authors": [
        "Hyun Kwon",
        "H. Yoon",
        "D. Choi"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/74ae9bff0f3ddc8b2ae19bcd27fe94687e19210c",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08710245.pdf",
      "publication_date": "2019-05-09",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e64f2ae7f44ef07aec01448b1ca8e5e039364cd6",
      "title": "Weighted-Sampling Audio Adversarial Example Attack",
      "abstract": "Recent studies have highlighted audio adversarial examples as a ubiquitous threat to state-of-the-art automatic speech recognition systems. Thorough studies on how to effectively generate adversarial examples are essential to prevent potential attacks. Despite many research on this, the efficiency and the robustness of existing works are not yet satisfactory. In this paper, we propose weighted-sampling audio adversarial examples, focusing on the numbers and the weights of distortion to reinforce the attack. Further, we apply a denoising method in the loss function to make the adversarial attack more imperceptible. Experiments show that our method is the first in the field to generate audio adversarial examples with low noise and high audio robustness at the minute time-consuming level 1.",
      "year": 2019,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaolei Liu",
        "Kun Wan",
        "Yufei Ding",
        "Xiaosong Zhang",
        "Qingxin Zhu"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/e64f2ae7f44ef07aec01448b1ca8e5e039364cd6",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/5928/5784",
      "publication_date": "2019-01-26",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "caf9352e3a795313052d132b15a6255bbb885791",
      "title": "Random Untargeted Adversarial Example on Deep Neural Network",
      "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance in machine learning areas such as image recognition, speech recognition, intrusion detection, and pattern analysis. However, it has been revealed that DNNs have weaknesses in the face of adversarial examples, which are created by adding a little noise to an original sample to cause misclassification by the DNN. Such adversarial examples can lead to fatal accidents in applications such as autonomous vehicles and disease diagnostics. Thus, the generation of adversarial examples has attracted extensive research attention recently. An adversarial example is categorized as targeted or untargeted. In this paper, we focus on the untargeted adversarial example scenario because it has a faster learning time and less distortion compared with the targeted adversarial example. However, there is a pattern vulnerability with untargeted adversarial examples: Because of the similarity between the original class and certain specific classes, it may be possible for the defending system to determine the original class by analyzing the output classes of the untargeted adversarial examples. To overcome this problem, we propose a new method for generating untargeted adversarial examples, one that uses an arbitrary class in the generation process. Moreover, we show that our proposed scheme can be applied to steganography. Through experiments, we show that our proposed scheme can achieve a 100% attack success rate with minimum distortion (1.99 and 42.32 using the MNIST and CIFAR10 datasets, respectively) and without the pattern vulnerability. Using a steganography test, we show that our proposed scheme can be used to fool humans, as demonstrated by the probability of their detecting hidden classes being equal to that of random selection.",
      "year": 2018,
      "venue": "Symmetry",
      "authors": [
        "Hyun Kwon",
        "Yongchul Kim",
        "H. Yoon",
        "D. Choi"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/caf9352e3a795313052d132b15a6255bbb885791",
      "pdf_url": "https://www.mdpi.com/2073-8994/10/12/738/pdf?version=1545130619",
      "publication_date": "2018-12-10",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "adbf5b414019510cb2d22ff166c36e413cf6d866",
      "title": "Adversarial Example Generation",
      "abstract": "Deep Neural Networks have achieved remarkable success in computer vision, and audio tasks, etc. However, in classification domains, deep neural models are easily fooled by adversarial examples. Many attack methods generate adversarial examples with large image distortion and low similarity between origin and corresponding adversarial examples, to address these issues, we propose an adversarial method with an adaptive gradient in a direction to generate perturbations, it generates perturbations which can escape local minimal. In this paper, we evaluate several traditional perturbations creating methods in image classification with ours. Experimental results show that our approach works well and outperform recent techniques in the change of misclassifying image classification, and excellent efficiency in fooling deep network models.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Yatie Xiao",
        "Chi-Man Pun"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/adbf5b414019510cb2d22ff166c36e413cf6d866",
      "pdf_url": "",
      "publication_date": "2019-02-01",
      "keywords_matched": [
        "adversarial example"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bbdd22096277d3592a8aa326b93b270ae8d22735",
      "title": "When Deep Learning-Based Soft Sensors Encounter Reliability Challenges: A Practical Knowledge-Guided Adversarial Attack and Its Defense",
      "abstract": "Deep learning-based soft sensors (DLSSs) have been demonstrated to exhibit significantly improved sensing accuracy; however, their vulnerability to adversarial attacks affects their reliability, thus hindering their widespread application. To improve the reliability of DLSSs, in this article, we conducted a systematic investigation of the adversarial attack and defense of DLSSs. By considering the task requirements of DLSSs and the actual scenarios that attackers may encounter, a framework based on black-box attack and proactive defense was proposed to realize the adversarial attack and defense of soft sensors. The adversarial attack was implemented through the proposed knowledge-guided adversarial attack (KGAA) method. By reconstructing the optimization model and introducing the mechanism knowledge into the objective function, the KGAA method could overcome the ill-posed problem of adversarial attack optimization when attacking a regression model. Moreover, based on the KGAA, a corresponding KGAA adversarial training defense method was proposed to achieve proactive defense. The attack and defense methods were verified in terms of the thermal deformation sensing of an air preheater rotor. Compared to other attacks, the KGAA exhibited higher imperceptibility, rationality, and stability; it can thus be considered a practical attack. The implementation of KGAA adversarial training enhances the adversarial robustness of DLSSs, thus aiding the defense of DLSSs to various attacks and improving their reliability.",
      "year": 2024,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Runyuan Guo",
        "Han Liu",
        "Ding Liu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/bbdd22096277d3592a8aa326b93b270ae8d22735",
      "pdf_url": "",
      "publication_date": "2024-02-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8733fe2371b615609b04e2e910b1ecfa8e77cbc2",
      "title": "Square Attack: a query-efficient black-box adversarial attack via random search",
      "abstract": "We propose the Square Attack, a score-based black-box $l_2$- and $l_\\infty$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least $1.8$ and up to $3$ compared to the recent state-of-the-art $l_\\infty$-attack of Al-Dujaili & O'Reilly. Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at this https URL.",
      "year": 2019,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Maksym Andriushchenko",
        "Francesco Croce",
        "Nicolas Flammarion",
        "Matthias Hein"
      ],
      "citation_count": 1155,
      "url": "https://www.semanticscholar.org/paper/8733fe2371b615609b04e2e910b1ecfa8e77cbc2",
      "pdf_url": "",
      "publication_date": "2019-11-29",
      "keywords_matched": [
        "adversarial attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
      "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
      "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xilie Xu",
        "Keyi Kong",
        "Ninghao Liu",
        "Li-zhen Cui",
        "Di Wang",
        "Jingfeng Zhang",
        "Mohan S. Kankanhalli"
      ],
      "citation_count": 125,
      "url": "https://www.semanticscholar.org/paper/df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
      "pdf_url": "",
      "publication_date": "2023-10-20",
      "keywords_matched": [
        "adversarial attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b14617abc61ca8c3d02f1d44eddcd11365ce61e9",
      "title": "Diffusion Models for Imperceptible and Transferable Adversarial Attack",
      "abstract": "Many existing adversarial attacks generate <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"shi-ieq1-3480519.gif\"/></alternatives></inline-formula>-norm perturbations on image RGB space. Despite some achievements in transferability and attack success rate, the crafted adversarial examples are easily perceived by human eyes. Towards visual imperceptibility, some recent works explore unrestricted attacks without <inline-formula><tex-math notation=\"LaTeX\">$L_{p}$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"shi-ieq2-3480519.gif\"/></alternatives></inline-formula>-norm constraints, yet lacking transferability of attacking black-box models. In this work, we propose a novel imperceptible and transferable attack by leveraging both the generative and discriminative power of diffusion models. Specifically, instead of direct manipulation in pixel space, we craft perturbations in the latent space of diffusion models. Combined with well-designed content-preserving structures, we can generate human-insensitive perturbations embedded with semantic clues. For better transferability, we further \u201cdeceive\u201d the diffusion model which can be viewed as an implicit recognition surrogate, by distracting its attention away from the target regions. To our knowledge, our proposed method, <italic>DiffAttack</italic>, is the first that introduces diffusion models into the adversarial attack field. Extensive experiments conducted across diverse model architectures (CNNs, Transformers, and MLPs), datasets (ImageNet, CUB-200, and Standford Cars), and defense mechanisms underscore the superiority of our attack over existing methods such as iterative attacks, GAN-based attacks, and ensemble attacks. Furthermore, we provide a comprehensive discussion on future research avenues in diffusion-based adversarial attacks, aiming to chart a course for this burgeoning field.",
      "year": 2023,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Jianqi Chen",
        "H. Chen",
        "Keyan Chen",
        "Yilan Zhang",
        "Zhengxia Zou",
        "Z. Shi"
      ],
      "citation_count": 118,
      "url": "https://www.semanticscholar.org/paper/b14617abc61ca8c3d02f1d44eddcd11365ce61e9",
      "pdf_url": "https://arxiv.org/pdf/2305.08192",
      "publication_date": "2023-05-14",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7f77058976e2fe75e98280371962c43d98c98321",
      "title": "Adversarial Attack on Graph Structured Data",
      "abstract": "Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",
      "year": 2018,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "H. Dai",
        "Hui Li",
        "Tian Tian",
        "Xin Huang",
        "L. Wang",
        "Jun Zhu",
        "Le Song"
      ],
      "citation_count": 841,
      "url": "https://www.semanticscholar.org/paper/7f77058976e2fe75e98280371962c43d98c98321",
      "pdf_url": "",
      "publication_date": "2018-06-06",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1202e36a30a3bec484aa7a9c8f1ccc39c483db2",
      "title": "Survey on Adversarial Attack and Defense for Medical Image Analysis: Methods and Challenges",
      "abstract": "Deep learning techniques have achieved superior performance in computer-aided medical image analysis, yet they are still vulnerable to imperceptible adversarial attacks, resulting in potential misdiagnosis in clinical practice. Oppositely, recent years have also witnessed remarkable progress in defense against these tailored adversarial examples in deep medical diagnosis systems. In this exposition, we present a comprehensive survey on recent advances in adversarial attacks and defenses for medical image analysis with a systematic taxonomy in terms of the application scenario. We also provide a unified framework for different types of adversarial attack and defense methods in the context of medical image analysis. For a fair comparison, we establish a new benchmark for adversarially robust medical diagnosis models obtained by adversarial training under various scenarios. To the best of our knowledge, this is the first survey article that provides a thorough evaluation of adversarially robust medical diagnosis models. By analyzing qualitative and quantitative results, we conclude this survey with a detailed discussion of current challenges for adversarial attack and defense in medical image analysis systems to shed light on future research directions. Code is available on GitHub.",
      "year": 2023,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Junhao Dong",
        "Junxi Chen",
        "Xiaohua Xie",
        "Jianhuang Lai",
        "H. Chen"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/f1202e36a30a3bec484aa7a9c8f1ccc39c483db2",
      "pdf_url": "",
      "publication_date": "2023-03-24",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b4bbaf4f040fd3c2fd0491d750ce6b96c517c615",
      "title": "Adversarial Attack Mitigation Strategy for Machine Learning-Based Network Attack Detection Model in Power System",
      "abstract": "The network attack detection model based on machine learning (ML) has received extensive attention and research in PMU measurement data protection of power systems. However, well-trained ML-based detection models are vulnerable to adversarial attacks. By adding meticulously designed perturbations to the original data, the attacker can significantly decrease the accuracy and reliability of the model, causing the control center to receive unreliable PMU measurement data. This paper takes the network attack detection model in the power system as a case study to analyze the vulnerability of the ML-based detection model under adversarial attacks. And then, a mitigation strategy for adversarial attacks based on causal theory is proposed, which can enhance the robustness of the detection model under different adversarial attack scenarios. Unlike adversarial training, this mitigation strategy does not require adversarial samples to train models, saving computing resources. Furthermore, the strategy only needs a small amount of detection model information and can be migrated to various models. Simulation experiments on the IEEE node systems verify the threat of adversarial attacks against different ML-based detection models and the effectiveness of the proposed mitigation strategy.",
      "year": 2023,
      "venue": "IEEE Transactions on Smart Grid",
      "authors": [
        "Rong Huang",
        "Yuancheng Li"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/b4bbaf4f040fd3c2fd0491d750ce6b96c517c615",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ffb40ed2bd99e032eff6d9d15291a9575ad3dfc",
      "title": "LESSON: Multi-Label Adversarial False Data Injection Attack for Deep Learning Locational Detection",
      "abstract": "Deep learning methods can not only detect false data injection attacks (FDIA) but also locate attacks of FDIA. Although adversarial false data injection attacks (AFDIA) based on deep learning vulnerabilities have been studied in the field of single-label FDIA detection, the adversarial attack and defense against multi-label FDIA locational detection are still not involved. To bridge this gap, this paper first explores the multi-label adversarial example attacks against multi-label FDIA locational detectors and proposes a general multi-label adversarial attack framework, namely muLti-labEl adverSarial falSe data injectiON attack (LESSON). The proposed LESSON attack framework includes three key designs, namely Perturbing State Variables, Tailored Loss Function Design, and Change of Variables, which can help find suitable multi-label adversarial perturbations within the physical constraints to circumvent both Bad Data Detection (BDD) and Neural Attack Location (NAL). Four typical LESSON attacks based on the proposed framework and two dimensions of attack objectives are examined, and the experimental results demonstrate the effectiveness of the proposed attack framework, posing serious and pressing security concerns in smart grids.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Jiwei Tian",
        "Chao Shen",
        "Buhong Wang",
        "Xiaofang Xia",
        "Meng Zhang",
        "Chenhao Lin",
        "Qian Li"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/7ffb40ed2bd99e032eff6d9d15291a9575ad3dfc",
      "pdf_url": "",
      "publication_date": "2024-01-29",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a73366d88ff966d0e7bf232c9a12ba2a21381460",
      "title": "A Survey of Adversarial Attack and Defense Methods for Malware Classification in Cyber Security",
      "abstract": "Malware poses a severe threat to cyber security. Attackers use malware to achieve their malicious purposes, such as unauthorized access, stealing confidential data, blackmailing, etc. Machine learning-based defense methods are applied to classify malware examples. However, such methods are vulnerable to adversarial attacks, where attackers aim to generate adversarial examples that can evade detection. Defenders also develop various approaches to enhance the robustness of malware classifiers against adversarial attacks. Both attackers and defenders evolve in the continuous confrontation of malware classification. In this paper, we firstly summarize a unified malware classification framework. Then, based on the framework, we systematically survey the Defense-Attack-Enhanced-Defense process and provide a comprehensive review of (i) machine learning-based malware classification, (ii) adversarial attacks on malware classifiers, and (iii) robust malware classification. Finally, we highlight the main challenges faced by both attackers and defenders and discuss some promising future work directions.",
      "year": 2023,
      "venue": "IEEE Communications Surveys and Tutorials",
      "authors": [
        "Senming Yan",
        "Jing Ren",
        "Wei Wang",
        "Limin Sun",
        "W. Zhang",
        "Quan Yu"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/a73366d88ff966d0e7bf232c9a12ba2a21381460",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "531549b87297b6489e8b9e310ca86e368acb1d35",
      "title": "Average Gradient-Based Adversarial Attack",
      "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks which can fool the classifiers by adding small perturbations to the original example. The added perturbations in most existing attacks are mainly determined by the gradient of the loss function with respect to the current example. In this paper, a new average gradient-based adversarial attack is proposed. In our proposed method, via utilizing the gradient of each iteration in the past, a dynamic set of adversarial examples is constructed first in each iteration. Then, according to the gradient of the loss function with respect to all the examples in the constructed dynamic set and the current adversarial example, the average gradient can be calculated, which is used to determine the added perturbations. Different from the existing adversarial attacks, the proposed average gradient-based attack optimizes the added perturbations through a dynamic set of adversarial examples, where the size of the dynamic set increases with the number of iterations. Our proposed method possesses good extensibility and can be integrated into most existing gradient-based attacks. Extensive experiments demonstrate that, compared with the state-of-the-art gradient-based adversarial attacks, the proposed attack can achieve higher attack success rates and exhibit better transferability, which is helpful to evaluate the robustness of the network and the effectiveness of the defense method.",
      "year": 2023,
      "venue": "IEEE transactions on multimedia",
      "authors": [
        "Chen Wan",
        "Fangjun Huang",
        "Xianfeng Zhao"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/531549b87297b6489e8b9e310ca86e368acb1d35",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8275b605a1363bbb4e19028c5b3bd6ad4ce25c99",
      "title": "Discrete Adversarial Attack to Models of Code",
      "abstract": "The pervasive brittleness of deep neural networks has attracted significant attention in recent years. A particularly interesting finding is the existence of adversarial examples, imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art neural models. In this paper, we study a different type of adversarial examples specific to code models, called discrete adversarial examples, which are created through program transformations that preserve the semantics of original inputs.In particular, we propose a novel, general method that is highly effective in attacking a broad range of code models. From the defense perspective, our primary contribution is a theoretical foundation for the application of adversarial training \u2014 the most successful algorithm for training robust classifiers \u2014 to defending code models against discrete adversarial attack. Motivated by the theoretical results, we present a simple realization of adversarial training that substantially improves the robustness of code models against adversarial attacks in practice. We extensively evaluate both our attack and defense methods. Results show that our discrete attack is significantly more effective than state-of-the-art whether or not defense mechanisms are in place to aid models in resisting attacks. In addition, our realization of adversarial training improves the robustness of all evaluated models by the widest margin against state-of-the-art adversarial attacks as well as our own.",
      "year": 2023,
      "venue": "Proc. ACM Program. Lang.",
      "authors": [
        "Fengjuan Gao",
        "Yu Wang",
        "Ke Wang"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/8275b605a1363bbb4e19028c5b3bd6ad4ce25c99",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3591227",
      "publication_date": "2023-06-06",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dad518125f6297ebfd9f3af2e5700997cb110b32",
      "title": "Adversarial Attack with Raindrops",
      "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial examples, which are usually designed artificially to fool DNNs, but rarely exist in real-world scenarios. In this paper, we study the adversarial examples caused by raindrops, to demonstrate that there exist plenty of natural phenomena being able to work as adversarial attackers to DNNs. Moreover, we present a new approach to generate adversarial raindrops, denoted as AdvRD, using the generative adversarial network (GAN) technique to simulate natural raindrops. The images crafted by our AdvRD look very similar to the real-world raindrop images, statistically close to the distribution of true raindrop images, and more importantly, can perform strong adversarial attack to the state-of-the-art DNN models. On the other side, we show that the adversarial training using our AdvRD images can significantly improve the robustness of DNNs to the real-world raindrop attacks. Extensive experiments are carried out to demonstrate that the images crafted by AdvRD are visually and statistically close to the natural raindrop images, can work as strong attackers to DNN models, and also help improve the robustness of DNNs to raindrop attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jiyuan Liu",
        "Bingyi Lu",
        "Mingkang Xiong",
        "T. Zhang",
        "Huilin Xiong"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/dad518125f6297ebfd9f3af2e5700997cb110b32",
      "pdf_url": "https://arxiv.org/pdf/2302.14267",
      "publication_date": "2023-02-28",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4d46be305392998365cb7e9ad0304bf8996ad7fe",
      "title": "Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon",
      "abstract": "Estimating the risk level of adversarial examples is essential for safely deploying machine learning models in the real world. One popular approach for physical-world attacks is to adopt the \u201csticker-pasting\u201d strategy, which however suffers from some limitations, including difficulties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artificial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily noticed by humans. In this paper, we study a new type of optical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adversarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simulated and real-world environments. Experimental results on traffic sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets respectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mechanism of this attack11Our code is available at https://github.com/hncszyq/ShadowAttack.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yiqi Zhong",
        "Xianming Liu",
        "Deming Zhai",
        "Junjun Jiang",
        "Xiangyang Ji"
      ],
      "citation_count": 182,
      "url": "https://www.semanticscholar.org/paper/4d46be305392998365cb7e9ad0304bf8996ad7fe",
      "pdf_url": "https://arxiv.org/pdf/2203.03818",
      "publication_date": "2022-03-08",
      "keywords_matched": [
        "adversarial attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "472cd41fa2ba2e520706f232cae12db4a7b5e60a",
      "title": "Contextualized Perturbation for Textual Adversarial Attack",
      "abstract": "Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.",
      "year": 2020,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Dianqi Li",
        "Yizhe Zhang",
        "Hao Peng",
        "Liqun Chen",
        "Chris Brockett",
        "Ming-Ting Sun",
        "Bill Dolan"
      ],
      "citation_count": 261,
      "url": "https://www.semanticscholar.org/paper/472cd41fa2ba2e520706f232cae12db4a7b5e60a",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.400.pdf",
      "publication_date": "2020-09-16",
      "keywords_matched": [
        "adversarial attack",
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c6e7cae95425c6a1e2362d61c3ae9948fd27b72f",
      "title": "Adversarial Attack and Defense: A Survey",
      "abstract": "In recent years, artificial intelligence technology represented by deep learning has achieved remarkable results in image recognition, semantic analysis, natural language processing and other fields. In particular, deep neural networks have been widely used in different security-sensitive tasks. Fields, such as facial payment, smart medical and autonomous driving, which accelerate the construction of smart cities. Meanwhile, in order to fully unleash the potential of edge big data, there is an urgent need to push the AI frontier to the network edge. Edge AI, the combination of artificial intelligence and edge computing, supports the deployment of deep learning algorithms to edge devices that generate data, and has become a key driver of smart city development. However, the latest research shows that deep neural networks are vulnerable to attacks from adversarial example and output wrong results. This type of attack is called adversarial attack, which greatly limits the promotion of deep neural networks in tasks with extremely high security requirements. Due to the influence of adversarial attacks, researchers have also begun to pay attention to the research in the field of adversarial defense. In the game process of adversarial attacks and defense technologies, both attack and defense technologies have been developed rapidly. This article first introduces the principles and characteristics of adversarial attacks, and summarizes and analyzes the adversarial example generation methods in recent years. Then, it introduces the adversarial example defense technology in detail from the three directions of model, data, and additional network. Finally, combined with the current status of adversarial example generation and defense technology development, put forward challenges and prospects in this field.",
      "year": 2022,
      "venue": "Electronics",
      "authors": [
        "Hongshuo Liang",
        "Erlu He",
        "Yang Zhao",
        "Zhengchang Jia",
        "Hao Li"
      ],
      "citation_count": 96,
      "url": "https://www.semanticscholar.org/paper/c6e7cae95425c6a1e2362d61c3ae9948fd27b72f",
      "pdf_url": "https://www.mdpi.com/2079-9292/11/8/1283/pdf?version=1650278650",
      "publication_date": "2022-04-18",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "94e6dcf908ca9029ae9ddf0b120642a4140f8a71",
      "title": "SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness",
      "abstract": "Deep neural network-based image classifications are vulnerable to adversarial perturbations. The image classifications can be easily fooled by adding artificial small and imperceptible perturbations to input images. As one of the most effective defense strategies, adversarial training was proposed to address the vulnerability of classification models, where the adversarial examples are created and injected into training data during training. The attack and defense of classification models have been intensively studied in past years. Semantic segmentation, as an extension of classifications, has also received great attention recently. Recent work shows a large number of attack iterations are required to create effective adversarial examples to fool segmentation models. The observation makes both robustness evaluation and adversarial training on segmentation models challenging. In this work, we propose an effective and efficient segmentation attack method, dubbed SegPGD. Besides, we provide a convergence analysis to show the proposed SegPGD can create more effective adversarial examples than PGD under the same number of attack iterations. Furthermore, we propose to apply our SegPGD as the underlying attack method for segmentation adversarial training. Since SegPGD can create more effective adversarial examples, the adversarial training with our SegPGD can boost the robustness of segmentation models. Our proposals are also verified with experiments on popular Segmentation model architectures and standard segmentation datasets.",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Jindong Gu",
        "Hengshuang Zhao",
        "Volker Tresp",
        "Philip H. S. Torr"
      ],
      "citation_count": 90,
      "url": "https://www.semanticscholar.org/paper/94e6dcf908ca9029ae9ddf0b120642a4140f8a71",
      "pdf_url": "https://arxiv.org/pdf/2207.12391",
      "publication_date": "2022-07-25",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2a89dbafb45e4f811114685a6327558bdb4d1141",
      "title": "Physical Adversarial Attack Meets Computer Vision: A Decade Survey",
      "abstract": "Despite the impressive achievements of Deep Neural Networks (DNNs) in computer vision, their vulnerability to adversarial attacks remains a critical concern. Extensive research has demonstrated that incorporating sophisticated perturbations into input images can lead to a catastrophic degradation in DNNs\u2019 performance. This perplexing phenomenon not only exists in the digital space but also in the physical world. Consequently, it becomes imperative to evaluate the security of DNNs-based systems to ensure their safe deployment in real-world scenarios, particularly in security-sensitive applications. To facilitate a profound understanding of this topic, this paper presents a comprehensive overview of physical adversarial attacks. First, we distill four general steps for launching physical adversarial attacks. Building upon this foundation, we uncover the pervasive role of artifacts carrying adversarial perturbations in the physical world. These artifacts influence each step. To denote them, we introduce a new term: adversarial medium. Then, we take the first step to systematically evaluate the performance of physical adversarial attacks, taking the adversarial medium as a first attempt. Our proposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness, Stealthiness, Robustness, Practicability, Aesthetics, and Economics. We also provide comparative results across task categories, together with insightful observations and suggestions for future research directions.",
      "year": 2022,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Hui Wei",
        "Hao Tang",
        "Xuemei Jia",
        "Han-Bing Yu",
        "Zhubo Li",
        "Zhixiang Wang",
        "Shin\u2019ichi Satoh",
        "Zheng Wang"
      ],
      "citation_count": 109,
      "url": "https://www.semanticscholar.org/paper/2a89dbafb45e4f811114685a6327558bdb4d1141",
      "pdf_url": "https://arxiv.org/pdf/2209.15179",
      "publication_date": "2022-09-30",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a9b56389129c8caeb7719664bbf27fc3185604b5",
      "title": "An Approximated Gradient Sign Method Using Differential Evolution for Black-Box Adversarial Attack",
      "abstract": "Recent studies show that deep neural networks are vulnerable to adversarial attacks in the form of subtle perturbations to the input image, which leads the model to output wrong prediction. Such an attack can easily succeed by the existing white-box attack methods, where the perturbation is calculated based on the gradient of the target network. Unfortunately, the gradient is often unavailable in the real-world scenarios, which makes the black-box adversarial attack problems practical and challenging. In fact, they can be formulated as high-dimensional black-box optimization problems at the pixel level. Although evolutionary algorithms are well known for solving black-box optimization problems, they cannot efficiently deal with the high-dimensional decision space. Therefore, we propose an approximated gradient sign method using differential evolution (DE) for solving black-box adversarial attack problems. Unlike most existing methods, it is novel that the proposed method searches the gradient sign rather than the perturbation by a DE algorithm. Also, we transform the pixel-based decision space into a dimension-reduced decision space by combining the pixel differences from the input image to neighbor images, and two different techniques for selecting neighbor images are introduced to build the transferred decision space. In addition, six variants of the proposed method are designed according to the different neighborhood selection and optimization search strategies. Finally, the performance of the proposed method is compared with a number of the state-of-the-art adversarial attack algorithms on CIFAR-10 and ImageNet datasets. The experimental results suggest that the proposed method shows superior performance for solving black-box adversarial attack problems, especially nontargeted attack problems.",
      "year": 2022,
      "venue": "IEEE Transactions on Evolutionary Computation",
      "authors": [
        "C. Li",
        "Handing Wang",
        "Jun Zhang",
        "W. Yao",
        "Tingsong Jiang"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/a9b56389129c8caeb7719664bbf27fc3185604b5",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f1e61051e6b0568fe36ec3dcf8a42cd0447ff1c",
      "title": "Adversarial Attack and Defense Strategies of Speaker Recognition Systems: A Survey",
      "abstract": "Speaker recognition is a task that identifies the speaker from multiple audios. Recently, advances in deep learning have considerably boosted the development of speech signal processing techniques. Speaker or speech recognition has been widely adopted in such applications as smart locks, smart vehicle-mounted systems, and financial services. However, deep neural network-based speaker recognition systems (SRSs) are susceptible to adversarial attacks, which fool the system to make wrong decisions by small perturbations, and this has drawn the attention of researchers to the security of SRSs. Unfortunately, there is no systematic review work in this domain. In this work, we conduct a comprehensive survey to fill this gap, which includes the development of SRSs, adversarial attacks and defenses against SRSs. Specifically, we first introduce the mainstream frameworks of SRSs and some commonly used datasets. Then, from the perspectives of adversarial example generation and evaluation, we introduce different attack tasks, the prior knowledge of attacks, perturbation objects, perturbation constraints, and attack effect evaluation indicators. Next, we focus on some effective defense strategies, including adversarial training, attack detection, and input refactoring against existing attacks, and analyze their strengths and weaknesses in terms of fidelity and robustness. Finally, we discuss the challenges posed by audio adversarial examples in SRSs and some valuable research topics in the future.",
      "year": 2022,
      "venue": "Electronics",
      "authors": [
        "Hao Tan",
        "Le Wang",
        "Huan Zhang",
        "Junjian Zhang",
        "Muhammad Shafiq",
        "Zhaoquan Gu"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/4f1e61051e6b0568fe36ec3dcf8a42cd0447ff1c",
      "pdf_url": "https://www.mdpi.com/2079-9292/11/14/2183/pdf?version=1657683477",
      "publication_date": "2022-07-12",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "220fa769af995f47a71132c0140fc1fc52085790",
      "title": "A Survey on Physical Adversarial Attack in Computer Vision",
      "abstract": "Over the past decade, deep learning has revolutionized conventional tasks that rely on hand-craft feature extraction with its strong feature learning capability, leading to substantial enhancements in traditional tasks. However, deep neural networks (DNNs) have been demonstrated to be vulnerable to adversarial examples crafted by malicious tiny noise, which is imperceptible to human observers but can make DNNs output the wrong result. Existing adversarial attacks can be categorized into digital and physical adversarial attacks. The former is designed to pursue strong attack performance in lab environments while hardly remaining effective when applied to the physical world. In contrast, the latter focus on developing physical deployable attacks, thus exhibiting more robustness in complex physical environmental conditions. Recently, with the increasing deployment of the DNN-based system in the real world, strengthening the robustness of these systems is an emergency, while exploring physical adversarial attacks exhaustively is the precondition. To this end, this paper reviews the evolution of physical adversarial attacks against DNN-based computer vision tasks, expecting to provide beneficial information for developing stronger physical adversarial attacks. Specifically, we first proposed a taxonomy to categorize the current physical adversarial attacks and grouped them. Then, we discuss the existing physical attacks and focus on the technique for improving the robustness of physical attacks under complex physical environmental conditions. Finally, we discuss the issues of the current physical adversarial attacks to be solved and give promising directions.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Donghua Wang",
        "Wen Yao",
        "Tingsong Jiang",
        "Guijian Tang",
        "Xiaoqian Chen"
      ],
      "citation_count": 49,
      "url": "https://www.semanticscholar.org/paper/220fa769af995f47a71132c0140fc1fc52085790",
      "pdf_url": "https://arxiv.org/pdf/2209.14262",
      "publication_date": "2022-09-28",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85af65c9355dc1ff8585354e014543751b498ee3",
      "title": "Imperceptible Adversarial Attack via Invertible Neural Networks",
      "abstract": "Adding perturbations via utilizing auxiliary gradient information or discarding existing details of the benign images are two common approaches for generating adversarial examples. Though visual imperceptibility is the desired property of adversarial examples, conventional adversarial attacks still generate traceable adversarial perturbations. In this paper, we introduce a novel Adversarial Attack via Invertible Neural Networks (AdvINN) method to produce robust and imperceptible adversarial examples. Specifically, AdvINN fully takes advantage of the information preservation property of Invertible Neural Networks and thereby generates adversarial examples by simultaneously adding class-specific semantic information of the target class and dropping discriminant information of the original class. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that the proposed AdvINN method can produce less imperceptible adversarial images than the state-of-the-art methods and AdvINN yields more robust adversarial examples with high confidence compared to other adversarial attacks. Code is available at https://github.com/jjhuangcs/AdvINN.",
      "year": 2022,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Zihan Chen",
        "Zifan Wang",
        "Junjie Huang",
        "Wentao Zhao",
        "Xiao Liu",
        "Dejian Guan"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/85af65c9355dc1ff8585354e014543751b498ee3",
      "pdf_url": "http://arxiv.org/pdf/2211.15030",
      "publication_date": "2022-11-28",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "781e7db0a90209734d59c8732fda330e3a171e68",
      "title": "Optical Adversarial Attack",
      "abstract": "We introduce OPtical ADversarial attack (OPAD). OPAD is an adversarial attack in the physical space aiming to fool image classifiers without physically touching the objects (e.g., moving or painting the objects). The principle of OPAD is to use structured illumination to alter the appearance of the target objects. The system consists of a low-cost projector, a camera, and a computer. The challenge of the problem is the non-linearity of the radiometric response of the projector and the spatially varying spectral response of the scene. Attacks generated in a conventional approach do not work in this setting unless they are calibrated to compensate for such a projector-camera model. The proposed solution incorporates the projector-camera model into the adversarial attack optimization, where a new attack formulation is derived. Experimental results prove the validity of the solution. It is demonstrated that OPAD can optically attack a real 3D object in the presence of background lighting for white-box, black-box, targeted, and untargeted attacks. Theoretical analysis is presented to quantify the fundamental performance limit of the system.",
      "year": 2021,
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "authors": [
        "Abhiram Gnanasambandam",
        "A. Sherman",
        "Stanley H. Chan"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/781e7db0a90209734d59c8732fda330e3a171e68",
      "pdf_url": "https://arxiv.org/pdf/2108.06247",
      "publication_date": "2021-08-13",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7cce15b7ac80d5e3e574323367a5cf7bbafd2313",
      "title": "IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking",
      "abstract": "Adversarial attack arises due to the vulnerability of deep neural networks to perceive input samples injected with imperceptible perturbations. Recently, adversarial attack has been applied to visual object tracking to evaluate the robustness of deep trackers. Assuming that the model structures of deep trackers are known, a variety of white-box attack approaches to visual tracking have demonstrated promising results. However, the model knowledge about deep trackers is usually unavailable in real applications. In this paper, we propose a decision-based black-box attack method for visual object tracking. In contrast to existing black-box adversarial attack methods that deal with static images for image classification, we propose IoU attack that sequentially generates perturbations based on the predicted IoU scores from both current and historical frames. By decreasing the IoU scores, the proposed attack method degrades the accuracy of temporal coherent bounding boxes (i.e., object motions) accordingly. In addition, we transfer the learned perturbations to the next few frames to initialize temporal motion attack. We validate the proposed IoU attack on state-of-the-art deep trackers (i.e., detection based, correlation filter based, and long-term trackers). Extensive experiments on the benchmark datasets indicate the effectiveness of the proposed IoU attack method. The source code is available at https://github.com/VISION-SJTU/IoUattack.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Shuai Jia",
        "Yibing Song",
        "Chao Ma",
        "Xiaokang Yang"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/7cce15b7ac80d5e3e574323367a5cf7bbafd2313",
      "pdf_url": "https://arxiv.org/pdf/2103.14938",
      "publication_date": "2021-03-27",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95968b89040146cb015827aee8ff6f77d67bbaf1",
      "title": "OpenAttack: An Open-source Textual Adversarial Attack Toolkit",
      "abstract": "Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack.",
      "year": 2020,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Guoyang Zeng",
        "Fanchao Qi",
        "Qianrui Zhou",
        "Ting Zhang",
        "Bairu Hou",
        "Yuan Zang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/95968b89040146cb015827aee8ff6f77d67bbaf1",
      "pdf_url": "https://aclanthology.org/2021.acl-demo.43.pdf",
      "publication_date": "2020-09-19",
      "keywords_matched": [
        "adversarial attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18939eadc9c4460c8385e0591cde214a1ead067b",
      "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
      "abstract": "The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\\%$, identifying several broken defenses.",
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Francesco Croce",
        "Matthias Hein"
      ],
      "citation_count": 2153,
      "url": "https://www.semanticscholar.org/paper/18939eadc9c4460c8385e0591cde214a1ead067b",
      "pdf_url": "",
      "publication_date": "2020-03-03",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
      "title": "Certified Adversarial Robustness via Randomized Smoothing",
      "abstract": "We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
      "year": 2019,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Jeremy M. Cohen",
        "Elan Rosenfeld",
        "J. Z. Kolter"
      ],
      "citation_count": 2269,
      "url": "https://www.semanticscholar.org/paper/f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
      "pdf_url": "",
      "publication_date": "2019-02-08",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f27fc2ea3d3491deded642a5de247d167a03d15",
      "title": "Dissecting Adversarial Robustness of Multimodal LM Agents",
      "abstract": "As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation scripts in a realistic threat model on top of VisualWebArena, a real environment for web agents. To systematically examine the robustness of agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. We find that we can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search. With imperceptible perturbations to a single image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness. An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases the attack success relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are at https://github.com/ChenWu98/agent-attack",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Chen Henry Wu",
        "Jing Yu Koh",
        "Ruslan Salakhutdinov",
        "Daniel Fried",
        "Aditi Raghunathan"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/4f27fc2ea3d3491deded642a5de247d167a03d15",
      "pdf_url": "",
      "publication_date": "2024-06-18",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
      "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models",
      "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness ($\\epsilon=4/255$) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Lin Li",
        "Haoyan Guan",
        "Jianing Qiu",
        "Michael W. Spratling"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/3a391dfd536625e068f3888c817cc6cbe7fcea9c",
      "pdf_url": "http://arxiv.org/pdf/2403.01849",
      "publication_date": "2024-03-04",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2aab97e35c43d961d645e650808d5b052ec180ab",
      "title": "RobustBench: a standardized adversarial robustness benchmark",
      "abstract": "Evaluation of adversarial robustness is often error-prone leading to overestimation of the true robustness of models. While adaptive attacks designed for a particular defense are a way out of this, there are only approximate guidelines on how to perform them. Moreover, adaptive evaluations are highly customized for particular models, which makes it difficult to compare different defenses. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. This requires to impose some restrictions on the admitted models to rule out defenses that only make gradient-based attacks ineffective without improving actual robustness. We evaluate robustness of models for our benchmark with AutoAttack, an ensemble of white- and black-box attacks which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. Our leaderboard, hosted at this http URL, aims at reflecting the current state of the art on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models with possible extensions in the future. Additionally, we open-source the library this http URL that provides unified access to state-of-the-art robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze general trends in $\\ell_p$-robustness and its impact on other tasks such as robustness to various distribution shifts and out-of-distribution detection.",
      "year": 2020,
      "venue": "NeurIPS Datasets and Benchmarks",
      "authors": [
        "Francesco Croce",
        "Maksym Andriushchenko",
        "Vikash Sehwag",
        "Edoardo Debenedetti",
        "Edoardo Debenedetti",
        "M. Chiang",
        "Prateek Mittal",
        "Matthias Hein"
      ],
      "citation_count": 811,
      "url": "https://www.semanticscholar.org/paper/2aab97e35c43d961d645e650808d5b052ec180ab",
      "pdf_url": "",
      "publication_date": "2020-10-19",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
      "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
      "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yunqing Zhao",
        "Tianyu Pang",
        "Chao Du",
        "Xiao Yang",
        "Chongxuan Li",
        "Ngai-Man Cheung",
        "Min Lin"
      ],
      "citation_count": 257,
      "url": "https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
      "pdf_url": "http://arxiv.org/pdf/2305.16934",
      "publication_date": "2023-05-26",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "764eff31d9596033859895d9513b838d2c57a6fb",
      "title": "Improving Adversarial Robustness Requires Revisiting Misclassified Examples",
      "abstract": null,
      "year": 2020,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Yisen Wang",
        "Difan Zou",
        "Jinfeng Yi",
        "J. Bailey",
        "Xingjun Ma",
        "Quanquan Gu"
      ],
      "citation_count": 777,
      "url": "https://www.semanticscholar.org/paper/764eff31d9596033859895d9513b838d2c57a6fb",
      "pdf_url": "",
      "publication_date": "2020-04-30",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4528ba823b40c9032bbd75ad27a032135450aa17",
      "title": "Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness",
      "abstract": "With the increasing demand for practical applications of Large Language Models (LLMs), many attention-efficient models have been developed to balance performance and computational cost. However, the adversarial robustness of these models remains under-explored. In this work, we design a framework to investigate the trade-off between efficiency, performance, and adversarial robustness of LLMs and conduct extensive experiments on three prominent models with varying levels of complexity and efficiency \u2013 Transformer++, Gated Linear Attention (GLA) Transformer, and MatMul-Free LM \u2013 utilizing the GLUE and AdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to challenge model robustness. Our results show that while the GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate higher efficiency and either superior or comparative robustness on AdvGLUE tasks compared to Transformer++ across different attack levels. These findings highlight the potential of simplified architectures to achieve a compelling balance between efficiency, performance, and adversarial robustness, offering valuable insights for applications where resource constraints and resilience to adversarial attacks are critical.",
      "year": 2024,
      "venue": "Artificial Intelligence and Cloud Computing Conference",
      "authors": [
        "Xiaojing Fan",
        "Chunliang Tao"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/4528ba823b40c9032bbd75ad27a032135450aa17",
      "pdf_url": "",
      "publication_date": "2024-08-08",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b3f1aa12dde233aaf543bb9ccb27213c494e0fd5",
      "title": "Unlabeled Data Improves Adversarial Robustness",
      "abstract": "We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.",
      "year": 2019,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Y. Carmon",
        "Aditi Raghunathan",
        "Ludwig Schmidt",
        "Percy Liang",
        "John C. Duchi"
      ],
      "citation_count": 789,
      "url": "https://www.semanticscholar.org/paper/b3f1aa12dde233aaf543bb9ccb27213c494e0fd5",
      "pdf_url": "",
      "publication_date": "2019-05-31",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41071dbbbcbb27af3fec70de045f19c28535f5b7",
      "title": "Feature Denoising for Improving Adversarial Robustness",
      "abstract": "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.",
      "year": 2018,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Cihang Xie",
        "Yuxin Wu",
        "L. Maaten",
        "A. Yuille",
        "Kaiming He"
      ],
      "citation_count": 982,
      "url": "https://www.semanticscholar.org/paper/41071dbbbcbb27af3fec70de045f19c28535f5b7",
      "pdf_url": "https://arxiv.org/pdf/1812.03411",
      "publication_date": "2018-12-09",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "762752eb9a9a92b028026b17c46d50474ddf3f06",
      "title": "Fixing Data Augmentation to Improve Adversarial Robustness",
      "abstract": "Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against $\\ell_\\infty$ and $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Sylvestre-Alvise Rebuffi",
        "Sven Gowal",
        "D. A. Calian",
        "Florian Stimberg",
        "Olivia Wiles",
        "Timothy A. Mann"
      ],
      "citation_count": 297,
      "url": "https://www.semanticscholar.org/paper/762752eb9a9a92b028026b17c46d50474ddf3f06",
      "pdf_url": "",
      "publication_date": "2021-03-02",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5",
      "title": "On Evaluating Adversarial Robustness",
      "abstract": "Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect. \nWe believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Nicholas Carlini",
        "Anish Athalye",
        "Nicolas Papernot",
        "Wieland Brendel",
        "Jonas Rauber",
        "Dimitris Tsipras",
        "I. Goodfellow",
        "A. Ma\u0327dry",
        "Alexey Kurakin"
      ],
      "citation_count": 955,
      "url": "https://www.semanticscholar.org/paper/be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5",
      "pdf_url": "",
      "publication_date": "2019-02-18",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5690e35b8beab92a80055fe2530c29c24e495379",
      "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
      "abstract": "Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\\left({{\\varepsilon _\\infty } = 1/255}\\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.",
      "year": 2023,
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "authors": [
        "Christian Schlarmann",
        "Matthias Hein"
      ],
      "citation_count": 134,
      "url": "https://www.semanticscholar.org/paper/5690e35b8beab92a80055fe2530c29c24e495379",
      "pdf_url": "http://arxiv.org/pdf/2308.10741",
      "publication_date": "2023-08-21",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6fb280681c331a01f93947883d22424f46ad9ecb",
      "title": "Towards quantum enhanced adversarial robustness in machine learning",
      "abstract": "Machine learning algorithms are powerful tools for data-driven tasks such as image classification and feature detection. However, their vulnerability to adversarial examples\u2014input samples manipulated to fool the algorithm\u2014remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum-mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges in building robust real-world QAML tools. In this Perspective, we discuss recent progress in QAML and identify key challenges. We also suggest future research directions that could determine the route to practicality for QAML approaches as quantum computing hardware scales up and noise levels are reduced. To fulfil the potential of quantum machine learning for practical applications in the near future, it needs to be robust against adversarial attacks. West and colleagues give an overview of recent developments in quantum adversarial machine learning, and outline key challenges and future research directions to advance the field.",
      "year": 2023,
      "venue": "Nature Machine Intelligence",
      "authors": [
        "Maxwell T. West",
        "S. Tsang",
        "J. S. Low",
        "C. Hill",
        "C. Leckie",
        "L. Hollenberg",
        "S. Erfani",
        "Muhammad Usman"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/6fb280681c331a01f93947883d22424f46ad9ecb",
      "pdf_url": "https://arxiv.org/pdf/2306.12688",
      "publication_date": "2023-05-25",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "29b64e41bbea997019b64696d2374c3c96dbdf5c",
      "title": "Delving into the Adversarial Robustness of Federated Learning",
      "abstract": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, we conduct comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, we reveal the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. In this work, we propose a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "J Zhang",
        "Bo Li",
        "Chen Chen",
        "L. Lyu",
        "Shuang Wu",
        "Shouhong Ding",
        "Chao Wu"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/29b64e41bbea997019b64696d2374c3c96dbdf5c",
      "pdf_url": "http://arxiv.org/pdf/2302.09479",
      "publication_date": "2023-02-19",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e8be9b187eaf718757a139a2ec85e26688f092f0",
      "title": "Feature Separation and Recalibration for Adversarial Robustness",
      "abstract": "Deep neural networks are susceptible to adversarial attacks due to the accumulation of perturbations in the feature level, and numerous works have boosted model robustness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional useful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Feature Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust feature maps through Separation and Recalibration. The Separation part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activations that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are available at https://github.com/wkim97/FSR",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Woo Jae Kim",
        "Yoonki Cho",
        "Junsik Jung",
        "Sung-eui Yoon"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/e8be9b187eaf718757a139a2ec85e26688f092f0",
      "pdf_url": "https://arxiv.org/pdf/2303.13846",
      "publication_date": "2023-03-24",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09c0fc14d057b2d2a30ca82f71f183c568f37aff",
      "title": "Physical-Layer Adversarial Robustness for Deep Learning-Based Semantic Communications",
      "abstract": "End-to-end semantic communications (ESC) rely on deep neural networks (DNN) to boost communication efficiency by only transmitting the semantics of data, showing great potential for high-demand mobile applications. We argue that central to the success of ESC is the robust interpretation of conveyed semantics at the receiver side, especially for security-critical applications such as automatic driving and smart healthcare. However, robustifying semantic interpretation is challenging as ESC is extremely vulnerable to physical-layer adversarial attacks due to the openness of wireless channels and the fragileness of neural models. Toward ESC robustness in practice, we ask the following two questions: Q1: For attacks, is it possible to generate semantic-oriented physical-layer adversarial attacks that are imperceptible, input-agnostic and controllable? Q2: Can we develop a defense strategy against such semantic distortions and previously proposed adversaries? To this end, we first present MobileSC, a novel semantic communication framework that considers the computation and memory efficiency in wireless environments. Equipped with this framework, we propose SemAdv, a physical-layer adversarial perturbation generator that aims to craft semantic adversaries over the air with the abovementioned criteria, thus answering the Q1. To better characterize the real-world effects for robust training and evaluation, we further introduce a novel adversarial training method $\\texttt {SemMixed}$ to harden the ESC against SemAdv attacks and existing strong threats, thus answering the Q2. Extensive experiments on three public benchmarks verify the effectiveness of our proposed methods against various physical adversarial attacks. We also show some interesting findings, e.g., our MobileSC can even be more robust than classical block-wise communication systems in the low SNR regime.",
      "year": 2023,
      "venue": "IEEE Journal on Selected Areas in Communications",
      "authors": [
        "Gu Nan",
        "Zhichun Li",
        "Jinli Zhai",
        "Qimei Cui",
        "Gong Chen",
        "Xin Du",
        "Xuefei Zhang",
        "Xiaofeng Tao",
        "Zhu Han",
        "Tony Q. S. Quek"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/09c0fc14d057b2d2a30ca82f71f183c568f37aff",
      "pdf_url": "https://arxiv.org/pdf/2305.07220",
      "publication_date": "2023-05-12",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ea67b180970a9c44d9df9255ae073cf510b880b8",
      "title": "Theoretically Grounded Loss Functions and Algorithms for Adversarial Robustness",
      "abstract": null,
      "year": 2023,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "authors": [
        "Pranjal Awasthi",
        "Anqi Mao",
        "M. Mohri",
        "Yutao Zhong"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ea67b180970a9c44d9df9255ae073cf510b880b8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "72ce1f2b6c2f66f548607faf53ffccf1f5a2acd5",
      "title": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach",
      "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Kai Zhao",
        "Qiyu Kang",
        "Yang Song",
        "Rui She",
        "Sijie Wang",
        "Wee Peng Tay"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/72ce1f2b6c2f66f548607faf53ffccf1f5a2acd5",
      "pdf_url": "https://arxiv.org/pdf/2310.06396",
      "publication_date": "2023-10-10",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "858f9254a59daaa00808c642c987ec76b3150638",
      "title": "Robustness in deep learning models for medical diagnostics: security and adversarial challenges towards robust AI applications",
      "abstract": null,
      "year": 2024,
      "venue": "Artificial Intelligence Review",
      "authors": [
        "Haseeb Javed",
        "Shaker El-Sappagh",
        "Tamer Abuhmed"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/858f9254a59daaa00808c642c987ec76b3150638",
      "pdf_url": "",
      "publication_date": "2024-11-08",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d2ae1f3a742d2b9d5455204807ac5b2f2e128ed0",
      "title": "Adversarial Robustness with Partial Isometry",
      "abstract": "Despite their remarkable performance, deep learning models still lack robustness guarantees, particularly in the presence of adversarial examples. This significant vulnerability raises concerns about their trustworthiness and hinders their deployment in critical domains that require certified levels of robustness. In this paper, we introduce an information geometric framework to establish precise robustness criteria for l2 white-box attacks in a multi-class classification setting. We endow the output space with the Fisher information metric and derive criteria on the input\u2013output Jacobian to ensure robustness. We show that model robustness can be achieved by constraining the model to be partially isometric around the training points. We evaluate our approach using MNIST and CIFAR-10 datasets against adversarial attacks, revealing its substantial improvements over defensive distillation and Jacobian regularization for medium-sized perturbations and its superior robustness performance to adversarial training for large perturbations, all while maintaining the desired accuracy.",
      "year": 2024,
      "venue": "Entropy",
      "authors": [
        "Lo\u00efc Shi-Garrier",
        "N. Bouaynaya",
        "Daniel Delahaye"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d2ae1f3a742d2b9d5455204807ac5b2f2e128ed0",
      "pdf_url": "https://www.mdpi.com/1099-4300/26/2/103/pdf?version=1706092974",
      "publication_date": "2024-01-24",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c60c9928b4c1dfd259d190a24c338eadb8b4b5fd",
      "title": "On the Adversarial Robustness of Camera-based 3D Object Detection",
      "abstract": "In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the potential to show stronger robustness; (c) accurate depth estimation effectively improves robustness for depth-estimation-based methods; (d) incorporating multi-frame benign inputs can effectively mitigate adversarial attacks. We hope our findings can steer the development of future camera-based object detection models with enhanced adversarial robustness.",
      "year": 2023,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Shaoyuan Xie",
        "Zichao Li",
        "Zeyu Wang",
        "Cihang Xie"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/c60c9928b4c1dfd259d190a24c338eadb8b4b5fd",
      "pdf_url": "http://arxiv.org/pdf/2301.10766",
      "publication_date": "2023-01-25",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "17af9510a38e4dec93398707f11d833c8af36254",
      "title": "Recent Advances in Adversarial Training for Adversarial Robustness",
      "abstract": "Adversarial training is one of the most effective approaches for deep learning models to defend against adversarial examples.\n\nUnlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically.\n\nDuring the past few years, adversarial training has been studied and discussed from various aspects, which deserves a comprehensive review.\n\nFor the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy.\n\nThen we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled.\n\nFinally, we present potential future directions.",
      "year": 2021,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Tao Bai",
        "Jinqi Luo",
        "Jun Zhao",
        "B. Wen",
        "Qian Wang"
      ],
      "citation_count": 572,
      "url": "https://www.semanticscholar.org/paper/17af9510a38e4dec93398707f11d833c8af36254",
      "pdf_url": "https://www.ijcai.org/proceedings/2021/0591.pdf",
      "publication_date": "2021-02-02",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46269888f9c9f38efa8d763103510771bc7f140d",
      "title": "Adversarial robustness of amortized Bayesian inference",
      "abstract": "Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Manuel Gl\u00f6ckler",
        "Michael Deistler",
        "J. Macke"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/46269888f9c9f38efa8d763103510771bc7f140d",
      "pdf_url": "http://arxiv.org/pdf/2305.14984",
      "publication_date": "2023-05-24",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb9eec791922d755667c0c7216f5c4d5135cb54e",
      "title": "Sharpness-Aware Minimization Alone can Improve Adversarial Robustness",
      "abstract": "Sharpness-Aware Minimization (SAM) is an effective method for improving generalization ability by regularizing loss sharpness. In this paper, we explore SAM in the context of adversarial robustness. We find that using only SAM can achieve superior adversarial robustness without sacrificing clean accuracy compared to standard training, which is an unexpected benefit. We also discuss the relation between SAM and adversarial training (AT), a popular method for improving the adversarial robustness of DNNs. In particular, we show that SAM and AT differ in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model. Finally, while AT suffers from decreased clean accuracy and computational overhead, we suggest that SAM can be regarded as a lightweight substitute for AT under certain requirements. Code is available at https://github.com/weizeming/SAM_AT.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Zeming Wei",
        "Jingyu Zhu",
        "Yihao Zhang"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/eb9eec791922d755667c0c7216f5c4d5135cb54e",
      "pdf_url": "",
      "publication_date": "2023-05-09",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5c493a976f724ea8f238509f6fe087a7bde8c93d",
      "title": "Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better",
      "abstract": "Adversarial training is one effective approach for training robust deep neural networks against adversarial attacks. While being able to bring reliable robustness, adversarial training (AT) methods in general favor high capacity models, i.e., the larger the model the better the robustness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices). In this paper, we leverage the concept of knowledge distillation to improve the robustness of small models by distilling from adversarially trained large models. We first revisit several state-of-the-art AT methods from a distillation perspective and identify one common technique that can lead to improved robustness: the use of robust soft labels \u2013 predictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student\u2019s learning on both natural and adversarial examples in all loss terms. We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distillation methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack. We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robustness distillation. Code: https://github.com/zibojia/RSLAD.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Bojia Zi",
        "Shihao Zhao",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "citation_count": 121,
      "url": "https://www.semanticscholar.org/paper/5c493a976f724ea8f238509f6fe087a7bde8c93d",
      "pdf_url": "https://arxiv.org/pdf/2108.07969",
      "publication_date": "2021-08-18",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "676e40050453ddeb1387f8314478c0ac3681a8c6",
      "title": "Improving Adversarial Robustness via Promoting Ensemble Diversity",
      "abstract": "Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.",
      "year": 2019,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Tianyu Pang",
        "Kun Xu",
        "Chao Du",
        "Ning Chen",
        "Jun Zhu"
      ],
      "citation_count": 479,
      "url": "https://www.semanticscholar.org/paper/676e40050453ddeb1387f8314478c0ac3681a8c6",
      "pdf_url": "",
      "publication_date": "2019-01-25",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "94e3b22566ba78f3c6894eae3082f46317ab9bbb",
      "title": "Phase-aware Adversarial Defense for Improving Adversarial Robustness",
      "abstract": null,
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Dawei Zhou",
        "Nannan Wang",
        "Heng Yang",
        "Xinbo Gao",
        "Tongliang Liu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/94e3b22566ba78f3c6894eae3082f46317ab9bbb",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af05afea7a7e99b8b20a743a49d5b33efc041100",
      "title": "Adversarial Robustness Toolbox v1.0.0",
      "abstract": "Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (this http URL).",
      "year": 2018,
      "venue": "",
      "authors": [
        "Maria-Irina Nicolae",
        "M. Sinn",
        "Minh-Ngoc Tran",
        "Beat Buesser",
        "Ambrish Rawat",
        "Martin Wistuba",
        "Valentina Zantedeschi",
        "Nathalie Baracaldo",
        "Bryant Chen",
        "Heiko Ludwig",
        "Ian Molloy",
        "Ben Edwards"
      ],
      "citation_count": 525,
      "url": "https://www.semanticscholar.org/paper/af05afea7a7e99b8b20a743a49d5b33efc041100",
      "pdf_url": "",
      "publication_date": "2018-07-03",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ac8e45a0451ac578f17f631fc2663ee4b98b83a9",
      "title": "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients",
      "abstract": "\n \n Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more \"legitimate,\" interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.\n \n",
      "year": 2017,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "A. Ross",
        "F. Doshi-Velez"
      ],
      "citation_count": 720,
      "url": "https://www.semanticscholar.org/paper/ac8e45a0451ac578f17f631fc2663ee4b98b83a9",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/11504/11363",
      "publication_date": "2017-11-26",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5c7353fac22a8fdc43fc2f5c006b5d6902c47e75",
      "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
      "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.",
      "year": 2023,
      "venue": "IEEE Data Engineering Bulletin",
      "authors": [
        "Jindong Wang",
        "Xixu Hu",
        "Wenxin Hou",
        "Hao Chen",
        "Runkai Zheng",
        "Yidong Wang",
        "Linyi Yang",
        "Haojun Huang",
        "Weirong Ye",
        "Xiubo Geng",
        "Binxing Jiao",
        "Yue Zhang",
        "Xingxu Xie"
      ],
      "citation_count": 285,
      "url": "https://www.semanticscholar.org/paper/5c7353fac22a8fdc43fc2f5c006b5d6902c47e75",
      "pdf_url": "https://arxiv.org/pdf/2302.12095",
      "publication_date": "2023-02-22",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5b381a8104e7ea90b412e93f19482320c1b4b665",
      "title": "On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles",
      "abstract": "Trajectory prediction is a critical component for autonomous vehicles (AVs) to perform safe planning and navigation. However, few studies have analyzed the adversarial robustness of trajectory prediction or investigated whether the worst-case prediction can still lead to safe planning. To bridge this gap, we study the adversarial robustness of trajectory prediction models by proposing a new adversarial attack that perturbs normal vehicle trajectories to maximize the prediction error. Our experiments on three models and three datasets show that the adversarial prediction increases the prediction error by more than 150%. Our case studies show that if an adversary drives a vehicle close to the target AV following the adversarial trajectory, the AV may make an inaccurate prediction and even make unsafe driving decisions. We also explore possible mitigation techniques via data augmentation and trajectory smoothing.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Qingzhao Zhang",
        "Shengtuo Hu",
        "Jiachen Sun",
        "Qi Alfred Chen",
        "Z. Morley Mao"
      ],
      "citation_count": 177,
      "url": "https://www.semanticscholar.org/paper/5b381a8104e7ea90b412e93f19482320c1b4b665",
      "pdf_url": "https://arxiv.org/pdf/2201.05057",
      "publication_date": "2022-01-13",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "25e9fa483a048607131a5a0e3287e8f457fb4807",
      "title": "Benchmarking Adversarial Robustness on Image Classification",
      "abstract": "Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important research problems in the development of deep learning. While a lot of efforts have been made in recent years, it is of great significance to perform correct and complete evaluations of the adversarial attack and defense algorithms. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classification tasks. After briefly reviewing plenty of representative attack and defense methods, we perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods. Based on the evaluation results, we draw several important findings that can provide insights for future research, including: 1) The relative robustness between models can change across different attack configurations, thus it is encouraged to adopt the robustness curves to evaluate adversarial robustness; 2) As one of the most effective defense techniques, adversarial training can generalize across different threat models; 3) Randomization-based defenses are more robust to query-based black-box attacks.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yinpeng Dong",
        "Qi-An Fu",
        "Xiao Yang",
        "Tianyu Pang",
        "Hang Su",
        "Zihao Xiao",
        "Jun Zhu"
      ],
      "citation_count": 287,
      "url": "https://www.semanticscholar.org/paper/25e9fa483a048607131a5a0e3287e8f457fb4807",
      "pdf_url": "",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3a41a5bfec0d8ccb597d9313416130a2e0b2459",
      "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kaijie Zhu",
        "Jindong Wang",
        "Jiaheng Zhou",
        "Zichen Wang",
        "Hao Chen",
        "Yidong Wang",
        "Linyi Yang",
        "Weirong Ye",
        "Neil Zhenqiang Gong",
        "Yue Zhang",
        "Xing Xie"
      ],
      "citation_count": 218,
      "url": "https://www.semanticscholar.org/paper/f3a41a5bfec0d8ccb597d9313416130a2e0b2459",
      "pdf_url": "https://arxiv.org/pdf/2306.04528",
      "publication_date": null,
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2ce9963f1f072d578b1a1f1b995fec75e8c2247",
      "title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
      "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptRobust, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis, beneficial to both researchers and everyday users. Code is available at https://github.com/microsoft/promptbench.",
      "year": 2023,
      "venue": "LAMPS@CCS",
      "authors": [
        "Kaijie Zhu",
        "Jindong Wang",
        "Jiaheng Zhou",
        "Zichen Wang",
        "Hao Chen",
        "Yidong Wang",
        "Linyi Yang",
        "Weirong Ye",
        "N. Gong",
        "Yue Zhang",
        "Xingxu Xie"
      ],
      "citation_count": 207,
      "url": "https://www.semanticscholar.org/paper/a2ce9963f1f072d578b1a1f1b995fec75e8c2247",
      "pdf_url": "http://arxiv.org/pdf/2306.04528",
      "publication_date": "2023-06-07",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2c0564ca47262f76a46bfacc6d587a968be53712",
      "title": "A Self-supervised Approach for Adversarial Robustness",
      "abstract": "Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model's parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the \\textbf{unseen} adversarial attacks (\\eg by reducing the success rate of translation-invariant \\textbf{ensemble} attack from 82.6\\% to 31.9\\% in comparison to previous state-of-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection.",
      "year": 2020,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Muzammal Naseer",
        "Salman Hameed Khan",
        "Munawar Hayat",
        "F. Khan",
        "F. Porikli"
      ],
      "citation_count": 327,
      "url": "https://www.semanticscholar.org/paper/2c0564ca47262f76a46bfacc6d587a968be53712",
      "pdf_url": "http://arxiv.org/pdf/2006.04924",
      "publication_date": "2020-06-01",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "83cebf919635504786fc220d569284842b0f0a09",
      "title": "A Survey of Adversarial Defenses and Robustness in NLP",
      "abstract": "In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model\u2019s predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.",
      "year": 2023,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Shreyansh Goyal",
        "Sumanth Doddapaneni",
        "Mitesh M. Khapra",
        "B. Ravindran"
      ],
      "citation_count": 156,
      "url": "https://www.semanticscholar.org/paper/83cebf919635504786fc220d569284842b0f0a09",
      "pdf_url": "",
      "publication_date": "2023-04-20",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70c121af9818efd7d3e1d2894f744392506adb88",
      "title": "Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective",
      "abstract": "Neural networks have been widely applied in security applications such as spam and phishing detection, intrusion prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications. Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in security and machine learning. In this work, we survey existing literature in adversarial robustness verification for neural networks and collect 39 diversified research works across machine learning, security, and software engineering domains. We systematically analyze their approaches, including how robustness is formulated, what verification techniques are used, and the strengths and limitations of each technique. We provide a taxonomy from a formal verification perspective for a comprehensive understanding of this topic. We classify the existing techniques based on property specification, problem reduction, and reasoning strategies. We also demonstrate representative techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.",
      "year": 2022,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "M. H. Meng",
        "Guangdong Bai",
        "S. Teo",
        "Zhe Hou",
        "Yan Xiao",
        "Yun Lin",
        "J. Dong"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/70c121af9818efd7d3e1d2894f744392506adb88",
      "pdf_url": "https://research-repository.griffith.edu.au/bitstreams/76896173-953b-4771-af4a-e5ab89133f23/download",
      "publication_date": "2022-06-24",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d7c401de74174d19685ed52dc9a5e323c9d3a721",
      "title": "Adversarial attacks and adversarial robustness in computational pathology",
      "abstract": "Artificial Intelligence (AI) can support diagnostic workflows in oncology by aiding diagnosis and providing biomarkers. AI applications are therefore expected to evolve from academic prototypes to commercial products in the coming years. However, AI applications are vulnerable to adversarial attacks, such as malicious interference with test data aiming to cause misclassifications. Therefore, it is essential for the use of AI-based diagnostic devices to secure them against such attacks before widespread use. Unfortunately, no resistant systems exist in computational pathology so far. To address this problem, we investigate the susceptibility of convolutional neural networks (CNNs) to multiple types of white- and black-box attacks. We demonstrate that both attacks can easily confuse CNNs in clinically relevant pathology tasks and impair classification performance. Classical adversarially robust training and dual batch normalization (DBN) are possible mitigation strategies but require precise knowledge of the type of attack used in the inference. We demonstrate that vision transformers (ViTs) perform equally well compared to CNNs at baseline and are orders of magnitude more robust to different types of white-box and black-box attacks. At a mechanistic level, we show that this is associated with a more robust latent representation of clinically relevant categories in ViTs compared to CNNs. Our results are in line with previous theoretical studies. We show that ViTs are robust learners in computational pathology. This implies that large-scale rollout of AI models in computational pathology should rely on ViTs rather than CNN-based classifiers to provide inherent protection against adversaries.",
      "year": 2022,
      "venue": "bioRxiv",
      "authors": [
        "Narmin Ghaffari Laleh",
        "D. Truhn",
        "G. P. Veldhuizen",
        "T. Han",
        "Marko van Treeck",
        "R. D. B\u00fclow",
        "R. Langer",
        "B. Dislich",
        "P. Boor",
        "V. Schulz",
        "Jakob Nikolas Kather"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/d7c401de74174d19685ed52dc9a5e323c9d3a721",
      "pdf_url": "https://www.nature.com/articles/s41467-022-33266-0.pdf",
      "publication_date": "2022-03-18",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "02f1243778bced398c4949cf90629742175ad79b",
      "title": "Revisiting Residual Networks for Adversarial Robustness",
      "abstract": "Efforts to improve the adversarial robustness of convolutional neural networks have primarily focused on developing more effective adversarial training methods. In contrast, little attention was devoted to analyzing the role of architectural elements (e.g., topology, depth, and width) on adversarial robustness. This paper seeks to bridge this gap and present a holistic study on the impact of architectural design on adversarial robustness. We focus on residual networks and consider architecture design at the block level as well as at the network scaling level. In both cases, we first derive insights through systematic experiments. Then we design a robust residual block, dubbed RobustResBlock, and a compound scaling rule, dubbed RobustScaling, to distribute depth and width at the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling and present a portfolio of adversarially robust residual networks, RobustResNets, spanning a broad spectrum of model capacities. Experimental validation across multiple datasets and adversarial attacks demonstrate that RobustResNets consistently outperform both the standard WRNs and other existing robust architectures, achieving state-of-the-art AutoAttack robust accuracy 63.7% with 500K external data while being 2\u00d7 more compact in terms of parameters. Code is available at this URL.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Shihua Huang",
        "Zhichao Lu",
        "K. Deb",
        "Vishnu Naresh Boddeti"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/02f1243778bced398c4949cf90629742175ad79b",
      "pdf_url": "",
      "publication_date": "2022-12-21",
      "keywords_matched": [
        "adversarial robustness"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90022c80ea85a41d8d1a7765fd95824bf3a9830f",
      "title": "Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task",
      "abstract": "With the increasing capabilities of large language models (LLMs), these high-performance models have achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks. However, the models' performance on commonly-used benchmark datasets often fails to accurately reflect their reliability and robustness when applied to real-world noisy data. To address these challenges, we propose a unified robustness evaluation framework based on the slot-filling task to systematically evaluate the dialogue understanding capability of LLMs in diverse input perturbation scenarios. Specifically, we construct a input perturbation evaluation dataset, Noise-LLM, which contains five types of single perturbation and four types of mixed perturbation data. Furthermore, we utilize a multi-level data augmentation method (character, word, and sentence levels) to construct a candidate data pool, and carefully design two ways of automatic task demonstration construction strategies (instance-level and entity-level) with various prompt templates. Our aim is to assess how well various robustness methods of LLMs perform in real-world noisy scenarios. The experiments have demonstrated that the current open-source LLMs generally achieve limited perturbation robustness performance. Based on these experimental observations, we make some forward-looking suggestions to fuel the research in this direction.",
      "year": 2023,
      "venue": "Natural Language Processing and Chinese Computing",
      "authors": [
        "Guanting Dong",
        "Jinxu Zhao",
        "Tingfeng Hui",
        "Daichi Guo",
        "Wenlong Wan",
        "Boqi Feng",
        "Yueyan Qiu",
        "Zhuoma Gongque",
        "Keqing He",
        "Zechen Wang",
        "Weiran Xu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/90022c80ea85a41d8d1a7765fd95824bf3a9830f",
      "pdf_url": "",
      "publication_date": "2023-10-10",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "52f98f2be3a26cc74f1f03c46c417c7efb17749d",
      "title": "Sensitivity of Spiking Neural Networks Due to Input Perturbation",
      "abstract": "Background: To investigate the behavior of spiking neural networks (SNNs), the sensitivity of input perturbation serves as an effective metric for assessing the influence on the network output. However, existing methods fall short in evaluating the sensitivity of SNNs featuring biologically plausible leaky integrate-and-fire (LIF) neurons due to the intricate neuronal dynamics during the feedforward process. Methods: This paper first defines the sensitivity of a temporal-coded spiking neuron (SN) as the deviation between the perturbed and unperturbed output under a given input perturbation with respect to overall inputs. Then, the sensitivity algorithm of an entire SNN is derived iteratively from the sensitivity of each individual neuron. Instead of using the actual firing time, the desired firing time is employed to derive a more precise analytical expression of the sensitivity. Moreover, the expectation of the membrane potential difference is utilized to quantify the magnitude of the input deviation. Results/Conclusions: The theoretical results achieved with the proposed algorithm are in reasonable agreement with the simulation results obtained with extensive input data. The sensitivity also varies monotonically with changes in other parameters, except for the number of time steps, providing valuable insights for choosing appropriate values to construct the network. Nevertheless, the sensitivity exhibits a piecewise decreasing tendency with respect to the number of time steps, with the length and starting point of each piece contingent upon the specific parameter values of the neuron.",
      "year": 2024,
      "venue": "Brain Science",
      "authors": [
        "Haoran Zhu",
        "Xiaoqin Zeng",
        "Yang Zou",
        "Jinfeng Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/52f98f2be3a26cc74f1f03c46c417c7efb17749d",
      "pdf_url": "https://www.mdpi.com/2076-3425/14/11/1149/pdf?version=1731921274",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4b8b62adb68546e2b7f8ec07b765f8f4c2a069b5",
      "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
      "abstract": "Neural network calibration is an essential task in deep learning to ensure consistency between the confidence of model prediction and the true correctness likelihood. In this paper, we propose a new post-processing calibration method called Neural Clamping, which employs a simple joint input-output transformation on a pre-trained classifier via a learnable universal input perturbation and an output temperature scaling parameter. Moreover, we provide theoretical explanations on why Neural Clamping is provably better than temperature scaling. Evaluated on BloodMNIST, CIFAR-100, and ImageNet image recognition datasets and a variety of deep neural network models, our empirical results show that Neural Clamping significantly outperforms state-of-the-art post-processing calibration methods. The code is available at github.com/yungchentang/NCToolkit, and the demo is available at huggingface.co/spaces/TrustSafeAI/NCTV.",
      "year": 2022,
      "venue": "Trans. Mach. Learn. Res.",
      "authors": [
        "Yu Tang",
        "Pin-Yu Chen",
        "Tsung-Yi Ho"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4b8b62adb68546e2b7f8ec07b765f8f4c2a069b5",
      "pdf_url": "http://arxiv.org/pdf/2209.11604",
      "publication_date": "2022-09-23",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b05c9b6876adebf429aaa07b1ff0c099b929853d",
      "title": "Computation of CNN\u2019s Sensitivity to Input Perturbation",
      "abstract": null,
      "year": 2021,
      "venue": "Neural Processing Letters",
      "authors": [
        "Lin Xiang",
        "Xiaoqin Zeng",
        "Shengli Wu",
        "Yanjun Liu",
        "Baohua Yuan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b05c9b6876adebf429aaa07b1ff0c099b929853d",
      "pdf_url": "",
      "publication_date": "2021-01-03",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "699d4dc6d06079df283eda4d6a8fdc4c698f9edb",
      "title": "On robustness of radial basis function network with input perturbation",
      "abstract": null,
      "year": 2019,
      "venue": "Neural computing & applications (Print)",
      "authors": [
        "P. Dey",
        "Madhumita Gopal",
        "Payal P. Pradhan",
        "T. Pal"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/699d4dc6d06079df283eda4d6a8fdc4c698f9edb",
      "pdf_url": "",
      "publication_date": "2019-02-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee307c33a67e37163d8c345108da8d0d0cb43950",
      "title": "Robust Stability of Recurrent Neural Networks With Time-Varying Delays and Input Perturbation",
      "abstract": "This paper addresses the robust stability of recurrent neural networks (RNNs) with time-varying delays and input perturbation, where the time-varying delays include discrete and distributed delays. By employing the new <inline-formula> <tex-math notation=\"LaTeX\">$\\psi $ </tex-math></inline-formula>-type integral inequality, several sufficient conditions are derived for the robust stability of RNNs with discrete and distributed delays. Meanwhile, the robust boundedness of neural networks is explored by the bounded input perturbation and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {L}^{1}$ </tex-math></inline-formula>-norm constraint. Moreover, RNNs have a strong anti-jamming ability to input perturbation, and the robustness of RNNs is suitable for associative memory. Specifically, when input perturbation belongs to the specified and well-characterized space, the results cover both monostability and multistability as special cases. It is revealed that there is a relationship between the stability of neural networks and input perturbation. Compared with the existing results, these conditions proposed in this paper improve and extend the existing stability in some literature. Finally, the numerical examples are given to substantiate the effectiveness of the theoretical results.",
      "year": 2019,
      "venue": "IEEE Transactions on Cybernetics",
      "authors": [
        "Fanghai Zhang",
        "Z. Zeng"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/ee307c33a67e37163d8c345108da8d0d0cb43950",
      "pdf_url": "",
      "publication_date": "2019-07-22",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7aa8a6f7e2cf24aadd16990f17a1b0f4f886011a",
      "title": "Learning Representations for Axis-Aligned Decision Forests through Input Perturbation",
      "abstract": "Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Sebastian Bruch",
        "Jan Pfeifer",
        "Mathieu Guillame-Bert"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7aa8a6f7e2cf24aadd16990f17a1b0f4f886011a",
      "pdf_url": "",
      "publication_date": "2020-07-29",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e653e6bf47b423b41316a9bc3fd03fc11ece91ba",
      "title": "Input-Perturbation-Sensitivity for Performance Analysis of CNNS on Image Recognition",
      "abstract": "Performance assessment is critical to learning systems, but it is tough to explain the relationship between data, model, and performance. In this paper, the Input-Perturbation-Sensitivity (IPS) is proposed to investigate this problem in a class of Convolutional Neural Networks (CNNs) for image recognition and try to explain their relationships. First, IPS is defined and the CNNs parameters are divided into groups according to the layers of the model. Second, the output perturbations of the CNNs caused by input perturbations are analyzed with a group of local sensitivities (LS). Third, global sensitivity (GS) is obtained over all local IPS. Finally, experiments are carried out on a few CNNs with different hyper-parameters on the image recognition datasets. The analytic and experimental results show that the proposed method correlates well with data, model, and performance. Moreover, the IPS can provide a reasonable explanation of the different networks for image classification, showing the potential to evaluate other learning systems.",
      "year": 2019,
      "venue": "International Conference on Information Photonics",
      "authors": [
        "Zhibo Rao",
        "Mingyi He",
        "Zhidong Zhu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/e653e6bf47b423b41316a9bc3fd03fc11ece91ba",
      "pdf_url": "",
      "publication_date": "2019-09-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b8519f78b0bf742c56ce444d16b4b206ad1f9905",
      "title": "A2C: Self Destructing Exploit Executions via Input Perturbation",
      "abstract": "Malicious payload injection attacks have been a serious threat to software for decades. Unfortunately, protection against these attacks remains challenging due to the ever increasing diversity and sophistication of payload injection and triggering mechanisms used by adversaries. In this paper, we develop A2C, a system that provides general protection against payload injection attacks. A2C is based on the observation that payloads are highly fragile and thus any mutation would likely break their functionalities. Therefore, A2C mutates inputs from untrusted sources. Malicious payloads that reside in these inputs are hence mutated and broken. To assure that the program continues to function correctly when benign inputs are provided, A2C divides the state space into exploitable and post-exploitable sub-spaces, where the latter is much larger than the former, and decodes the mutated values only when they are transmitted from the former to the latter. A2C does not rely on any knowledge of malicious payloads or their injection and triggering mechanisms. Hence, its protection is general. We evaluate A2C with 30 realworld applications, including apache on a real-world work-load, and our results show that A2C effectively prevents a variety of payload injection attacks on these programs with reasonably low overhead (6.94%).",
      "year": 2017,
      "venue": "Proceedings 2017 Network and Distributed System Security Symposium",
      "authors": [
        "Yonghwi Kwon",
        "Brendan Saltaformaggio",
        "I. L. Kim",
        "K. H. Lee",
        "X. Zhang",
        "Dongyan Xu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/b8519f78b0bf742c56ce444d16b4b206ad1f9905",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa2ce2bc9155b30ed15006ad9d9210b88872523c",
      "title": "Computation of multilayer perceptron sensitivity to input perturbation",
      "abstract": null,
      "year": 2013,
      "venue": "Neurocomputing",
      "authors": [
        "J. Yang",
        "Xiaoqin Zeng",
        "Shuiming Zhong"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/fa2ce2bc9155b30ed15006ad9d9210b88872523c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "23c884679d59c5557413ff0932f3580faae3f42d",
      "title": "Interpretable Adversarial Perturbation in Input Embedding Space for Text",
      "abstract": "Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field.\n\nOne promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts.\n\nHowever, this approach abandons such interpretability as generating adversarial texts to significantly improve the performance of NLP tasks.\n\nThis paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space.\n\nAs a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance.",
      "year": 2018,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Motoki Sato",
        "Jun Suzuki",
        "Hiroyuki Shindo",
        "Yuji Matsumoto"
      ],
      "citation_count": 203,
      "url": "https://www.semanticscholar.org/paper/23c884679d59c5557413ff0932f3580faae3f42d",
      "pdf_url": "https://www.ijcai.org/proceedings/2018/0601.pdf",
      "publication_date": "2018-05-08",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ddb6a2b84b5ccd3991a78df26e666e650cacaa0",
      "title": "A Quantified Sensitivity Measure for Multilayer Perceptron to Input Perturbation",
      "abstract": null,
      "year": 2003,
      "venue": "Neural Computation",
      "authors": [
        "Xiaoqin Zeng",
        "D. Yeung"
      ],
      "citation_count": 58,
      "url": "https://www.semanticscholar.org/paper/7ddb6a2b84b5ccd3991a78df26e666e650cacaa0",
      "pdf_url": "",
      "publication_date": "2003-01-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca29acb4a4ada1f2d486a4595322401a736077da",
      "title": "Computation of two-layer perceptron networks\u2019 sensitivity to input perturbation",
      "abstract": null,
      "year": 2008,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Jing Yang",
        "Xiao-Qin Zeng",
        "W.W.Y. Ng",
        "D. Yeung"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/ca29acb4a4ada1f2d486a4595322401a736077da",
      "pdf_url": "",
      "publication_date": "2008-07-12",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "73c050687ec73ec3bf173aa66b6923a4f31f4496",
      "title": "Sensitivity analysis of multilayer perceptron to input perturbation",
      "abstract": null,
      "year": 2000,
      "venue": "Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0",
      "authors": [
        "Xiaoqin Zeng",
        "D. Yeung",
        "Xuequan Sun"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/73c050687ec73ec3bf173aa66b6923a4f31f4496",
      "pdf_url": "",
      "publication_date": "2000-10-08",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d52d1572bfd07362ef7b7f485ed2a46522f21cc",
      "title": "An upper bound of input perturbation for RBFNNS sensitivity analysis",
      "abstract": null,
      "year": 2005,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Xizhao Wang",
        "Hui Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/9d52d1572bfd07362ef7b7f485ed2a46522f21cc",
      "pdf_url": "",
      "publication_date": "2005-11-07",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bff7caa595dc15d86831c0e7018cc75232d129a5",
      "title": "Statistical Sensitivity Measure of Single Layer Perceptron Neural Networks to Input Perturbation",
      "abstract": null,
      "year": 2006,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Xiao-Qin Zeng",
        "W.W.Y. Ng",
        "D. Yeung"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bff7caa595dc15d86831c0e7018cc75232d129a5",
      "pdf_url": "",
      "publication_date": "2006-08-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5",
      "title": "Adversarial Weight Perturbation Helps Robust Generalization",
      "abstract": null,
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Dongxian Wu",
        "Shutao Xia",
        "Yisen Wang"
      ],
      "citation_count": 812,
      "url": "https://www.semanticscholar.org/paper/9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5",
      "pdf_url": "",
      "publication_date": "2020-04-13",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18a9bb863e3110e2e981b53618b214585a32f877",
      "title": "Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond",
      "abstract": null,
      "year": 2020,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Kaidi Xu",
        "Zhouxing Shi",
        "Huan Zhang",
        "Yihan Wang",
        "Kai-Wei Chang",
        "Minlie Huang",
        "B. Kailkhura",
        "Xue Lin",
        "Cho-Jui Hsieh"
      ],
      "citation_count": 325,
      "url": "https://www.semanticscholar.org/paper/18a9bb863e3110e2e981b53618b214585a32f877",
      "pdf_url": "",
      "publication_date": "2020-02-28",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "647609b9608feee2817b18cc1b7b4f652390d223",
      "title": "Identification of multi-input systems using simultaneous perturbation by pseudorandom input signals",
      "abstract": null,
      "year": 2015,
      "venue": "",
      "authors": [
        "Ai Hui Tan",
        "Keith R. Godfrey",
        "H. Anthony Barker"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/647609b9608feee2817b18cc1b7b4f652390d223",
      "pdf_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-cta.2014.0795",
      "publication_date": "2015-07-23",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b4ce98cca9528db494e30e84bc28918b3bc0d081",
      "title": "Stability of Input-Perturbation Extremum-Seeking Systems",
      "abstract": null,
      "year": 1968,
      "venue": "",
      "authors": [
        "J. W. White",
        "G. Foley",
        "R. J. Altpeter"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b4ce98cca9528db494e30e84bc28918b3bc0d081",
      "pdf_url": "",
      "publication_date": "1968-05-01",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d73faa38b00209662fb46280ccc332d98c2d542a",
      "title": "Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models",
      "abstract": "Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related features, e.g., gender and ethnicity. The key advantage is that FAAP does not modify deployed models in terms of param-eters and structures. To achieve this, we design a discriminator to distinguish fairness-related attributes based on latent representations from deployed models. Meanwhile, a perturbation generator is trained against the discriminator, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model pa-rameters), which shows the transferability of FAAP, foreseeing the potential of black-box adaptation.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zhibo Wang",
        "Xiaowei Dong",
        "Henry Xue",
        "Zhifei Zhang",
        "Weifeng Chiu",
        "Tao Wei",
        "Kui Ren"
      ],
      "citation_count": 83,
      "url": "https://www.semanticscholar.org/paper/d73faa38b00209662fb46280ccc332d98c2d542a",
      "pdf_url": "https://arxiv.org/pdf/2203.01584",
      "publication_date": "2022-03-03",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8f771cb51b97a207b9c737abcfbc253eab6f9f7f",
      "title": "A Black-Box Targeted Misclassification Attack on Edge AI with Adversarial Examples Generated from RAW Image Sensor Data",
      "abstract": "Deep learning has witnessed pervasive deployment on edge devices over the past decade, especially for computer vision applications. However, its vulnerability to adversarial attacks, where visually imperceptible patterns cause machine learning models to malfunction, has raised significant security concerns. The connection between the image sensor and the application processors is typically not encrypted nor signed for data integrity verification, leaving the data link exposed to tampering threats. Previous works have demonstrated how these threats can be exploited. However, these methods typically inject attack patterns into the RAW image data without considering the effect of the image signal processing (ISP) pipeline, which can undesirably weaken the adversarial effects. Insofar such attacks have not succeeded in more powerful targeted misclassification fraud attacks where a selected target can be misclassified into the attacker's intended output. In this work, we propose a novel RAW image domain black-box attack that incorporates a differentiable ISP to train a knowledge-distilled substitute classifier to generate adaptive adversarial perturbations that survive the ISP. We show that such an attack is feasible by attacking the edge implementations of ResNet18 and MobileNetV2 with adversarial examples generated from their knowledge-distilled models by applying differentiable ISP on RAW formatted GTSRB test images captured by a Raspberry Pi camera. Our results demonstrate that its attack success rate surpasses previous direct mapping techniques by 10.37% and 13.07%, respectively for ResNet18 and MobileNetV2 in untargeted misclassification attack tasks with greater stealthiness when the adversarial examples are displayed on an LCD monitor for comparison. More importantly, it can achieve the targeted misclassification at an attack success rate of 95.09 % and 51.98 % respectively, which is currently impossible with existing camera-link attack methods. The results are reproducible with our code, which can be downloaded from https://github.com/BOWEN-HU/diff-isp.",
      "year": 2024,
      "venue": "Asian Hardware-Oriented Security and Trust Symposium",
      "authors": [
        "Bowen Hu",
        "Weiyang He",
        "Kuo Wang",
        "Chip-Hong Chang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8f771cb51b97a207b9c737abcfbc253eab6f9f7f",
      "pdf_url": "",
      "publication_date": "2024-12-16",
      "keywords_matched": [
        "misclassification attack",
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7380a6a0121b691c49565cfca0e2f12173defc75",
      "title": "Imperceptible Misclassification Attack on Deep Learning Accelerator by Glitch Injection",
      "abstract": "The convergence of edge computing and deep learning empowers endpoint hardwares or edge devices to perform inferences locally with the help of deep neural network (DNN) accelerator. This trend of edge intelligence invites new attack vectors, which are methodologically different from the well-known software oriented deep learning attacks like the input of adversarial examples. Current studies of threats on DNN hardware focus mainly on model parameters interpolation. Such kind of manipulation is not stealthy as it will leave non-erasable traces or create conspicuous output patterns. In this paper, we present and investigate an imperceptible misclassification attack on DNN hardware by introducing infrequent instantaneous glitches into the clock signal. Comparing with falsifying model parameters by permanent faults, corruption of targeted intermediate results of convolution layer(s) by disrupting associated computations intermittently leaves no trace. We demonstrated our attack on nine state-of-the-art ImageNet models running on Xilinx FPGA based deep learning accelerator. With no knowledge about the models, our attack can achieve over 98% misclassification on 8 out of 9 models with only 10% glitches launched into the computation clock cycles. Given the model details and inputs, all the test images applied to ResNet50 can be successfully misclassified with no more than 1.7% glitch injection.",
      "year": 2020,
      "venue": "Design Automation Conference",
      "authors": [
        "Wenye Liu",
        "Chip-Hong Chang",
        "Fan Zhang",
        "Xiaoxuan Lou"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/7380a6a0121b691c49565cfca0e2f12173defc75",
      "pdf_url": "https://dr.ntu.edu.sg/bitstream/10356/145856/2/21_2_Liu_finalpaper_05_22_2020_09_48.pdf",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4499c550c9bda6622ac6feb2e723974b13a4e8f2",
      "title": "A Novel Misclassification Attack Against Black Box Neural Network Classifiers",
      "abstract": "It is generally believed that neural network classifiers are vulnerable to misclassification attacks. An adversary generates adversarial samples by adding small perturbations to the original samples. These adversarial samples will mislead classifiers, although they are almost the same as the original samples from the perspective of human observation. However, the existing misclassification attacks need either the details of classifier or a fake classifier to craft the adversarial samples. One may think a black box classifier is robust enough against misclassification attacks. We demonstrate that black box classifier is still vulnerable to our proposed misclassification attack. We conceal the details of classifier. The only thing an adversary can do is to query samples' classification results. We proposed a particle swarm optimization based misclassification attack. Using this attack an adversary can make black box classifiers yield erroneous results. The experiments show that LeNet and GoogLeNet are vulnerable to our proposed attack. The misclassification rate on MNIST and ImageNet ILSVRC 2012 dataset are 99.1% and 98.4%. Finally, we give some defense strategies against misclassification attacks.",
      "year": 2018,
      "venue": "International Conference on Semantics, Knowledge and Grid",
      "authors": [
        "Zibo Yi",
        "Shasha Li",
        "Jie Yu",
        "Yusong Tan",
        "Q. Wu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4499c550c9bda6622ac6feb2e723974b13a4e8f2",
      "pdf_url": "",
      "publication_date": "2018-09-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d540099cd632b773b85a1213b42d778e0b990226",
      "title": "A Non-Targeted Attack Approach for the Coarse Misclassification Problem",
      "abstract": "The evaluation of classifiers' robustness against adversarial attacks is typically performed through metrics based on the minimal perturbation required for misclassification. The conventional method of generating these perturbations relies on setting a limit on the maximum allowed perturbation (restricted attack method) and a non-targeted attack formulation. This approach, however, disregards any relationships between classes. Our paper introduces a novel, non-targeted, bound-restricted method for achieving coarse misclassification, so that the perturbed feature is classified outside its true coarse class. We present an efficient, single-step solution to the coarse misclassification problem and analyze its computational requirements. Our experiments showcase the superiority of our method, surpassing state-of-the-art in terms of both the perceptibility of adversarial examples and runtime.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Ismail R. Alkhouri",
        "Alvaro Velasquez",
        "George K. Atia"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d540099cd632b773b85a1213b42d778e0b990226",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f15662437b0cf9eaa1d9838d35143a7e127a1df5",
      "title": "Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack",
      "abstract": "With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks, such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model\u2019s robustness against such attacks. The empirical results reveal that the current detection model can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model\u2019s robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.",
      "year": 2024,
      "venue": "International Conference on Language Resources and Evaluation",
      "authors": [
        "Ying Zhou",
        "Ben He",
        "Le Sun"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/f15662437b0cf9eaa1d9838d35143a7e127a1df5",
      "pdf_url": "",
      "publication_date": "2024-04-02",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "553d9afb64466f3a0424cfc6165353a918007250",
      "title": "Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger",
      "abstract": "Recent research works have demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks. The existing backdoor attacks can only cause targeted misclassification on backdoor instances, which makes them can be easily detected by defense methods. In this article, we propose an untargeted backdoor attack (UBA) against DNNs, where the backdoor instances are randomly misclassified by the backdoored model to any incorrect label. To achieve the goal of UBA, we propose to utilize autoencoder as the trigger generation model and train the target model and the autoencoder simultaneously. We also propose a special loss function (Evasion Loss) to train the autoencoder and the target model, in order to make the target model predict backdoor instances as random incorrect classes. During the inference stage, the trained autoencoder is used to generate backdoor instances. For different backdoor instances, the generated triggers are different and the corresponding predicted labels are random incorrect labels. Experimental results demonstrate that the proposed UBA is effective. On the ResNet-18 model, the attack success rate (ASR) of the proposed UBA is 96.48%, 91.27%, and 90.83% on CIFAR-10, GTSRB, and ImageNet datasets, respectively. On the VGG-16 model, the ASR of the proposed UBA is 89.72% and 97.78% on CIFAR-10 and ImageNet datasets, respectively. Moreover, the proposed UBA is robust against existing backdoor defense methods, which are designed to detect targeted backdoor attacks. We hope this article can promote the research of corresponding backdoor defense works.",
      "year": 2024,
      "venue": "IEEE Transactions on Industrial Informatics",
      "authors": [
        "Mingfu Xue",
        "Ying-Chang Wu",
        "S. Ni",
        "Leo Yu Zhang",
        "Yushu Zhang",
        "Weiqiang Liu"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/553d9afb64466f3a0424cfc6165353a918007250",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e04b2adc535c4071d5b3400b6054cd8fca5957e6",
      "title": "Universal Adversarial Attack Against Speaker Recognition Models",
      "abstract": "In recent years, deep learning-based speaker recognition (SR) models have received a large amount of attention from the machine learning (ML) community. Their increasing popularity derives in large part from their effectiveness in identifying speakers in many security-sensitive applications. Researchers have attempted to challenge the robustness of SR models, and they have revealed the models\u2019 vulnerability to adversarial ML attacks. However, the studies performed mainly proposed tailor-made perturbations that are only effective for the speakers they were trained on (i.e., a closed-set). In this paper, we propose the Anonymous Speakers attack, a universal adversarial perturbation that fools SR models on all speakers in an open-set environment, i.e., including speakers that were not part of the training phase of the attack. Using a custom optimization process, we craft a single perturbation that can be applied to the original recording of any speaker and results in misclassification by the SR model. We examined the attack\u2019s effectiveness on various state-of-the-art SR models with a wide range of speaker identities. The results of our experiments show that our attack largely reduces the embeddings\u2019 similarity to the speaker\u2019s original embedding representation while maintaining a high signal-to-noise ratio value.",
      "year": 2024,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Shoham Hanina",
        "Alon Zolfi",
        "Y. Elovici",
        "A. Shabtai"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e04b2adc535c4071d5b3400b6054cd8fca5957e6",
      "pdf_url": "",
      "publication_date": "2024-04-14",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a787cd3ec9a725bc36ecaec157ba2656779c2ab9",
      "title": "Cascaded Adversarial Attack: Simultaneously Fooling Rain Removal and Semantic Segmentation Networks",
      "abstract": "When applying high-level visual algorithms to rainy scenes, it is customary to preprocess the rainy images using low-level rain removal networks, followed by visual networks to achieve the desired objectives. Such a setting has never been explored by adversarial attack methods, which are only limited to attacking one kind of them. Considering the deficiency of multi-functional attacking strategies and the significance for open-world perception scenarios, we are the first to propose a Cascaded Adversarial Attack (CAA) setting, where the adversarial example can simultaneously attack different-level tasks, such as rain removal and semantic segmentation in an integrated system. Specifically, our attack on the rain removal network aims to preserve rain streaks in the output image, while for the semantic segmentation network, we employ powerful existing adversarial attack methods to induce misclassification of the image content. Importantly, CAA innovatively utilizes binary masks to effectively concentrate the aforementioned two significantly disparate perturbation distributions on the input image, enabling attacks on both networks. Additionally, we propose two variants of CAA, which minimize the differences between the two generated perturbations by introducing a carefully designed perturbation interaction mechanism, resulting in enhanced attack performance. Extensive experiments validate the effectiveness of our methods, demonstrating their superior ability to significantly degrade the performance of the downstream task compared to methods that solely attack a single network.",
      "year": 2024,
      "venue": "ACM Multimedia",
      "authors": [
        "Zhiwen Wang",
        "Yuhui Wu",
        "Zheng Wang",
        "Jiwei Wei",
        "Tianyu Li",
        "Guoqing Wang",
        "Yang Yang",
        "H. Shen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a787cd3ec9a725bc36ecaec157ba2656779c2ab9",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1dafd2ee9339e41efdc6cbcf36e1741c5d10accd",
      "title": "Evolutionary Multilabel Adversarial Examples: An Effective Black-Box Attack",
      "abstract": "Studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attack. Minor malicious modifications of examples will lead to the DNN misclassification. Such maliciously modified examples are called adversarial examples. So far, the work on adversarial examples is mainly focused on multiclass classification tasks; there is less work in the field of multilabel classification. In this article, for the first time, a differential evolution (DE) algorithm that can effectively generate multilabel adversarial examples is proposed, which is called MLAE-DE. Different from traditional DE, we designed a complementary mutation operator for MLAE-DE, which can improve attack performance and reduce the number of fitness evaluations. As a black-box attack, MLAE-DE does not need to access model parameters and only uses model outputs to generate adversarial examples. Experiments on two typical multilabel classification models and three typical datasets under the black-box settings are conducted in this article. Experimental results demonstrate that, comparing with the existing black-box attack algorithms for multilabel classification models, the attack success rate of our proposed algorithm is much better.",
      "year": 2023,
      "venue": "IEEE Transactions on Artificial Intelligence",
      "authors": [
        "Linghao Kong",
        "Wenjian Luo",
        "Hongwei Zhang",
        "Yang Liu",
        "Yuhui Shi"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/1dafd2ee9339e41efdc6cbcf36e1741c5d10accd",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4251e5e1602e4627f8b9eb07ea0b5ddb08be1ddb",
      "title": "Robustness with Query-efficient Adversarial Attack using Reinforcement Learning",
      "abstract": "A measure of robustness against naturally occurring distortions is key to the safety, success, and trustworthiness of machine learning models on deployment. We propose an adversarial black-box attack that adds minimum Gaussian noise distortions to input images to make machine learning models misclassify. We used a Reinforcement Learning (RL) agent as a smart hacker to explore the input images to add minimum distortions to the most sensitive regions to induce misclassification. The agent employs a smart policy also to remove noises introduced earlier, which has less impact on the trained model at a given state. This novel approach is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to faster and optimal convergence. Also, this adversarial attack method effectively measures the robustness of image classification models with the misclassification inducing minimum L2 distortion of Gaussian noise similar to many naturally occurring distortions. Furthermore, the proposed black-box L2 adversarial attack tool beats state-of-the-art competitors in terms of the average number of queries by a significant margin with a 100% success rate while maintaining a very competitive L2 score, despite limiting distortions to Gaussian noise. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31% better than the state-of-the-art \"Square-Attack\" approach while maintaining a competitive L2.Demo: https://tinyurl.com/yr8f7x9t",
      "year": 2023,
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "S. Sarkar",
        "Ashwin Ramesh Babu",
        "Sajad Mousavi",
        "Sahand Ghorbanpour",
        "Vineet Gundecha",
        "Antonio Guillen",
        "Ricardo Luna",
        "Avisek Naug"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/4251e5e1602e4627f8b9eb07ea0b5ddb08be1ddb",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7613c5641eec43cdad87e9c129ef5f96d1a87d0f",
      "title": "Reinforcement Learning Based Black-Box Adversarial Attack for Robustness Improvement",
      "abstract": "We propose a Reinforcement Learning (RL) based adversarial Black-box attack (RLAB) that aims at adding minimum distortion to the input iteratively to deceive image classification models. The RL agent learns to identify highly sensitive regions in the input's feature space to add distortions to induce misclassification with minimum steps and L2 norm. The agent also selectively removes noises introduced at earlier steps in the iteration, which has less impact on the model at a given state. This novel dual-action method is equivalent to doing a deep tree search to add noises without an exhaustive search, leading to the faster generation of an optimum adversarial sample. This black-box method focuses on naturally occurring distortion to effectively measure the robustness of models, a key element of trustworthiness. The proposed method beats existing heuristic based state-of-the-art black-box adversarial attacks on metrics such as the number of queries, L2 norm, and success rate on ImageNet and CIFAR-10 datasets. For the ImageNet dataset, the average number of queries achieved by the proposed method for ResNet-50, Inception-V3, and VGG-16 models are 42%, 32%, and 31 % better than the popular \u201cSquare Attack\u201d. Furthermore, retraining the model with adversarial samples significantly improved robustness when evaluated on benchmark datasets such as CIFAR-10-C with the metrics of adversarial error and mean corruption error (mCE). Demo: https://tinyurl.com/yr8f7x9t",
      "year": 2023,
      "venue": "2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)",
      "authors": [
        "S. Sarkar",
        "Ashwin Ramesh Babu",
        "Sajad Mousavi",
        "Sahand Ghorbanpour",
        "Vineet Gundecha",
        "Ricardo Luna Guti\u00e9rrez",
        "Antonio Guillen-Perez",
        "Avisek Naug"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/7613c5641eec43cdad87e9c129ef5f96d1a87d0f",
      "pdf_url": "",
      "publication_date": "2023-08-26",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "612a0e4af38b273fbd29070a4a769f551cfd1029",
      "title": "Robustness with Black-Box Adversarial Attack using Reinforcement Learning",
      "abstract": null,
      "year": 2023,
      "venue": "SafeAI@AAAI",
      "authors": [
        "S. Sarkar",
        "Ashwin Ramesh Babu",
        "Sajad Mousavi",
        "Vineet Gundecha",
        "Sahand Ghorbanpour",
        "A. Shmakov",
        "Ricardo Luna Gutierrez",
        "Antonio Guillen",
        "Avisek Naug"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/612a0e4af38b273fbd29070a4a769f551cfd1029",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2bbd0cf97953f972db56afcc73fadaf113dcb8b0",
      "title": "Artistic image adversarial attack via style perturbation",
      "abstract": null,
      "year": 2023,
      "venue": "Multimedia Systems",
      "authors": [
        "Haiyan Zhang",
        "Quan Wang",
        "Guorui Feng"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2bbd0cf97953f972db56afcc73fadaf113dcb8b0",
      "pdf_url": "",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ce0312d3773ee8e873d5eeeada19ec369c5aa9e",
      "title": "Rearranging Pixels is a Powerful Black-Box Attack for RGB and Infrared Deep Learning Models",
      "abstract": "Recent research has found that neural networks for computer vision are vulnerable to several types of external attacks that modify the input of the model, with the malicious intent of producing a misclassification. With the increase in the number of feasible attacks, many defence approaches have been proposed to mitigate the effect of these attacks and protect the models. Mainly, the research on both attack and defence has focused on RGB images, while other domains, such as the infrared domain, are currently underexplored. In this paper, we propose two attacks, and we evaluate them on multiple datasets and neural network models, showing that the results outperform others established attacks, on both RGB as well as infrared domains. In addition, we show that our proposal can be used in an adversarial training protocol to produce more robust models, with respect to both adversarial attacks and natural perturbations that can be applied to input images. Lastly, we study if a successful attack in a domain can be transferred to an aligned image in another domain, without any further tuning. The code, containing all the files and the configurations used to run the experiments, is available https://github.com/jaryP/IR-RGB-domain-attackonline.",
      "year": 2023,
      "venue": "IEEE Access",
      "authors": [
        "Jary Pomponi",
        "Daniele Dantoni",
        "Nicolosi Alessandro",
        "Simone Scardapane"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7ce0312d3773ee8e873d5eeeada19ec369c5aa9e",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10034751.pdf",
      "publication_date": null,
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "862ee8ee9997d5aafe062a624b6de1b5bf534a1b",
      "title": "Single-Class Target-Specific Attack against Interpretable Deep Learning Systems",
      "abstract": "In this paper, we present a novel Single-class target-specific Adversarial attack called SingleADV. The goal of SingleADV is to generate a universal perturbation that deceives the target model into confusing a specific category of objects with a target category while ensuring highly relevant and accurate interpretations. The universal perturbation is stochastically and iteratively optimized by minimizing the adversarial loss that is designed to consider both the classifier and interpreter costs in targeted and non-targeted categories. In this optimization framework, ruled by the first- and second-moment estimations, the desired loss surface promotes high confidence and interpretation score of adversarial samples. By avoiding unintended misclassification of samples from other categories, SingleADV enables more effective targeted attacks on interpretable deep learning systems in both white-box and black-box scenarios. To evaluate the effectiveness of SingleADV, we conduct experiments using four different model architectures (ResNet-50, VGG-16, DenseNet-169, and Inception-V3) coupled with three interpretation models (CAM, Grad, and MASK). Through extensive empirical evaluation, we demonstrate that SingleADV effectively deceives the target deep learning models and their associated interpreters under various conditions and settings. Our experimental results show that the performance of SingleADV is effective, with an average fooling ratio of 0.74 and an adversarial confidence level of 0.78 in generating deceptive adversarial samples. Furthermore, we discuss several countermeasures against SingleADV, including a transfer-based learning approach and existing preprocessing defenses.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Eldor Abdukhamidov",
        "Mohammed Abuhamad",
        "G. Thiruvathukal",
        "Hyoungshick Kim",
        "Tamer Abuhmed"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/862ee8ee9997d5aafe062a624b6de1b5bf534a1b",
      "pdf_url": "https://arxiv.org/pdf/2307.06484",
      "publication_date": "2023-07-12",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "799397aabb0389315c1181268978ceceb88e704c",
      "title": "Graph-based methods coupled with specific distributional distances for adversarial attack detection",
      "abstract": "Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These adversarial attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an input image, we compute an associated sparse graph using the layer-wise relevance propagation algorithm (Bach et al., 2015). Specifically, we only keep edges of the neural network with the highest relevance values. Three quantities are then computed from the graph which are then compared against those computed from the training set. The result of the comparison is a classification of the image as benign or adversarial. To make the comparison, two classification methods are introduced: (1) an explicit formula based on Wasserstein distance applied to the degree of node and (2) a logistic regression. Both classification methods produce strong results which lead us to believe that a graph-based interpretation of adversarial attacks is valuable.",
      "year": 2023,
      "venue": "Neural Networks",
      "authors": [
        "dwight nwaigwe",
        "Lucrezia Carboni",
        "Martial Mermillod",
        "S. Achard",
        "M. Dojat"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/799397aabb0389315c1181268978ceceb88e704c",
      "pdf_url": "https://arxiv.org/pdf/2306.00042",
      "publication_date": "2023-05-31",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ce1e00da74f54027ec000d0f08b519176014ac18",
      "title": "Evolutionary Art Attack for Black-Box Adversarial Example Generation",
      "abstract": "Deep neural networks (DNNs) have achieved remarkable performance in various tasks, including image classification. However, recent research has revealed the susceptibility of trained DNNs to subtle perturbations introduced into input images. Addressing these vulnerabilities is pivotal, leading to a significant area of study focused on developing attack algorithms capable of generating potent adversarial images. In scenarios where access to gradient information is restricted (black-box scenario), many existing methods introduce optimized perturbations to each individual pixels of an image to cause trained DNNs to misclassify. However, due to the high-dimensional nature of this approach, current methods have inherent limitations. In contrast, our proposed approach involves the construction of perturbations by concatenating a series of overlapping semi-transparent shapes. Through the optimization of these shapes\u2019 characteristics, we generate perturbations that result in the desired misclassification by the DNN. By conducting a series of attacks on state-of-the-art DNNs trained of CIFAR-10 and Imagenet datasets, our method consistently outperforms existing attack algorithms in terms of both query efficiency and success rate.",
      "year": 2025,
      "venue": "IEEE Transactions on Evolutionary Computation",
      "authors": [
        "P. Williams",
        "Ke Li",
        "Geyong Min"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/ce1e00da74f54027ec000d0f08b519176014ac18",
      "pdf_url": "",
      "publication_date": "2025-08-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0198ca426c5e264f21f8772f4897e6ae0bd85f67",
      "title": "GreedyPixel: Fine-Grained Black-Box Adversarial Attack via Greedy Algorithm",
      "abstract": "Deep neural networks are highly vulnerable to adversarial examples, which are inputs with small, carefully crafted perturbations that cause misclassification\u2014making adversarial attacks a critical tool for evaluating robustness. Existing black-box methods typically entail a trade-off between precision and flexibility: pixel-sparse attacks (e.g., single- or few-pixel attacks) provide fine-grained control but lack adaptability, whereas patch- or frequency-based attacks improve efficiency or transferability, but at the cost of producing larger and less precise perturbations. We present GreedyPixel, a fine-grained black-box attack method that performs brute-force-style, per-pixel greedy optimization guided by a surrogate-derived priority map and refined by means of query feedback. It evaluates each coordinate directly without any gradient information, guaranteeing monotonic loss reduction and convergence to a coordinate-wise optimum, while also yielding near white-box-level precision and pixel-wise sparsity and perceptual quality. On the CIFAR-10 and ImageNet datasets, spanning convolutional neural networks (CNNs) and Transformer models, GreedyPixel achieved state-of-the-art success rates with visually imperceptible perturbations, effectively bridging the gap between black-box practicality and white-box performance. The implementation is available at https://github.com/azrealwang/greedypixel",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Hanrui Wang",
        "Ching-Chun Chang",
        "Chun-Shien Lu",
        "Christopher Leckie",
        "Isao Echizen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0198ca426c5e264f21f8772f4897e6ae0bd85f67",
      "pdf_url": "",
      "publication_date": "2025-01-24",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8395eaad22b603891f68100e000d97e4c50342bf",
      "title": "Random Perturbation Attack on LLMs for Code Generation",
      "abstract": "Large language models (LLMs) have shown impressive capabilities in coding tasks, including code understanding and generation. However, these models are also susceptible to input perturbations, such as case changes, whitespace or typo modifications, which can affect their performance. This study investigates the impact of different types of perturbations on code and natural language on the performance of LLM code generation tasks. In addition to evaluating individual perturbations, the research examines combined perturbation attacks, where multiple perturbations from different categories are applied together. While combined attacks showed only marginal overall improvement over individual ones, they demonstrated a synergistic effect in specific scenarios, exploiting complementary vulnerabilities in the models.",
      "year": 2025,
      "venue": "2025 IEEE/ACM 4th International Conference on AI Engineering \u2013 Software Engineering for AI (CAIN)",
      "authors": [
        "Qiulu Peng",
        "Chi Zhang",
        "Ravi Mangal",
        "Corina S. P\u0103s\u0103reanu",
        "Limin Jia"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8395eaad22b603891f68100e000d97e4c50342bf",
      "pdf_url": "",
      "publication_date": "2025-04-27",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df6e0f138915817a0cf228cd4448747c2b72188d",
      "title": "A Perturbation Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow",
      "abstract": "Recent optical flow methods are almost exclusively judged in terms of accuracy, while their robustness is often neglected. Although adversarial attacks offer a useful tool to perform such an analysis, current attacks on optical flow methods focus on real-world attacking scenarios rather than a worst case robustness assessment. Hence, in this work, we propose a novel adversarial attack - the Perturbation-Constrained Flow Attack (PCFA) - that emphasizes destructivity over applicability as a real-world attack. PCFA is a global attack that optimizes adversarial perturbations to shift the predicted flow towards a specified target flow, while keeping the L2 norm of the perturbation below a chosen bound. Our experiments demonstrate PCFA's applicability in white- and black-box settings, and show it finds stronger adversarial samples than previous attacks. Based on these strong samples, we provide the first joint ranking of optical flow methods considering both prediction quality and adversarial robustness, which reveals state-of-the-art methods to be particularly vulnerable. Code is available at https://github.com/cv-stuttgart/PCFA.",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Jenny Schmalfuss",
        "Philipp Scholze",
        "Andr\u00e9s Bruhn"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/df6e0f138915817a0cf228cd4448747c2b72188d",
      "pdf_url": "http://arxiv.org/pdf/2203.13214",
      "publication_date": "2022-03-24",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0004e150d93d52624a2caf7a9d37e2be9d4dae3f",
      "title": "Sparse Adversarial Attack via Perturbation Factorization",
      "abstract": null,
      "year": 2020,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Yanbo Fan",
        "Baoyuan Wu",
        "T. Li",
        "Yong Zhang",
        "Mingyang Li",
        "Zhifeng Li",
        "Yujiu Yang"
      ],
      "citation_count": 88,
      "url": "https://www.semanticscholar.org/paper/0004e150d93d52624a2caf7a9d37e2be9d4dae3f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f5256b83f435b4d79b3c48a33b5fffc3cbf0209b",
      "title": "Perturbation-Based Diagnosis of False Data Injection Attack Using Distributed Energy Resources",
      "abstract": "Modern smart grid relies on various sensor measurements for its operational control. In a successful false data injection attack, the attacker manipulates the measurements from the grid sensors such that undetected errors are introduced into the estimates of the system parameters leading to catastrophic situations. This article proposes a novel perturbation based false data injection attack detection mechanism that utilizes inverter based distributed energy resources (DERs) to create low magnitude perturbation signal in the distribution system voltage that is inconsequential to the normal grid operation. Two voltage sensitivity analysis based algorithms are designed to identify the optimal set of DERs that can create the voltage perturbation signal of desired magnitude. An analytical method of voltage sensitivity analysis is used to compute the magnitude of voltage perturbation signal at each node in a computationally efficient manner. Then, a detection mechanism is developed that checks for the presence of the perturbation sequence in each sensor measurement. A sensor measurement is deemed authentic if the voltage perturbation signal is present in the data. In case of sensor malfunction or cyber-attack, the perturbation signal will not be present in the measurement data. Performance of the proposed attack detection mechanism is validated via simulation of the IEEE 69 bus test system.",
      "year": 2021,
      "venue": "IEEE Transactions on Smart Grid",
      "authors": [
        "K. Jhala",
        "P. Pradhan",
        "B. Natarajan"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/f5256b83f435b4d79b3c48a33b5fffc3cbf0209b",
      "pdf_url": "https://doi.org/10.1109/tsg.2020.3029954",
      "publication_date": "2021-03-01",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "07968742c9ae269e0023a7540b0a7e51bbc7f29a",
      "title": "Towards Feature Space Adversarial Attack by Style Perturbation",
      "abstract": "We propose a new adversarial attack to Deep Neural Networks for image classification. Different from most existing attacks that directly perturb input pixels, our attack focuses on perturbing abstract features, more specifically, features that denote styles, including interpretable styles such as vivid colors and sharp outlines, and uninterpretable ones. It induces model misclassification by injecting imperceptible style changes through an optimization procedure.\nWe show that our attack can generate adversarial samples that are more natural-looking than the state-of-the-art unbounded attacks. The experiment also supports that existing pixel-space adversarial attack detection and defense techniques can hardly ensure robustness in the style-related feature space.",
      "year": 2021,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Qiuling Xu",
        "Guanhong Tao",
        "Siyuan Cheng",
        "Xiangyu Zhang"
      ],
      "citation_count": 31,
      "url": "https://www.semanticscholar.org/paper/07968742c9ae269e0023a7540b0a7e51bbc7f29a",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17259/17066",
      "publication_date": "2021-05-18",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3653bd782d142836d770eac78536d07780534bde",
      "title": "Robust Network Alignment via Attack Signal Scaling and Adversarial Perturbation Elimination",
      "abstract": "Recent studies have shown that graph learning models are highly vulnerable to adversarial attacks, and network alignment methods are no exception. How to enhance the robustness of network alignment against adversarial attacks remains an open research problem. In this paper, we propose a robust network alignment solution, RNA, for offering preemptive protection of existing network alignment algorithms, enhanced with the guidance of effective adversarial attacks. First, we analyze how popular iterative gradient-based adversarial attack techniques suffer from gradient vanishing issues and show a fake sense of attack effectiveness. Based on dynamical isometry theory, an attack signal scaling (ASS) method with established upper bound of feasible signal scaling is introduced to alleviate the gradient vanishing issues for effective adversarial attacks while maintaining the decision boundary of network alignment. Second, we develop an adversarial perturbation elimination (APE) model to neutralize adversarial nodes in vulnerable space to adversarial-free nodes in safe area, by integrating Dirac delta approximation (DDA) techniques and the LSTM models. Our proposed APE method is able to provide proactive protection to existing network alignment algorithms against adversarial attacks. The theoretical analysis demonstrates the existence of an optimal distribution for the APE model to reach a lower bound. Last but not least, extensive evaluation on real datasets presents that RNA is able to offer the preemptive protection to trained network alignment methods against three popular adversarial attack models.",
      "year": 2021,
      "venue": "The Web Conference",
      "authors": [
        "Yang Zhou",
        "Zeru Zhang",
        "Sixing Wu",
        "Victor Sheng",
        "Xiaoying Han",
        "Zijie Zhang",
        "R. Jin"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/3653bd782d142836d770eac78536d07780534bde",
      "pdf_url": "",
      "publication_date": "2021-04-19",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "097f055e2734db8e15c86f2a2f5e5173d6572c34",
      "title": "Adaptive Perturbation for Adversarial Attack",
      "abstract": "In recent years, the security of deep learning models achieves more and more attentions with the rapid development of neural networks, which are vulnerable to adversarial examples. Almost all existing gradient-based attack methods use the sign function in the generation to meet the requirement of perturbation budget on <inline-formula><tex-math notation=\"LaTeX\">$L_\\infty$</tex-math><alternatives><mml:math><mml:msub><mml:mi>L</mml:mi><mml:mi>\u221e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"yuan-ieq1-3367773.gif\"/></alternatives></inline-formula> norm. However, we find that the sign function may be improper for generating adversarial examples since it modifies the exact gradient direction. Instead of using the sign function, we propose to directly utilize the exact gradient direction with a scaling factor for generating adversarial perturbations, which improves the attack success rates of adversarial examples even with fewer perturbations. At the same time, we also theoretically prove that this method can achieve better black-box transferability. Moreover, considering that the best scaling factor varies across different images, we propose an adaptive scaling factor generator to seek an appropriate scaling factor for each image, which avoids the computational cost for manually searching the scaling factor. Our method can be integrated with almost all existing gradient-based attack methods to further improve their attack success rates. Extensive experiments on the CIFAR10 and ImageNet datasets show that our method exhibits higher transferability and outperforms the state-of-the-art methods.",
      "year": 2021,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Zheng Yuan",
        "J. Zhang",
        "S. Shan"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/097f055e2734db8e15c86f2a2f5e5173d6572c34",
      "pdf_url": "https://arxiv.org/pdf/2111.13841",
      "publication_date": "2021-11-27",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97339c2eae697ef35f940d35cc189603b2fb35df",
      "title": "Rethinking Perturbation Directions for Imperceptible Adversarial Attacks on Point Clouds",
      "abstract": "Adversarial attacks have been successfully extended to the field of point clouds. Besides applying the common perturbation guided by the gradient, adversarial attacks on point clouds can be conducted by applying directional perturbations, e.g., along normal and along the tangent plane. In this article, we first investigate whether adversarial attacks with these two orthogonal directional perturbations are more imperceptible than that with the gradient-aware perturbation. Second, we investigate the deeper difference between adversarial attacks with these two directional perturbations, and whether they are applicable to the same scenarios. Third, based on the verification results that the above two directional perturbations have different sensitiveness to curvature, we devise a novel normal-tangent attack (NTA) framework with a hybrid directional perturbation scheme that adaptively chooses the direction according to the curvature of the local shape around the point. Extensive experiments on two publicly available data sets, e.g., ModelNet40 and ShapeNet Part, with classifiers in three representative networks, e.g., PointNet++, DGCNN, PointConv, validate the effectiveness of NTA, and the superiority to the state-of-the-art methods.",
      "year": 2023,
      "venue": "IEEE Internet of Things Journal",
      "authors": [
        "Keke Tang",
        "Yawen Shi",
        "Tianrui Lou",
        "Weilong Peng",
        "Xu He",
        "Peican Zhu",
        "Zhaoquan Gu",
        "Zhihong Tian"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/97339c2eae697ef35f940d35cc189603b2fb35df",
      "pdf_url": "",
      "publication_date": "2023-03-15",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8e62bc255683670fabf2dc9536e14499c44242b3",
      "title": "Only Once Attack: Fooling the Tracker With Adversarial Template",
      "abstract": "Adversarial attacks in visual object tracking aims to fool trackers via injecting invisible perturbations for the video frames. Most adversarial methods advocate generating perturbations for each video frame, but frequent attacks may increase the computational load and the risk of exposure. Unfortunately, less works are about only attacking the initial frame and their attack effects are insufficient. To tackle this, we focus on the initialization phase of tracking and propose an only once attack framework. It can effectively fool the tracker via only generating invisible perturbations for the initial template, rather than each frame. Specifically, considering the tracking mechanism of the Siamese-based trackers, we design the minimum score-based and the minimum IoU-based loss functions. Both of them are used for training the UNet-based perturbation generator instead of the tracker, achieving the non-targeted attack. Additionally, we propose the location and direction offsets as the base attacks of sophisticated targeted attack. Combined with the two basic attacks, the tracker can be easily hijacked to move towards the fake target predefined by users. Extensive experimental results demonstrate that our only once attack framework costs the least number of attacks yet achieves better attack effect, with the maximum drop of 68.7%. The transferability experiments illustrate that our attack framework with good generalization ability can be directly applicable to the CNN-based, Siamese-based, deep discriminative-based and Transformer-based trackers, without retraining.",
      "year": 2023,
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "authors": [
        "Ze Zhou",
        "Yinghui Sun",
        "Quansen Sun",
        "Chaobo Li",
        "Zhenwen Ren"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/8e62bc255683670fabf2dc9536e14499c44242b3",
      "pdf_url": "",
      "publication_date": "2023-07-01",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "91a05cb84f1c7dbb0354da2ff11ae92549152435",
      "title": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack",
      "abstract": "The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \\in \\{1,2,\\infty\\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient masking.",
      "year": 2019,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Francesco Croce",
        "Matthias Hein"
      ],
      "citation_count": 558,
      "url": "https://www.semanticscholar.org/paper/91a05cb84f1c7dbb0354da2ff11ae92549152435",
      "pdf_url": "",
      "publication_date": "2019-07-03",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1e0484607a57687da6f663954292b0c848141f1",
      "title": "Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving",
      "abstract": "In Autonomous Vehicles (AVs), one fundamental pillar is perception,which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process.Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function.We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility.We also discuss defense directions at the AV system, sensor, and machine learning model levels.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Yulong Cao",
        "Chaowei Xiao",
        "Benjamin Cyr",
        "Yimeng Zhou",
        "Wonseok Park",
        "Sara Rampazzi",
        "Qi Alfred Chen",
        "Kevin Fu",
        "Z. Morley Mao"
      ],
      "citation_count": 597,
      "url": "https://www.semanticscholar.org/paper/f1e0484607a57687da6f663954292b0c848141f1",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3319535.3339815",
      "publication_date": "2019-07-16",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d74d87789b993d84b1e2356be3b2e6ef43c7f79",
      "title": "Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph learning tasks. However, recent studies show that GNNs are vulnerable to both test-time evasion and training-time poisoning attacks that perturb the graph structure. While existing attack methods have shown promising attack performance, we would like to design an attack framework to further enhance the performance. In particular, our attack framework is inspired by certified robustness, which was originally used by defenders to defend against adversarial attacks. We are the first, from the attacker perspective, to leverage its properties to better attack GNNs. Specifically, we first derive nodes' certified perturbation sizes against graph evasion and poisoning attacks based on randomized smoothing, respectively. A larger certified perturbation size of a node indicates this node is theoretically more robust to graph perturbations. Such a property motivates us to focus more on nodes with smaller certified perturbation sizes, as they are easier to be attacked after graph perturbations. Accordingly, we design a certified robustness inspired attack loss, when incorporated into (any) existing attacks, produces our certified robustness inspired attack counterpart. We apply our frame-work to the existing attacks and results show it can significantly enhance the existing base attacks' performance.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Binghui Wang",
        "Meng Pang",
        "Yun Dong"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/9d74d87789b993d84b1e2356be3b2e6ef43c7f79",
      "pdf_url": "http://arxiv.org/pdf/2303.06199",
      "publication_date": "2023-03-10",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "03215fa5b945ee7ab1ac7dba1576c43f96665a11",
      "title": "Adversarial Image Perturbation for Privacy Protection A Game Theory Perspective",
      "abstract": "Users like sharing personal photos with others through social media. At the same time, they might want to make automatic identification in such photos difficult or even impossible. Classic obfuscation methods such as blurring are not only unpleasant but also not as effective as one would expect [28, 37, 18]. Recent studies on adversarial image perturbations (AIP) suggest that it is possible to confuse recognition systems effectively without unpleasant artifacts. However, in the presence of counter measures against AIPs [7], it is unclear how effective AIP would be in particular when the choice of counter measure is unknown. Game theory provides tools for studying the interaction between agents with uncertainties in the strategies. We introduce a general game theoretical framework for the user-recogniser dynamics, and present a case study that involves current state of the art AIP and person recognition techniques. We derive the optimal strategy for the user that assures an upper bound on the recognition rate independent of the recogniser\u2019s counter measure. Code is available at https://goo.gl/hgvbNK.",
      "year": 2017,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Seong Joon Oh",
        "Mario Fritz",
        "B. Schiele"
      ],
      "citation_count": 166,
      "url": "https://www.semanticscholar.org/paper/03215fa5b945ee7ab1ac7dba1576c43f96665a11",
      "pdf_url": "https://arxiv.org/pdf/1703.09471",
      "publication_date": "2017-03-28",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d745e2fefd86d0134692606b40e3286cf38adc83",
      "title": "Adversarial Image Perturbation with a Genetic Algorithm",
      "abstract": "The quality of image recognition with neural network models relies heavily on filters and parameters optimized through the training process. These filters are di\u02d9erent compared to how humans see and recognize objects around them. The di\u02d9erence in machine and human recognition yields a noticeable gap, which is prone to exploitation. The workings of these algorithms can be compromised with adversarial perturbations of images. This is where images are seemingly modified imperceptibly, such that humans see little to no di\u02d9erence, but the neural network classifies t he m otif i ncorrectly. This paper explores the adversarial image modifica-tion with an evolutionary algorithm, so that the AlexNet convolutional neural network cannot recognize previously clear motifs while preserving the human perceptibility of the image. The ex-periment was implemented in Python and tested on the ILSVRC dataset. Original images and their recreated counterparts were compared and contrasted using visual assessment and statistical metrics. The findings s uggest t hat t he human eye, without prior knowledge, will hardly spot the di\u02d9erence compared to the original images.",
      "year": 2021,
      "venue": "Proceedings of the 2021 7th Student Computer Science Research Conference (StuCoSReC)",
      "authors": [
        "Rok Kukovec",
        "\u0160pela Pe\u010dnik",
        "Iztok Fister Jr.",
        "S. Karakati\u010d"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d745e2fefd86d0134692606b40e3286cf38adc83",
      "pdf_url": "https://doi.org/10.18690/978-961-286-516-0.6",
      "publication_date": "2021-09-13",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f57d4e7712d0932137d7da7ba4f7a2fb72115e8",
      "title": "A Novel Image Perturbation Approach: Perturbing Latent Representation",
      "abstract": "Deep neural networks are state-of-art models in computer vision and image recognition problems. However, it is shown that these models are highly vulnerable to intentionally perturbed inputs named adversarial examples. This problem has attracted a lot of attention in recent years. In this paper, a novel approach is proposed for generating adversarial examples by perturbing latent representation of an input image that causes to mislead trained classifier network. Also, it is shown that perturbing dense representation of image results in transforming key features of it with respect to classification task. Our experimental results show that this slight transformation in the features of the image can easily fool the classifier network. We also show the impact of adding perturbations with the large magnitude to the corresponding generated adversarial example.",
      "year": 2019,
      "venue": "2019 27th Iranian Conference on Electrical Engineering (ICEE)",
      "authors": [
        "Nader Asadi",
        "M. Eftekhari"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f57d4e7712d0932137d7da7ba4f7a2fb72115e8",
      "pdf_url": "",
      "publication_date": "2019-04-01",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "063bc629bcb1baac01f938019bece4c63be1b047",
      "title": "Deep Perturbation Learning: Enhancing the Network Performance via Image Perturbations",
      "abstract": null,
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zifan Song",
        "Xiao Gong",
        "Guosheng Hu",
        "Cairong Zhao"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/063bc629bcb1baac01f938019bece4c63be1b047",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aa942f5c4d5022dcdc197a14e93ac05f7bd61d43",
      "title": "Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation",
      "abstract": "Unpaired image-to-image translation (I2I) is an ill-posed problem, as an infinite number of translation functions can map the source domain distribution to the target distribution. Therefore, much effort has been put into designing suitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency (GCGAN), and contrastive learning-based constraints (CUTGAN), that help better pose the problem. However, these well-known constraints have limitations: (1) they are either too restrictive or too weak for specific I2I tasks; (2) these methods result in content distortion when there is a significant spatial variation between the source and target domains. This paper proposes a universal regularization technique called maximum spatial perturbation consistency (MSPC), which enforces a spatial perturbation function $(T)$ and the translation operator $(G)$ to be commutative (i.e., $T\\circ G=G\\circ T)$. In addition, we introduce two adversarial training components for learning the spatial perturbation function. The first one lets $T$ compete with $G$ to achieve maximum perturbation. The second one lets $G$ and $T$ compete with discriminators to align the spatial variations caused by the change of object size, object distortion, background interruptions, etc. Our method outperforms the state-of-the-art methods on most I2I benchmarks. We also introduce a new benchmark, namely the front face to profile face dataset, to emphasize the underlying challenges of I2I for real-world applications. We finally perform ablation experiments to study the sensitivity of our method to the severity of spatial perturbation and its effectiveness for distribution alignment.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Yanwu Xu",
        "Shaoan Xie",
        "Wenhao Wu",
        "Kun Zhang",
        "Mingming Gong",
        "K. Batmanghelich"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/aa942f5c4d5022dcdc197a14e93ac05f7bd61d43",
      "pdf_url": "https://arxiv.org/pdf/2203.12707",
      "publication_date": "2022-03-23",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ce2b85de012ba134d186e5793e3b7b4c18e10890",
      "title": "Adversarial perturbation in remote sensing image recognition",
      "abstract": null,
      "year": 2021,
      "venue": "Applied Soft Computing",
      "authors": [
        "Shanshan Ai",
        "Arthur Sandor Voundi Koe",
        "Teng Huang"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/ce2b85de012ba134d186e5793e3b7b4c18e10890",
      "pdf_url": "",
      "publication_date": "2021-03-10",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7e42674800902d8361a210992322b1c3b47c9c36",
      "title": "Fooled by Imagination: Adversarial Attack to Image Captioning Via Perturbation in Complex Domain",
      "abstract": "Adversarial attacks are very successful on image classification, but there are few researches on vision-language systems, such as image captioning. In this paper, we study the robustness of a CNN+RNN based image captioning system being subjected to adversarial noises in complex domain. In particular, we propose Fooled-by-Imagination, a novel algorithm for crafting adversarial examples with semantic embedding of targeted caption as perturbation in complex domain. The proposed algorithm explores the great merit of complex values in introducing imaginary part for modeling adversarial perturbation, and maintains the similarity of the image in real part. Our approach provides two evaluation approaches, which check whether neural image captioning systems can be fooled to output some randomly chosen captions or keywords. Besides, our method has good transferability under black-box setting. At last, our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords at a higher success rate.",
      "year": 2020,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Shaofeng Zhang",
        "Zheng Wang",
        "Xing Xu",
        "Xiang Guan",
        "Yang Yang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/7e42674800902d8361a210992322b1c3b47c9c36",
      "pdf_url": "",
      "publication_date": "2020-07-01",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7380e343dd4547e21d5118b16daf03d021d98c4e",
      "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation",
      "abstract": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \u201clook\u201d in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.",
      "year": 2017,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Ruth C. Fong",
        "A. Vedaldi"
      ],
      "citation_count": 1616,
      "url": "https://www.semanticscholar.org/paper/7380e343dd4547e21d5118b16daf03d021d98c4e",
      "pdf_url": "https://arxiv.org/pdf/1704.03296",
      "publication_date": "2017-04-11",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e141e29d068af4bdb929768ba949044590ac33c7",
      "title": "MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model",
      "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where an imperceptible perturbation is added to the image that can fool the DNNs. Diffusion-based adversarial purification uses the diffusion model to generate a clean image against such adversarial attacks. Un-fortunately, the generative process of the diffusion model is also inevitably affected by adversarial perturbation since the diffusion model is also a deep neural network where its input has adversarial perturbation. In this work, we propose MimicDiffusion, a new diffusion-based adversarial purification technique that directly approximates the generative process of the diffusion model with the clean image as input. Concretely, we analyze the differences between the guided terms using the clean image and the ad-versarial sample. After that, we first implement MimicD-iffusion based on Manhattan distance. Then, we propose two guidance to purify the adversarial perturbation and ap-proximate the clean diffusion model. Extensive experiments on three image datasets, including CIFAR-10, CIFAR-100, and ImageNet, with three classifier backbones including WideResNet-70-16, WideResNet-28-10, and ResNet-50 demonstrate that MimicDiffusion significantly performs better than the state-of-the-art baselines. On CIFAR -10, CIFAR-100, and ImageNet, it achieves 92.67%, 61.35%, and 61.53% average robust accuracy, which are 18.49%, 13.23%, and 17.64% higher, respectively. The code is available at https://github.com/psky1111/MimicDiffusion.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Kaiyu Song",
        "Hanjiang Lai"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/e141e29d068af4bdb929768ba949044590ac33c7",
      "pdf_url": "http://arxiv.org/pdf/2312.04802",
      "publication_date": "2023-12-08",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d544dd9a6ba9ea5f2c1217bc554cf7fded732fbf",
      "title": "Anti-DreamBooth: Protecting users from personalized text-to-image synthesis",
      "abstract": "Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user\u2019s image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of Dream-Booth and Diffusion-based text-to-image models, our methods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse conditions, such as model or prompt/term mismatching between training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.git.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Van Thanh Le",
        "Hao Phung",
        "Thuan Hoang Nguyen",
        "Quan Dao",
        "Ngoc N. Tran",
        "A. Tran"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/d544dd9a6ba9ea5f2c1217bc554cf7fded732fbf",
      "pdf_url": "https://arxiv.org/pdf/2303.15433",
      "publication_date": "2023-03-27",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "199f8fbf558d4eadaa86bdf4201f560ca69a0bf0",
      "title": "Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis",
      "abstract": "A plethora of attribution methods have recently been developed to explain deep neural networks. These methods use different classes of perturbations (e.g, occlusion, blurring, masking, etc) to estimate the importance of individual image pixels to drive a model's decision. Nevertheless, the space of possible perturbations is vast and current attribution methods typically require significant computation time to accurately sample the space in order to achieve high-quality explanations. In this work, we introduce EVA (Explaining using Verified Perturbation Analysis) - the first explainability method which comes with guarantees that an entire set of possible perturbations has been exhaustively searched. We leverage recent progress in verified perturbation analysis methods to directly propagate bounds through a neural network to exhaustively probe a - potentially infinite-size - set of perturbations in a single forward pass. Our approach takes advantage of the beneficial properties of verified perturbation analysis, i.e., time efficiency and guaranteed complete - sampling agnostic - coverage of the perturbation space - to identify image pixels that drive a model's decision. We evaluate EVA systematically and demonstrate state-of-the-art results on multiple benchmarks. Our code isfreely available: github.com/deel-ai/formal-explainability",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Thomas Fel",
        "M\u00e9lanie Ducoffe",
        "David Vigouroux",
        "R'emi Cadene",
        "Mikael Capelle",
        "C. Nicodeme",
        "Thomas Serre"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/199f8fbf558d4eadaa86bdf4201f560ca69a0bf0",
      "pdf_url": "https://arxiv.org/pdf/2202.07728",
      "publication_date": "2022-02-15",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "31d76bc7724ad3b3d44a5bbffcfdef09b11f0565",
      "title": "Color Backdoor: A Robust Poisoning Attack in Color Space",
      "abstract": "Backdoor attacks against neural networks have been intensively investigated, where the adversary compromises the integrity of the victim model, causing it to make wrong predictions for inference samples containing a specific trigger. To make the trigger more imperceptible and human-unnoticeable, a variety of stealthy backdoor attacks have been proposed, some works employ imperceptible perturbations as the backdoor triggers, which restrict the pixel differences of the triggered image and clean image. Some works use special image styles (e.g., reflection, Instagram filter) as the backdoor triggers. However, these attacks sacrifice the robustness, and can be easily defeated by common preprocessing-based defenses. This paper presents a novel color backdoor attack, which can exhibit robustness and stealthiness at the same time. The key insight of our attack is to apply a uniform color space shift for all pixels as the trigger. This global feature is robust to image transformation operations and the triggered samples maintain natural-looking. To find the optimal trigger, we first define naturalness restrictions through the metrics of PSNR, SSIM and LPIPS. Then we employ the Particle Swarm Optimization (PSO) algorithm to searchfor the optimal trigger that can achieve high attack effectiveness and robustness while satisfying the restrictions. Extensive experiments demonstrate the superiority of PSO and the robustness of color backdoor against different main-stream backdoor defenses.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Wenbo Jiang",
        "Hong-wei Li",
        "Guowen Xu",
        "Tianwei Zhang"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/31d76bc7724ad3b3d44a5bbffcfdef09b11f0565",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d3ee9ab2b145e9e6a94441c5fff61a95bb875bb2",
      "title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models",
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Jiong Wang",
        "Junlin Wu",
        "Muhao Chen",
        "Yevgeniy Vorobeychik",
        "Chaowei Xiao"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/d3ee9ab2b145e9e6a94441c5fff61a95bb875bb2",
      "pdf_url": "https://arxiv.org/pdf/2311.09641",
      "publication_date": "2023-11-16",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a7caa04c1d5fe264ee24d913f9292201fb87b642",
      "title": "Black-box Detection of Backdoor Attacks with Limited Information and Data",
      "abstract": "Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a black-box backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.",
      "year": 2021,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Yinpeng Dong",
        "Xiao Yang",
        "Zhijie Deng",
        "Tianyu Pang",
        "Zihao Xiao",
        "Hang Su",
        "Jun Zhu"
      ],
      "citation_count": 123,
      "url": "https://www.semanticscholar.org/paper/a7caa04c1d5fe264ee24d913f9292201fb87b642",
      "pdf_url": "https://arxiv.org/pdf/2103.13127",
      "publication_date": "2021-03-24",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "81af3058f2df78be332f50ea3813be5aa1f02f58",
      "title": "The Hidden Vulnerability of Distributed Learning in Byzantium",
      "abstract": "While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine-resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers. \nWe show in this paper that convergence is not enough. In high dimension $d \\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine-resilient schemes leave a margin of poisoning of $\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR-10 and MNIST. \nWe introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non-Byzantine gradients had been used to update the model.",
      "year": 2018,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "El Mahdi El Mhamdi",
        "R. Guerraoui",
        "S\u00e9bastien Rouault"
      ],
      "citation_count": 902,
      "url": "https://www.semanticscholar.org/paper/81af3058f2df78be332f50ea3813be5aa1f02f58",
      "pdf_url": "",
      "publication_date": "2018-02-22",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32867b58fb395dd8f271b75e2c554355dba43d75",
      "title": "Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Zhen Xiang",
        "David J. Miller",
        "G. Kesidis"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/32867b58fb395dd8f271b75e2c554355dba43d75",
      "pdf_url": "",
      "publication_date": "2019-11-18",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "68c490a6f44fec270674683f3b21f4f725af8123",
      "title": "Certified Robustness to Label-Flipping Attacks via Randomized Smoothing",
      "abstract": "Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we present a unifying view of randomized smoothing over arbitrary functions, and we leverage this novel characterization to propose a new strategy for building classifiers that are pointwise-certifiably robust to general data poisoning attacks. As a specific instantiation, we utilize our framework to build linear classifiers that are robust to a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Randomized smoothing has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier; we derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.",
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Elan Rosenfeld",
        "Ezra Winston",
        "Pradeep Ravikumar",
        "J. Z. Kolter"
      ],
      "citation_count": 171,
      "url": "https://www.semanticscholar.org/paper/68c490a6f44fec270674683f3b21f4f725af8123",
      "pdf_url": "",
      "publication_date": "2020-02-07",
      "keywords_matched": [
        "label flipping"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fdc226967a237487eaa182c94e4344a47ac69e9a",
      "title": "Privacy Leakage via Unrestricted Motion-Position Sensors in the Age of Virtual Reality: A Study of Snooping Typed Input on Virtual Keyboards",
      "abstract": "Virtual Reality (VR) has gained popularity in numerous fields, including gaming, social interactions, shopping, and education. In this paper, we conduct a comprehensive study to assess the trustworthiness of the embedded sensors on VR, which embed various forms of sensitive data that may put users\u2019 privacy at risk. We find that accessing most on-board sensors (e.g., motion, position, and button sensors) on VR SDKs/APIs, such as OpenVR, Oculus Platform, and WebXR, requires no security permission, exposing a huge attack surface for an adversary to steal the user\u2019s privacy. We validate this vulnerability through developing malware programs and malicious websites and specifically explore to what extent it exposes the user\u2019s information in the context of keystroke snooping. To examine its actual threat in practice, the adversary in the considered attack model doesn\u2019t possess any labeled data from the user nor knowledge about the user\u2019s VR settings. Extensive experiments, involving two mainstream VR systems and four keyboards with different typing mechanisms, demonstrate that our proof-of-concept attack can recognize the user\u2019s virtual typing with over 89.7% accuracy. The attack can recover the user\u2019s passwords with up to 84.9% recognition accuracy if three attempts are allowed and achieve an average of 87.1% word recognition rate for paragraph inference. We hope this study will help the community gain awareness of the vulnerability in the sensor management of current VR systems and provide insights to facilitate the future design of more comprehensive and restricted sensor access control mechanisms.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yi Wu",
        "Cong Shi",
        "Tian-Di Zhang",
        "Pa Walker",
        "Jian Liu",
        "Nitesh Saxena",
        "Ying Chen"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/fdc226967a237487eaa182c94e4344a47ac69e9a",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "privacy leakage"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "30a9b441c553eef479a2ee7775569132852eb474",
      "title": "Sabre: Cutting through Adversarial Noise with Adaptive Spectral Filtering and Input Reconstruction",
      "abstract": "The adoption of neural networks (NNs) across critical sectors including transportation, medicine, communications infrastructure, etc. is inexorable. However, NNs remain highly susceptible to adversarial perturbations, whereby seemingly minimal or imperceptible changes to their inputs cause gross misclassifications, which questions their practical use. Although a growing body of work focuses on defending against such attacks, adversarial robustness remains an open challenge, especially as the effectiveness of existing solutions against increasingly sophisticated input manipulations comes at the cost of degrading ability to recognize benign samples, as we reveal. In this work we introduce Sabre, an adversarial defense framework that closes the gap between benign and robust accuracy in NN classification tasks, without sacrificing benign sample recognition performance. In particular, through spectral decomposition of the input and selective energy-based filtering, Sabre extracts robust features that serve in input reconstruction prior to feeding existing NN architectures. We demonstrate the performance of our approach across multiple domains, by evaluating it on image classification, network intrusion detection, and speech command recognition tasks, showing that Sabre not only outperforms existing defense mechanisms, but also behaves consistently with different neural architectures, data types, (un)known attacks, and adversarial perturbation strengths. Through these extensive experiments, we make the case for Sabre\u2019s adoption in deploying robust and reliable neural classifiers.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "A. Diallo",
        "Paul Patras"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/30a9b441c553eef479a2ee7775569132852eb474",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4672c38c3a3208508358610860e06438e0342dac",
      "title": "Adaptive Input Reconstruction Based Resilient MPC Against Deception Attacks",
      "abstract": "This article proposes an adaptive input reconstruction based resilient model predictive control (MPC) strategy for continuous-time nonlinear cyber-physical systems (CPS) against deception attacks. The input reconstruction mechanism is first developed based on the self-triggered sampling mechanism which could not only relax the assumptions on the attack energy limitation utilized by the existing resilient MPC methods, but also significantly reduce the cyber resource consumption of the resultant CPS. Besides, the adaptive prediction horizon mechanism is incorporated into the proposed MPC method to reduce its computational complexity. Furthermore, the feasibility and closed-loop stability of the developed MPC algorithm under deception attacks are strictly proven. Finally, the effectiveness of the designed algorithm in defending against deception attacks and reducing resource consumption is tested through both simulation and robot experiments.",
      "year": 2025,
      "venue": "IEEE transactions on industrial electronics (1982. Print)",
      "authors": [
        "Ning He",
        "Kai Ma",
        "Huiping Li",
        "Zhao Fan"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/4672c38c3a3208508358610860e06438e0342dac",
      "pdf_url": "",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ccaa24756ea97fb6849ac518d77169c873190e37",
      "title": "Input reconstruction for networked control systems subject to deception attacks and data losses on control signals",
      "abstract": null,
      "year": 2016,
      "venue": "International Journal of Systems Science",
      "authors": [
        "J. Keller",
        "K. Chabir",
        "D. Sauter"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/ccaa24756ea97fb6849ac518d77169c873190e37",
      "pdf_url": "https://hal.archives-ouvertes.fr/hal-01094322/file/IJSS-version-hal.pdf",
      "publication_date": "2016-03-01",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "504fd40ceb3b000c2a4e0b9a389cb80c128f972b",
      "title": "Functional unknown input reconstruction of descriptor systems: Application to fault detection",
      "abstract": null,
      "year": 2015,
      "venue": "at - Automatisierungstechnik",
      "authors": [
        "F. Bejarano"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/504fd40ceb3b000c2a4e0b9a389cb80c128f972b",
      "pdf_url": "",
      "publication_date": "2015-07-01",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "451b3d7c25110b9e5062a88b539994ab9c6806c7",
      "title": "Input reconstruction for statistical\u2010based fault detection and isolation",
      "abstract": null,
      "year": 2012,
      "venue": "",
      "authors": [
        "U. Schubert",
        "U. Kruger",
        "G. Wozny",
        "H. Arellano\u2010Garcia"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/451b3d7c25110b9e5062a88b539994ab9c6806c7",
      "pdf_url": "",
      "publication_date": "2012-05-01",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf7cc0159323c774702db6c44b54315b272d5a31",
      "title": "Partial unknown input reconstruction for linear systems",
      "abstract": null,
      "year": 2011,
      "venue": "at - Automatisierungstechnik",
      "authors": [
        "F. Bejarano"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/cf7cc0159323c774702db6c44b54315b272d5a31",
      "pdf_url": "",
      "publication_date": "2011-08-01",
      "keywords_matched": [
        "input reconstruction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8ba6bf5dd8de260d72abb9042216e617653443b3",
      "title": "A Cascade Methodology to Evaluate Black-Box Recognition Systems Based on a Copycat Algorithm",
      "abstract": ": With the significant advancements of deep learning (DL) and convolutional neural networks (CNNs), many complex systems in the field of computer vision (CV) have been effectively solved with promising performance, even equivalent to human capabilities. Images sophistically perturbed in order to cause accurately trained deep learning systems to misclassify have emerged as a significant challenge and major concern in application domains requiring high reliability. These samples are referred to as adversarial examples. Many studies apply white-box attack methods to create these adversarial images. However, white-box attacks might be impractical in real-world applications. In this paper, a cascade methodology is deployed in which the Copy-cat algorithm is utilized to replicate the behavior of a black-box model (known as an original model) by using a substitute model. The substitute model is employed to generate white-box perturbations, which are then used to evaluate the black-box models. The experiments are conducted with benchmark datasets as MNIST and CIFAR10 and a facial recognition system as a real use-case. The results show impressive outcomes, as the majority of the adversarial samples generated can significantly reduce the overall accuracy and reliability of facial recognition systems up to over 80%.",
      "year": 2024,
      "venue": "VISIGRAPP : VISAPP",
      "authors": [
        "Dinh Nguyen",
        "Tam Le Nhan",
        "Mai Van Hoan",
        "Tuong Quan Nguyen",
        "Van Nguyen",
        "The Cuong Nguyen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8ba6bf5dd8de260d72abb9042216e617653443b3",
      "pdf_url": "https://doi.org/10.5220/0012402500003660",
      "publication_date": null,
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c8c504cf22cde8d1d1e4f8690e3233c877e2cb39",
      "title": "COPYCAT: Practical Adversarial Attacks on Visualization-Based Malware Detection",
      "abstract": "Despite many attempts, the state-of-the-art of adversarial machine learning on malware detection systems generally yield unexecutable samples. In this work, we set out to examine the robustness of visualization-based malware detection system against adversarial examples (AEs) that not only are able to fool the model, but also maintain the executability of the original input. As such, we first investigate the application of existing off-the-shelf adversarial attack approaches on malware detection systems through which we found that those approaches do not necessarily maintain the functionality of the original inputs. Therefore, we proposed an approach to generate adversarial examples, COPYCAT, which is specifically designed for malware detection systems considering two main goals; achieving a high misclassification rate and maintaining the executability and functionality of the original input. We designed two main configurations for COPYCAT, namely AE padding and sample injection. While the first configuration results in untargeted misclassification attacks, the sample injection configuration is able to force the model to generate a targeted output, which is highly desirable in the malware attribution setting. We evaluate the performance of COPYCAT through an extensive set of experiments on two malware datasets, and report that we were able to generate adversarial samples that are misclassified at a rate of 98.9% and 96.5% with Windows and IoT binary datasets, respectively, outperforming the misclassification rates in the literature. Most importantly, we report that those AEs were executable unlike AEs generated by off-the-shelf approaches. Our transferability study demonstrates that the generated AEs through our proposed method can be generalized to other models.",
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Aminollah Khormali",
        "Ahmed A. Abusnaina",
        "Songqing Chen",
        "Daehun Nyang",
        "Aziz Mohaisen"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/c8c504cf22cde8d1d1e4f8690e3233c877e2cb39",
      "pdf_url": "",
      "publication_date": "2019-09-20",
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "51d9ebc2b96ea721616810478cf8f25081ca8910",
      "title": "Copycat : A cooperative gaming environment for programming adversarial agents by children",
      "abstract": null,
      "year": 0,
      "venue": "",
      "authors": [
        "Omid Banyasad"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/51d9ebc2b96ea721616810478cf8f25081ca8910",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "copycat model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "843959ffdccf31c6694d135fad07425924f785b1",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "abstract": null,
      "year": 2008,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Pascal Vincent",
        "H. Larochelle",
        "Yoshua Bengio",
        "Pierre-Antoine Manzagol"
      ],
      "citation_count": 7904,
      "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
      "pdf_url": "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
      "publication_date": "2008-07-05",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "title": "Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Siyu Luan",
        "Zhenyi Wang",
        "Li Shen",
        "Zonghua Gu",
        "Chao Wu",
        "Dacheng Tao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "title": "Proof of Spacetime as a Defensive Technique Against Model Extraction Attacks",
      "abstract": "\u2014When providing a service that utilizes a machine learning model, the countermeasures against cyber-attacks are required. The model extraction attack is one of the attacks, in which an attacker attempts to replicate the model by obtaining a large number of input-output pairs. While a defense using Proof of Work has already been proposed, an attacker can still conduct model extraction attacks by increasing their computational power. Moreover, this approach leads to unnecessary energy consumption and might not be environmentally friendly. In this paper, the defense method using Proof of Spacetime instead of Proof of Work is proposed to reduce the energy consumption. The Proof of Spacetime is a method to impose spatial and temporal costs on the users of the service. While the Proof of Work makes a user to calculate until permission is granted, the Proof of Spacetime makes a user to keep a result of calculation, so the energy consumption is reduced. Through computer simulations, it was found that systems with Proof of Spacetime, compared to those with Proof of Work, impose 0.79 times the power consumption and 1.07 times the temporal cost on the attackers, while 0.73 times and 0.64 times on the non-attackers. Therefore, the system with Proof of Spacetime can prevent model extraction attacks with lower energy consumption.",
      "year": 2023,
      "venue": "International Journal of Advanced Computer Science and Applications",
      "authors": [
        "Tatsuki Fukuda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "pdf_url": "http://thesai.org/Downloads/Volume14No6/Paper_11-Proof_of_Spacetime_as_a_Defensive_Technique_Against_Model.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4246a8311b0bc2aa5cf49507c2c142fd01798ba",
      "title": "FP-ZOO: Fast Patch-Based Zeroth Order Optimization for Black-Box Adversarial Attacks on Vision Models",
      "abstract": "Deep neural networks have outperformed conventional methods in various fields such as image recognition, natural language processing, and speech recognition. In particular, vision models are widely applied to real-world domains including medical image analysis, autonomous driving, smart factories, and security surveillance. However, these models are vulnerable to adversarial attacks, which pose serious threats to safety and reliability. Among different attack types, this study focuses on evasion attacks that perturb the inputs of deployed models, with an emphasis on black-box settings. The zeroth order optimization (ZOO) attack can approximate gradients and execute attacks without access to internal model information, but it becomes inefficient and exhibits low success rates on high-resolution images due to its dependence on image resizing and its high memory complexity. To address these limitations, this study proposes a patch-based fast zeroth order optimization attack, FP-ZOO. FP-ZOO partitions images into patches and generates perturbations effectively by employing probability-based sampling and an \u03f5-greedy scheduling strategy. We conducted a large-scale evaluation of the FP-ZOO attack on the CIFAR-10, CIFAR-100, and ImageNet datasets. In this evaluation, we adopted attack success rate, L2 distance, and adversarial example generation time as performance metrics. The evaluation results showed that the FP-ZOO attack not only achieved an attack success rate of 97\u2013100% against ImageNet in untargeted attacks, but also demonstrated performance up to 10 s faster compared to the ZOO attack. However, in targeted attacks, it showed relatively lower performance compared to baseline attacks, leaving it as a future research topic.",
      "year": 2025,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Junho Seo",
        "Seungho Jeon"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c4246a8311b0bc2aa5cf49507c2c142fd01798ba",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "27cc4668ddc5b6862e73cf1f089449ae27506ce7",
      "title": "Natural Light Can Also be Dangerous: Traffic Sign Misinterpretation Under Adversarial Natural Light Attacks",
      "abstract": "Common illumination sources like sunlight or artificial light may introduce hidden vulnerabilities to AI systems. Our paper delves into these potential threats, offering a novel approach to simulate varying light conditions, including sunlight, headlights, and flashlight illuminations. Moreover, unlike typical physical adversarial attacks requiring conspicuous alterations, our method utilizes a model-agnostic black-box attack integrated with the Zeroth Order Optimization (ZOO) algorithm to identify deceptive patterns in a physically-applicable space. Consequently, attackers can recreate these simulated conditions, deceiving machine learning models with seemingly natural light. Empirical results demonstrate the efficacy of our method, misleading models trained on the GTSRB and LISA datasets under natural-like physical environments with an attack success rate exceeding 70% across all digital datasets, and remaining effective against all evaluated real-world traffic signs. Importantly, after adversarial training using samples generated from our approach, models showcase enhanced robustness, underscoring the dual value of our work in both identifying and mitigating potential threats.1",
      "year": 2024,
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "authors": [
        "Teng-Fang Hsiao",
        "Bo-Lun Huang",
        "Zi-Xiang Ni",
        "Yan-Ting Lin",
        "Hong-Han Shuai",
        "Yung-Hui Li",
        "Wen-Huang Cheng"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/27cc4668ddc5b6862e73cf1f089449ae27506ce7",
      "pdf_url": "",
      "publication_date": "2024-01-03",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "205d0d64c9d1ee291ff0f29bd55c8d129d4fc075",
      "title": "Surrogate-Guided Adversarial Attacks: Enabling White-Box Methods in Black-Box Scenarios",
      "abstract": "Adversarial attacks pose significant threats to machine learning models, with white-box attacks such as Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Basic Iterative Method (BIM) achieving high success rates when model gradients are accessible. However, in real-world scenarios, direct access to model internals is often restricted, necessitating black-box attack strategies that typically suffer from lower effectiveness. In this work, we propose a novel approach to transform white-box attacks into black-box attacks by leveraging state-of-the-art surrogate models, including MultiLayer Perceptrons (MLP) and XGBoost (XGB). Our method involves training a surrogate model to mimic the decision boundaries of an inaccessible target model using pseudo-labeling, thereby enabling the application of gradient-based white-box attacks in a black-box setting. We systematically compare our approach against conventional black-box attacks, such as Zero Order Optimization (ZOO), evaluating their effectiveness in terms of attack success rates, transferability, and computational efficiency. The results demonstrate that surrogate-assisted attacks perform as good as standard black-box methods, bridging the performance gap between white-box and black-box adversarial attacks. This study highlights the power of surrogate models in enhancing adversarial transferability and provides insights into the robustness of different machine learning architectures against adversarial threats.",
      "year": 2025,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "D. Asimopoulos",
        "Panagiotis I. Radoglou-Grammatikis",
        "Panagiotis E. Fouliras",
        "Konstandinos Panitsidis",
        "G. Efstathopoulos",
        "Thomas D. Lagkas",
        "Vasileios Argyriou",
        "Igor Kotsiuba",
        "Panagiotis G. Sarigiannidis"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/205d0d64c9d1ee291ff0f29bd55c8d129d4fc075",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5cb7516734fcb757b0bdc18908594f80b932b547",
      "title": "Adversarial Human Context Recognition: Evasion Attacks and Defenses",
      "abstract": "Human Context Recognition (HCR) from smartphone sensor data is a crucial task for Context-Aware (CA) systems, such as those targeting the healthcare and security domains. HCR models deployed in the wild are susceptible to adversarial attacks, wherein an adversary perturbs input sensor values to cause malicious mis-classifications. In this study, we demonstrate evasion attacks that can be perpetuated during model inference, particularly input perturbations that are adversarially calibrated to fool classifiers. In contrast to white-box methods that require impractical levels of system access, black-box evasion attacks merely require the ability to query the model with arbitrary inputs. Specifically, we generate adversarial perturbations using only class confidence scores, as in the Zoo attack, or only class decisions, as in the HopSkipJump (HSJ) attack that correspond with plausible scenarios of possible adversarial attacks. We empirically demonstrate that sophisticated adversarial evasion attacks can significantly impair the accuracy of HCR models, resulting in a performance drop of up to 60% in f1-score. We also propose RobustHCR, an innovative framework for demonstrating and defending against black box evasion threats using a provable defense based on a duality-based network. RobustHCR is able to make reliable predictions regardless of whether its input is under attack or not, effectively mitigating the potential negative impacts caused by adversarial attacks. Rigorous evaluation on both scripted and in-the-wild smartphone HCR datasets demonstrates that RobustHCR can significantly improve the HCR model\u2019s robustness and protect it from possible evasion attacks while maintaining acceptable performance on \"clean\" inputs. In particular, an HCR model with integrated RobustHCR defenses experienced an f1-score reduction of about 3% as opposed to a reduction of over 50% for an HCR model without a defense.",
      "year": 2023,
      "venue": "Annual International Computer Software and Applications Conference",
      "authors": [
        "Abdulaziz Alajaji",
        "Walter Gerych",
        "kar 2402565399 ku",
        "Luke Buquicchio",
        "E. Agu",
        "Elke A. Rundensteiner"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/5cb7516734fcb757b0bdc18908594f80b932b547",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a01aa16ba8f8b9541577e49f54f701df5bfb6105",
      "title": "An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents",
      "abstract": "Much human and computational effort has aimed to improve how deep reinforcement learning (DRL) algorithms perform on benchmarks such as the Atari Learning Environment. Comparatively less effort has focused on understanding what has been learned by such methods, and investigating and comparing the representations learned by different families of DRL algorithms. Sources of friction include the onerous computational requirements, and general logistical and architectural complications for running DRL algorithms at scale. We lessen this friction, by (1) training several algorithms at scale and releasing trained models, (2) integrating with a previous DRL model release, and (3) releasing code that makes it easy for anyone to load, visualize, and analyze such models. This paper introduces the Atari Zoo framework, which contains models trained across benchmark Atari games, in an easy-to-use format, as well as code that implements common modes of analysis and connects such models to a popular neural network visualization library. Further, to demonstrate the potential of this dataset and software package, we show initial quantitative and qualitative comparisons between the performance and representations of several DRL algorithms, highlighting interesting and previously unknown distinctions between them.",
      "year": 2018,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "F. Such",
        "Vashisht Madhavan",
        "Rosanne Liu",
        "Rui Wang",
        "P. S. Castro",
        "Yulun Li",
        "Ludwig Schubert",
        "Marc G. Bellemare",
        "J. Clune",
        "J. Lehman"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/a01aa16ba8f8b9541577e49f54f701df5bfb6105",
      "pdf_url": "https://www.ijcai.org/proceedings/2019/0452.pdf",
      "publication_date": "2018-12-17",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70762a912d07f4c11dbc5d02a2eb416dd21eaa5c",
      "title": "Stealthy Backdoor Attack for Code Models",
      "abstract": "Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose Afraidoor (Adversarial Feature as Adaptive Backdoor). Afraidoor achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We apply Afraidoor to three widely adopted code models (CodeBERT, PLBART, and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used defense methods and find that Afraidoor is more unlikely to be detected by the defense methods than by baseline methods. More specifically, when using spectral signature as defense, around 85% of adaptive triggers in Afraidoor bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both Afraidoor and baselines have almost perfect attack success rates. However, once a defense is applied, the attack success rates of baselines decrease dramatically, while the success rate of Afraidoor remains high. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that state-of-the-art defense methods cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.",
      "year": 2023,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Zhou Yang",
        "Bowen Xu",
        "J Zhang",
        "Hong Jin Kang",
        "Jieke Shi",
        "Junda He",
        "David Lo"
      ],
      "citation_count": 89,
      "url": "https://www.semanticscholar.org/paper/70762a912d07f4c11dbc5d02a2eb416dd21eaa5c",
      "pdf_url": "https://ink.library.smu.edu.sg/context/sis_research/article/9702/viewcontent/StealthyBackdoor_av.pdf",
      "publication_date": "2023-01-06",
      "keywords_matched": [
        "downstream attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6",
      "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
      "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "F\u00e1bio Perez",
        "Ian Ribeiro"
      ],
      "citation_count": 608,
      "url": "https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6",
      "pdf_url": "https://arxiv.org/pdf/2211.09527",
      "publication_date": "2022-11-17",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "db4cf9f6a653d5c15973e836c800ea47743251ae",
      "title": "Prompt Injection attack against LLM-integrated Applications",
      "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Yuekang Li",
        "Kailong Wang",
        "Tianwei Zhang",
        "Yepang Liu",
        "Haoyu Wang",
        "Yanhong Zheng",
        "Yang Liu"
      ],
      "citation_count": 545,
      "url": "https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae",
      "pdf_url": "http://arxiv.org/pdf/2306.05499",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82c9b8d7e4664ee1e5a5beaf322e0989bc687f63",
      "title": "A Comparative Analysis of Adversarial Attack Methods on Machine Learning Models",
      "abstract": "The broad adoption of machine learning (ML) models in many applications has sparked worries about their susceptibility to adversarial attacks, in which slight alterations to input data result in inaccurate model predictions. This study does a comparative examination of adversarial attack techniques on machine learning models, assessing their efficacy, complexity, and current mitigation measures. The analysis explores several attack methodologies, such as gradient-based, decision-based, and optimization-based methods. Each method exploits different flaws in machine learning models to create adversarial instances. An assessment is conducted to determine the vulnerability of different ML model designs, such as neural networks, support vector machines, and decision trees, to manipulation in light of these assaults. Additionally, the study investigates D\u00e9fense measures, such as adversarial training, input pre- processing, and model robustness verification, that are designed to reduce the effects of adversarial attacks and improve the resilience of the model. This comparative research offers valuable insights into the changing environment of adversarial attacks on machine learning models, emphasizing the importance of implementing strong D\u00e9fense mechanisms to protect against possible threats. This research aims to enhance the security and dependability of machine learning systems against hostile manipulation, hence promoting trust and confidence in their practical implementation. Key Words: MNIST, artificial intelligence, dataset adversarial Attacks, Machine Learning, Adversarial Examples, robustness, Fast Gradient Sign Method, DeepFool, Carlini & Wagner (C&W), Zoo-Adversarial Instance Optimization,",
      "year": 2024,
      "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
      "authors": [
        "Abdirashid Abukar Ahmed",
        "Dr. Nirvair Neeru"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/82c9b8d7e4664ee1e5a5beaf322e0989bc687f63",
      "pdf_url": "",
      "publication_date": "2024-09-04",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d7c3987e632cff8a01f512724da6eaf61897c1d8",
      "title": "AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack",
      "abstract": "We present AdversariaLib, an open-source python library for the security evaluation of machine learning (ML) against carefully-targeted attacks. It supports the implementation of several attacks proposed thus far in the literature of adversarial learning, allows for the evaluation of a wide range of ML algorithms, runs on multiple platforms, and has multi-processing enabled. The library has a modular architecture that makes it easy to use and to extend by implementing novel attacks and countermeasures. It relies on other widely-used open-source ML libraries, including scikit-learn and FANN. Classification algorithms are implemented and optimized in C/C++, allowing for a fast evaluation of the simulated attacks. The package is distributed under the GNU General Public License v3, and it is available for download at this http URL",
      "year": 2016,
      "venue": "arXiv.org",
      "authors": [
        "Igino Corona",
        "B. Biggio",
        "Davide Maiorca"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/d7c3987e632cff8a01f512724da6eaf61897c1d8",
      "pdf_url": "",
      "publication_date": "2016-11-15",
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59d9318f07331ec15e54fe2a4218bc4a5c247a38",
      "title": "Foolbox: A Python toolbox to benchmark the robustness of machine learning models",
      "abstract": null,
      "year": 2017,
      "venue": "",
      "authors": [
        "Jonas Rauber",
        "Wieland Brendel",
        "M. Bethge"
      ],
      "citation_count": 504,
      "url": "https://www.semanticscholar.org/paper/59d9318f07331ec15e54fe2a4218bc4a5c247a38",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "package attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "78db1529bd67ef885fe550ab3ed7f965067e8928",
      "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
      "abstract": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.",
      "year": 2020,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Sahar Abdelnabi",
        "Mario Fritz"
      ],
      "citation_count": 184,
      "url": "https://www.semanticscholar.org/paper/78db1529bd67ef885fe550ab3ed7f965067e8928",
      "pdf_url": "https://arxiv.org/pdf/2009.03015",
      "publication_date": "2020-09-07",
      "keywords_matched": [
        "model provenance"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d403c77364df70115f0f905ef1f9f4891bf40e9d",
      "title": "PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning",
      "abstract": "Fine-tuning is an essential process to improve the performance of Large Language Models (LLMs) in specific domains, with Parameter-Efficient Fine-Tuning (PEFT) gaining popularity due to its capacity to reduce computational demands through the integration of low-rank adapters. These lightweight adapters, such as LoRA, can be shared and utilized on open-source platforms. However, adversaries could exploit this mechanism to inject backdoors into these adapters, resulting in malicious behaviors like incorrect or harmful outputs, which pose serious security risks to the community. Unfortunately, few current efforts concentrate on analyzing the backdoor patterns or detecting the backdoors in the adapters. To fill this gap, we first construct and release PADBench, a comprehensive benchmark that contains 13, 300 benign and backdoored adapters fine-tuned with various datasets, attack strategies, PEFT methods, and LLMs. Moreover, we propose PEFTGuard, the first backdoor detection framework against PEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard outperforms existing detection methods, achieving nearly perfect detection accuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot transferability on three aspects, including different attacks, PEFT methods, and adapter ranks. In addition, we consider various adaptive attacks to demonstrate the high robustness of PEFTGuard. We further explore several possible backdoor mitigation defenses, finding fine-mixing to be the most effective method. We envision that our benchmark and method can shed light on future LLM backdoor detection research. 11Our code and dataset are available at: https://github.com/Vincent-HKUSTGZ/PEFTGuard.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Zhen Sun",
        "Tianshuo Cong",
        "Yule Liu",
        "Chenhao Lin",
        "Xinlei He",
        "Rongmao Chen",
        "Xingshuo Han",
        "Xinyi Huang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/d403c77364df70115f0f905ef1f9f4891bf40e9d",
      "pdf_url": "",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "341a95e9039ad0fed83b6067cd37eae3f8f2bce8",
      "title": "Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm",
      "abstract": "Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6%$\\downarrow$). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks. Source code will be obtained at https://github.com/obliviateARR/Obliviate.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Jaehan Kim",
        "Minkyoo Song",
        "S. Na",
        "Seungwon Shin"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/341a95e9039ad0fed83b6067cd37eae3f8f2bce8",
      "pdf_url": "",
      "publication_date": "2024-09-21",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e60225712cb14e92b235b85f6fad888257dae573",
      "title": "Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples",
      "abstract": null,
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Bingxu Mu",
        "Le Wang",
        "Zhenxing Niu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e60225712cb14e92b235b85f6fad888257dae573",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c7ba94eb5d96b665397db77f765910e5acde5c6",
      "title": "Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection Attacks via the Fine-Tuning Interface",
      "abstract": "We surface a new threat to closed-weight Large Language Models (LLMs) that enables an attacker to compute optimization-based prompt injections. Specifically, we characterize how an attacker can leverage the loss-like information returned from the remote fine-tuning interface to guide the search for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor and allows developers to fine-tune LLMs for their tasks, thus providing utility, but also exposes enough information for an attacker to compute adversarial prompts. Through an experimental analysis, we characterize the loss-like values returned by the Gemini fine-tuning API and demonstrate that they provide a useful signal for discrete optimization of adversarial prompts using a greedy search algorithm. Using the PurpleLlama prompt injection benchmark, we demonstrate attack success rates between 65% and 82% on Google's Gemini family of LLMs. These attacks exploit the classic utility-security tradeoff - the fine-tuning interface provides a useful feature for developers but also exposes the LLMs to powerful attacks.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Andrey Labunets",
        "Nishit V. Pandya",
        "Ashish Hooda",
        "Xiaohan Fu",
        "Earlence Fernandes"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7c7ba94eb5d96b665397db77f765910e5acde5c6",
      "pdf_url": "",
      "publication_date": "2025-01-16",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2818ef5d5fdafa2760a9fa088e1ee362ad766669",
      "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks",
      "abstract": "A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning to separate instructions and data, so that the LLM does not follow instructions that might be present with data. We evaluate the robustness of this approach in the whitebox setting by constructing strong optimization-based attacks, and show that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for textual LLMs and apply it to three recent whitebox defenses SecAlign (CCS 2025), SecAlign++, and StruQ (USENIX Security 2025), showing attacks with success rates of up to \\textbf{85-95\\%} on unseen prompts with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Nishit V. Pandya",
        "Andrey Labunets",
        "Sicun Gao",
        "Earlence Fernandes"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2818ef5d5fdafa2760a9fa088e1ee362ad766669",
      "pdf_url": "",
      "publication_date": "2025-07-10",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bf06547a7296a994cf77c1baa793dee6e337a865",
      "title": "Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization",
      "abstract": "Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Wenjun Cao"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bf06547a7296a994cf77c1baa793dee6e337a865",
      "pdf_url": "",
      "publication_date": "2025-05-07",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a02eda08e1084ceb9a3b82a80217855d94094249",
      "title": "Adversarial Reprogramming of Pretrained Neural Networks for Fraud Detection",
      "abstract": "Machine learning models have been widely used for fraud detection, while developing and maintaining these models often suffers from significant limitations in terms of training data scarcity and constrained resources. To address these issues, in this paper, we leverage machine learning vulnerability to adversarial attacks, and design a novel model AdvRFD that Adversarially Reprograms an ImageNet classification neural network for Fraud Detection task. AdvRFD first embeds transaction features into a host image to construct new ImageNet data, and then learns a universal perturbation to be added to all inputs, such that the outputs of the pretrained model can be accordingly mapped to the final detection decisions for all transactions. Extensive experiments on two transaction datasets made over Ethereum and credit cards have demonstrated that AdvRFD is effective to detect fraud using limited data and resources.",
      "year": 2021,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Lingwei Chen",
        "Yujie Fan",
        "Yanfang Ye"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/a02eda08e1084ceb9a3b82a80217855d94094249",
      "pdf_url": "",
      "publication_date": "2021-10-26",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9ed4aa13f84e646b869eb2d1b5249b4b1be86bc8",
      "title": "README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model",
      "abstract": "Deep learning-based watermarking has emerged as a promising solution for robust image authentication and protection. However, existing models are limited by low embedding capacity and vulnerability to bit-level errors, making them unsuitable for cryptographic applications such as digital signatures, which require over 2048 bits of error-free data. In this paper, we propose README (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a novel framework that enables robust, verifiable, and error-tolerant digital signatures within images. Our method combines a simple yet effective cropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a lightweight error correction module designed to localize and correct bit errors using Distinct Circular Subsum Sequences (DCSS). Without requiring any fine-tuning of existing pretrained watermarking models, README significantly boosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when embedding 2048-bit digital signatures into a single image, even under real-world distortions. Moreover, our use of perceptual hash-based signature verification ensures public verifiability and robustness against tampering. The proposed framework unlocks a new class of high-assurance applications for deep watermarking, bridging the gap between signal-level watermarking and cryptographic security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hyunwook Choi",
        "Sangyun Won",
        "Daeyeon Hwang",
        "Junhyeok Choi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9ed4aa13f84e646b869eb2d1b5249b4b1be86bc8",
      "pdf_url": "",
      "publication_date": "2025-07-06",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fe8e1c1765bc1edda1100de281224892f4197f70",
      "title": "A Transfer Attack to Image Watermarks",
      "abstract": "Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector based on existing watermarking methods is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API. Our code is available at: https://github.com/hifi-hyp/Watermark-Transfer-Attack.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Yuepeng Hu",
        "Zhengyuan Jiang",
        "Moyang Guo",
        "N. Gong"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/fe8e1c1765bc1edda1100de281224892f4197f70",
      "pdf_url": "",
      "publication_date": "2024-03-22",
      "keywords_matched": [
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f9ec4390cba65c32c44dd9524e4d45ea79e49113",
      "title": "Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient",
      "abstract": "Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motioninformed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Zhengzhi Lu",
        "He Wang",
        "Ziyi Chang",
        "Guoan Yang",
        "Hubert P. H. Shum"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/f9ec4390cba65c32c44dd9524e4d45ea79e49113",
      "pdf_url": "https://durham-repository.worktribe.com/preview/1715180/1715085AAM.pdf",
      "publication_date": "2023-08-10",
      "keywords_matched": [
        "transfer attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "831bc1da60ff41e8901faf1354aea3cac19f3d2b",
      "title": "NeRFail: Neural Radiance Fields-Based Multiview Adversarial Attack",
      "abstract": "Adversarial attacks, i.e., generating adversarial perturbations with a small magnitude to deceive deep neural networks, are important for investigating and improving model trustworthiness. Traditionally, the topic was scoped within 2D images without considering 3D multiview information. Benefiting from Neural Radiance Fields (NeRF), one can easily reconstruct a 3D scene with a Multi-Layer Perceptron (MLP) from given 2D views and synthesize photo-realistic renderings of novel vantages. This opens up a door to discussing the possibility of undertaking to attack multiview NeRF network with downstream tasks from different rendering angles, which we denote Neural Radiance Fiels-based multiview adversarial Attack (NeRFail). The goal is, given one scene and a subset of views, to deceive the recognition results of agnostic view angles as well as given views. To do so, we propose a transformation mapping from pixels to 3D points such that our attack generates multiview adversarial perturbations by attacking a subset of images with different views, intending to prevent the downstream classifier from correctly predicting images rendered by NeRF from other views. Experiments show that our multiview adversarial perturbations successfully obfuscate the downstream classifier at both known and unknown views. Notably, when retraining another NeRF on the perturbed training data, we show that the perturbation can be inherited and reproduced. The code can be found at https://github.com/jiang-wenxiang/NeRFail.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Wenxiang Jiang",
        "Hanwei Zhang",
        "Xi Wang",
        "Zhongwen Guo",
        "Hao Wang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/831bc1da60ff41e8901faf1354aea3cac19f3d2b",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/30113/31966",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "490c30b18071273e281be600c2bb6071bc2f239d",
      "title": "TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems",
      "abstract": "While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model\u2019s performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.",
      "year": 2023,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "W. Zong",
        "Yang-Wai Chow",
        "Willy Susilo",
        "Kien Do",
        "S. Venkatesh"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/490c30b18071273e281be600c2bb6071bc2f239d",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a8b5f22d728bfb507e27064e7af700411283b869",
      "title": "Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples",
      "abstract": "Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full white-box setting and transfer-based black-box settings. Beyond the basic goal of mask removal, we further investigate and find that it is possible to generate any desired mask by the adversarial attack.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Taegoo Kang",
        "Donghun Kim",
        "S. Bae",
        "In-So Kweon"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/a8b5f22d728bfb507e27064e7af700411283b869",
      "pdf_url": "http://arxiv.org/pdf/2305.00866",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "downstream attack",
        "downstream task attack",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "10fd5a9964856df5059478edb042cd5c1b5b42bd",
      "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models",
      "abstract": "In this paper, we present a new form of backdoor attack against Large Language Models (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks is that the language itself serves as the trigger to hijack the infected LLMs to generate inflammatory speech. They enable the precise targeting of a specific language-speaking group, exacerbating racial discrimination by malicious entities. We first implement a baseline lingual-backdoor attack, which is carried out by poisoning a set of training data for specific downstream tasks through translation into the trigger language. However, this baseline attack suffers from poor task generalization and is impractical in real-world settings. To address this challenge, we design BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any downstream tasks within the chat LLMs, regardless of the specific questions of these tasks. We design a new approach using PPL-constrained Greedy Coordinate Gradient-based Search (PGCG) based adversarial training to expand the decision boundary of lingual-backdoor, thereby enhancing the generalization ability of lingual-backdoor across various tasks. We perform extensive experiments to validate the effectiveness of our proposed attacks. Specifically, the baseline attack achieves an ASR of over 90% on the specified tasks. However, its ASR reaches only 37.61% across six tasks in the task-agnostic scenario. In contrast, BadLingual brings up to 37.35% improvement over the baseline. Our study sheds light on a new perspective of vulnerabilities in LLMs with multilingual capabilities and is expected to promote future research on the potential defenses to enhance the LLMs' robustness",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zihan Wang",
        "Hongwei Li",
        "Rui Zhang",
        "Wenbo Jiang",
        "Kangjie Chen",
        "Tianwei Zhang",
        "Qingchuan Zhao",
        "Guowen Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/10fd5a9964856df5059478edb042cd5c1b5b42bd",
      "pdf_url": "",
      "publication_date": "2025-05-06",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41c22924317ff8827a7df465d6ed0986b28c3979",
      "title": "Robust backdoor injection with the capability of resisting network transfer",
      "abstract": null,
      "year": 2022,
      "venue": "Information Sciences",
      "authors": [
        "Le Feng",
        "Sheng Li",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/41c22924317ff8827a7df465d6ed0986b28c3979",
      "pdf_url": "",
      "publication_date": "2022-10-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7a39ef498765313bc8e3c92704e40b35a0b842df",
      "title": "The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems",
      "abstract": "Backdoor Attacks have been shown to pose significant threats to automatic speech recognition systems (ASRs). Existing success largely assumes backdoor triggering in the digital domain, or the victim will not notice the presence of triggering sounds in the physical domain. However, in practical victim-present scenarios, the over-the-air distortion of the backdoor trigger and the victim awareness raised by its audibility may invalidate such attacks. In this paper, we propose SMA, an inaudible grey-box backdoor attack that can be generalized to real-world scenarios where victims are present by exploiting both the vulnerability of microphones and neural networks. Specifically, we utilize the nonlinear effects of microphones to inject an inaudible ultrasonic trigger. To accurately characterize the microphone response to the crafted ultrasound, we construct a novel nonlinear transfer function for effective optimization. We also design optimization objectives to ensure triggers' robustness in the physical world and transferability on unseen ASR models. In practice, SMA can bypass the microphone's built-in filters and human perception, activating the implanted trigger in the ASRs inaudibly, regardless of whether the user is speaking. Extensive experiments show that the attack success rate of SMA can reach nearly 100% in the digital domain and over 85% against most microphones in the physical domains by only poisoning about 0.5% of the training audio dataset. Moreover, our attack can resist typical defense countermeasures to backdoor attacks.",
      "year": 2023,
      "venue": "ACM Multimedia",
      "authors": [
        "Zhicong Zheng",
        "Xinfeng Li",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7a39ef498765313bc8e3c92704e40b35a0b842df",
      "pdf_url": "",
      "publication_date": "2023-10-26",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f4f22cbcf6980bcf536cae110fce45865c4f0ba2",
      "title": "Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes",
      "abstract": "Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustment can effectively mitigate for confounding shift.",
      "year": 2023,
      "venue": "AMIA ... Annual Symposium proceedings. AMIA Symposium",
      "authors": [
        "Xiruo Ding",
        "Zhecheng Sheng",
        "Meliha Yetisgen-Yildiz",
        "Serguei V. S. Pakhomov",
        "Trevor Cohen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/f4f22cbcf6980bcf536cae110fce45865c4f0ba2",
      "pdf_url": "https://arxiv.org/pdf/2310.02451",
      "publication_date": "2023-10-03",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd9031b34d301197f8ed7fd3a25dd9a22602dfb0",
      "title": "Robust Federated Learning against Backdoor Attackers",
      "abstract": "Federated learning is a privacy-preserving alter-native for distributed learning with no involvement of data transfer. As the server does not have any control on clients' actions, some adversaries may participate in learning to introduce corruption into the underlying model. Backdoor attacker is one such adversary who injects a trigger pattern into the data to manipulate the model outcomes on a specific sub-task. This work aims to identify backdoor attackers and to mitigate their effects by isolating their weight updates. Leveraging the correlation between clients' gradients, we propose two graph theoretic algorithms to separate out attackers from the benign clients. Under a classification task, the experimental results show that our algorithms are effective and robust to the attackers who add backdoor trigger patterns at different location in targeted images. The results also evident that our algorithms are superior than existing methods especially when numbers of attackers are more than the normal clients.",
      "year": 2023,
      "venue": "Conference on Computer Communications Workshops",
      "authors": [
        "Priyesh Ranjan",
        "Ashish Gupta",
        "Federico Cor\u00f3",
        "Sajal Kumar Das"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/cd9031b34d301197f8ed7fd3a25dd9a22602dfb0",
      "pdf_url": "",
      "publication_date": "2023-05-20",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aaf307cca2c441cbb4579e45f79f3bdd10c9bc90",
      "title": "on the Abstract of Latent Backdoor Attacks on Deep Neural Networks",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/aaf307cca2c441cbb4579e45f79f3bdd10c9bc90",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d1e86f592845d0fc8b1eb0c75c36473cc4dec13",
      "title": "Black-box and Target-specific Attack Against Interpretable Deep Learning Systems",
      "abstract": "Deep neural network models are susceptible to malicious manipulations even in the black-box settings. Providing explanations for DNN models offers a sense of security by human involvement, which reveals whether the sample is benign or adversarial even though previous studies achieved a high attack success rate. However, interpretable deep learning systems (IDLSes) are shown to be susceptible to adversarial manipulations in white-box settings. Attacking IDLSes in black-box settings is challenging and remains an open research domain. In this work, we propose a black-box version of the white-box AdvEdge approach against IDLSes, which is query-efficient and gradient-free without obtaining any knowledge of the target DNN model and its coupled interpreter. Our approach takes advantage of transfer-based and score-based techniques using the effective microbial genetic algorithm (MGA). We achieve a high attack success rate with a small number of queries and high similarity in interpretations between adversarial and benign samples.",
      "year": 2022,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Eldor Abdukhamidov",
        "Firuz Juraev",
        "Mohammed Abuhamad",
        "Tamer Abuhmed"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/8d1e86f592845d0fc8b1eb0c75c36473cc4dec13",
      "pdf_url": "",
      "publication_date": "2022-05-30",
      "keywords_matched": [
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2d827c286e32dbf0739e8c796b119b1074809b4",
      "title": "Meta SAC-Lag: Towards Deployable Safe Reinforcement Learning via MetaGradient-based Hyperparameter Tuning",
      "abstract": "Safe Reinforcement Learning (Safe RL) is one of the prevalently studied subcategories of trial-and-error-based methods with the intention to be deployed on real-world systems. In safe RL, the goal is to maximize reward performance while minimizing constraints, often achieved by setting bounds on constraint functions and utilizing the Lagrangian method. However, deploying Lagrangian-based safe RL in real-world scenarios is challenging due to the necessity of threshold fine-tuning, as imprecise adjustments may lead to suboptimal policy convergence. To mitigate this challenge, we propose a unified Lagrangian-based model-free architecture called Meta Soft Actor-Critic Lagrangian (Meta SAC-Lag). Meta SAC-Lag uses meta-gradient optimization to automatically update the safety-related hyperparameters. The proposed method is designed to address safe exploration and threshold adjustment with minimal hyperparameter tuning requirement. In our pipeline, the inner parameters are updated through the conventional formulation and the hyperparameters are adjusted using the meta-objectives which are defined based on the updated parameters. Our results show that the agent can reliably adjust the safety performance due to the relatively fast convergence rate of the safety threshold. We evaluate the performance of Meta SAC-Lag in five simulated environments against Lagrangian baselines, and the results demonstrate its capability to create synergy between parameters, yielding better or competitive results. Furthermore, we conduct a real-world experiment involving a robotic arm tasked with pouring coffee into a cup without spillage. Meta SAC-Lag is successfully trained to execute the task, while minimizing effort constraints. The success of Meta SAC-Lag in performing the experiment is intended to be a step toward practical deployment of safe RL algorithms to learn the control process of safety-critical real-world systems without explicit engineering.",
      "year": 2024,
      "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
      "authors": [
        "Homayoun Honari",
        "Amir M. Soufi Enayati",
        "Mehran Ghafarian Tamizi",
        "Homayoun Najjaran"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b2d827c286e32dbf0739e8c796b119b1074809b4",
      "pdf_url": "",
      "publication_date": "2024-08-15",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "84fc714f22535f82c3fbfa28f2883292d2a02167",
      "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
      "abstract": "Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications. Videos and code are available at our project website https://cccedric.github.io/conrft/.",
      "year": 2025,
      "venue": "Robotics",
      "authors": [
        "Yuhui Chen",
        "Shuai Tian",
        "Shugao Liu",
        "Yingting Zhou",
        "Haoran Li",
        "Dongbin Zhao"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/84fc714f22535f82c3fbfa28f2883292d2a02167",
      "pdf_url": "",
      "publication_date": "2025-02-08",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "876af103c08ed5dc08e044bfb26c19656ce5fbab",
      "title": "Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications",
      "abstract": "Integrating learning-based techniques, especially reinforcement learning, into robotics is promising for solving complex problems in unstructured environments. Most of the existing approaches rely on training in carefully calibrated simulators before being deployed on real robots, often without real-world fine-tuning. While effective in controlled settings, this framework falls short in applications where precise simulation is unavailable or the environment is too complex to model. Instead, on-robot learning, which learns by interacting directly with the real world, offers a promising alternative. One major problem for on-robot reinforcement learning is ensuring safety, as uncontrolled exploration can cause catastrophic damage to the robot or the environment. Indeed, safety specifications, often represented as constraints, can be complex and nonlinear, making safety challenging to guarantee in learning systems. In this article, we show how we can impose complex safety constraints on learning-based robotics systems in a principled manner, both from theoretical and practical points of view. Our approach is based on the concept of the constraint manifold, representing the set of safe robot configurations. Exploiting differential geometry techniques, i.e., the tangent space, we can construct a safe action space, allowing learning agents to sample arbitrary actions while ensuring safety. We demonstrate the method's effectiveness in a real-world robot air hockey task, showing that our method can handle high-dimensional tasks with complex constraints.",
      "year": 2024,
      "venue": "IEEE Transactions on robotics",
      "authors": [
        "Puze Liu",
        "Haitham Bou-Ammar",
        "Jan Peters",
        "Davide Tateo"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/876af103c08ed5dc08e044bfb26c19656ce5fbab",
      "pdf_url": "http://arxiv.org/pdf/2404.09080",
      "publication_date": "2024-04-13",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d51890be35abb728adf6d9be5ff3b9670f48ff9",
      "title": "Safety Alignment of Large Language Models via Contrasting Safe and Harmful Distributions",
      "abstract": "With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can effectively reduce harmful responses from LLMs, they often require high-quality datasets and heavy computational overhead during model training. Another way to align language models is to modify the logit of tokens in model outputs without heavy training. Recent studies have shown that contrastive decoding can enhance the performance of language models by reducing the likelihood of confused tokens. However, these methods require the manual selection of contrastive models or instruction templates, limiting the degree of contrast. To this end, we propose Adversarial Contrastive Decoding (ACD), an optimization-based framework to generate two opposite soft system prompts, the Safeguarding Prompt (SP) and the Adversarial Prompt (AP), for prompt-based contrastive decoding. The SP aims to promote safer outputs while the AP aims to exploit the harmful parts of the model, providing a strong contrast to align the model with safety. ACD only needs to apply a lightweight prompt tuning on a rather small anchor dataset without training the target model. Experiments conducted on extensive models and benchmarks demonstrate that the proposed method achieves much better safety performance than previous model training-free decoding methods without sacrificing its original generation ability.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Xiaoyun Zhang",
        "Zhengyue Zhao",
        "Wenxuan Shi",
        "Kaidi Xu",
        "Di Huang",
        "Xing Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/9d51890be35abb728adf6d9be5ff3b9670f48ff9",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "886f41c1efc8139aadb6bf83a807d05bc96d9a65",
      "title": "SteerDiff: Steering towards Safe Text-to-Image Diffusion Models",
      "abstract": "Text-to-image (T2I) diffusion models have drawn attention for their ability to generate high-quality images with precise text alignment. However, these models can also be misused to produce inappropriate content. Existing safety measures, which typically rely on text classifiers or ControlNet-like approaches, are often insufficient. Traditional text classifiers rely on large-scale labeled datasets and can be easily bypassed by rephrasing. As diffusion models continue to scale, fine-tuning these safeguards becomes increasingly challenging and lacks flexibility. Recent red-teaming attack researches further underscore the need for a new paradigm to prevent the generation of inappropriate content. In this paper, we introduce SteerDiff, a lightweight adaptor module designed to act as an intermediary between user input and the diffusion model, ensuring that generated images adhere to ethical and safety standards with little to no impact on usability. SteerDiff identifies and manipulates inappropriate concepts within the text embedding space to guide the model away from harmful outputs. We conduct extensive experiments across various concept unlearning tasks to evaluate the effectiveness of our approach. Furthermore, we benchmark SteerDiff against multiple red-teaming strategies to assess its robustness. Finally, we explore the potential of SteerDiff for concept forgetting tasks, demonstrating its versatility in text-conditioned image generation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hongxiang Zhang",
        "Yifeng He",
        "Hao Chen"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/886f41c1efc8139aadb6bf83a807d05bc96d9a65",
      "pdf_url": "",
      "publication_date": "2024-10-03",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f16764ce971e94e69d64527d91975f75c5123a0",
      "title": "Hard Adversarial Example Mining for Improving Robust Fairness",
      "abstract": "Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AEs). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems. Recent works in this field usually apply class-wise regularization methods to enhance the fairness of AT. However, this paper discovers that these paradigms can be sub-optimal in improving robust fairness. Specifically, we empirically observe that the AEs that are already robust (referred to as \u201ceasy AEs\u201d in this paper) are useless and even harmful in improving robust fairness. To this end, we propose the hard adversarial example mining (HAM) technique which concentrates on mining hard AEs while discarding the easy AEs in AT. Specifically, HAM identifies the easy AEs and hard AEs with a fast adversarial attack method. By discarding the easy AEs and reweighting the hard AEs, the robust fairness of the model can be efficiently and effectively improved. Extensive experimental results on four image classification datasets demonstrate the improvement of HAM in robust fairness and training efficiency compared to several state-of-the-art fair adversarial training methods. Our code is available at https://github.com/yyl-github-1896/HAM.",
      "year": 2025,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Chenhao Lin",
        "Xiang Ji",
        "Yulong Yang",
        "Qian Li",
        "Zhengyu Zhao",
        "Zhe Peng",
        "Run Wang",
        "Liming Fang",
        "Chao Shen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4f16764ce971e94e69d64527d91975f75c5123a0",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40dbd97cfba2d2d819ed3194d6a3b5717cfbe44d",
      "title": "FARMUR: Fair Adversarial Retraining to Mitigate Unfairness in Robustness",
      "abstract": null,
      "year": 2023,
      "venue": "Symposium on Advances in Databases and Information Systems",
      "authors": [
        "S. A. Mousavi",
        "H. Mousavi",
        "M. Daneshtalab"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/40dbd97cfba2d2d819ed3194d6a3b5717cfbe44d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "159552426d23922cd2e5b1337af2e3cae2a7c8a9",
      "title": "Rethinking Word-level Adversarial Attack: The Trade-off between Efficiency, Effectiveness, and Imperceptibility",
      "abstract": null,
      "year": 2024,
      "venue": "International Conference on Language Resources and Evaluation",
      "authors": [
        "Pengwei Zhan",
        "Jing Yang",
        "He Wang",
        "Chao Zheng",
        "Liming Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/159552426d23922cd2e5b1337af2e3cae2a7c8a9",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bdf434f475654ee0a99fe11fd63405b038244f69",
      "title": "Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction",
      "abstract": "Recidivism prediction scores are used across the USA to determine sentencing and supervision for hundreds of thousands of inmates. One such generator of recidivism prediction scores is Northpointe's Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) score, used in states like California and Florida, which past research has shown to be biased against black inmates according to certain measures of fairness. To counteract this racial bias, we present an adversarially-trained neural network that predicts recidivism and is trained to remove racial bias. When comparing the results of our model to COMPAS, we gain predictive accuracy and get closer to achieving two out of three measures of fairness: parity and equality of odds. Our model can be generalized to any prediction and demographic. This piece of research contributes an example of scientific replication and simplification in a high-stakes real-world application like recidivism prediction.",
      "year": 2018,
      "venue": "arXiv.org",
      "authors": [
        "C. Wadsworth",
        "Francesca Vera",
        "C. Piech"
      ],
      "citation_count": 191,
      "url": "https://www.semanticscholar.org/paper/bdf434f475654ee0a99fe11fd63405b038244f69",
      "pdf_url": "",
      "publication_date": "2018-06-30",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a02ecdabc0e43b37bd0dac32b6b6916ae7ab60af",
      "title": "SMOTEBoost: Improving Prediction of the Minority Class in Boosting",
      "abstract": null,
      "year": 2003,
      "venue": "European Conference on Principles of Data Mining and Knowledge Discovery",
      "authors": [
        "N. Chawla",
        "A. Lazarevic",
        "L. Hall",
        "K. Bowyer"
      ],
      "citation_count": 1728,
      "url": "https://www.semanticscholar.org/paper/a02ecdabc0e43b37bd0dac32b6b6916ae7ab60af",
      "pdf_url": "",
      "publication_date": "2003-09-22",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b751a61cd968307a02b9bc91231999a47e44f2c3",
      "title": "Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing",
      "abstract": "Machine learning (ML) systems have achieved remarkable performance across a wide area of applications. However, they frequently exhibit unfair behaviors in sensitive application domains (e.g., employment and loan), raising severe fairness concerns. To evaluate and test fairness, engineers often generate individual discriminatory instances to expose unfair behaviors before model deployment. However, existing baselines ignore the naturalness of generation and produce instances that deviate from the real data distribution, which may fail to reveal the actual model fairness since these unnatural discriminatory instances are unlikely to appear in practice. To address the problem, this paper proposes a framework named Latent Imitator (LIMI) to generate more natural individual discriminatory instances with the help of a generative adversarial network (GAN), where we imitate the decision boundary of the target model in the semantic latent space of GAN and further samples latent instances on it. Specifically, we first derive a surrogate linear boundary to coarsely approximate the decision boundary of the target model, which reflects the nature of the original data distribution. Subsequently, to obtain more natural instances, we manipulate random latent vectors to the surrogate boundary with a one-step movement, and further conduct vector calculation to probe two potential discriminatory candidates that may be more closely located in the real decision boundary. Extensive experiments on various datasets demonstrate that our LIMI outperforms other baselines largely in effectiveness (\u00d79.42 instances), efficiency (\u00d78.71 speeds), and naturalness (+19.65%) on average. In addition, we empirically demonstrate that retraining on test samples generated by our approach can lead to improvements in both individual fairness (45.67% on IFr and 32.81% on IFo) and group fairness (9.86% on SPD and 28.38% on AOD). Our codes can be found on our website.",
      "year": 2023,
      "venue": "International Symposium on Software Testing and Analysis",
      "authors": [
        "Yisong Xiao",
        "Aishan Liu",
        "Tianlin Li",
        "Xianglong Liu"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/b751a61cd968307a02b9bc91231999a47e44f2c3",
      "pdf_url": "https://arxiv.org/pdf/2305.11602",
      "publication_date": "2023-05-19",
      "keywords_matched": [
        "discriminatory model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "515a4cff44d879b56adec7d16a0b70610ece665d",
      "title": "On Adversarial Bias and the Robustness of Fair Machine Learning",
      "abstract": "Optimizing prediction accuracy can come at the expense of fairness. Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. However, we show that giving the same importance to groups of different sizes and distributions, to counteract the effect of bias in training data, can be in conflict with robustness. We analyze data poisoning attacks against group-based fair machine learning, with the focus on equalized odds. An adversary who can control sampling or labeling for a fraction of training data, can reduce the test accuracy significantly beyond what he can achieve on unconstrained models. Adversarial sampling and adversarial labeling attacks can also worsen the model's fairness gap on test data, even though the model satisfies the fairness constraint on training data. We analyze the robustness of fair machine learning through an empirical evaluation of attacks on multiple algorithms and benchmark datasets.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Hong Chang",
        "Ta Duy Nguyen",
        "S. K. Murakonda",
        "Ehsan Kazemi",
        "R. Shokri"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/515a4cff44d879b56adec7d16a0b70610ece665d",
      "pdf_url": "",
      "publication_date": "2020-06-15",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5994e3c50166b0428eb5a00061e20425b675ede5",
      "title": "Towards Robust and Fair Machine Learning",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Anshuman Chhabra"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5994e3c50166b0428eb5a00061e20425b675ede5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "416a2740cb023cdd86c07456578ed311d7a30e52",
      "title": "Explainability for fair machine learning",
      "abstract": "As the decisions made or influenced by machine learning models increasingly impact our lives, it is crucial to detect, understand, and mitigate unfairness. But even simply determining what ``unfairness'' should mean in a given context is non-trivial: there are many competing definitions, and choosing between them often requires a deep understanding of the underlying task. It is thus tempting to use model explainability to gain insights into model fairness, however existing explainability tools do not reliably indicate whether a model is indeed fair. In this work we present a new approach to explaining fairness in machine learning, based on the Shapley value paradigm. Our fairness explanations attribute a model's overall unfairness to individual input features, even in cases where the model does not operate on sensitive attributes directly. Moreover, motivated by the linearity of Shapley explainability, we propose a meta algorithm for applying existing training-time fairness interventions, wherein one trains a perturbation to the original model, rather than a new model entirely. By explaining the original model, the perturbation, and the fair-corrected model, we gain insight into the accuracy-fairness trade-off that is being made by the intervention. We further show that this meta algorithm enjoys both flexibility and stability benefits with no loss in performance.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "T. Begley",
        "Tobias Schwedes",
        "Christopher Frye",
        "Ilya Feige"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/416a2740cb023cdd86c07456578ed311d7a30e52",
      "pdf_url": "",
      "publication_date": "2020-10-14",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0d75694788ecefd5ef584a54b0217b024e8c4b0c",
      "title": "Label Bias, Label Shift: Fair Machine Learning with Unreliable Labels",
      "abstract": null,
      "year": 2020,
      "venue": "",
      "authors": [
        "Jessica Dai",
        "Sarah M. Brown"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/0d75694788ecefd5ef584a54b0217b024e8c4b0c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1fa32503bce4f01ab2ccb65dedd374310c488fe8",
      "title": "Fair Machine Learning Under Partial Compliance",
      "abstract": "Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.",
      "year": 2020,
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
      "authors": [
        "Jessica Dai",
        "S. Fazelpour",
        "Zachary Chase Lipton"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/1fa32503bce4f01ab2ccb65dedd374310c488fe8",
      "pdf_url": "https://philpapers.org/archive/DAIFML.pdf",
      "publication_date": "2020-11-07",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8f99196e78c6285db671c60e12d24abf0a887473",
      "title": "Long-Term Impacts of Fair Machine Learning",
      "abstract": "Machine learning models developed from real-world data can inherit potential, preexisting bias in the dataset. When these models are used to inform decisions involving human beings, fairness concerns inevitably arise. Imposing certain fairness constraints in the training of models can be effective only if appropriate criteria are applied. However, a fairness criterion can be defined/assessed only when the interaction between the decisions and the underlying population is well understood. We introduce two feedback models describing how people react when receiving machine-aided decisions and illustrate that some commonly used fairness criteria can end with undesirable consequences while reinforcing discrimination.",
      "year": 2019,
      "venue": "Ergonomics in design",
      "authors": [
        "Xueru Zhang",
        "Mohammad Mahdi Khalili",
        "M. Liu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/8f99196e78c6285db671c60e12d24abf0a887473",
      "pdf_url": "",
      "publication_date": "2019-10-25",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6275aa21331a2712222b7ab2116e9589e21ae82c",
      "title": "Prediction of Manipulation Actions",
      "abstract": "By looking at a person\u2019s hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor\u2019s intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as \u201csqueezing\u201d, \u201cflipping\u201d, \u201cwashing\u201d, \u201cwiping\u201d and \u201cscratching\u201d with a sponge. In psychophysical experiments, we evaluated human observers\u2019 skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed.",
      "year": 2016,
      "venue": "International Journal of Computer Vision",
      "authors": [
        "C. Ferm\u00fcller",
        "Fang Wang",
        "Yezhou Yang",
        "Konstantinos Zampogiannis",
        "Yi Zhang",
        "Francisco Barranco",
        "Michael Pfeiffer"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/6275aa21331a2712222b7ab2116e9589e21ae82c",
      "pdf_url": "https://www.zora.uzh.ch/id/eprint/132636/1/ZORA132636.pdf",
      "publication_date": "2016-10-03",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c780dfe8ff9c5a0d51893b5eaf41591299b3b8b",
      "title": "SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency",
      "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed scaled prediction consistency) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at https://github.com/JunfengGo/SCALE-UP.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Junfeng Guo",
        "Yiming Li",
        "Xun Chen",
        "Hanqing Guo",
        "Lichao Sun",
        "Cong Liu"
      ],
      "citation_count": 133,
      "url": "https://www.semanticscholar.org/paper/6c780dfe8ff9c5a0d51893b5eaf41591299b3b8b",
      "pdf_url": "http://arxiv.org/pdf/2302.03251",
      "publication_date": "2023-02-07",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32d9445d87da8c05d99a64750a71787c261cf5c5",
      "title": "Visual Stability Prediction and Its Application to Manipulation",
      "abstract": "Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel objects and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. \nIn this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an {\\em end-to-end} approach that directly predicts stability from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way---bypassing the need for an explicit simulation at run-time. \nWe present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. We first evaluate the approach on synthetic data and compared the results to human judgments on the same stimuli. Further, we extend this approach to reason about future states of such towers that in turn enables successful stacking.",
      "year": 2016,
      "venue": "AAAI Spring Symposia",
      "authors": [
        "Wenbin Li",
        "A. Leonardis",
        "Mario Fritz"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/32d9445d87da8c05d99a64750a71787c261cf5c5",
      "pdf_url": "",
      "publication_date": "2016-09-15",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95d90f6e0b2b8d2e44726b3389fd201f0df6199f",
      "title": "VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors",
      "abstract": "We introduce VIOLA, an object-centric imitation learning approach to learning closed-loop visuomotor policies for robot manipulation. Our approach constructs object-centric representations based on general object proposals from a pre-trained vision model. VIOLA uses a transformer-based policy to reason over these representations and attend to the task-relevant visual factors for action prediction. Such object-based structural priors improve deep imitation learning algorithm's robustness against object variations and environmental perturbations. We quantitatively evaluate VIOLA in simulation and on real robots. VIOLA outperforms the state-of-the-art imitation learning methods by $45.8\\%$ in success rate. It has also been deployed successfully on a physical robot to solve challenging long-horizon tasks, such as dining table arrangement and coffee making. More videos and model details can be found in supplementary material and the project website: https://ut-austin-rpl.github.io/VIOLA .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Yifeng Zhu",
        "Abhishek Joshi",
        "P. Stone",
        "Yuke Zhu"
      ],
      "citation_count": 188,
      "url": "https://www.semanticscholar.org/paper/95d90f6e0b2b8d2e44726b3389fd201f0df6199f",
      "pdf_url": "http://arxiv.org/pdf/2210.11339",
      "publication_date": "2022-10-20",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0df4cec837ce53db955d698dfbc972d76c1abfdf",
      "title": "SAFL-Net: Semantic-Agnostic Feature Learning Network with Auxiliary Plugins for Image Manipulation Detection",
      "abstract": "Since image editing methods in real world scenarios cannot be exhausted, generalization is a core challenge for image manipulation detection, which could be severely weakened by semantically related features. In this paper we propose SAFL-Net, which constrains a feature extractor to learn semantic-agnostic features by designing specific modules with corresponding auxiliary tasks. Applying constraints directly to the features extracted by the encoder helps it learn semantic-agnostic manipulation trace features, which prevents the biases related to semantic information within the limited training data and improves generalization capabilities. The consistency of auxiliary boundary prediction task and original region prediction task is guaranteed by a feature transformation structure. Experiments on various public datasets and comparisons in multiple dimensions demonstrate that SAFL-Net is effective for image manipulation detection.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Zhihao Sun",
        "Haoran Jiang",
        "Danding Wang",
        "Xirong Li",
        "Juan Cao"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/0df4cec837ce53db955d698dfbc972d76c1abfdf",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1ceac37e39287b313cc98658ce1e97b5113a46f6",
      "title": "Passivity-based Attack Identification and Mitigation with Event-triggered Observer Feedback and Switching Controller",
      "abstract": "This paper addresses the problem of output consensus in linear passive multi-agent systems under a False Data Injection (FDI) attack, considering the unavailability of complete state information. Our formulation relies on an event-based cryptographic authentication scheme for sensor integrity and considers FDI attacks at the actuator end, inspired by their practical nature and usages. For secure consensus, we propose (i) a passivity-based approach for detecting FDI attacks on the system and (ii) a Zeno-free event-triggered observer-based switching controller, which switches between the normal and the defense modes following an attack detection. We show that the closed-loop system achieves practical consensus under the controller's action in the defense mode. Simulation examples are provided to support the theoretical findings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Pushkal Purohit",
        "Anoop Jain"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1ceac37e39287b313cc98658ce1e97b5113a46f6",
      "pdf_url": "",
      "publication_date": "2024-03-23",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e9e04213512dba7c3fcd6949d6a83375e2bb8351",
      "title": "Black-box Lossless Fragile Watermarking Based on Hidden Space Search for DNN Integrity Authentication",
      "abstract": "A lossless and fragile watermarking scheme based on black-box with hidden space search is proposed for DNN. This is a publicized remote verification scheme, any authorized party can generate and verify the watermark of the target model without knowing the internal situation of the model and only by the model ground-truth input and output results. Specifically, we generate some sensitive samples near the decision boundary of the target model and exploit these samples as the fingerprints of the model. Once these target models are attacked or tampered to a certain extent, the labels corresponding to these sensitive samples will be flipped, thus accomplishing the purpose of authentication. Experimental results show that the proposed scheme is sensitive to the some current common model attack schemes, and only a small number of sensitive samples are needed to efficiently identify whether the watermarked model is intact or not.",
      "year": 2023,
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "authors": [
        "Gejian Zhao",
        "Chuan Qin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e9e04213512dba7c3fcd6949d6a83375e2bb8351",
      "pdf_url": "",
      "publication_date": "2023-10-31",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4492c22513c21baef29d01815aa52c1a60c083ef",
      "title": "Spatio-Temporal Correlation-Based False Data Injection Attack Detection Using Deep Convolutional Neural Network",
      "abstract": "There are lots of cyber-attack, especially false data injection attacks, in modern power systems. This attack can circumvent traditional residual-based detection methods, and destroy the integrity of control information, thus hindering the stability of the power system. In this paper, a novel Spatio-temporal detection mechanism is proposed to evaluate and locate false data injection attacks. In the proposed method, temporal correlation and spatial correlation are analyzed by cubature Kalman filter and Gaussian process regression, respectively, to capture the dynamic features of state vectors. Then, a deep convolutional neural network is trained to depict the functional relationship between Spatio-temporal correlation functions and the output, which is set as the detection indicator to access whether the power system under attack or not. Furthermore, the performance of the proposed mechanism is evaluated with comprehensive numerical simulation on IEEE 39-bus test system. The results of the case studies showed that the proposed method can achieve 99.84%-100% accuracy.",
      "year": 2022,
      "venue": "IEEE Transactions on Smart Grid",
      "authors": [
        "Guangdou Zhang",
        "Jian Li",
        "O. Bamisile",
        "Dongsheng Cai",
        "Weihao Hu",
        "Qi Huang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/4492c22513c21baef29d01815aa52c1a60c083ef",
      "pdf_url": "",
      "publication_date": "2022-01-01",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "40ec00716fefa3d7aa2f678793cf6ba3998cb7de",
      "title": "Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation",
      "abstract": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining. Despite their significant utility in various applications, RAG systems present new security risks. In this work, we propose a novel attack that allows an adversary to inject a single malicious document into a RAG system's knowledge base, and mount a backdoor poisoning attack. We design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model's output. First, the document is constructed to be retrieved only when a specific naturally occurring trigger sequence of tokens appears in the victim's queries. Second, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful behaviors.We demonstrate our attacks on multiple open-source LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to closed-source models such as GPT-3.5 Turbo and GPT-4. Finally, we successfully demonstrate our attack on an end-to-end black-box production RAG system: NVIDIA's\"Chat with RTX''.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Harsh Chaudhari",
        "Giorgio Severi",
        "John Abascal",
        "Matthew Jagielski",
        "Christopher A. Choquette-Choo",
        "Milad Nasr",
        "C. Nita-Rotaru",
        "Alina Oprea"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/40ec00716fefa3d7aa2f678793cf6ba3998cb7de",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b9ec7e741dd89c345908161b7835f8aa4fc24bb1",
      "title": "Rushing at SPDZ: On the Practical Security of Malicious MPC Implementations",
      "abstract": "Secure multi-party computation (MPC) enables parties to compute a function over private inputs while maintaining confidentiality. Although MPC has advanced significantly and attracts a growing industry interest, open-source imple-mentations are still at an early stage, with no production-ready code and a poor understanding of their actual security guarantees. In this work, we study the real-world security of modern MPC implementations, focusing on the SPDZ protocol (Damgard et al., CRYPTO 2012, ESORICS 2013), which provides security against malicious adversaries when all-but-one of the participants may be corrupted. We identify a novel type of MAC key leakage in the MAC check protocol of SPDZ, which can be exploited in concurrent, multi-threaded settings, com-promising output integrity and, in some cases, input privacy. In our analysis of three SPDZ implementations (MP-SPDZ, SCALE-MAMBA, and FRESCO), two are vulnerable to this attack, while we also uncover further issues and vulnerabilities with all implementations. We propose mitigation strategies and some recommendations for researchers, developers and users, which we hope can bring more awareness to these issues and avoid them reoccurring in future.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Alexander Kyster",
        "Frederik Huss Nielsen",
        "Sabine Oechsner",
        "Peter Scholl"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b9ec7e741dd89c345908161b7835f8aa4fc24bb1",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "75e7d221cebcf9cd10c140b596469283424eeffb",
      "title": "Detection of Integrity Attacks in Cyper Physical Systems Based On Reservoir Networks",
      "abstract": "This paper presents an anomaly-based methodology for reliable detection of integrity attacks in cyber-physical critical infrastructures. Such malicious events compromise the smooth operation of the infrastructure while the attacker is able to exploit the respective resources according to his/her purposes. Even though the operator may not understand the attack, since the overall system appears to remain in a steady state, the consequences may be of catastrophic nature with a huge negative impact. Here, we apply a deep learning technique and more specifically reservoir networks. They follow the supervised learning principle for recurrent neural networks, while the fundamental logic is to steer a random, large, fixed recurrent neural network with the input signal to the desired direction (class, probability, etc.). Their great advantage is the fact that the only part in need of training is the output layer which is a linear combination of all of the response signals. In addition we consider both temporal and functional dependencies existing among the elements of an infrastructure. The experimental platform includes a simulator of both a power grid and a cyber-network of the IEEE-9 bus model. Subsequently we implemented a wide range of integrity attacks (replay, ramp, pulse, scaling, and random) with different intensity levels. A thorough evaluation procedure is carried out while the results demonstrate the ability of the proposed method to produce a desired result in terms of false positive rate, false negative rate and detection delay.",
      "year": 2015,
      "venue": "",
      "authors": [
        "S. Ntalampiras",
        "Yannis Soupionis"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/75e7d221cebcf9cd10c140b596469283424eeffb",
      "pdf_url": "https://sciforum.net/paper/download/3274/manuscript",
      "publication_date": "2015-11-13",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ad0b2aa3999f0b312bdea24dd24f37ffc6716a32",
      "title": "Realistic GPS Spoofing via Customized CARLA GPS Navigation and Controller Systems",
      "abstract": "The vulnerability of autonomous vehicle (AV) systems to Global Positioning System (GPS)-based attacks presents a significant challenge in ensuring reliable navigation and sensor integrity. This work addresses the lack of modifiable GPS data output in CARLA which limits the evaluation of sensor vulnerabilities under different attack scenarios. To overcome this limitation, we introduce a novel unbounded GPS navigation framework within the CARLA simulator. A Proportional-Integral-Derivative (PID) controller is integrated with the GPS approximation method to improve navigation performance under adversarial conditions. The framework has been tested under two GPS attack models, demonstrating its ability to handle small deviations while revealing limitations in correcting larger biases. Experimental results demonstrate successful navigation in both straight-line and waypoint-based scenarios, establishing a foundation for future work focused on mitigating sensor manipulation through reverseengineering techniques and improved GPS tracking systems.",
      "year": 2025,
      "venue": "Midwest Symposium on Circuits and Systems",
      "authors": [
        "Thomas D. Robertson",
        "Wesam Al Amiri",
        "Abhijeet Solanki",
        "S. R. Hasan",
        "Terry N. Guo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ad0b2aa3999f0b312bdea24dd24f37ffc6716a32",
      "pdf_url": "",
      "publication_date": "2025-08-10",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d075109534302de96f3d386dcb6626905f0a33aa",
      "title": "Estimate-Based Dynamic Memory-Event-Triggered Control for Nonlinear Networked Control Systems Subject to Hybrid Attacks",
      "abstract": "Within the framework of a dynamic memory-event-triggered mechanism (DMETM), this paper proposes an estimate-based secure control algorithm for nonlinear networked control systems (NNCSs) that suffer from hybrid attacks. Firstly, a sampled-data observer is employed utilizing the output signals to estimate the states. Secondly, due to the limitation of data transmission capacity in NNCSs, a novel DMETM with auxiliary variable is proposed, which effectively leverages the benefits of historical sampled data. In the process of network data transmission, a hybrid attack model that simultaneously considers the impact of both deception and denial of service (DoS) attacks is introduced, which can undermine signal integrity and disrupt data transmission. Then, a memory-event-triggered controller is developed, and the mean square stability of the NNCSs can be ensured by selecting some appropriate values. Finally, a numerical simulation and a practical example are given to illustrate the meaning of the designed dynamic memory-event-triggered control (DMETC) algorithm.",
      "year": 2025,
      "venue": "Mathematics",
      "authors": [
        "Bo Zhang",
        "Tao Zhang",
        "Zesheng Xi",
        "Yunfan Wang",
        "Meng Yang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d075109534302de96f3d386dcb6626905f0a33aa",
      "pdf_url": "",
      "publication_date": "2025-09-02",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dc86c68484f6cfe0f47e36d6ace47668c5417d77",
      "title": "MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks",
      "abstract": "The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing finely tuned perturbations that are imperceptible to the human eye. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approach contributes to the responsible and ethical use of generative models. This work provides a foundation for future research in countering cyber threats in medical imaging. Models and codes are publicly available.1.",
      "year": 2024,
      "venue": "Comput. Biol. Medicine",
      "authors": [
        "Giovanni Pasqualino",
        "Luca Guarnera",
        "Alessandro Ortis",
        "S. Battiato"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/dc86c68484f6cfe0f47e36d6ace47668c5417d77",
      "pdf_url": "",
      "publication_date": "2024-01-17",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4c9541ce59d53d7b21fe644656b65698006a860",
      "title": "A Two-Stage Dual-Path Framework for Text Tampering Detection and Recognition",
      "abstract": "Document tamper detection has always been an important aspect of tamper detection. Before the advent of deep learning, document tamper detection was difficult. We have made some explorations in the field of text tamper detection based on deep learning. Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition. It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered). By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.). The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results. Audit point positioning uses detection frameworks and controls thresholds for high and low density detection. Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction. After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Guandong Li",
        "Xian Yang",
        "Wenpin Ma"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c4c9541ce59d53d7b21fe644656b65698006a860",
      "pdf_url": "",
      "publication_date": "2024-02-21",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c77f9086c82dfc9a354397ccf43b4882cbe81c00",
      "title": "Operation-wise Attention Network for Tampering Localization Fusion",
      "abstract": "In this work, we present a deep learning-based approach for image tampering localization fusion. This approach is designed to combine the outcomes of multiple image forensics algorithms and provides a fused tampering localization map, which requires no expert knowledge and is easier to interpret by end users. Our fusion framework includes a set of five individual tampering localization methods for splicing localization on JPEG images. The proposed deep learning fusion model is an adapted architecture, initially proposed for the image restoration task, that performs multiple operations in parallel, weighted by an attention mechanism to enable the selection of proper operations depending on the input signals. This weighting process can be very beneficial for cases where the input signal is very diverse, as in our case where the output signals of multiple image forensics algorithms are combined. Evaluation in three publicly available forensics datasets demonstrates that the performance of the proposed approach is competitive, outperforming the individual forensics techniques as well as another recently proposed fusion framework in the majority of cases.",
      "year": 2021,
      "venue": "International Conference on Content-Based Multimedia Indexing",
      "authors": [
        "P. Charitidis",
        "Giorgos Kordopatis-Zilos",
        "S. Papadopoulos",
        "Y. Kompatsiaris"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c77f9086c82dfc9a354397ccf43b4882cbe81c00",
      "pdf_url": "https://arxiv.org/pdf/2105.05515",
      "publication_date": "2021-05-12",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fe92b72d5d76c25b4cd713fa431114b4238ea63",
      "title": "Output-correlated adversarial attack for image translation network",
      "abstract": "Abstract. Maliciously forged images generated by image translation networks can cause significant security threats to personal privacy and national security. An emerging solution for forged images is preventing image forgery models from tampering with user images through adversarial attacks. Currently, conventional adversarial generation algorithms use random noise as the starting point, which makes the final adversarial output similar to the original output but does not prevent image tampering. The output-correlated initialization is applied to improve the adversarial attack algorithm for the image translation network and improve the visual effect of adversarial attacks. Moreover, a comparative experiment is performed on multiple loss functions, and the loss function with the best performance is selected as the adversarial loss function to complete the adversarial attack on the image translation network. The selected initialization method makes the search process of adversarial examples more comprehensive and makes the generation results of adversarial examples more diverse. The analysis of the visual effects of the attack reveals how the proposed adversarial attack methods affect the forgery results of different image translation frameworks and generate more chaotic images. Comparison of multiple indicators demonstrates that the proposed method has a high attack success rate and expands the image distance between the adversarial output and the original output, thereby improving the attack efficiency, preventing malicious tampering with the image, and protecting the user image.",
      "year": 2022,
      "venue": "J. Electronic Imaging",
      "authors": [
        "Peiyuan Liu",
        "Lei Sun",
        "Xiuqing Mao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3fe92b72d5d76c25b4cd713fa431114b4238ea63",
      "pdf_url": "",
      "publication_date": "2022-03-01",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fc913085a09769fca220e95bd4b45811cfd5fbb6",
      "title": "Learning under p-Tampering Attacks",
      "abstract": null,
      "year": 2018,
      "venue": "International Symposium on Artificial Intelligence and Mathematics",
      "authors": [
        "Saeed Mahloujifar",
        "Dimitrios I. Diochnos",
        "Mohammad Mahmoody"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/fc913085a09769fca220e95bd4b45811cfd5fbb6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d00c1e6c5d45d177053e8c8f14fba7103055ddae",
      "title": "Hybrid Logic Locking Using Horned Lizard Optimization and Q-Learning for Secure Hardware Design",
      "abstract": ": Logic locking has become an essential technique for safeguarding intellectual property (IP) in hardware designs against reverse engineering and tampering. However, existing methods often rely on the random insertion of key gates in original circuits, neglecting critical factors such as area overhead and output corruption rates. This paper presents a novel hybrid logic locking technique aimed at maximizing security while minimizing the overhead of key gate insertion. The proposed method integrates the Horned Lizard Optimization (HLO), a metaheuristic inspired by the adaptive defense strategies of horned lizards, with a Q -learning model. The Q -learning component enhances the exploration stage of HLO, enabling the optimal selection of insertion points for key gates in the circuit. Experimental evaluations on benchmark circuits demonstrate that the proposed technique achieves an average area, delay, and power overhead of 16.85%, 0.0475%, and 2.3345%, respectively, outperforming state-of-the-art methods in terms of efficiency and security.",
      "year": 2025,
      "venue": "Tehni\u010dki Vjesnik",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d00c1e6c5d45d177053e8c8f14fba7103055ddae",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "00d10f78da8d9f41a932fb6f91a97e98fe04a017",
      "title": "Finite-time adaptive control of output constrained nonlinear systems under deception attacks",
      "abstract": null,
      "year": 2024,
      "venue": "Nonlinear dynamics",
      "authors": [
        "Linfang Shao",
        "Weiwei Sun",
        "Lusong Ding"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/00d10f78da8d9f41a932fb6f91a97e98fe04a017",
      "pdf_url": "",
      "publication_date": "2024-07-12",
      "keywords_matched": [
        "output corruption"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "53e2f37df6cec7b005705410184029860af7dfc8",
      "title": "Generating and Predicting Output Perturbations in Image Segmenters",
      "abstract": "Image segmentation applications are a core component of safety-critical autonomous software pipelines. Sensor data input noise can lead to segmentation output corruption that threatens safety in both DNN- and transformer-based segmenters. Previous work has proposed methods for generating malicious noise to cause DNN- and transformer-based object detection and classification output corruption. We perform the same task for image segmentation applications using genetic algorithms for optimization. We then propose a novel method to predict whether an input image will yield a corrupted segmentation output due to noise. We evaluate the optimal noise generation and corruption prediction on state-of-the-art image segmenters YOLOv8 and DETR. We observe that we can (a) cause segmentation output corruption with noise that is undetectable to the human eye and unrelated to the corrupted region of the image; and (b) predict output corruption due to image noise with over 96% accuracy.",
      "year": 2025,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Matthew Bozoukov",
        "Nguyen Anh Vu Doan",
        "Bryan Donyanavard"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/53e2f37df6cec7b005705410184029860af7dfc8",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "output corruption"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6c8da172a54da210c838677f05d04c66a51d14e",
      "title": "Better Trigger Inversion Optimization in Backdoor Scanning",
      "abstract": "Backdoor attacks aim to cause misclassification of a subject model by stamping a trigger to inputs. Backdoors could be injected through malicious training and naturally exist. Deriving backdoor trigger for a subject model is critical to both attack and defense. A popular trigger inversion method is by optimization. Existing methods are based on finding a smallest trigger that can uniformly flip a set of input samples by minimizing a mask. The mask defines the set of pixels that ought to be perturbed. We develop a new optimization method that directly minimizes individual pixel changes, without using a mask. Our experiments show that compared to existing methods, the new one can generate triggers that require a smaller number of input pixels to be perturbed, have a higher attack success rate, and are more robust. They are hence more desirable when used in real-world attacks and more effective when used in defense. Our method is also more cost-effective.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Guanhong Tao",
        "Guangyu Shen",
        "Yingqi Liu",
        "Shengwei An",
        "Qiuling Xu",
        "Shiqing Ma",
        "X. Zhang"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/f6c8da172a54da210c838677f05d04c66a51d14e",
      "pdf_url": "",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "999f2f51dc800501efce5c7e12039f276e17e883",
      "title": "A multitarget backdooring attack on deep neural networks with random location trigger",
      "abstract": "Machine learning has made tremendous progress and applied to various critical practical applications. However, recent studies have shown that machine learning models are vulnerable to malicious attackers, such as neural network backdoor triggering. A successful backdoor triggering behavior may cause serious consequences, such as allowing the attacker to bypass the identity verification and directly enter the system. In image classification, there is always only one target label triggered by one backdoor trigger in previous works. The position of the backdoor trigger is also fixed, which brings limitations to the attack. In this paper, we propose a novel method that utilizes one trigger pattern to correspond to multiple target labels, and the location of the trigger is not limited. In our method, the trigger guarantees that the malicious output is within the range of multiple targets chosen by the attacker, but the specific target depends on the original image where the trigger is pasted. Due to the original images' diversity, it is difficult for the defender to predict which target the image with the trigger is classified as. Besides, the attacker can use only one trigger pattern to achieve multitarget attacks at different locations, which brings more flexibility. We also proposed to train a neural network as a detector to distinguish backdoor images and clean images for multitarget backdooring attacks. Experiment results show that the detection method can also successfully detect the backdoor image with a trigger at a random location of the image, and the detection success rate is as high as 86.02%.",
      "year": 2021,
      "venue": "International Journal of Intelligent Systems",
      "authors": [
        "Xiaohui Yu",
        "Liu Cong",
        "Mingwen Zheng",
        "Yajie Wang",
        "Xinrui Liu",
        "Song Shuxiao",
        "Ma Yuexuan",
        "Zheng Jun"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/999f2f51dc800501efce5c7e12039f276e17e883",
      "pdf_url": "https://doi.org/10.1002/int.22785",
      "publication_date": "2021-12-28",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42658c812d60d26a0bdad91b4d81e8620b994bf6",
      "title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks",
      "abstract": "Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classification to produce unexpected results. For example, a model with a backdoor always identifies a face as Bill Gates if a specific symbol is present in the input. Backdoors can stay hidden indefinitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g. biometric authentication systems or self-driving cars. We present the first robust and generalizable detection and mitigation system for DNN backdoor attacks. Our techniques identify backdoors and reconstruct possible triggers. We identify multiple mitigation techniques via input filters, neuron pruning and unlearning. We demonstrate their efficacy via extensive experiments on a variety of DNNs, against two types of backdoor injection methods identified by prior work. Our techniques also prove robust against a number of variants of the backdoor attack.",
      "year": 2019,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Bolun Wang",
        "Yuanshun Yao",
        "Shawn Shan",
        "Huiying Li",
        "Bimal Viswanath",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 1664,
      "url": "https://www.semanticscholar.org/paper/42658c812d60d26a0bdad91b4d81e8620b994bf6",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/8826229/8835208/08835365.pdf",
      "publication_date": "2019-04-01",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f706f4ecf40b695f202177607d6fbf28ff97d1b8",
      "title": "Countering Adversarial Zeroth Order Optimization Attacks Based on Neural-Cleanse, Gaussian and Poisson Noise Adding",
      "abstract": "Nowadays, machine learning is becoming an increasingly widely used artificial intelligence technology and is being actively implemented in various fields of science and technology, such as defense against cyber attacks, image recognition, computer vision, autonomous vehicles and other complex tasks. However, despite the benefits of machine learning, it attracts the attention of attackers. Cyberattacks against machine learning models, such as classifiers or artificial neural networks, can seriously distort the results of these models and cause irreversible damage to machine learning-based decision-making systems. Therefore, research aimed at countering cyber attacks using machine learning models is becoming very important nowadays. The paper presents a method for protecting against adversarial attacks, Zeroth Order Optimization (ZOO), which is based on the use of Neural-Cleanse technology and the addition of Gaussian and Poisson noise. The proposed approach is aimed at increasing the resistance of neural networks to malicious attacks by additional processing of input data and complicating the optimization problem for attackers. An experimental evaluation of the proposed approach was carried out on the PC Parts Images Dataset using the K-Nearest Neighbors, Random Forest, Naive Bayes Classifier, and Decision Trees classifiers. The study results showed the high efficiency of the proposed approach to protecting models from the effects of adversarial ZOO attacks. The findings may be useful for developing more reliable systems and improving security in the field of machine learning.",
      "year": 2024,
      "venue": "International Conference on Industrial Engineering, Applications and Manufacturing",
      "authors": [
        "I. Kotenko",
        "I. Saenko",
        "V. Sadovnikov"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f706f4ecf40b695f202177607d6fbf28ff97d1b8",
      "pdf_url": "",
      "publication_date": "2024-05-20",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "57356c38394a7849c616d467600a19bcba88a6e9",
      "title": "Approach to Detecting Attacks against Machine Learning Systems with a Generative Adversarial Network",
      "abstract": null,
      "year": 2024,
      "venue": "Pattern Recognition and Image Analysis",
      "authors": [
        "I. V. Kotenko",
        "I. Saenko",
        "O. Lauta",
        "N. A. Vasilev",
        "V. Sadovnikov"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/57356c38394a7849c616d467600a19bcba88a6e9",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2068a0be69dffc72adf2a102f88755aa515521a2",
      "title": "Stealthy Backdoor Attack with Adversarial Training",
      "abstract": "Research shows that deep neural networks are vulnerable to back-door attacks. The backdoor network behaves normally on clean examples, but once backdoor patterns are attached to examples, back-door examples will be classified into the target class. In the previous backdoor attack schemes, backdoor patterns are not stealthy and may be detected. Thus, to achieve the stealthiness of backdoor patterns, we explore an invisible and example-dependent backdoor attack scheme. Specifically, we employ the backdoor generation network to generate the invisible backdoor pattern for each example, and backdoor patterns are not generic to each other. However, without other measures, the backdoor attack scheme cannot bypass the neural cleanse detection. Thus, we propose adversarial training to bypass neural cleanse detection. Experiments show that the proposed backdoor attack achieves a considerable attack success rate, invisibility, and can bypass the existing defense strategies.",
      "year": 2022,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Le Feng",
        "Sheng Li",
        "Zhenxing Qian",
        "Xinpeng Zhang"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/2068a0be69dffc72adf2a102f88755aa515521a2",
      "pdf_url": "",
      "publication_date": "2022-05-23",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4dac4feac17a56290f79443be5c011f88b48850d",
      "title": "Adversarial Robustness via Runtime Masking and Cleansing",
      "abstract": null,
      "year": 2020,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Yi-Hsuan Wu",
        "Chia-Hung Yuan",
        "Shan-Hung Wu"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/4dac4feac17a56290f79443be5c011f88b48850d",
      "pdf_url": "",
      "publication_date": "2020-07-12",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ed005193b3050770155c79ea51ffb1ece1dfd56c",
      "title": "Efficient Estimation of Influence of a Training Instance",
      "abstract": "Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model\u2019s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influence. Our method is inspired by dropout, which zero-masks a sub-network and prevents the sub-network from learning each training instance. By switching between dropout masks, we can use sub-networks that learned or did not learn each training instance and estimate its influence. Through experiments with BERT and VGGNet on classification datasets, we demonstrate that the proposed method can capture training influences, enhance the interpretability of error predictions, and cleanse the training dataset for improving generalization.",
      "year": 2020,
      "venue": "SUSTAINLP",
      "authors": [
        "Sosuke Kobayashi",
        "Sho Yokoi",
        "Jun Suzuki",
        "Kentaro Inui"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/ed005193b3050770155c79ea51ffb1ece1dfd56c",
      "pdf_url": "https://www.aclweb.org/anthology/2020.sustainlp-1.6.pdf",
      "publication_date": "2020-11-01",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "449310e3538b08b43227d660227dfd2875c3c3c1",
      "title": "Neural Ordinary Differential Equations",
      "abstract": "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",
      "year": 2018,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "T. Chen",
        "Yulia Rubanova",
        "J. Bettencourt",
        "D. Duvenaud"
      ],
      "citation_count": 6146,
      "url": "https://www.semanticscholar.org/paper/449310e3538b08b43227d660227dfd2875c3c3c1",
      "pdf_url": "",
      "publication_date": "2018-06-19",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "701c10f6df1ace15c321d265e3488c613f009c19",
      "title": "PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning",
      "abstract": "Pre-trained language models (PLMs) have attracted enormous attention over the past few years with their unparalleled performances. Meanwhile, the soaring cost to train PLMs as well as their amazing generalizability have jointly contributed to few-shot fine-tuning and prompting as the most popular training paradigms for natural language processing (NLP) models. Nevertheless, existing studies have shown that these NLP models can be backdoored such that model behavior is manipulated when trigger tokens are presented.In this paper, we propose PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial prompt-tuning in few-shot settings.Unlike existing NLP backdoor removal methods, which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix keeps the model parameters intact and only utilizes two extra sets of soft tokens which approximate the trigger and counteract it respectively. The use of soft tokens and adversarial optimization eliminates the need to enumerate possible backdoor configurations and enables an adaptive balance between trigger finding and preservation of performance.Experiments with various backdoor attacks validate the effectiveness of the proposed method and the performances when domain shift is present further shows PromptFix\u2019s applicability to models pretrained on unknown data source which is the common case in prompt tuning scenarios.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Tianrong Zhang",
        "Zhaohan Xi",
        "Ting Wang",
        "Prasenjit Mitra",
        "Jinghui Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/701c10f6df1ace15c321d265e3488c613f009c19",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d3ecd098d386080ee71a9a4f8cad7b73a6faa3a4",
      "title": "Data-free Backdoor Removal based on Channel Lipschitzness",
      "abstract": "Recent studies have shown that Deep Neural Networks (DNNs) are vulnerable to the backdoor attacks, which leads to malicious behaviors of DNNs when specific triggers are attached to the input images. It was further demonstrated that the infected DNNs possess a collection of channels, which are more sensitive to the backdoor triggers compared with normal channels. Pruning these channels was then shown to be effective in mitigating the backdoor behaviors. To locate those channels, it is natural to consider their Lipschitzness, which measures their sensitivity against worst-case perturbations on the inputs. In this work, we introduce a novel concept called Channel Lipschitz Constant (CLC), which is defined as the Lipschitz constant of the mapping from the input images to the output of each channel. Then we provide empirical evidences to show the strong correlation between an Upper bound of the CLC (UCLC) and the trigger-activated change on the channel activation. Since UCLC can be directly calculated from the weight matrices, we can detect the potential backdoor channels in a data-free manner, and do simple pruning on the infected DNN to repair the model. The proposed Channel Lipschitzness based Pruning (CLP) method is super fast, simple, data-free and robust to the choice of the pruning threshold. Extensive experiments are conducted to evaluate the efficiency and effectiveness of CLP, which achieves state-of-the-art results among the mainstream defense methods even without any data. Source codes are available at https://github.com/rkteddy/channel-Lipschitzness-based-pruning.",
      "year": 2022,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Runkai Zheng",
        "Rong Tang",
        "Jianze Li",
        "Li Liu"
      ],
      "citation_count": 124,
      "url": "https://www.semanticscholar.org/paper/d3ecd098d386080ee71a9a4f8cad7b73a6faa3a4",
      "pdf_url": "https://arxiv.org/pdf/2208.03111",
      "publication_date": "2022-08-05",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a771a7eeb5edc5071c2ad9fa25bd5ecbe0f4882e",
      "title": "BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning",
      "abstract": null,
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xuan Chen",
        "Wenbo Guo",
        "Guanhong Tao",
        "Xiangyu Zhang",
        "D. Song"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/a771a7eeb5edc5071c2ad9fa25bd5ecbe0f4882e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1b2ac7789ea8bed2550ac3f930d339b7bd2273cf",
      "title": "Efficient Method for Robust Backdoor Detection and Removal in Feature Space Using Clean Data",
      "abstract": "The steady increase of proposed backdoor attacks on deep neural networks highlights the need for robust defense methods for their detection and removal. A backdoor attack is a type of attack where hidden triggers are added to the input data during training, with the goal of changing the behavior of the model during inference. These attacks pose a significant security threat in critical applications, such as street sign or pedestrian recognition for autonomous vehicles, biometric authentication, image retrieval, semantic labeling, etc. To combat these threats, many defense mechanisms have been proposed. These methods target different areas, such as computer vision (CV), natural language processing (NLP), and thus utilize different assumptions about the nature of the input data and the type of backdoor trigger used in the attack. However, the attacker can exploit these assumptions, which reduces their successfulness in real-world scenarios. Thus, a robust method for backdoor detection needs to have broad and simple assumptions. Furthermore, detection methods that rely on the input data suffer from the fact that they are constrained to the modality of the input and cannot apply to different modalities. In this work, a novel method for backdoor detection and removal for classification tasks using features extracted by the attacked model called FEAT-IN is proposed. This method can detect and reconstruct the feature representation of the possible triggers used in attacking the neural network. Using these reconstructed trigger features, the method can be used to efficiently mitigate the effects of an attack. Extensive experiments on multiple datasets and attack methods demonstrate that, when compared to state-of-the-art methods such as Neural Cleanse, Neural Attention Distillation, I-BAU, BTI-DBF etc. the FEAT-IN method provides several benefits. It can more consistently detect and mitigate backdoor attacks than similar trigger inversion defense methods that conduct the defense in the input space instead of feature space (where, on average, it achieves approx. 10% higher decrease in attack success rate during mitigation compared to the second-best method). Secondly, it reduces the memory footprint and the computation time by at least an order of magnitude compared to other methods, which allows FEAT-IN to be used practically in real-world scenarios. Finally, it is not constrained to only computer vision tasks, as this assumption holds for feature spaces of different problems, which is demonstrated by applying it without any change to semantic analysis on the SST-2 dataset.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Donik Vr\u0161nak",
        "Marko Suba\u0161i\u0107",
        "Sven Lon\u010dari\u0107"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1b2ac7789ea8bed2550ac3f930d339b7bd2273cf",
      "pdf_url": "https://doi.org/10.1109/access.2025.3531716",
      "publication_date": null,
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cde66dfcfc818249f94aa0d186ab9a91beeff448",
      "title": "Unlearning Backdoor Attacks in Federated Learning",
      "abstract": "Federated learning systems are constantly under the looming threat of backdoor attacks. Despite significant progress in mitigating such attacks, the challenge of effectively removing a potential attacker\u2019s influence from the trained global model remains unresolved. In this paper, we present a novel federated unlearning method that is suitable for backdoor removal. By leveraging historical updates subtraction and knowledge distillation, our approach can maintain the models\u2019s performance while completely removing the backdoors implanted by the attacker from the model. It can be seamlessly applied to various types of neural networks and does not require clients\u2019 participation in the unlearning process. Through experiments on diverse computer vision and natural language processing datasets, we demonstrate the effectiveness and efficiency of our proposed method. The promising results obtained validate the potential of our approach to bolster the security of federated learning systems against backdoor threats.",
      "year": 2024,
      "venue": "IEEE Conference on Communications and Network Security",
      "authors": [
        "Chen Wu",
        "Sencun Zhu",
        "P. Mitra",
        "Wei Wang"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/cde66dfcfc818249f94aa0d186ab9a91beeff448",
      "pdf_url": "",
      "publication_date": "2024-09-30",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "159ac24db1e19d230ff326b75cddb98230bc9327",
      "title": "Fisher Information guided Purification against Backdoor Attacks",
      "abstract": "Studies on backdoor attacks in recent years suggest that an adversary can compromise the integrity of a deep neural network (DNN) by manipulating a small set of training samples. Our analysis shows that such manipulation can make the backdoor model converge to a bad local minima, i.e., sharper minima as compared to a benign model. Intuitively, the backdoor can be purified by re-optimizing the model to smoother minima. However, a na\u00efve adoption of any optimization targeting smoother minima can lead to sub-optimal purification techniques hampering the clean test accuracy. Hence, to effectively obtain such re-optimization, inspired by our novel perspective establishing the connection between backdoor removal and loss smoothness, we propose Fisher Information guided Purification (FIP), a novel backdoor purification framework. Proposed FIP consists of a couple of novel regularizers that aid the model in suppressing the backdoor effects and retaining the acquired knowledge of clean data distribution throughout the backdoor removal procedure through exploiting the knowledge of Fisher Information Matrix (FIM). In addition, we introduce an efficient variant of FIP, dubbed as Fast FIP, which reduces the number of tunable parameters significantly and obtains an impressive runtime gain of almost 5\u00d7. Extensive experiments show that the proposed method achieves state-of-the-art (SOTA) performance on a wide range of backdoor defense benchmarks: 5 different tasks---Image Recognition, Object Detection, Video Action Recognition, 3D point Cloud, Language Generation ; 11 different datasets including ImageNet, PASCAL VOC, UCF101 ; diverse model architectures spanning both CNN and vision transformer; 14 different backdoor attacks, e.g., Dynamic, WaNet, LIRA, ISSBA, etc. Our code is available in this https://github.com/nazmul-karim170/FIP-Fisher-Backdoor-Removal GitHub Repository.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Nazmul Karim",
        "Abdullah Al Arafat",
        "A. S. Rakin",
        "Zhishan Guo",
        "Nazanin Rahnavard"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/159ac24db1e19d230ff326b75cddb98230bc9327",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690250",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "22c22edabdbe5253a76063dc6c0d3eeb5ccab64a",
      "title": "Stealthy and Robust Backdoor Attack against 3D Point Clouds through Additional Point Features",
      "abstract": "Recently, 3D backdoor attacks have posed a substantial threat to 3D Deep Neural Networks (3D DNNs) designed for 3D point clouds, which are extensively deployed in various security-critical applications. Although the existing 3D backdoor attacks achieved high attack performance, they remain vulnerable to preprocessing-based defenses (e.g., outlier removal and rotation augmentation) and are prone to detection by human inspection. In pursuit of a more challenging-to-defend and stealthy 3D backdoor attack, this paper introduces the Stealthy and Robust Backdoor Attack (SRBA), which ensures robustness and stealthiness through intentional design considerations. The key insight of our attack involves applying a uniform shift to the additional point features of point clouds (e.g., reflection intensity) widely utilized as part of inputs for 3D DNNs as the trigger. Without altering the geometric information of the point clouds, our attack ensures visual consistency between poisoned and benign samples, and demonstrate robustness against preprocessing-based defenses. In addition, to automate our attack, we employ Bayesian Optimization (BO) to identify the suitable trigger. Extensive experiments suggest that SRBA achieves an attack success rate (ASR) exceeding 94% in all cases, and significantly outperforms previous SOTA methods when multiple preprocessing operations are applied during training.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xiaoyang Ning",
        "Qing Xie",
        "Jinyu Xu",
        "Wenbo Jiang",
        "Jiachen Li",
        "Yanchun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/22c22edabdbe5253a76063dc6c0d3eeb5ccab64a",
      "pdf_url": "",
      "publication_date": "2024-12-10",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "afbf3975f5f774a9eaead1c54389a63542b9d474",
      "title": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment",
      "abstract": "With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-discriminator pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting and successful installation of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are more challenging to remove. These findings underscore an urgent need for the research community to develop more robust defenses against adversarial backdoor threats in LLM alignment.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Pankayaraj Pathmanathan",
        "Udari Madhushani Sehwag",
        "Michael-Andrei Panaitescu-Liess",
        "Furong Huang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/afbf3975f5f774a9eaead1c54389a63542b9d474",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5cc4d950ae66e70bc54f0ecebbad1d42777473b2",
      "title": "On the Robustness of Backdoor-based Watermarking in Deep Neural Networks",
      "abstract": "Watermarking algorithms have been introduced in the past years to protect deep learning models against unauthorized re-distribution. We investigate the robustness and reliability of state-of-the-art deep neural network watermarking schemes. We focus on backdoor-based watermarking and propose two simple yet effective attacks -- a black-box and a white-box -- that remove these watermarks without any labeled data from the ground truth. Our black-box attack steals the model and removes the watermark with only API access to the labels. Our white-box attack proposes an efficient watermark removal when the parameters of the marked model are accessible, and improves the time to steal a model up to twenty times over the time to train a model from scratch. We conclude that these watermarking algorithms are insufficient to defend against redistribution by a motivated attacker.",
      "year": 2021,
      "venue": "Information Hiding and Multimedia Security Workshop",
      "authors": [
        "Masoumeh Shafieinejad",
        "Nils Lukas",
        "Jiaqi Wang",
        "Xinda Li",
        "F. Kerschbaum"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/5cc4d950ae66e70bc54f0ecebbad1d42777473b2",
      "pdf_url": "https://arxiv.org/pdf/1906.07745",
      "publication_date": "2021-06-17",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e5ee77cb1b43fe42af050d29b5a2b424882c842c",
      "title": "Backdoor Unlearning by Linear Task Decomposition",
      "abstract": "Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Amel Abdelraheem",
        "Alessandro Favero",
        "G\u00e9r\u00f4me Bovet",
        "Pascal Frossard"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e5ee77cb1b43fe42af050d29b5a2b424882c842c",
      "pdf_url": "",
      "publication_date": "2025-10-16",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}