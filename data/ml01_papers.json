{
  "owasp_id": "ML01",
  "owasp_name": "Input Manipulation Attack",
  "total": 117,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "https://openalex.org/W2969333443",
      "title": "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries",
      "abstract": "We study adversarial examples in a black-box setting where the adversary only has API access to the target model and each query is expensive. Prior work on black-box adversarial examples follows one of two main strategies: (1) transfer attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model, and (2) optimization-based attacks use queries to the target model and apply optimization techniques to search for adversarial examples. We propose hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization-based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates. We empirically demonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack strategy reduces cost and improves success rates. We also introduce a seed prioritization strategy which enables attackers to focus their resources on the most promising seeds. Combining hybrid attacks with our seed prioritization strategy enables batch attacks that can reliably find adversarial examples with only a handful of queries.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Fnu Suya",
        "Jianfeng Chi",
        "David Evans",
        "Yuan Tian"
      ],
      "url": "https://openalex.org/W2969333443",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3015625436",
      "title": "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
      "abstract": "The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
      "year": 2020,
      "venue": "IEEE S&P",
      "authors": [
        "Jianbo Chen",
        "Michael I. Jordan",
        "Martin J. Wainwright"
      ],
      "url": "https://arxiv.org/abs/1904.02144",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3093131568",
      "title": "PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking",
      "abstract": "Localized adversarial patches aim to induce misclassification in machine learning models by arbitrarily modifying pixels within a restricted region of an image. Such attacks can be realized in the physical world by attaching the adversarial patch to the object to be misclassified, and defending against such attacks is an unsolved/open problem. In this paper, we propose a general defense framework called PatchGuard that can achieve high provable robustness while maintaining high clean accuracy against localized adversarial patches. The cornerstone of PatchGuard involves the use of CNNs with small receptive fields to impose a bound on the number of features corrupted by an adversarial patch. Given a bounded number of corrupted features, the problem of designing an adversarial patch defense reduces to that of designing a secure feature aggregation mechanism. Towards this end, we present our robust masking defense that robustly detects and masks corrupted features to recover the correct prediction. Notably, we can prove the robustness of our defense against any adversary within our threat model. Our extensive evaluation on ImageNet, ImageNette (a 10-class subset of ImageNet), and CIFAR-10 datasets demonstrates that our defense achieves state-of-the-art performance in terms of both provable robust accuracy and clean accuracy.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Chong Xiang",
        "Arjun Nitin Bhagoji",
        "Vikash Sehwag",
        "Prateek Mittal"
      ],
      "url": "https://openalex.org/W3093131568",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3088733693",
      "title": "Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks",
      "abstract": "Deep neural networks (DNN) are known to be vulnerable to adversarial attacks. Numerous efforts either try to patch weaknesses in trained models, or try to make it difficult or costly to compute adversarial examples that exploit them. In our work, we explore a new \"honeypot\" approach to protect DNN models. We intentionally inject trapdoors, honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. Attackers' optimization algorithms gravitate towards trapdoors, leading them to produce attacks similar to trapdoors in the feature space. Our defense then identifies attacks by comparing neuron activation signatures of inputs to those of trapdoors. In this paper, we introduce trapdoors and describe an implementation of a trapdoor-enabled defense. First, we analytically prove that trapdoors shape the computation of adversarial attacks so that attack inputs will have feature representations very similar to those of trapdoors. Second, we experimentally show that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks (PGD, optimization-based CW, Elastic Net, BPDA), with negligible impact on normal classification. These results generalize across classification domains, including image, facial, and traffic-sign recognition. We also present significant results measuring trapdoors' robustness against customized adaptive attacks (countermeasures).",
      "year": 2020,
      "venue": null,
      "authors": [
        "Shawn Shan",
        "Emily Wenger",
        "Bolun Wang",
        "Bo Li",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W3088733693",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3024103409",
      "title": "A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models",
      "abstract": "Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing \"mutual reinforcement\" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.",
      "year": 2020,
      "venue": "ACM CCS",
      "authors": [
        "Ren Pang",
        "Hua Shen",
        "Xinyang Zhang",
        "Shouling Ji",
        "Yevgeniy Vorobeychik",
        "Xiapu Luo",
        "Alex Liu",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/1911.01559",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3213785680",
      "title": "Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense",
      "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial attacks. A great effort has been directed to developing effective defenses against adversarial attacks and finding vulnerabilities of proposed defenses. A recently proposed defense called Trapdoor-enabled Detection (TeD) deliberately injects trapdoors into DNN models to trap and detect adversarial examples targeting categories protected by TeD. TeD can effectively detect existing state-of-the-art adversarial attacks. In this paper, we propose a novel black-box adversarial attack on TeD, called Feature-Indistinguishable Attack (FIA). It circumvents TeD by crafting adversarial examples indistinguishable in the feature (i.e., neuron-activation) space from benign examples in the target category. To achieve this goal, FIA jointly minimizes the distance to the expectation of feature representations of benign samples in the target category and maximizes the distances to positive adversarial examples generated to query TeD in the preparation phase. A constraint is used to ensure that the feature vector of a generated adversarial example is within the distribution of feature vectors of benign examples in the target category. Our extensive empirical evaluation with different configurations and variants of TeD indicates that our proposed FIA can effectively circumvent TeD. FIA opens a door for developing much more powerful adversarial attacks. The FIA code is available at: https://github.com/CGCL-codes/FeatureIndistinguishableAttack.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Chaoxiang He",
        "Bin Zhu",
        "Xiaojing Ma",
        "Hai Jin",
        "Shengshan Hu"
      ],
      "url": "https://openalex.org/W3213785680",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3212077589",
      "title": "DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks",
      "abstract": "State-of-the-art object detectors are vulnerable to localized patch hiding attacks, where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard as the first general framework for building provably robust object detectors against localized patch hiding attacks. DetectorGuard is inspired by recent advancements in robust image classification research; we ask: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (<1%) of clean performance.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Chong Xiang",
        "Prateek Mittal"
      ],
      "url": "https://arxiv.org/abs/2102.02956",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3211902004",
      "title": "It's Not What It Looks Like: Manipulating Perceptual Hashing based Applications",
      "abstract": "Perceptual hashing is widely used to search or match similar images for digital forensics and cybercrime study. Unfortunately, the robustness of perceptual hashing algorithms is not well understood in these contexts. In this paper, we examine the robustness of perceptual hashing and its dependent security applications both experimentally and empirically. We first develop a series of attack algorithms to subvert perceptual hashing based image search. This is done by generating attack images that effectively enlarge the hash distance to the original image while introducing minimal visual changes. To make the attack practical, we design the attack algorithms under a black-box setting, augmented with novel designs (e.g., grayscale initialization) to improve the attack efficiency and transferability. We then evaluate our attack against the standard pHash as well as its robust variant using three different datasets. After confirming the attack effectiveness experimentally, we then empirically test against real-world reverse image search engines including TinEye, Google, Microsoft Bing, and Yandex. We find that our attack is highly successful on TinEye and Bing, and is moderately successful on Google and Yandex. Based on our findings, we discuss possible countermeasures and recommendations.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Qingying Hao",
        "Licheng Luo",
        "Steve T.K. Jan",
        "Gang Wang"
      ],
      "url": "https://openalex.org/W3211902004",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4225817286",
      "title": "RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit",
      "abstract": "Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low distortion adversarial employing gradient estimation methods. We develop a robust query efficient attack capable of avoiding entrapment in a local minimum and misdirection from noisy gradients seen in gradient estimation methods. The attack method we propose, RamBoAttack, exploits the notion of Randomized Block Coordinate Descent to explore the hidden classifier manifold, targeting perturbations to manipulate only localized input features to address the issues of gradient estimation methods. Importantly, the RamBoAttack is more robust to the different sample inputs available to an adversary and the targeted class. Overall, for a given target class, RamBoAttack is demonstrated to be more robust at achieving a lower distortion within a given query budget. We curate our extensive results using the large-scale high-resolution ImageNet dataset and open-source our attack, test samples and artifacts on GitHub.",
      "year": 2021,
      "venue": "NDSS",
      "authors": [
        "Viet Quoc Vo",
        "Ehsan Abbasnejad",
        "Damith C. Ranasinghe"
      ],
      "url": "https://arxiv.org/abs/2112.05282",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4226086258",
      "title": "What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction",
      "abstract": "Adversarial examples (AEs) pose severe threats to the applications of deep neural networks (DNNs) to safety-critical domains, e.g., autonomous driving. While there has been a vast body of AE defense solutions, to the best of our knowledge, they all suffer from some weaknesses, e.g., defending against only a subset of AEs or causing a relatively high accuracy loss for legitimate inputs. Moreover, most existing solutions cannot defend against adaptive attacks, wherein attackers are knowledgeable about the defense mechanisms and craft AEs accordingly. In this paper, we propose a novel AE detection framework based on the very nature of AEs, i.e., their semantic information is inconsistent with the discriminative features extracted by the target DNN model. To be specific, the proposed solution, namely ContraNet, models such contradiction by first taking both the input and the inference result to a generator to obtain a synthetic output and then comparing it against the original input. For legitimate inputs that are correctly inferred, the synthetic output tries to reconstruct the input. On the contrary, for AEs, instead of reconstructing the input, the synthetic output would be created to conform to the wrong label whenever possible. Consequently, by measuring the distance between the input and the synthetic output with metric learning, we can differentiate AEs from legitimate inputs. We perform comprehensive evaluations under various AE attack scenarios, and experimental results show that ContraNet outperforms existing solutions by a large margin, especially under adaptive attacks. Moreover, our analysis shows that successful AEs that can bypass ContraNet tend to have much-weakened adversarial semantics. We have also shown that ContraNet can be easily combined with adversarial training techniques to achieve further improved AE defense capabilities.",
      "year": 2022,
      "venue": "NDSS",
      "authors": [
        "Yijun Yang",
        "Ruiyuan Gao",
        "Yu Li",
        "Qiuxia Lai",
        "Qiang Xu"
      ],
      "url": "https://arxiv.org/abs/2201.09650",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391418171",
      "title": "AutoDA: Automated Decision-based Iterative Adversarial Attacks",
      "abstract": "In the rapidly evolving field of machine learning, adversarial attacks\\npresent a significant challenge to model robustness and security.\\nDecision-based attacks, which only require feedback on the decision of a model\\nrather than detailed probabilities or scores, are particularly insidious and\\ndifficult to defend against. This work introduces L-AutoDA (Large Language\\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\\nleveraging the generative capabilities of Large Language Models (LLMs) to\\nautomate the design of these attacks. By iteratively interacting with LLMs in\\nan evolutionary framework, L-AutoDA automatically designs competitive attack\\nalgorithms efficiently without much human effort. We demonstrate the efficacy\\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\\nmethods in both success rate and computational efficiency. Our findings\\nunderscore the potential of language models as tools for adversarial attack\\ngeneration and highlight new avenues for the development of robust AI systems.\\n",
      "year": 2024,
      "venue": "Proceedings of the Genetic and Evolutionary Computation Conference Companion",
      "authors": [
        "Ping Guo",
        "Fei Liu",
        "Xi Lin",
        "Qingchuan Zhao",
        "Qingfu Zhang"
      ],
      "url": "https://openalex.org/W4391418171",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4287752693",
      "title": "Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks",
      "abstract": "Deep learning systems are known to be vulnerable to adversarial examples. In particular, query-based black-box attacks do not require knowledge of the deep learning model, but can compute adversarial examples over the network by submitting queries and inspecting returns. Recent work largely improves the efficiency of those attacks, demonstrating their practicality on today's ML-as-a-service platforms. We propose Blacklight, a new defense against query-based black-box adversarial attacks. The fundamental insight driving our design is that, to compute adversarial examples, these attacks perform iterative optimization over the network, producing image queries highly similar in the input space. Blacklight detects query-based black-box attacks by detecting highly similar queries, using an efficient similarity engine operating on probabilistic content fingerprints. We evaluate Blacklight against eight state-of-the-art attacks, across a variety of models and image classification tasks. Blacklight identifies them all, often after only a handful of queries. By rejecting all detected queries, Blacklight prevents any attack to complete, even when attackers persist to submit queries after account ban or query rejection. Blacklight is also robust against several powerful countermeasures, including an optimal black-box attack that approximates white-box attacks in efficiency. Finally, we illustrate how Blacklight generalizes to other domains like text classification.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Huiying Li",
        "Shawn Shan",
        "Emily Wenger",
        "Jiayun Zhang",
        "Hai-Tao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://openalex.org/W4287752693",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4308411153",
      "title": "Physical Hijacking Attacks against Object Trackers",
      "abstract": "Modern autonomous systems rely on both object detection and object tracking in their visual perception pipelines. Although many recent works have attacked the object detection component of autonomous vehicles, these attacks do not work on full pipelines that integrate object tracking to enhance the object detector\u2019s accuracy. Meanwhile, existing attacks against object tracking either lack real-world applicability or do not work against a powerful class of object trackers, Siamese trackers. In this paper, we present AttrackZone, a new physically-realizable tracker hijacking attack against Siamese trackers that systematically determines valid regions in an environment that can be used for physical perturbations. AttrackZone exploits the heatmap generation process of Siamese Region Proposal Networks in order to take control of an object\u2019s bounding box, resulting in physical consequences including vehicle collisions and masked intrusion of pedestrians into unauthorized areas. Evaluations in both the digital and physical domain show that AttrackZone achieves its attack goals 92% of the time, requiring only 0.3-3 seconds on average.",
      "year": 2022,
      "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
      "authors": [
        "Raymond J. Muller",
        "Yanmao Man",
        "Z. Berkay Celik",
        "Ming Li",
        "Ryan Gerdes"
      ],
      "url": "https://openalex.org/W4308411153",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4281487639",
      "title": "Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models",
      "abstract": "Server breaches are an unfortunate reality on today's Internet. In the context of deep neural network (DNN) models, they are particularly harmful, because a leaked model gives an attacker \"white-box\" access to generate adversarial examples, a threat model that has no practical robust defenses. For practitioners who have invested years and millions into proprietary DNNs, e.g. medical imaging, this seems like an inevitable disaster looming on the horizon.   In this paper, we consider the problem of post-breach recovery for DNN models. We propose Neo, a new system that creates new versions of leaked models, alongside an inference time filter that detects and removes adversarial examples generated on previously leaked models. The classification surfaces of different model versions are slightly offset (by introducing hidden distributions), and Neo detects the overfitting of attacks to the leaked model used in its generation. We show that across a variety of tasks and attack methods, Neo is able to filter out attacks from leaked models with very high accuracy, and provides strong protection (7--10 recoveries) against attackers who repeatedly breach the server. Neo performs well against a variety of strong adaptive attacks, dropping slightly in # of breaches recoverable, and demonstrates potential as a complement to DNN defenses in the wild.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Shawn Shan",
        "Wenxin Ding",
        "Emily Wenger",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "url": "https://arxiv.org/abs/2205.10686",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4395083462",
      "title": "Squint Hard Enough: Attacking Perceptual Hashing with Adversarial Machine Learning",
      "abstract": "PhotoDNA is a widely utilized hash designed to counteract Child Sexual Abuse Material (CSAM). However, there has been a scarcity of detailed information regarding its performance. In this paper, we present a comprehensive analysis of its robustness and susceptibility to false positives, along with fundamental insights into its structure. Our findings reveal its resilience to common image processing techniques like lossy compression. Conversely, its robustness is limited when confronted with cropping. Additionally, we propose recommendations for enhancing the algorithm or optimizing its application. This work is an extension on our paper [21].",
      "year": 2024,
      "venue": "Journal of Cyber Security and Mobility",
      "authors": [
        "Martin Steinebach"
      ],
      "url": "https://openalex.org/W4395083462",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4295698813",
      "title": "The Space of Adversarial Strategies",
      "abstract": "Adversarial examples, inputs designed to induce worst-case behavior in machine learning models, have been extensively studied over the past decade. Yet, our understanding of this phenomenon stems from a rather fragmented pool of knowledge; at present, there are a handful of attacks, each with disparate assumptions in threat models and incomparable definitions of optimality. In this paper, we propose a systematic approach to characterize worst-case (i.e., optimal) adversaries. We first introduce an extensible decomposition of attacks in adversarial machine learning by atomizing attack components into surfaces and travelers. With our decomposition, we enumerate over components to create 576 attacks (568 of which were previously unexplored). Next, we propose the Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack performance. With our new attacks, we measure performance relative to the PEA on: both robust and non-robust models, seven datasets, and three extended lp-based threat models incorporating compute costs, formalizing the Space of Adversarial Strategies. From our evaluation we find that attack performance to be highly contextual: the domain, model robustness, and threat model can have a profound influence on attack efficacy. Our investigation suggests that future studies measuring the security of machine learning should: (1) be contextualized to the domain &amp; threat models, and (2) go beyond the handful of known attacks used today.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Patrick McDaniel"
      ],
      "url": "https://openalex.org/W4295698813",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4324302739",
      "title": "Stateful Defenses for Machine Learning Models Are Not Yet Secure Against Black-box Attacks",
      "abstract": "Recent work has proposed stateful defense models (SDMs) as a compelling strategy to defend against a black-box attacker who only has query access to the model, as is common for online machine learning platforms. Such stateful defenses aim to defend against black-box attacks by tracking the query history and detecting and rejecting queries that are \"similar\" and thus preventing black-box attacks from finding useful gradients and making progress towards finding adversarial attacks within a reasonable query budget. Recent SDMs (e.g., Blacklight and PIHA) have shown remarkable success in defending against state-of-the-art black-box attacks. In this paper, we show that SDMs are highly vulnerable to a new class of adaptive black-box attacks. We propose a novel adaptive black-box attack strategy called Oracle-guided Adaptive Rejection Sampling (OARS) that involves two stages: (1) use initial query patterns to infer key properties about an SDM's defense; and, (2) leverage those extracted properties to design subsequent query patterns to evade the SDM's defense while making progress towards finding adversarial inputs. OARS is broadly applicable as an enhancement to existing black-box attacks - we show how to apply the strategy to enhance six common black-box attacks to be more effective against current class of SDMs. For example, OARS-enhanced versions of black-box attacks improved attack success rate against recent stateful defenses from almost 0% to to almost 100% for multiple datasets within reasonable query budgets.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Feng",
        "Ashish Hooda",
        "Neal Mangaokar",
        "Kassem Fawaz",
        "Somesh Jha",
        "Atul Prakash"
      ],
      "url": "https://arxiv.org/abs/2303.06280",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4402264076",
      "title": "Sabre: Cutting through Adversarial Noise with Adaptive Spectral Filtering and Input Reconstruction",
      "abstract": "<p class=\"MsoNormal\">The adoption of neural networks (NNs) across critical sectors including transportation, medicine, communications infrastructure, etc.is inexorable. However, NNs remain highly susceptible to adversarial perturbations, whereby seemingly minimal or imperceptible changes to their inputs cause gross misclassifications, which questions their practical use.Although a growing body of work focuses on defending against such attacks,adversarial robustness remains an open challenge, especially as the effectiveness of existing solutions against increasingly sophisticated input manipulations comes at the cost of degrading ability to recognize benign samples, as we reveal. In this work we introduce SABRE, an adversarial defense framework that closes the gap between benign and robust accuracy in NN classification tasks, without sacrificing benign sample recognition performance. In particular, through spectral decomposition of the input and selective energy-based filtering, SABRE extracts robust features that serve in input reconstruction prior to feeding existing NN architectures. We demonstrate the performance of our approach across multiple domains, by evaluating it on image classification, network intrusion detection, and speech command recognition tasks, showing that SABRE not only outperforms existing defense mechanisms, but also behaves consistently with different neural architectures,data types, (un)known attacks, and adversarial perturbation strengths. Through these extensive experiments, we make the case for SABRE\u2019s adoption in deploying robust and reliable neural classifiers.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Alec F. Diallo",
        "Paul Patras"
      ],
      "url": "https://openalex.org/W4402264076",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4402264342",
      "title": "Why Does Little Robustness Help? A Further Step Towards Understanding Adversarial Transferability",
      "abstract": "Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in adversarial training explains the degradation of gradient similarity. Building on these insights, we explore the impacts of data augmentation and gradient regularization on transferability and identify that the trade-off generally exists in the various training mechanisms, thus building a comprehensive blueprint for the regulation mechanism behind transferability. Finally, we provide a general route for constructing better surrogates to boost transferability which optimizes both model smoothness and gradient similarity simultaneously, e.g., the combination of input gradient regularization and sharpness-aware minimization (SAM), validated by extensive experiments. In summary, we call for attention to the united impacts of these two factors for launching effective transfer attacks, rather than optimizing one while ignoring the other, and emphasize the crucial role of manipulating surrogate models.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yechao Zhang",
        "Shengshan Hu",
        "Leo Yu Zhang",
        "Junyu Shi",
        "Minghui Li",
        "Xiaogeng Liu",
        "Wei Wan",
        "Hai Jin"
      ],
      "url": "https://arxiv.org/abs/2307.07873",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391725277",
      "title": "Group-based Robustness: A General Framework for Customized Robustness in the Real World",
      "abstract": "Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss functions and 2) identify three new attack strategies. We show empirically that with comparable success rates, finding evasive samples using our new loss functions saves computation by a factor as large as the number of targeted classes, and finding evasive samples using our new attack strategies saves time by up to 99\\% compared to brute-force search methods. Finally, we propose a defense method that increases group-based robustness by up to 3.52$\\times$.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Weiran Lin",
        "Keane Lucas",
        "Neo Eyal",
        "Lujo Bauer",
        "Michael K. Reiter",
        "Mahmood Sharif"
      ],
      "url": "https://arxiv.org/abs/2306.16614",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391725251",
      "title": "DorPatch: Distributed and Occlusion-Robust Adversarial Patch to Evade Certifiable Defenses",
      "abstract": "Adversarial patch attacks are among the most practical adversarial attacks.Recent efforts focus on providing a certifiable guarantee on correct predictions in the presence of white-box adversarial patch attacks.In this paper, we propose DorPatch, an effective adversarial patch attack to evade both certifiably robust defenses and empirical defenses.DorPatch employs group lasso on a patch's mask, image dropout, density regularization, and structural loss to generate a fully optimized, distributed, occlusion-robust, and inconspicuous adversarial patch that can be deployed in physical-world adversarial patch attacks.Our extensive experimental evaluation with both digitaldomain and physical-world tests indicates that DorPatch can effectively evade PatchCleanser [64], the state-of-the-art certifiable defense, and empirical defenses against adversarial patch attacks.More critically, mispredicted results of adversarially patched examples generated by DorPatch can receive certification from PatchCleanser, producing a false trust in guaranteed predictions.DorPatch achieves state-of-the-art attacking performance and perceptual quality among all adversarial patch attacks.DorPatch poses a significant threat to real-world applications of DNN models and calls for developing effective defenses to thwart the attack.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Chaoxiang He",
        "Xiaojing Ma",
        "Bin Zhu",
        "Yimiao Zeng",
        "Hanqing Hu",
        "Xiaofan Bai",
        "Hai Jin",
        "Dongmei Zhang"
      ],
      "url": "https://openalex.org/W4391725251",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391724743",
      "title": "UniID: Spoofing Face Authentication System by Universal Identity",
      "abstract": "Face authentication systems are widely employed in access control systems to ensure the security of confidential facilities.Recent works have demonstrated their vulnerabilities to adversarial attacks.However, such attacks typically require adversaries to wear disguises such as glasses or hats during every authentication, which may raise suspicion and reduce their attack impacts.In this paper, we propose the UniID attack, which allows multiple adversaries to perform face spoofing attacks without any additional disguise by enabling an insider to register a universal identity into the face authentication database by wearing an adversarial patch.To achieve it, we first select appropriate adversaries through feature engineering, then generate the desired adversarial patch with a multi-target joint-optimization approach, and finally overcome practical challenges such as improving the transferability of the adversarial patch towards black-box systems and enhancing its robustness in the physical world.We implement UniID in laboratory setups and evaluate its effectiveness with six face recognition models (FaceNet, Mobile-FaceNet, ArcFace-18/50, and MagFace-18/50) and two commercial face authentication systems (ArcSoft and Face++).Simulation and real-world experimental results demonstrate that UniID can achieve a max attack success rate of 100% and 79% in 3-user scenarios under the white-box setting and black-box setting respectively, and it can be extended to more than 8 users.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Zhihao Wu",
        "Yushi Cheng",
        "Shibo Zhang",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4391724743",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391725334",
      "title": "Enhance Stealthiness and Transferability of Adversarial Attacks with Class Activation Mapping Ensemble Attack",
      "abstract": "Although there has been extensive research on the transferability of adversarial attacks, existing methods for generating adversarial examples suffer from two significant drawbacks: poor stealthiness and low attack efficacy under low-round attacks.To address the above issues, we creatively propose an adversarial example generation method that ensembles the class activation maps of multiple models, called class activation mapping ensemble attack.We first use the class activation mapping method to discover the relationship between the decision of the Deep Neural Network and the image region.Then we calculate the class activation score for each pixel and use it as the weight for perturbation to enhance the stealthiness of adversarial examples and improve attack performance under low attack rounds.In the optimization process, we also ensemble class activation maps of multiple models to ensure the transferability of the adversarial attack algorithm.Experimental results show that our method generates adversarial examples with high perceptibility, transferability, attack performance under low-round attacks, and evasiveness.Specifically, when our attack capability is comparable to the most potent attack (VMIFGSM), our perceptibility is close to the best-performing attack (TPGD).For non-targeted attacks, our method outperforms the VMIFGSM by an average of 11.69% in attack capability against 13 target models and outperforms the TPGD by an average of 37.15%.For targeted attacks, our method achieves the fastest convergence, the most potent attack efficacy, and significantly outperforms the eight baseline methods in lowround attacks.Furthermore, our method can evade defenses and be used to assess the robustness of models 1 .",
      "year": 2024,
      "venue": null,
      "authors": [
        "Hui Xia",
        "Rui Zhang",
        "Zi Kang",
        "Shuliang Jiang",
        "Shuo Xu"
      ],
      "url": "https://openalex.org/W4391725334",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4400667173",
      "title": "Self-interpreting Adversarial Images",
      "abstract": "We introduce a new type of indirect, cross-modal injection attacks against visual language models that enable creation of self-interpreting images. These images contain hidden \"meta-instructions\" that control how models answer users' questions about the image and steer models' outputs to express an adversary-chosen style, sentiment, or point of view. Self-interpreting images act as soft prompts, conditioning the model to satisfy the adversary's (meta-)objective while still producing answers based on the image's visual content. Meta-instructions are thus a stronger form of prompt injection. Adversarial images look natural and the model's answers are coherent and plausible, yet they also follow the adversary-chosen interpretation, e.g., political spin, or even objectives that are not achievable with explicit text instructions. We evaluate the efficacy of self-interpreting images for a variety of models, interpretations, and user prompts. We describe how these attacks could cause harm by enabling creation of self-interpreting content that carries spam, misinformation, or spin. Finally, we discuss defenses.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Tingwei Zhang",
        "Collin Zhang",
        "John X. Morris",
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W4400667173",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3176393001",
      "title": "Bad Characters: Imperceptible NLP Attacks",
      "abstract": "Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Nicholas Boucher",
        "Ilia Shumailov",
        "Ross Anderson",
        "Nicolas Papernot"
      ],
      "url": "https://arxiv.org/abs/2106.09898",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4295989754",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "url": "https://arxiv.org/abs/2209.06506",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4380866510",
      "title": "No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning",
      "abstract": "The number of papers submitted to academic conferences is steadily rising in many scientific disciplines. To handle this growth, systems for automatic paper-reviewer assignments are increasingly used during the reviewing process. These systems use statistical topic models to characterize the content of submissions and automate the assignment to reviewers. In this paper, we show that this automation can be manipulated using adversarial learning. We propose an attack that adapts a given paper so that it misleads the assignment and selects its own reviewers. Our attack is based on a novel optimization strategy that alternates between the feature space and problem space to realize unobtrusive changes to the paper. To evaluate the feasibility of our attack, we simulate the paper-reviewer assignment of an actual security conference (IEEE S&P) with 165 reviewers on the program committee. Our results show that we can successfully select and remove reviewers without access to the assignment system. Moreover, we demonstrate that the manipulated papers remain plausible and are often indistinguishable from benign submissions.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Thorsten Eisenhofer",
        "Erwin Quiring",
        "Jonas M\u00f6ller",
        "Doreen Riepel",
        "Thorsten Holz",
        "Konrad Rieck"
      ],
      "url": "https://arxiv.org/abs/2303.14443",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3135197931",
      "title": "WaveGuard: Understanding and Mitigating Audio Adversarial Examples",
      "abstract": "There has been a recent surge in adversarial attacks on deep learning based automatic speech recognition (ASR) systems. These attacks pose new challenges to deep learning security and have raised significant concerns in deploying ASR systems in safety-critical applications. In this work, we introduce WaveGuard: a framework for detecting adversarial inputs that are crafted to attack ASR systems. Our framework incorporates audio transformation functions and analyses the ASR transcriptions of the original and transformed audio to detect adversarial inputs. We demonstrate that our defense framework is able to reliably detect adversarial examples constructed by four recent audio adversarial attacks, with a variety of audio transformation functions. With careful regard for best practices in defense evaluations, we analyze our proposed defense and its strength to withstand adaptive and robust attacks in the audio domain. We empirically demonstrate that audio transformations that recover audio from perceptually informed representations can lead to a strong defense that is robust against an adaptive adversary even in a complete white-box setting. Furthermore, WaveGuard can be used out-of-the box and integrated directly with any ASR model to efficiently detect audio adversarial examples, without the need for model retraining.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Shlomo Dubnov",
        "Julian McAuley",
        "Farinaz Koushanfar"
      ],
      "url": "https://openalex.org/W3135197931",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3127994681",
      "title": "Dompteur: Taming Audio Adversarial Examples",
      "abstract": "Adversarial examples seem to be inevitable. These specifically crafted inputs allow attackers to arbitrarily manipulate machine learning systems. Even worse, they often seem harmless to human observers. In our digital society, this poses a significant threat. For example, Automatic Speech Recognition (ASR) systems, which serve as hands-free interfaces to many kinds of systems, can be attacked with inputs incomprehensible for human listeners. The research community has unsuccessfully tried several approaches to tackle this problem. In this paper we propose a different perspective: We accept the presence of adversarial examples against ASR systems, but we require them to be perceivable by human listeners. By applying the principles of psychoacoustics, we can remove semantically irrelevant information from the ASR input and train a model that resembles human perception more closely. We implement our idea in a tool named DOMPTEUR and demonstrate that our augmented system, in contrast to an unmodified baseline, successfully focuses on perceptible ranges of the input signal. This change forces adversarial examples into the audible range, while using minimal computational overhead and preserving benign performance. To evaluate our approach, we construct an adaptive attacker that actively tries to avoid our augmentations and demonstrate that adversarial examples from this attacker remain clearly perceivable. Finally, we substantiate our claims by performing a hearing test with crowd-sourced human listeners.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Thorsten Eisenhofer",
        "Lea Sch\u00f6nherr",
        "J. Howard Frank",
        "Lars Speckemeier",
        "Dorothea Kolossa",
        "Thorsten Holz"
      ],
      "url": "https://openalex.org/W3127994681",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3138076532",
      "title": "EarArray: Defending against DolphinAttack via Acoustic Attenuation",
      "abstract": "DolphinAttacks (i.e., inaudible voice commands) modulate audible voices over ultrasounds to inject malicious commands silently into voice assistants and manipulate controlled systems (e.g., doors or smart speakers).Eliminating DolphinAttacks is challenging if ever possible since it requires to modify the microphone hardware.In this paper, we design EarArray, a lightweight method that can not only detect such attacks but also identify the direction of attackers without requiring any extra hardware or hardware modification.Essentially, inaudible voice commands are modulated on ultrasounds that inherently attenuate faster than the one of audible sounds.By inspecting the command sound signals via the built-in multiple microphones on smart devices, EarArray is able to estimate the attenuation rate and thus detect the attacks.We propose a model of the propagation of audible sounds and ultrasounds from the sound source to a voice assistant, e.g., a smart speaker, and illustrate the underlying principle and its feasibility.We implemented EarArray using two specially-designed microphone arrays and our experiments show that EarArray can detect inaudible voice commands with an accuracy of 99% and recognize the direction of the attackers with an accuracy of 97.89%.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Guoming Zhang",
        "Xiaoyu Ji",
        "Xinfeng Li",
        "Gang Qu",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W3138076532",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2985489290",
      "title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems",
      "abstract": "Speaker recognition (SR) is widely used in our daily life as a biometric authentication or identification mechanism. The popularity of SR brings in serious security concerns, as demonstrated by recent adversarial attacks. However, the impacts of such threats in the practical black-box setting are still open, since current attacks consider the white-box setting only. In this paper, we conduct the first comprehensive and systematic study of the adversarial attacks on SR systems (SRSs) to understand their security weakness in the practical blackbox setting. For this purpose, we propose an adversarial attack, named FAKEBOB, to craft adversarial samples. Specifically, we formulate the adversarial sample generation as an optimization problem, incorporated with the confidence of adversarial samples and maximal distortion to balance between the strength and imperceptibility of adversarial voices. One key contribution is to propose a novel algorithm to estimate the score threshold, a feature in SRSs, and use it in the optimization problem to solve the optimization problem. We demonstrate that FAKEBOB achieves 99% targeted attack success rate on both open-source and commercial systems. We further demonstrate that FAKEBOB is also effective on both open-source and commercial systems when playing over the air in the physical world. Moreover, we have conducted a human study which reveals that it is hard for human to differentiate the speakers of the original and adversarial voices. Last but not least, we show that four promising defense methods for adversarial attack from the speech recognition domain become ineffective on SRSs against FAKEBOB, which calls for more effective defense methods. We highlight that our study peeks into the security implications of adversarial attacks on SRSs, and realistically fosters to improve the security robustness of SRSs.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Guangke Chen",
        "Sen Chen",
        "Lingling Fan",
        "Xiaoning Du",
        "Zhe Zhao",
        "Fu Song",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/1911.01840",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2979711137",
      "title": "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems",
      "abstract": "Automatic speech recognition and voice identification systems are being deployed in a wide array of applications, from providing control mechanisms to devices lacking traditional interfaces, to the automatic transcription of conversations and authentication of users. Many of these applications have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state of the art systems, with minimal impact on human comprehension. Processing pipelines for modern systems are comprised of signal preprocessing and feature extraction steps, whose output is fed to a machine-learned model. Prior work has focused on the models, using white-box knowledge to tailor model-specific attacks. We focus on the pipeline stages before the models, which (unlike the models) are quite similar across systems. As such, our attacks are black-box and transferable, and demonstrably achieve mistranscription and misidentification rates as high as 100% by modifying only a few frames of audio. We perform a study via Amazon Mechanical Turk demonstrating that there is no statistically significant difference between human perception of regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are generally not perceived by human subjects, but that are crucial for model accuracy. We also find that certain English language phonemes (in particular, vowels) are significantly more susceptible to our attack. We show that the attacks are effective when mounted over cellular networks, where signals are subject to degradation due to transcoding, jitter, and packet loss.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Muhammad Sajidur Rahman",
        "Washington Garcia",
        "Logan Blue",
        "Kevin Warren",
        "Anurag Swarnim Yadav",
        "Tom Shrimpton",
        "Patrick Traynor"
      ],
      "url": "https://arxiv.org/abs/1910.05262",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3042776162",
      "title": "SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems",
      "abstract": "Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible by the improved accuracy in neural networks. Like other systems based on neural networks, recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs. However, as we demonstrate in this paper, the end-to-end architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space. We demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work. We then demonstrate experimentally that attacks against these models almost universally fail to transfer. In so doing, we argue that substantial additional work is required to provide adequate mitigations in this space.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Hadi Abdullah",
        "Kevin Warren",
        "Vincent Bindschaedler",
        "Nicolas Papernot",
        "Patrick Traynor"
      ],
      "url": "https://arxiv.org/abs/2007.06622",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3109668151",
      "title": "AdvPulse: Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via Subsecond Perturbations",
      "abstract": "Existing efforts in audio adversarial attacks only focus on the scenarios where an adversary has prior knowledge of the entire speech input so as to generate an adversarial example by aligning and mixing the audio input with corresponding adversarial perturbation. In this work we consider a more practical and challenging attack scenario where the intelligent audio system takes streaming audio inputs (e.g., live human speech) and the adversary can deceive the system by playing adversarial perturbations simultaneously. This change in attack behavior brings great challenges, preventing existing adversarial perturbation generation methods from being applied directly. In practice, (1) the adversary cannot anticipate what the victim will say: the adversary cannot rely on their prior knowledge of the speech signal to guide how to generate adversarial perturbations; and (2) the adversary cannot control when the victim will speak: the synchronization between the adversarial perturbation and the speech cannot be guaranteed. To address these challenges, in this paper we propose AdvPulse, a systematic approach to generate subsecond audio adversarial perturbations, that achieves the capability to alter the recognition results of streaming audio inputs in a targeted and synchronization-free manner. To circumvent the constraints on speech content and time, we exploit penalty-based universal adversarial perturbation generation algorithm and incorporate the varying time delay into the optimization process. We further tailor the adversarial perturbation according to environmental sounds to make it inconspicuous to humans. Additionally, by considering the sources of distortions occurred during the physical playback, we are able to generate more robust audio adversarial perturbations that can remain effective even under over-the-air propagation. Extensive experiments on two representative types of intelligent audio systems (i.e., speaker recognition and speech command recognition) are conducted in various realistic environments. The results show that our attack can achieve an average attack success rate of over 89.6% in indoor environments and 76.0% in inside-vehicle scenarios even with loud engine and road noises.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Zhuohang Li",
        "Yi Wu",
        "Jian Liu",
        "Yingying Chen",
        "Bo Yuan"
      ],
      "url": "https://openalex.org/W3109668151",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3207651366",
      "title": "Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information",
      "abstract": "Adversarial attacks against commercial black-box speech platforms, including cloud speech APIs and voice control devices, have received little attention until recent years. The current \"black-box\" attacks all heavily rely on the knowledge of prediction/confidence scores to craft effective adversarial examples, which can be intuitively defended by service providers without returning these messages. In this paper, we propose two novel adversarial attacks in more practical and rigorous scenarios. For commercial cloud speech APIs, we propose Occam, a decision-only black-box adversarial attack, where only final decisions are available to the adversary. In Occam, we formulate the decision-only AE generation as a discontinuous large-scale global optimization problem, and solve it by adaptively decomposing this complicated problem into a set of sub-problems and cooperatively optimizing each one. Our Occam is a one-size-fits-all approach, which achieves 100% success rates of attacks with an average SNR of 14.23dB, on a wide range of popular speech and speaker recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and Jingdong, outperforming the state-of-the-art black-box attacks. For commercial voice control devices, we propose NI-Occam, the first non-interactive physical adversarial attack, where the adversary does not need to query the oracle and has no access to its internal information and training data. We combine adversarial attacks with model inversion attacks, and thus generate the physically-effective audio AEs with high transferability without any interaction with target devices. Our experimental results show that NI-Occam can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light on non-interactive physical attacks against voice control devices.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Baolin Zheng",
        "Peipei Jiang",
        "Qian Wang",
        "Qi Li",
        "Chao Shen",
        "Cong Wang",
        "Yunjie Ge",
        "Qingyang Teng",
        "Shenyi Zhang"
      ],
      "url": "https://arxiv.org/abs/2110.09714",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4288727615",
      "title": "Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception",
      "abstract": "Recently, adversarial machine learning attacks have posed serious security threats against practical audio signal classification systems, including speech recognition, speaker recognition, and music copyright detection. Previous studies have mainly focused on ensuring the effectiveness of attacking an audio signal classifier via creating a small noise-like perturbation on the original signal. It is still unclear if an attacker is able to create audio signal perturbations that can be well perceived by human beings in addition to its attack effectiveness. This is particularly important for music signals as they are carefully crafted with human-enjoyable audio characteristics.   In this work, we formulate the adversarial attack against music signals as a new perception-aware attack framework, which integrates human study into adversarial attack design. Specifically, we conduct a human study to quantify the human perception with respect to a change of a music signal. We invite human participants to rate their perceived deviation based on pairs of original and perturbed music signals, and reverse-engineer the human perception process by regression analysis to predict the human-perceived deviation given a perturbed signal. The perception-aware attack is then formulated as an optimization problem that finds an optimal perturbation signal to minimize the prediction of perceived deviation from the regressed human perception model. We use the perception-aware framework to design a realistic adversarial music attack against YouTube's copyright detector. Experiments show that the perception-aware attack produces adversarial music with significantly better perceptual quality than prior work.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Shangqing Zhao",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "url": "https://arxiv.org/abs/2207.13192",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4388856757",
      "title": "SpecPatch: Human-in-the-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition",
      "abstract": "The rapid development of deep neural networks and generative AI has catalyzed growth in realistic speech synthesis. While this technology has great potential to improve lives, it also leads to the emergence of ''DeepFake'' where synthesized speech can be misused to deceive humans and machines for nefarious purposes. In response to this evolving threat, there has been a significant amount of interest in mitigating this threat by DeepFake detection.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Zhiyuan Yu",
        "Shixuan Zhai",
        "Ning Zhang"
      ],
      "url": "https://openalex.org/W4388856757",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391725296",
      "title": "Learning Normality is Enough: A Software-based Mitigation against Inaudible Voice Attacks",
      "abstract": "Automatic speech recognition (ASR) systems have been shown to be vulnerable to adversarial examples (AEs).Recent success all assumes that users will not notice or disrupt the attack process despite the existence of music/noise-like sounds and spontaneous responses from voice assistants.Nonetheless, in practical user-present scenarios, user awareness may nullify existing attack attempts that launch unexpected sounds or ASR usage.In this paper, we seek to bridge the gap in existing research and extend the attack to user-present scenarios.We propose VRIFLE, an inaudible adversarial perturbation (IAP) attack via ultrasound delivery that can manipulate ASRs as a user speaks.The inherent differences between audible sounds and ultrasounds make IAP delivery face unprecedented challenges such as distortion, noise, and instability.In this regard, we design a novel ultrasonic transformation model to enhance the crafted perturbation to be physically effective and even survive long-distance delivery.We further enable VRIFLE's robustness by adopting a series of augmentation on user and real-world variations during the generation process.In this way, VRIFLE features an effective real-time manipulation of the ASR output from different distances and under any speech of users, with an alter-and-mute strategy that suppresses the impact of user disruption.Our extensive experiments in both digital and physical worlds verify VRIFLE's effectiveness under various configurations, robustness against six kinds of defenses, and universality in a targeted manner.We also show that VRIFLE can be delivered with a portable attack device and even everyday-life loudspeakers.Receiver UTM \u2460",
      "year": 2024,
      "venue": null,
      "authors": [
        "Xinfeng Li",
        "Yan Chen",
        "Xuancun Lu",
        "Zihan Zeng",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4391725296",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4388717917",
      "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models",
      "abstract": "Audio adversarial examples (AEs) have posed significant security challenges to real-world speaker recognition systems. Most black-box attacks still require certain information from the speaker recognition model to be effective (e.g., keeping probing and requiring the knowledge of similarity scores). This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model. Although it is not feasible for an attacker to succeed with completely zero knowledge, we assume that the attacker only knows a short (or a few seconds) speech sample of a target speaker. Without any probing to gain further knowledge about the target model, we propose a new mechanism, called parrot training, to generate AEs against the target model. Motivated by recent advancements in voice conversion (VC), we propose to use the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech. Then, we use these parrot speech samples to train a parrot-trained(PT) surrogate model for the attacker. Under a joint transferability and perception framework, we investigate different ways to generate AEs on the PT model (called PT-AEs) to ensure the PT-AEs can be generated with high transferability to a black-box target model with good human perceptual quality. Real-world experiments show that the resultant PT-AEs achieve the attack success rates of 45.8% - 80.8% against the open-source models in the digital-line scenario and 47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon Echo, and Google Home, in the over-the-air scenario.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Rui Duan",
        "Zhe Qu",
        "Leah Ding",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "url": "https://arxiv.org/abs/2311.07780",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3179761913",
      "title": "Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems",
      "abstract": "Widely deployed deep neural network (DNN) models have been proven to be vulnerable to adversarial perturbations in many applications (e.g., image, audio and text classifications). To date, there are only a few adversarial perturbations proposed to deviate the DNN models in video recognition systems by simply injecting 2D perturbations into video frames. However, such attacks may overly perturb the videos without learning the spatio-temporal features (across temporal frames), which are commonly extracted by DNN models for video recognition. To our best knowledge, we propose the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems. U3D has many advantages, such as (1) as the transfer-based attack, U3D can universally attack multiple DNN models for video recognition without accessing to the target DNN model; (2) the high transferability of U3D makes such universal black-box attack easy-to-launch, which can be further enhanced by integrating queries over the target model when necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the existing state-of-the-art defense schemes; (5) U3D can be efficiently generated with a few pre-learned parameters, and then immediately injected to attack real-time DNN-based video recognition systems. We have conducted extensive experiments to evaluate U3D on multiple DNN models and three large-scale video datasets. The experimental results demonstrate its superiority and practicality.",
      "year": 2022,
      "venue": "IEEE S&P",
      "authors": [
        "Shangyu Xie",
        "Han Wang",
        "Yu Kong",
        "Yuan Hong"
      ],
      "url": "https://arxiv.org/abs/2107.04284",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4385080387",
      "title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
      "abstract": "Video classification systems are vulnerable to adversarial attacks, which can create severe security problems in video verification. Current black-box attacks need a large number of queries to succeed, resulting in high computational overhead in the process of attack. On the other hand, attacks with restricted perturbations are ineffective against defenses such as denoising or adversarial training. In this paper, we focus on unrestricted perturbations and propose StyleFool, a black-box video adversarial attack via style transfer to fool the video classification system. StyleFool first utilizes color theme proximity to select the best style image, which helps avoid unnatural details in the stylized videos. Meanwhile, the target class confidence is additionally considered in targeted attacks to influence the output distribution of the classifier by moving the stylized video closer to or even across the decision boundary. A gradient-free method is then employed to further optimize the adversarial perturbations. We carry out extensive experiments to evaluate StyleFool on two standard datasets, UCF-101 and HMDB-51. The experimental results demonstrate that StyleFool outperforms the state-of-the-art adversarial attacks in terms of both the number of queries and the robustness against existing defenses. Moreover, 50% of the stylized videos in untargeted attacks do not need any query since they can already fool the video classification model. Furthermore, we evaluate the indistinguishability through a user study to show that the adversarial samples of StyleFool look imperceptible to human eyes, despite unrestricted perturbations.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Yuxin Cao",
        "Xi Xiao",
        "Ruoxi Sun",
        "Derui Wang",
        "Minhui Xue",
        "Sheng Wen"
      ],
      "url": "https://arxiv.org/abs/2203.16000",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3213927281",
      "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
      "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.   In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Jiaming Mu",
        "Binghui Wang",
        "Qi Li",
        "Kun Sun",
        "Mingwei Xu",
        "Zhuotao Liu"
      ],
      "url": "https://arxiv.org/abs/2108.09513",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2749572357",
      "title": "Evading Classifiers by Morphing in the Dark",
      "abstract": "Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
      "year": 2017,
      "venue": "ACM CCS",
      "authors": [
        "Hung Dang",
        "Yue Huang",
        "Ee-Chien Chang"
      ],
      "url": "https://arxiv.org/abs/1705.07535",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2947227164",
      "title": "Misleading Authorship Attribution of Source Code using Adversarial Learning",
      "abstract": "In this paper, we present a novel attack against authorship attribution of source code. We exploit that recent attribution methods rest on machine learning and thus can be deceived by adversarial examples of source code. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer. The attack is guided by Monte-Carlo tree search that enables us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack has a substantial effect on two recent attribution methods, whose accuracy drops from over 88% to 1% under attack. Furthermore, we show that our attack can imitate the coding style of developers with high accuracy and thereby induce false attributions. We conclude that current approaches for authorship attribution are inappropriate for practical application and there is a need for resilient analysis techniques.",
      "year": 2019,
      "venue": "USENIX Security",
      "authors": [
        "Erwin Quiring",
        "Alwin Maier",
        "Konrad Rieck"
      ],
      "url": "https://arxiv.org/abs/1905.12386",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2989093880",
      "title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
      "abstract": "Recent research efforts on adversarial machine learning (ML) have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This article makes three major contributions. Firstly, we propose a general formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, absent artifacts, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the by-product of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. Secondly, building on our general formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations in terms of semantics and artifacts. We have tested our approach on a dataset with 150K Android apps from 2016 and 2018 which show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Thirdly, we explore the effectiveness of adversarial training as a possible approach to enforce robustness against adversarial samples, evaluating its effectiveness on the considered machine learning models under different scenarios. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial instance.",
      "year": 2019,
      "venue": "IEEE S&P",
      "authors": [
        "Jacopo Cortellazzi",
        "Feargus Pendlebury",
        "Daniel Arp",
        "Erwin Quiring",
        "Fabio Pierazzi",
        "Lorenzo Cavallaro"
      ],
      "url": "https://arxiv.org/abs/1911.02142",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3212677680",
      "title": "Structural Attack against Graph Based Android Malware Detection",
      "abstract": "Malware detection techniques achieve great success with deeper insight into the semantics of malware. Among existing detection techniques, function call graph (FCG) based methods achieve promising performance due to their prominent representations of malware's functionalities. Meanwhile, recent adversarial attacks not only perturb feature vectors to deceive classifiers (i.e., feature-space attacks) but also investigate how to generate real evasive malware (i.e., problem-space attacks). However, existing problem-space attacks are limited due to their inconsistent transformations between feature space and problem space.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Kaifa Zhao",
        "Hao Zhou",
        "Yulin Zhu",
        "Xian Zhan",
        "Kai Zhou",
        "Jianfeng Li",
        "Le Yu",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "url": "https://openalex.org/W3212677680",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4385964791",
      "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
      "abstract": "Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Kevin Eykholt",
        "Taesung Lee",
        "Douglas Lee Schales",
        "Jiyong Jang",
        "Ian Molloy",
        "Masha Zorin"
      ],
      "url": "https://openalex.org/W4385964791",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4327671210",
      "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
      "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting \"never-executed\" function calls into malware code. The main challenges are two-fold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent.   To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges.   Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Heng Li",
        "Zhang Cheng",
        "Bang Wu",
        "Liheng Yuan",
        "Cuiying Gao",
        "Wei Yuan",
        "Xiapu Luo"
      ],
      "url": "https://arxiv.org/abs/2303.08509",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4388857307",
      "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting",
      "abstract": "The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Ping He",
        "Yifan Xia",
        "Xuhong Zhang",
        "Shouling Ji"
      ],
      "url": "https://arxiv.org/abs/2309.01866",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W2903544706",
      "title": "Interpretable Deep Learning under Fire",
      "abstract": "Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far. Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing \\imlses are highly vulnerable to adversarial manipulations. Specifically, we present ADV^2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV^2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability -- a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV^2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions.",
      "year": 2018,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinyang Zhang",
        "Ningfei Wang",
        "Hua Shen",
        "Shouling Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W2903544706",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_25e677aa",
      "title": "\u201cIs your explanation stable?\u201d: A Robustness Evaluation Framework for Feature Attribution",
      "abstract": "Understanding the decision process of neural networks is hard. One vital method for explanation is to attribute its decision to pivotal features. Although many algorithms are proposed, most of them solely improve the faithfulness to the model. However, the real environment contains many random noises, which may leads to great fluctuations in the explanations. More seriously, recent works show that explanation algorithms are vulnerable to adversarial attacks. All of these make the explanation hard to trust in real scenarios.   To bridge this gap, we propose a model-agnostic method \\emph{Median Test for Feature Attribution} (MeTFA) to quantify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees. MeTFA has the following two functions: (1) examine whether one feature is significantly important or unimportant and generate a MeTFA-significant map to visualize the results; (2) compute the confidence interval of a feature attribution score and generate a MeTFA-smoothed map to increase the stability of the explanation. Experiments show that MeTFA improves the visual quality of explanations and significantly reduces the instability while maintaining the faithfulness. To quantitatively evaluate the faithfulness of an explanation under different noise settings, we further propose several robust faithfulness metrics. Experiment results show that the MeTFA-smoothed explanation can significantly increase the robust faithfulness. In addition, we use two scenarios to show MeTFA's potential in the applications. First, when applied to the SOTA explanation method to locate context bias for semantic segmentation models, MeTFA-significant explanations use far smaller regions to maintain 99\\%+ faithfulness. Second, when tested with different explanation-oriented attacks, MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against explanations.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Yuyou Gan",
        "Yuhao Mao",
        "Xuhong Zhang",
        "Shouling Ji",
        "Yuwen Pu",
        "Meng Han",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2209.01782",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3042075786",
      "title": "SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations",
      "abstract": "Research into adversarial examples (AE) has developed rapidly, yet static adversarial patches are still the main technique for conducting attacks in the real world, despite being obvious, semi-permanent and unmodifiable once deployed. In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a novel technique that allows adversaries to realize physically robust real-world AE by using a light projector. Attackers can project a specifically crafted adversarial perturbation onto a real-world object, transforming it into an AE. This allows the adversary greater control over the attack compared to adversarial patches: (i) projections can be dynamically turned on and off or modified at will, (ii) projections do not suffer from the locality constraint imposed by patches, making them harder to detect. We study the feasibility of SLAP in the self-driving scenario, targeting both object detector and traffic sign recognition tasks, focusing on the detection of stop signs. We conduct experiments in a variety of ambient light conditions, including outdoors, showing how in non-bright settings the proposed method generates AE that are extremely robust, causing misclassifications on state-of-the-art networks with up to 99% success rate for a variety of angles and distances. We also demostrate that SLAP-generated AE do not present detectable behaviours seen in adversarial patches and therefore bypass SentiNet, a physical AE detection method. We evaluate other defences including an adaptive defender using adversarial learning which is able to thwart the attack effectiveness up to 80% even in favourable attacker conditions.",
      "year": 2020,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Giulio Lovisotto",
        "Henry Turner",
        "Ivo Sluganovic",
        "Martin Strohmeier",
        "Ivan Martinovi\u0107"
      ],
      "url": "https://openalex.org/W3042075786",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2209.09577",
      "title": "Understanding Real-world Threats to Deep Learning Models in Android Apps",
      "abstract": "Famous for its superior performance, deep learning (DL) has been popularly used within many applications, which also at the same time attracts various threats to the models. One primary threat is from adversarial attacks. Researchers have intensively studied this threat for several years and proposed dozens of approaches to create adversarial examples (AEs). But most of the approaches are only evaluated on limited models and datasets (e.g., MNIST, CIFAR-10). Thus, the effectiveness of attacking real-world DL models is not quite clear. In this paper, we perform the first systematic study of adversarial attacks on real-world DNN models and provide a real-world model dataset named RWM. Particularly, we design a suite of approaches to adapt current AE generation algorithms to the diverse real-world DL models, including automatically extracting DL models from Android apps, capturing the inputs and outputs of the DL models in apps, generating AEs and validating them by observing the apps' execution. For black-box DL models, we design a semantic-based approach to build suitable datasets and use them for training substitute models when performing transfer-based attacks. After analyzing 245 DL models collected from 62,583 real-world apps, we have a unique opportunity to understand the gap between real-world DL models and contemporary AE generation algorithms. To our surprise, the current AE generation algorithms can only directly attack 6.53% of the models. Benefiting from our approach, the success rate upgrades to 47.35%.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Zizhuang Deng",
        "Kai Chen",
        "Guozhu Meng",
        "Xiaodong Zhang",
        "Ke Xu",
        "Yao Cheng"
      ],
      "url": "https://arxiv.org/abs/2209.09577",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2302.09491",
      "title": "X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection",
      "abstract": "Adversarial attacks are valuable for evaluating the robustness of deep learning models. Existing attacks are primarily conducted on the visible light spectrum (e.g., pixel-wise texture perturbation). However, attacks targeting texture-free X-ray images remain underexplored, despite the widespread application of X-ray imaging in safety-critical scenarios such as the X-ray detection of prohibited items. In this paper, we take the first step toward the study of adversarial attacks targeted at X-ray prohibited item detection, and reveal the serious threats posed by such attacks in this safety-critical scenario. Specifically, we posit that successful physical adversarial attacks in this scenario should be specially designed to circumvent the challenges posed by color/texture fading and complex overlapping. To this end, we propose X-adv to generate physically printable metals that act as an adversarial agent capable of deceiving X-ray detectors when placed in luggage. To resolve the issues associated with color/texture fading, we develop a differentiable converter that facilitates the generation of 3D-printable objects with adversarial shapes, using the gradients of a surrogate model rather than directly generating adversarial textures. To place the printed 3D adversarial objects in luggage with complex overlapped instances, we design a policy-based reinforcement learning strategy to find locations eliciting strong attack performance in worst-case scenarios whereby the prohibited items are heavily occluded by other items. To verify the effectiveness of the proposed X-Adv, we conduct extensive experiments in both the digital and the physical world (employing a commercial X-ray security inspection system for the latter case). Furthermore, we present the physical-world X-ray adversarial attack dataset XAD.",
      "year": 2023,
      "venue": "USENIX Security",
      "authors": [
        "Aishan Liu",
        "Jun Guo",
        "Jiakai Wang",
        "Siyuan Liang",
        "Renshuai Tao",
        "Wenbo Zhou",
        "Cong Liu",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "url": "https://arxiv.org/abs/2302.09491",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4306887028",
      "title": "You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks",
      "abstract": "Autonomous Vehicles (AVs) increasingly use LiDAR-based object detection systems to perceive other vehicles and pedestrians on the road. While existing attacks on LiDAR-based autonomous driving architectures focus on lowering the confidence score of AV object detection models to induce obstacle misdetection, our research discovers how to leverage laser-based spoofing techniques to selectively remove the LiDAR point cloud data of genuine obstacles at the sensor level before being used as input to the AV perception. The ablation of this critical LiDAR information causes autonomous driving obstacle detectors to fail to identify and locate obstacles and, consequently, induces AVs to make dangerous automatic driving decisions. In this paper, we present a method invisible to the human eye that hides objects and deceives autonomous vehicles' obstacle detectors by exploiting inherent automatic transformation and filtering processes of LiDAR sensor data integrated with autonomous driving frameworks. We call such attacks Physical Removal Attacks (PRA), and we demonstrate their effectiveness against three popular AV obstacle detectors (Apollo, Autoware, PointPillars), and we achieve 45\u00b0 attack capability. We evaluate the attack impact on three fusion models (Frustum-ConvNet, AVOD, and Integrated-Semantic Level Fusion) and the consequences on the driving decision using LGSVL, an industry-grade simulator. In our moving vehicle scenarios, we achieve a 92.7% success rate removing 90\\% of a target obstacle's cloud points. Finally, we demonstrate the attack's success against two popular defenses against spoofing and object hiding attacks and discuss two enhanced defense strategies to mitigate our attack.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yulong Cao",
        "S. Hrushikesh Bhupathiraju",
        "Pirouz Naghavi",
        "Takeshi Sugawara",
        "Z. Morley Mao",
        "Sara Rampazzi"
      ],
      "url": "https://openalex.org/W4306887028",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4353012846",
      "title": "Exorcising \"Wraith\": Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks",
      "abstract": "Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., appearing attack). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of recent appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object. At the core of our module is a local objectness predictor, which explicitly incorporates the depth information to model the relation between depth and point density, and predicts each local part of an obstacle with an objectness score. Extensive experiments show, our proposed defense eliminates at least 70% cars forged by three known appearing attacks in most cases, while, for the best previous defense, less than 30% forged cars are eliminated. Meanwhile, under the same circumstance, our defense incurs less overhead for AP/precision on cars compared with existing defenses. Furthermore, We validate the effectiveness of our proposed defense on simulation-based closed-loop control driving tests in the open-source system of Baidu's Apollo.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Qi\u2010Fan Xiao",
        "Xudong Pan",
        "Yifan Lu",
        "Mi Zhang",
        "Jiarun Dai",
        "Min Yang"
      ],
      "url": "https://openalex.org/W4353012846",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2401.03582",
      "title": "Invisible Reflections: Leveraging Infrared Laser Reflections to Target Traffic Sign Perception",
      "abstract": "All vehicles must follow the rules that govern traffic behavior, regardless of whether the vehicles are human-driven or Connected Autonomous Vehicles (CAVs). Road signs indicate locally active rules, such as speed limits and requirements to yield or stop. Recent research has demonstrated attacks, such as adding stickers or projected colored patches to signs, that cause CAV misinterpretation, resulting in potential safety issues. Humans can see and potentially defend against these attacks. But humans can not detect what they can not observe. We have developed an effective physical-world attack that leverages the sensitivity of filterless image sensors and the properties of Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is designed to affect CAV cameras and perception, undermining traffic sign recognition by inducing misclassification. In this work, we formulate the threat model and requirements for an ILR-based traffic sign perception attack to succeed. We evaluate the effectiveness of the ILR attack with real-world experiments against two major traffic sign recognition architectures on four IR-sensitive cameras. Our black-box optimization methodology allows the attack to achieve up to a 100% attack success rate in indoor, static scenarios and a >80.5% attack success rate in our outdoor, moving vehicle scenarios. We find the latest state-of-the-art certifiable defense is ineffective against ILR attacks as it mis-certifies >33.5% of cases. To address this, we propose a detection strategy based on the physical properties of IR laser reflections which can detect 96% of ILR attacks.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Takami Sato",
        "Sri Hrushikesh Varma Bhupathiraju",
        "Michael Clifford",
        "Takeshi Sugawara",
        "Qi Alfred Chen",
        "Sara Rampazzi"
      ],
      "url": "https://arxiv.org/abs/2401.03582",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2402.03741",
      "title": "SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems",
      "abstract": "Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.",
      "year": 2024,
      "venue": "CCS",
      "authors": [
        "Oubo Ma",
        "Yuwen Pu",
        "Linkang Du",
        "Yang Dai",
        "Ruo Wang",
        "Xiaolei Liu",
        "Yingcai Wu",
        "Shouling Ji"
      ],
      "url": "https://arxiv.org/abs/2402.03741",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4406975609",
      "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
      "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training. Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Derui Wang",
        "Kristen Moore",
        "Diksha Goel",
        "Minjune Kim",
        "Gang Li",
        "Yang Li",
        "Robin Doss",
        "Minhui Xue",
        "Bo Li",
        "Seyit Camtepe",
        "Liming Zhu"
      ],
      "url": "https://openalex.org/W4406975609",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3026606204",
      "title": "Cost-Aware Robust Tree Ensembles for Security Applications",
      "abstract": "There are various costs for attackers to manipulate the features of security classifiers. The costs are asymmetric across features and to the directions of changes, which cannot be precisely captured by existing cost models based on $L_p$-norm robustness. In this paper, we utilize such domain knowledge to increase the attack cost of evading classifiers, specifically, tree ensemble models that are widely used by security tasks. We propose a new cost modeling method to capture the feature manipulation cost as constraint, and then we integrate the cost-driven constraint into the node construction process to train robust tree ensembles. During the training process, we use the constraint to find data points that are likely to be perturbed given the feature manipulation cost, and we use a new robust training algorithm to optimize the quality of the trees. Our cost-aware training method can be applied to different types of tree ensembles, including gradient boosted decision trees and random forest models. Using Twitter spam detection as the case study, our evaluation results show that we can increase the attack cost by 10.6X compared to the baseline. Moreover, our robust training method using cost-driven constraint can achieve higher accuracy, lower false positive rate, and stronger cost-aware robustness than the state-of-the-art training method using $L_\\infty$-norm cost model. Our code is available at https://github.com/surrealyz/growtrees.",
      "year": 2019,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Weifan Jiang",
        "Asaf Cidon",
        "Suman Jana"
      ],
      "url": "https://openalex.org/W3026606204",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2105.11363",
      "title": "Learning Security Classifiers with Verified Global Robustness Properties",
      "abstract": "Many recent works have proposed methods to train classifiers with local robustness properties, which can provably eliminate classes of evasion attacks for most inputs, but not all inputs. Since data distribution shift is very common in security applications, e.g., often observed for malware detection, local robustness cannot guarantee that the property holds for unseen inputs at the time of deploying the classifier. Therefore, it is more desirable to enforce global robustness properties that hold for all inputs, which is strictly stronger than local robustness.   In this paper, we present a framework and tools for training classifiers that satisfy global robustness properties. We define new notions of global robustness that are more suitable for security classifiers. We design a novel booster-fixer training framework to enforce global robustness properties. We structure our classifier as an ensemble of logic rules and design a new verifier to verify the properties. In our training algorithm, the booster increases the classifier's capacity, and the fixer enforces verified global robustness properties following counterexample guided inductive synthesis.   We show that we can train classifiers to satisfy different global robustness properties for three security datasets, and even multiple properties at the same time, with modest impact on the classifier's performance. For example, we train a Twitter spam account classifier to satisfy five global robustness properties, with 5.4% decrease in true positive rate, and 0.1% increase in false positive rate, compared to a baseline XGBoost model that doesn't satisfy any property.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Yizheng Chen",
        "Shiqi Wang",
        "Yue Qin",
        "Xiaojing Liao",
        "Suman Jana",
        "David Wagner"
      ],
      "url": "https://arxiv.org/abs/2105.11363",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2105.08619",
      "title": "On the Robustness of Domain Constraints",
      "abstract": "Machine learning is vulnerable to adversarial examples-inputs designed to cause models to perform poorly. However, it is unclear if adversarial examples represent realistic inputs in the modeled domains. Diverse domains such as networks and phishing have domain constraints-complex relationships between features that an adversary must satisfy for an attack to be realized (in addition to any adversary-specific goals). In this paper, we explore how domain constraints limit adversarial capabilities and how adversaries can adapt their strategies to create realistic (constraint-compliant) examples. In this, we develop techniques to learn domain constraints from data, and show how the learned constraints can be integrated into the adversarial crafting process. We evaluate the efficacy of our approach in network intrusion and phishing datasets and find: (1) up to 82% of adversarial examples produced by state-of-the-art crafting algorithms violate domain constraints, (2) domain constraints are robust to adversarial examples; enforcing constraints yields an increase in model accuracy by up to 34%. We observe not only that adversaries must alter inputs to satisfy domain constraints, but that these constraints make the generation of valid adversarial examples far more challenging.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Ryan Sheatsley",
        "Blaine Hoak",
        "Eric Pauley",
        "Yohan Beugin",
        "Michael J. Weisman",
        "Patrick McDaniel"
      ],
      "url": "https://arxiv.org/abs/2105.08619",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3214321642",
      "title": "Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",
      "abstract": "Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Tianyu Du",
        "Shouling Ji",
        "Lujia Shen",
        "Yao Zhang",
        "Jinfeng Li",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Raheem Beyah",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3214321642",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2002.12398",
      "title": "TSS: Transformation-Specific Smoothing for Robustness Certification",
      "abstract": "As machine learning (ML) systems become pervasive, safeguarding their security is critical. However, recently it has been demonstrated that motivated adversaries are able to mislead ML systems by perturbing test data using semantic transformations. While there exists a rich body of research providing provable robustness guarantees for ML models against $\\ell_p$ norm bounded adversarial perturbations, guarantees against semantic perturbations remain largely underexplored. In this paper, we provide TSS -- a unified framework for certifying ML robustness against general adversarial semantic transformations. First, depending on the properties of each transformation, we divide common transformations into two categories, namely resolvable (e.g., Gaussian blur) and differentially resolvable (e.g., rotation) transformations. For the former, we propose transformation-specific randomized smoothing strategies and obtain strong robustness certification. The latter category covers transformations that involve interpolation errors, and we propose a novel approach based on stratified sampling to certify the robustness. Our framework TSS leverages these certification strategies and combines with consistency-enhanced training to provide rigorous certification of robustness. We conduct extensive experiments on over ten types of challenging semantic transformations and show that TSS significantly outperforms the state of the art. Moreover, to the best of our knowledge, TSS is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset. For instance, our framework achieves 30.4% certified robust accuracy against rotation attack (within $\\pm 30^\\circ$) on ImageNet. Moreover, to consider a broader range of transformations, we show TSS is also robust against adaptive attacks and unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Linyi Li",
        "Maurice Weber",
        "Xiaojun Xu",
        "Luka Rimanic",
        "Bhavya Kailkhura",
        "Tao Xie",
        "Ce Zhang",
        "Bo Li"
      ],
      "url": "https://arxiv.org/abs/2002.12398",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4221166176",
      "title": "Transferring Adversarial Robustness Through Robust Representation Matching",
      "abstract": "With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassified. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a significant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher's robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model $\\sim 1.8\\times$ faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model $\\sim 18\\times$ faster than standard adversarial training.",
      "year": 2022,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "P. Vaishnavi",
        "Kevin Eykholt",
        "Amir Rahmati"
      ],
      "url": "https://openalex.org/W4221166176",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4386272826",
      "title": "DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing",
      "abstract": "Diffusion models have been leveraged to perform adversarial purification and thus provide both empirical and certified robustness for a standard model. On the other hand, different robustly trained smoothed models have been studied to improve the certified robustness. Thus, it raises a natural question: Can diffusion model be used to achieve improved certified robustness on those robustly trained smoothed models? In this work, we first theoretically show that recovered instances by diffusion models are in the bounded neighborhood of the original instance with high probability; and the \"one-shot\" denoising diffusion probabilistic models (DDPM) can approximate the mean of the generated distribution of a continuous-time diffusion model, which approximates the original instance under mild conditions. Inspired by our analysis, we propose a certifiably robust pipeline DiffSmooth, which first performs adversarial purification via diffusion models and then maps the purified instances to a common region via a simple yet effective local smoothing strategy. We conduct extensive experiments on different datasets and show that DiffSmooth achieves SOTA-certified robustness compared with eight baselines. For instance, DiffSmooth improves the SOTA-certified accuracy from $36.0\\%$ to $53.0\\%$ under $\\ell_2$ radius $1.5$ on ImageNet. The code is available at [https://github.com/javyduck/DiffSmooth].",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiawei Zhang",
        "Zhong\u2010Zhu Chen",
        "Huan Zhang",
        "Chaowei Xiao",
        "Bo Li"
      ],
      "url": "https://openalex.org/W4386272826",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4324007187",
      "title": "BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems",
      "abstract": "Deep learning (DL) performs well in many traffic analysis tasks.Nevertheless, the vulnerability of deep learning weakens the real-world performance of these traffic analyzers (e.g., suffering from evasion attack).Many studies in recent years focused on robustness certification for DL-based models.But existing methods perform far from perfectly in the traffic analysis domain.In this paper, we try to match three attributes of DL-based traffic analysis systems at the same time: (1) highly heterogeneous features, (2) varied model designs, (3) adversarial operating environments.Therefore, we propose BARS, a general robustness certification framework for DL-based traffic analysis systems based on boundary-adaptive randomized smoothing.To obtain tighter robustness guarantee, BARS uses optimized smoothing noise converging on the classification boundary.We firstly propose the Distribution Transformer for generating optimized smoothing noise.Then to optimize the smoothing noise, we propose some special distribution functions and two gradient based searching algorithms for noise shape and noise scale.We implement and evaluate BARS in three practical DL-based traffic analysis systems.Experiment results show that BARS can achieve tighter robustness guarantee than baseline methods.Furthermore, we illustrate the practicability of BARS through five application cases (e.g., quantitatively evaluating robustness).",
      "year": 2023,
      "venue": null,
      "authors": [
        "Kai Wang",
        "Zhiliang Wang",
        "Dongqi Han",
        "Wenqi Chen",
        "Jiahai Yang",
        "Xingang Shi",
        "Xia Yin"
      ],
      "url": "https://openalex.org/W4324007187",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2301.02905",
      "title": "REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service",
      "abstract": "Encoder as a service is an emerging cloud service. Specifically, a service provider first pre-trains an encoder (i.e., a general-purpose feature extractor) via either supervised learning or self-supervised learning and then deploys it as a cloud service API. A client queries the cloud service API to obtain feature vectors for its training/testing inputs when training/testing its classifier (called downstream classifier). A downstream classifier is vulnerable to adversarial examples, which are testing inputs with carefully crafted perturbation that the downstream classifier misclassifies. Therefore, in safety and security critical applications, a client aims to build a robust downstream classifier and certify its robustness guarantees against adversarial examples.   What APIs should the cloud service provide, such that a client can use any certification method to certify the robustness of its downstream classifier against adversarial examples while minimizing the number of queries to the APIs? How can a service provider pre-train an encoder such that clients can build more certifiably robust downstream classifiers? We aim to answer the two questions in this work. For the first question, we show that the cloud service only needs to provide two APIs, which we carefully design, to enable a client to certify the robustness of its downstream classifier with a minimal number of queries to the APIs. For the second question, we show that an encoder pre-trained using a spectral-norm regularization term enables clients to build more robust downstream classifiers.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Wenjie Qu",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "url": "https://arxiv.org/abs/2301.02905",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2202.01811",
      "title": "ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking",
      "abstract": "Object detectors, which are widely deployed in security-critical systems such as autonomous vehicles, have been found vulnerable to patch hiding attacks. An attacker can use a single physically-realizable adversarial patch to make the object detector miss the detection of victim objects and undermine the functionality of object detection applications. In this paper, we propose ObjectSeeker for certifiably robust object detection against patch hiding attacks. The key insight in ObjectSeeker is patch-agnostic masking: we aim to mask out the entire adversarial patch without knowing the shape, size, and location of the patch. This masking operation neutralizes the adversarial effect and allows any vanilla object detector to safely detect objects on the masked images. Remarkably, we can evaluate ObjectSeeker's robustness in a certifiable manner: we develop a certification procedure to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness. Our experiments demonstrate a significant (~10%-40% absolute and ~2-6x relative) improvement in certifiable robustness over the prior work, as well as high clean performance (~1% drop compared with undefended models).",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Chong Xiang",
        "Alexander Valtchanov",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "url": "https://arxiv.org/abs/2202.01811",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2202.03277",
      "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks",
      "abstract": "While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealistic attacks. We shed light on the patterns that discriminate which unrealistic examples can be used for effective hardening. We release our code, datasets and models to support future research in exploring how to reduce the gap between unrealistic and realistic adversarial attacks.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Salijona Dyrmishi",
        "Salah Ghamizi",
        "Thibault Simonetto",
        "Yves Le Traon",
        "Maxime Cordy"
      ],
      "url": "https://arxiv.org/abs/2202.03277",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2307.16630",
      "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
      "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Xinyu Zhang",
        "Hanbin Hong",
        "Yuan Hong",
        "Peng Huang",
        "Binghui Wang",
        "Zhongjie Ba",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2307.16630",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2309.11005",
      "title": "It's Simplex! Disaggregating Measures to Improve Certified Robustness",
      "abstract": "Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\\%$ more samples at noise scale $\u03c3= 1$, with greater relative improvements observed as the difficulty of the predictive task increases.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Andrew C. Cullen",
        "Paul Montague",
        "Shijie Liu",
        "Sarah M. Erfani",
        "Benjamin I. P. Rubinstein"
      ],
      "url": "https://arxiv.org/abs/2309.11005",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4407124074",
      "title": "AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification",
      "abstract": "Graph neural networks (GNNs) achieve the state-of-the-art on graph-relevant tasks such as node and graph classification. However, recent works show GNNs are vulnerable to adversarial perturbations include the perturbation on edges, nodes, and node features, the three components forming a graph. Empirical defenses against such attacks are soon broken by adaptive ones. While certified defenses offer robustness guarantees, they face several limitations: 1) almost all restrict the adversary's capability to only one type of perturbation, which is impractical; 2) all are designed for a particular GNN task, which limits their applicability; and 3) the robustness guarantees of all methods except one are not 100% accurate. We address all these limitations by developing AGNNCert, the first certified defense for GNNs against arbitrary (edge, node, and node feature) perturbations with deterministic robustness guarantees, and applicable to the two most common node and graph classification tasks. AGNNCert also encompass existing certified defenses as special cases. Extensive evaluations on multiple benchmark node/graph classification datasets and two real-world graph datasets, and multiple GNNs validate the effectiveness of AGNNCert to provably defend against arbitrary perturbations. AGNNCert also shows its superiority over the state-of-the-art certified defenses against the individual edge perturbation and node perturbation.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiate Li",
        "Binghui Wang"
      ],
      "url": "https://openalex.org/W4407124074",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4403594037",
      "title": "Robustifying ML-powered Network Classifiers with PANTS",
      "abstract": "Multiple network management tasks, from resource allocation to intrusion detection, rely on some form of ML-based network traffic classification (MNC). Despite their potential, MNCs are vulnerable to adversarial inputs, which can lead to outages, poor decision-making, and security violations, among other issues. The goal of this paper is to help network operators assess and enhance the robustness of their MNC against adversarial inputs. The most critical step for this is generating inputs that can fool the MNC while being realizable under various threat models. Compared to other ML models, finding adversarial inputs against MNCs is more challenging due to the existence of non-differentiable components e.g., traffic engineering and the need to constrain inputs to preserve semantics and ensure reliability. These factors prevent the direct use of well-established gradient-based methods developed in adversarial ML (AML). To address these challenges, we introduce PANTS, a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs. We also embed PANTS into an iterative adversarial training process that enhances the robustness of MNCs against adversarial inputs. PANTS is 70% and 2x more likely in median to find adversarial inputs against target MNCs compared to state-of-the-art baselines, namely Amoeba and BAP. PANTS improves the robustness of the target MNCs by 52.7% (even against attackers outside of what is considered during robustification) without sacrificing their accuracy.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Minhao Jin",
        "Maria Apostolaki"
      ],
      "url": "https://openalex.org/W4403594037",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2102.00918",
      "title": "Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
      "abstract": "Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.",
      "year": 2021,
      "venue": "ACM CCS",
      "authors": [
        "Alireza Bahramali",
        "Milad Nasr",
        "Amir Houmansadr",
        "Dennis Goeckel",
        "Don Towsley"
      ],
      "url": "https://arxiv.org/abs/2102.00918",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4324007133",
      "title": "Adversarial Robustness for Tabular Data through Cost and Utility Awareness",
      "abstract": "value of a person's salary, another to their age, and another to a categorical value representing their marital status.The properties of the image domain have shaped the way adversarial examples and adversarial robustness are approached in the literature [11] and have greatly influenced adversarial robustness research in the text domain.In this paper, we argue that adversarial examples in tabular domains are of a different nature, and adversarial robustness has a different meaning.Thus, the definitions and techniques used to study these phenomena need to be revisited to reflect the tabular context.We argue that two high-level differences need to be addressed.First, imperceptibility, which is the main requirement considered for image and text adversarial examples, is ill-defined and can be irrelevant for tabular data.Second, existing methods assume that all adversarial inputs have the same value for the adversary, whereas in tabular domains different adversarial examples can bring drastically different gains.Imperceptibility and semantic similarity are not necessarily the primary constraints in tabular domains.The existing literature commonly formalizes the concept of \"an example deliberately crafted to cause a misclassification\" as a natural example, i.e., an example coming from the data distribution, that is imperceptibly modified by an adversary in a way that the classifier's decision changes.Typically, imperceptibility is formalized as closeness according to a mathematical distance such as L p [21,22].In tabular data, however, imperceptibility is not necessarily relevant.Let us consider the following toy example of financialfraud detection: Assume a fraud detector takes as input two features: (1) transaction amount, and (2) device from which the transaction was sent.The adversary aims to create a fraudulent financial transaction.The adversary starts with a natural example (amount=$200, device=Android phone) and changes the feature values until the detector no longer classifies the example as fraud.In this example, imperceptibility is not well-defined.Is a modification to the amount feature from $200 to $201 imperceptible?What increase or decrease would we consider perceptible?The issue is even more apparent with categorical data, for which standard distances such as L 2 , L \u221e cannot even capture imperceptibility: Is a change of the device feature from Android to an iPhone imperceptible?Even if imperceptibility was well-defined, imperceptibility might not be relevant.Should we only be concerned about adversaries making \"imperceptible\" changes, e.g., modifying amount from $200 to $201?What about attack vectors in Abstract-Many safety-critical applications of machine learning, such as fraud or abuse detection, use data in tabular domains.Adversarial examples can be particularly damaging for these applications.Yet, existing works on adversarial robustness primarily focus on machine-learning models in image and text domains.We argue that, due to the differences between tabular data and images or text, existing threat models are not suitable for tabular domains.These models do not capture that the costs of an attack could be more significant than imperceptibility, or that the adversary could assign different values to the utility obtained from deploying different adversarial examples.We demonstrate that, due to these differences, the attack and defense methods used for images and text cannot be directly applied to tabular settings.We address these issues by proposing new cost and utility-aware threat models that are tailored to the adversarial capabilities and constraints of attackers targeting tabular domains.We introduce a framework that enables us to design attack and defense mechanisms that result in models protected against cost or utility-aware adversaries, for example, adversaries constrained by a certain financial budget.We show that our approach is effective on three datasets corresponding to applications for which adversarial examples can have economic and social implications.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Klim Kireev",
        "Bogdan Kulynych",
        "Carmela Troncoso"
      ],
      "url": "https://openalex.org/W4324007133",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2201.02775",
      "title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems",
      "abstract": "Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the saliency score of ``victim'' participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Qi Pang",
        "Yuanyuan Yuan",
        "Shuai Wang",
        "Wenting Zheng"
      ],
      "url": "https://arxiv.org/abs/2201.02775",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4384948583",
      "title": "AI-Guardian: Defeating Adversarial Attacks using Backdoors",
      "abstract": "Deep neural networks (DNNs) have been widely used in many fields due to their increasingly high accuracy. However, they are also vulnerable to adversarial attacks, posing a serious threat to security-critical applications such as autonomous driving, remote diagnosis, etc. Existing solutions are limited in detecting/preventing such attacks, and also impacting the performance on the original tasks. In this paper, we present AI-Guardian, a novel approach to defeating adversarial attacks that leverages intentionally embedded backdoors to fail the adversarial perturbations and maintain the performance of the original main task. We extensively evaluate AI-Guardian using five popular adversarial example generation approaches, and experimental results demonstrate its efficacy in defeating adversarial attacks. Specifically, AI-Guardian reduces the attack success rate from 97.3% to 3.2%, which outperforms the state-of-the-art works by 30.9%, with only a 0.9% decline on the clean data accuracy. Furthermore, AI-Guardian introduces only 0.36% overhead to the model prediction time, almost negligible in most cases.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Hong Zhu",
        "Shengzhi Zhang",
        "Kai Chen"
      ],
      "url": "https://openalex.org/W4384948583",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4406959651",
      "title": "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
      "abstract": "Large Language Models (LLMs) have raised increasing concerns about their misuse in generating hate speech. Among all the efforts to address this issue, hate speech detectors play a crucial role. However, the effectiveness of different detectors against LLM-generated hate speech remains largely unknown. In this paper, we propose HateBench, a framework for benchmarking hate speech detectors on LLM-generated hate speech. We first construct a hate speech dataset of 7,838 samples generated by six widely-used LLMs covering 34 identity groups, with meticulous annotations by three labelers. We then assess the effectiveness of eight representative hate speech detectors on the LLM-generated dataset. Our results show that while detectors are generally effective in identifying LLM-generated hate speech, their performance degrades with newer versions of LLMs. We also reveal the potential of LLM-driven hate campaigns, a new threat that LLMs bring to the field of hate speech detection. By leveraging advanced techniques like adversarial attacks and model stealing attacks, the adversary can intentionally evade the detector and automate hate campaigns online. The most potent adversarial attack achieves an attack success rate of 0.966, and its attack efficiency can be further improved by $13-21\\times$ through model stealing attacks with acceptable attack performance. We hope our study can serve as a call to action for the research community and platform moderators to fortify defenses against these emerging threats.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xinyue Shen",
        "Yixin Wu",
        "Yiting Qu",
        "Michael Backes",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "url": "https://openalex.org/W4406959651",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4415158376",
      "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
      "abstract": "Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Zhisheng Zhang",
        "Derui Wang",
        "Qianyi Yang",
        "Pengyang Huang",
        "Jialun Pu",
        "Yuxin Cao",
        "Kai Ye",
        "Jie Hao",
        "Yixian Yang"
      ],
      "url": "https://openalex.org/W4415158376",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2506.17162",
      "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
      "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability.   To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Side Liu",
        "Jiang Ming",
        "Guodong Zhou",
        "Xinyi Liu",
        "Jianming Fu",
        "Guojun Peng"
      ],
      "url": "https://arxiv.org/abs/2506.17162",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3094898851",
      "title": "Text Captcha Is Dead? A Large Scale Deployment and Empirical Studys",
      "abstract": "The development of deep learning techniques has significantly increased the ability of computers to recognize CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), thus breaking or mitigating the security of existing captcha schemes. To protect against these attacks, recent works have been proposed to leverage adversarial machine learning to perturb captcha pictures. However, they either require the prior knowledge of captcha solving models or lack adaptivity to the evolving behaviors of attackers. Most importantly, none of them has been deployed in practical applications, and their practical applicability and effectiveness are unknown.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Chenghui Shi",
        "Shouling Ji",
        "Qianjun Liu",
        "Changchang Liu",
        "Yuefeng Chen",
        "Yuan He",
        "Zhe Liu",
        "Raheem Beyah",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3094898851",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4324007038",
      "title": "Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems",
      "abstract": "Audio CAPTCHAs are supposed to provide a strong defense for online resources; however, advances in speech-totext mechanisms have rendered these defenses ineffective.Audio CAPTCHAs cannot simply be abandoned, as they are specifically named by the W3C as important enablers of accessibility.Accordingly, demonstrably more robust audio CAPTCHAs are important to the future of a secure and accessible Web.We look to recent literature on attacks on speech-to-text systems for inspiration for the construction of robust, principle-driven audio defenses.We begin by comparing 20 recent attack papers, classifying and measuring their suitability to serve as the basis of new \"robust to transcription\" but \"easy for humans to understand\" CAPTCHAs.After showing that none of these attacks alone are sufficient, we propose a new mechanism that is both comparatively intelligible (evaluated through a user study) and hard to automatically transcribe (i.e., P (transcription) = 4 \u00d7 10 -5 ).We also demonstrate that our audio samples have a high probability of being detected as CAPTCHAs when given to speech-to-text systems (P (evasion) = 1.77 \u00d7 10 -4 ).Finally, we show that our method can break WaveGuard, a mechanism designed to defend adversarial audio, with a 99% success rate.In so doing, we not only demonstrate a CAPTCHA that is approximately four orders of magnitude more difficult to crack, but that such systems can be designed based on the insights gained from attack papers using the differences between the ways that humans and computers process audio.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Hadi Abdullah",
        "Aditya Karlekar",
        "Saurabh Prasad",
        "Muhammad Sajidur Rahman",
        "Logan Blue",
        "Luke A. Bauer",
        "Vincent Bindschaedler",
        "Patrick Traynor"
      ],
      "url": "https://openalex.org/W4324007038",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4385679715",
      "title": "A Generic, Efficient, and Effortless Solver with Self-Supervised Learning for Breaking Text Captchas",
      "abstract": "Although text-based captcha, which is used to differentiate between human users and bots, has faced many attack methods, it remains a widely used security mechanism and is employed by some websites. Some deep learning-based text captcha solvers have shown excellent results, but the labor-intensive and time-consuming labeling process severely limits their viability. Previous works attempted to create easy-to-use solvers using a limited collection of labeled data. However, they are hampered by inefficient preprocessing procedures and inability to recognize the captchas with complicated security features.In this paper, we propose GeeSolver, a generic, efficient, and effortless solver for breaking text-based captchas based on self-supervised learning. Our insight is that numerous difficult-to-attack captcha schemes that \"damage\" the standard font of characters are similar to image masks. And we could leverage masked autoencoders (MAE) to improve the captcha solver to learn the latent representation from the \"unmasked\" part of the captcha images. Specifically, our model consists of a ViT encoder as latent representation extractor and a well-designed decoder for captcha recognition. We apply MAE paradigm to train our encoder, which enables the encoder to extract latent representation from local information (i.e., without masking part) that can infer the corresponding character. Further, we freeze the parameters of the encoder and leverage a few labeled captchas and many unlabeled captchas to train our captcha decoder with semi-supervised learning.Our experiments with real-world captcha schemes demonstrate that GeeSolver outperforms the state-of-the-art methods by a large margin using a few labeled captchas. We also show that GeeSolver is highly efficient as it can solve a captcha within 25 ms using a desktop CPU and 9 ms using a desktop GPU. Besides, thanks to latent representation extraction, we successfully break the hard-to-attack captcha schemes, proving the generality of our solver. We hope that our work will help security experts to revisit the design and availability of text-based captchas. The code is available at https://github.com/NSSL-SJTU/GeeSolver.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Ruijie Zhao",
        "Xianwen Deng",
        "Yanhao Wang",
        "Zhicong Yan",
        "Zhengguang Han",
        "Libo Chen",
        "Zhi Xue",
        "Yijun Wang"
      ],
      "url": "https://openalex.org/W4385679715",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2209.03463",
      "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
      "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",
      "year": 2022,
      "venue": "ACM CCS",
      "authors": [
        "Wai Man Si",
        "Michael Backes",
        "Jeremy Blackburn",
        "Emiliano De Cristofaro",
        "Gianluca Stringhini",
        "Savvas Zannettou",
        "Yang Zhang"
      ],
      "url": "https://arxiv.org/abs/2209.03463",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3206584998",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "PolyU Institutional Research Archive (Hong Kong Polytechnic University)",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "Shouling Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "url": "https://openalex.org/W3206584998",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4384948696",
      "title": "ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes",
      "abstract": "This paper presents a novel physical impersonating attack against face recognition systems. It aims at generating consistent style changes across multiple pictures of the attacker under different conditions and poses. Additionally, the style changes are required to be physically realizable by make-up and can induce the intended misclassification. To achieve the goal, we develop novel techniques to embed multiple pictures of the same physical person to vectors in the StyleGAN's latent space, such that the embedded latent vectors have some implicit correlations to make the search for consistent style changes feasible. Our digital and physical evaluation results show our approach can allow an outsider attacker to successfully impersonate the insiders with consistent and natural changes.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Shengwei An",
        "Yuan Yao",
        "Qiuling Xu",
        "Shiqing Ma",
        "Guanhong Tao",
        "Siyuan Cheng",
        "Kaiyuan Zhang",
        "Yingqi Liu",
        "Guangyu Shen",
        "Ian Kelk",
        "Xiangyu Zhang"
      ],
      "url": "https://openalex.org/W4384948696",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4384948616",
      "title": "DepthFake: Spoofing 3D Face Authentication with a 2D Photo",
      "abstract": "Face authentication has been widely used in access control, and the latest 3D face authentication systems employ 3D liveness detection techniques to cope with the photo replay attacks, whereby an attacker uses a 2D photo to bypass the authentication. In this paper, we analyze the security of 3D liveness detection systems that utilize structured light depth cameras and discover a new attack surface against 3D face authentication systems. We propose DepthFake attacks that can spoof a 3D face authentication using only one single 2D photo. To achieve this goal, DepthFake first estimates the 3D depth information of a target victim's face from his 2D photo. Then, DepthFake projects the carefully-crafted scatter patterns embedded with the face depth information, in order to empower the 2D photo with 3D authentication properties. We overcome a collection of practical challenges, e.g., depth estimation errors from 2D photos, depth images forgery based on structured light, the alignment of the RGB image and depth images for a face, and implemented DepthFake in laboratory setups. We validated DepthFake on 3 commercial face authentication systems (i.e., Tencent Cloud, Baidu Cloud, and 3DiVi) and one commercial access control device. The results over 50 users demonstrate that DepthFake achieves an overall Depth attack success rate of 79.4% and RGB-D attack success rate of 59.4% in the real world.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Zhihao Wu",
        "Yushi Cheng",
        "Jiahui Yang",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://openalex.org/W4384948616",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4385187425",
      "title": "Understanding the (In)Security of Cross-side Face Verification Systems in Mobile Apps: A System Perspective",
      "abstract": "Face Verification Systems (FVSes) are more and more deployed by real-world mobile applications (apps) to verify a human's claimed identity. One popular type of FVSes is called cross-side FVS (XFVS), which splits the FVS functionality into two sides: one at a mobile phone to take pictures or videos and the other at a trusted server for verification. Prior works have studied the security of XFVSes from the machine learning perspective, i.e., whether the learning models used by XFVSes are robust to adversarial attacks. However, the security of other parts of XFVSes, especially the design and implementation of the verification procedure used by XFVSes, is not well understood.In this paper, we conduct the first measurement study on the security of real-world XFVSes used by popular mobile apps from a system perspective. More specifically, we design and implement a semi-automated system, called XFVSChecker, to detect XFVSes in mobile apps and then inspect their compliance with four security properties. Our evaluation reveals that most of existing XFVS apps, including those with billions of downloads, are vulnerable to at least one of four types of attacks. These attacks require only easily available attack prerequisites, such as one photo of the victim, to pose significant security risks, including complete account takeover, identity fraud and financial loss. Our findings result in 14 Chinese National Vulnerability Database (CNVD) IDs and one of them, particularly CNVD-2021-86899, is awarded the most valuable vulnerability in 2021 among all the reported vulnerabilities to CNVD.",
      "year": 2023,
      "venue": null,
      "authors": [
        "Xiaohan Zhang",
        "H. Ye",
        "Ziqi Huang",
        "Ye Xiao",
        "Yinzhi Cao",
        "Yuan Zhang",
        "Min Yang"
      ],
      "url": "https://openalex.org/W4385187425",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2210.09421",
      "title": "Deepfake Text Detection: Limitations and Opportunities",
      "abstract": "Recent advances in generative models for language have enabled the creation of convincing synthetic text or deepfake text. Prior work has demonstrated the potential for misuse of deepfake text to mislead content consumers. Therefore, deepfake text detection, the task of discriminating between human and machine-generated text, is becoming increasingly critical. Several defenses have been proposed for deepfake text detection. However, we lack a thorough understanding of their real-world applicability. In this paper, we collect deepfake text from 4 online services powered by Transformer-based tools to evaluate the generalization ability of the defenses on content in the wild. We develop several low-cost adversarial attacks, and investigate the robustness of existing defenses against an adaptive attacker. We find that many defenses show significant degradation in performance under our evaluation scenarios compared to their original claimed performance. Our evaluation shows that tapping into the semantic information in the text content is a promising approach for improving the robustness and generalization performance of deepfake text detection schemes.",
      "year": 2023,
      "venue": "IEEE S&P",
      "authors": [
        "Jiameng Pu",
        "Zain Sarwar",
        "Sifat Muhammad Abdullah",
        "Abdullah Rehman",
        "Yoonjin Kim",
        "Parantapa Bhattacharya",
        "Mobin Javed",
        "Bimal Viswanath"
      ],
      "url": "https://arxiv.org/abs/2210.09421",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2510.05173",
      "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
      "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce SafeGuider, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, SafeGuider generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
      "year": 2025,
      "venue": "CCS",
      "authors": [
        "Peigui Qi",
        "Kunsheng Tang",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Nenghai Yu",
        "Tianwei Zhang",
        "Qing Guo",
        "Jie Zhang"
      ],
      "url": "https://arxiv.org/abs/2510.05173",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2302.05319",
      "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
      "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
      "year": 2023,
      "venue": "ACM CCS",
      "authors": [
        "Jingxuan He",
        "Martin Vechev"
      ],
      "url": "https://arxiv.org/abs/2302.05319",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2305.12082",
      "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
      "abstract": "Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E raise many ethical concerns due to the generation of harmful images such as Not-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety filters are often adopted to prevent the generation of NSFW images. In this work, we propose SneakyPrompt, the first automated attack framework, to jailbreak text-to-image generative models such that they generate NSFW images even if safety filters are adopted. Given a prompt that is blocked by a safety filter, SneakyPrompt repeatedly queries the text-to-image generative model and strategically perturbs tokens in the prompt based on the query results to bypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement learning to guide the perturbation of tokens. Our evaluation shows that SneakyPrompt successfully jailbreaks DALL$\\cdot$E 2 with closed-box safety filters to generate NSFW images. Moreover, we also deploy several state-of-the-art, open-source safety filters on a Stable Diffusion model. Our evaluation shows that SneakyPrompt not only successfully generates NSFW images, but also outperforms existing text adversarial attacks when extended to jailbreak text-to-image generative models, in terms of both the number of queries and qualities of the generated NSFW images. SneakyPrompt is open-source and available at this repository: \\url{https://github.com/Yuchen413/text2image_safety}.",
      "year": 2024,
      "venue": "IEEE S&P",
      "authors": [
        "Yuchen Yang",
        "Bo Hui",
        "Haolin Yuan",
        "Neil Gong",
        "Yinzhi Cao"
      ],
      "url": "https://arxiv.org/abs/2305.12082",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_1ba3949d",
      "title": "SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models",
      "abstract": "Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts -- inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Xinfeng Li",
        "Yuchen Yang",
        "Jiangyi Deng",
        "Chen Yan",
        "Yanjiao Chen",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "url": "https://arxiv.org/abs/2404.06666",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2309.14122",
      "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-to-Image Models via Substitution",
      "abstract": "Advanced text-to-image models such as DALL$\\cdot$E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Zhongjie Ba",
        "Jieming Zhong",
        "Jiachen Lei",
        "Peng Cheng",
        "Qinglong Wang",
        "Zhan Qin",
        "Zhibo Wang",
        "Kui Ren"
      ],
      "url": "https://arxiv.org/abs/2309.14122",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "seed_9a1e7666",
      "title": "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
      "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers.   In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. Jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of LLM chatbots.",
      "year": 2023,
      "venue": "NDSS",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Yuekang Li",
        "Kailong Wang",
        "Ying Zhang",
        "Zefeng Li",
        "Haoyu Wang",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "url": "https://arxiv.org/abs/2307.08715",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4399317489",
      "title": "Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries",
      "abstract": "Recent advances in Large Language Models (LLMs) have led to impressive alignment where models learn to distinguish harmful from harmless queries through supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In this paper, we reveal a subtle yet impactful weakness in these aligned models. We find that simply appending multiple end of sequence (eos) tokens can cause a phenomenon we call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space. Building on this observation, we propose a straightforward method to BOOST jailbreak attacks by appending eos tokens. Our systematic evaluation shows that this strategy significantly increases the attack success rate across 8 representative jailbreak techniques and 16 open-source LLMs, ranging from 2B to 72B parameters. Moreover, we develop a novel probing mechanism for commercial APIs and discover that major providers such as OpenAI, Anthropic, and Qwen do not filter eos tokens, making them similarly vulnerable. These findings highlight a hidden yet critical blind spot in existing alignment and content filtering approaches. We call for heightened attention to eos tokens' unintended influence on model behaviors, particularly in production systems. Our work not only calls for an input-filtering based defense, but also points to new defenses that make refusal boundaries more robust and generalizable, as well as fundamental alignment techniques that can defend against context segmentation attacks.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiahao Yu",
        "Haozheng Luo",
        "Jerry Yao-Chieh",
        "Wenbo Guo",
        "Han Liu",
        "Xinyu Xing"
      ],
      "url": "https://openalex.org/W4399317489",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4407124169",
      "title": "Activation Approximations Can Incur Safety Vulnerabilities in Aligned LLMs: Comprehensive Analysis and Defense",
      "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadA's effectiveness in enhancing the safety capabilities of LLMs after activation approximations.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jiawen Zhang",
        "Kejia Chen",
        "Lipeng He",
        "Jian Lou",
        "Dan Li",
        "Zunlei Feng",
        "Mingli Song",
        "Jian Liu",
        "Kui Ren",
        "Xiaohu Yang"
      ],
      "url": "https://openalex.org/W4407124169",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417133177",
      "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
      "abstract": "Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism that automatically rejects such harmful prompts. However, they can be bypassed through LLM jailbreaks. Current jailbreaks often require significant manual effort, high computational costs, or result in excessive model modifications that may degrade regular utility. We introduce TwinBreak, an innovative safety alignment removal method. Building on the idea that the safety mechanism operates like an embedded backdoor, TwinBreak identifies and prunes parameters responsible for this functionality. By focusing on the most relevant model layers, TwinBreak performs fine-grained analysis of parameters essential to model utility and safety. TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters. We present the TwinPrompt dataset containing 100 such twin prompts. Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success rates with minimal computational requirements across 16 LLMs from five vendors.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Krau\u00df, Torsten",
        "Dashtbani, Hamid",
        "Dmitrienko, Alexandra"
      ],
      "url": "https://openalex.org/W4417133177",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4400484590",
      "title": "Exploiting Task-Level Vulnerabilities: An Automatic Jailbreak Attack and Defense Benchmarking for LLMs",
      "abstract": "Natural language prompts serve as an essential interface between users and Large Language Models (LLMs) like GPT-3.5 and GPT-4, which are employed by ChatGPT to produce outputs across various tasks. However, prompts crafted with malicious intent, known as jailbreak prompts, can circumvent the restrictions of LLMs, posing a significant threat to systems integrated with these models. Despite their critical importance, there is a lack of systematic analysis and comprehensive understanding of jailbreak prompts. Our paper aims to address this gap by exploring key research questions to enhance the robustness of LLM systems: 1) What common patterns are present in jailbreak prompts? 2) How effectively can these prompts bypass the restrictions of LLMs? 3) With the evolution of LLMs, how does the effectiveness of jailbreak prompts change? To address our research questions, we embarked on an empirical study targeting the LLMs underpinning ChatGPT, one of today\u2019s most advanced chatbots. Our methodology involved categorizing 78 jailbreak prompts into 10 distinct patterns, further organized into three jailbreak strategy types, and examining their distribution.We assessed the effectiveness of these prompts on GPT-3.5 and GPT-4, using a set of 3,120 questions across 8 scenarios deemed prohibited by OpenAI. Additionally, our study tracked the performance of these prompts over a 3-month period, observing the evolutionary response of ChatGPT to such inputs. Our findings offer a comprehensive view of jailbreak prompts, elucidating their taxonomy, effectiveness, and temporal dynamics. Notably, we discovered that GPT-3.5 and GPT-4 could still generate inappropriate content in response to malicious prompts without the need for jailbreaking. This underscores the critical need for effective prompt management within LLM systems and provides valuable insights and data to spur further research in LLM testing and jailbreak prevention.",
      "year": 2024,
      "venue": null,
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Zhengzi Xu",
        "Yuekang Li",
        "Yaowen Zheng",
        "Ying Zhang",
        "Lida Zhao",
        "Tianwei Zhang",
        "Kailong Wang"
      ],
      "url": "https://openalex.org/W4400484590",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4403780392",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "abstract": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Xueluan Gong",
        "Mingzhe Li",
        "Yilin Zhang",
        "Fengyuan Ran",
        "Chen Chen",
        "Yanjiao Chen",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "url": "https://openalex.org/W4403780392",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4393932428",
      "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
      "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem",
        "Ronen Eldan"
      ],
      "url": "https://openalex.org/W4393932428",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4399554837",
      "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
      "abstract": "Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "X Wang",
        "Daoyuan Wu",
        "Zhenlan Ji",
        "Zongjie Li",
        "Pingchuan Ma",
        "Shuai Wang",
        "Yingjiu Li",
        "Yang Liu",
        "Ning Liu",
        "Juergen Rahmel"
      ],
      "url": "https://openalex.org/W4399554837",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4407425033",
      "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
      "abstract": "Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats. Some defenses have been proposed to detect or mitigate jailbreaks, but they are unable to withstand the test of time due to an insufficient understanding of jailbreak mechanisms. In this work, we investigate the mechanisms behind jailbreaks based on the Linear Representation Hypothesis (LRH), which states that neural networks encode high-level concepts as subspaces in their hidden representations. We define the toxic semantics in harmful and jailbreak prompts as toxic concepts and describe the semantics in jailbreak prompts that manipulate LLMs to comply with unsafe requests as jailbreak concepts. Through concept extraction and analysis, we reveal that LLMs can recognize the toxic concepts in both harmful and jailbreak prompts. However, unlike harmful prompts, jailbreak prompts activate the jailbreak concepts and alter the LLM output from rejection to compliance. Building on our analysis, we propose a comprehensive jailbreak defense framework, JBShield, consisting of two key components: jailbreak detection JBShield-D and mitigation JBShield-M. JBShield-D identifies jailbreak prompts by determining whether the input activates both toxic and jailbreak concepts. When a jailbreak prompt is detected, JBShield-M adjusts the hidden representations of the target LLM by enhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs produce safe content. Extensive experiments demonstrate the superior performance of JBShield, achieving an average detection accuracy of 0.95 and reducing the average attack success rate of various jailbreak attacks to 2% from 61% across distinct LLMs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Shenyi Zhang",
        "Yougang Zhai",
        "Keyan Guo",
        "Hongxin Hu",
        "Shengnan Guo",
        "Fang Zheng",
        "Lingchen Zhao",
        "Chao Shen",
        "Cong Wang",
        "Qian Wang"
      ],
      "url": "https://openalex.org/W4407425033",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2311.17400",
      "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
      "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
      "year": 2024,
      "venue": "NDSS",
      "authors": [
        "Lujia Shen",
        "Yuwen Pu",
        "Shouling Ji",
        "Changjiang Li",
        "Xuhong Zhang",
        "Chunpeng Ge",
        "Ting Wang"
      ],
      "url": "https://arxiv.org/abs/2311.17400",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417255476",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Wenlong Meng",
        "Shao\u2010Hua Fan",
        "Chengkun Wei",
        null,
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "url": "https://openalex.org/W4417255476",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4403996051",
      "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
      "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Hanna Kim",
        "Minkyoo Song",
        "Seung Ho Na",
        "Seungwon Shin",
        "K.-H Lee"
      ],
      "url": "https://openalex.org/W4403996051",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4407310131",
      "title": "Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search",
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly querying a URL will increase the number of main risk-inclusive responses, while querying with natural language will slightly mitigate such risk. Compared to traditional search engines, AIPSEs outperform in both utility and safety. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector. Our evaluation shows that our defense can effectively reduce the risk, with only a minor cost of reducing available information by approximately 10.7%. Our research highlights the urgent need for robust safety measures in AIPSEs.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Z. Luo",
        "Zhaoyun Peng",
        "Yule Liu",
        "Zhen Sun",
        "M Li",
        "Jingyi Zheng",
        "Xinlei He"
      ],
      "url": "https://openalex.org/W4407310131",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4391766701",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "abstract": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/StruQ.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "url": "https://openalex.org/W4391766701",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4399553840",
      "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
      "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query, ostensibly because it lacks relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. Our method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not employ an auxiliary LLM. We evaluate jamming attacks on several embeddings and LLMs and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.",
      "year": 2024,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Avital Shafran",
        "Roei Schuster",
        "Vitaly Shmatikov"
      ],
      "url": "https://openalex.org/W4399553840",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2410.05451",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign",
      "year": 2025,
      "venue": "ACM CCS",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "David Wagner",
        "Chuan Guo"
      ],
      "url": "https://arxiv.org/abs/2410.05451",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4406880263",
      "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
      "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Yining Wang",
        "Mi Zhang",
        "Junjie Sun",
        "Chenyue Wang",
        "Min Yang",
        "Hui Xue",
        "Jingli Tao",
        "Ranjie Duan",
        "Jiexi Liu"
      ],
      "url": "https://openalex.org/W4406880263",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2009.03015",
      "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
      "abstract": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.   AWT is the first end-to-end model to hide data in text by automatically learning -- without ground truth -- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.",
      "year": 2021,
      "venue": "IEEE S&P",
      "authors": [
        "Sahar Abdelnabi",
        "Mario Fritz"
      ],
      "url": "https://arxiv.org/abs/2009.03015",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417229822",
      "title": "Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems",
      "abstract": "The widespread application of automatic speech recognition (ASR) supports large-scale voice surveillance, raising concerns about privacy among users. In this paper, we concentrate on using adversarial examples to mitigate unauthorized disclosure of speech privacy thwarted by potential eavesdroppers in speech communications. While audio adversarial examples have demonstrated the capability to mislead ASR models or evade ASR surveillance, they are typically constructed through time-intensive offline optimization, restricting their practicality in real-time voice communication. Recent work overcame this limitation by generating universal adversarial perturbations (UAPs) and enhancing their transferability for black-box scenarios. However, they introduced excessive noise that significantly degrades audio quality and affects human perception, thereby limiting their effectiveness in practical scenarios. To address this limitation and protect live users' speech against ASR systems, we propose a novel framework, AudioShield. Central to this framework is the concept of Transferable Universal Adversarial Perturbations in the Latent Space (LS-TUAP). By transferring the perturbations to the latent space, the audio quality is preserved to a large extent. Additionally, we propose target feature adaptation to enhance the transferability of UAPs by embedding target text features into the perturbations. Comprehensive evaluation on four commercial ASR APIs (Google, Amazon, iFlytek, and Alibaba), three voice assistants, two LLM-powered ASR and one NN-based ASR demonstrates the protection superiority of AudioShield over existing competitors, and both objective and subjective evaluations indicate that AudioShield significantly improves the audio quality. Moreover, AudioShield also shows high effectiveness in real-time end-to-end scenarios, and demonstrates strong resilience against adaptive countermeasures.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Weifei Jin",
        "Yuxin Cao",
        "J.-C. Su",
        "Derui Wang",
        "Yedi Zhang",
        "Minhui Xue",
        "Jie Hao",
        "Jin Song Dong",
        "Yixian Yang"
      ],
      "url": "https://openalex.org/W4417229822",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3188750497",
      "title": "Fairness Properties of Face Recognition and Obfuscation Systems",
      "abstract": "The proliferation of automated face recognition in the commercial and government sectors has caused significant privacy concerns for individuals. One approach to address these privacy concerns is to employ evasion attacks against the metric embedding networks powering face recognition systems: Face obfuscation systems generate imperceptibly perturbed images that cause face recognition systems to misidentify the user. Perturbed faces are generated on metric embedding networks, which are known to be unfair in the context of face recognition. A question of demographic fairness naturally follows: are there demographic disparities in face obfuscation system performance? We answer this question with an analytical and empirical exploration of recent face obfuscation systems. Metric embedding networks are found to be demographically aware: face embeddings are clustered by demographic. We show how this clustering behavior leads to reduced face obfuscation utility for faces in minority groups. An intuitive analytical model yields insight into these phenomena.",
      "year": 2021,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Harrison Rosenberg",
        "Brian Tang",
        "Kassem Fawaz",
        "Somesh Jha"
      ],
      "url": "https://openalex.org/W3188750497",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3047561893",
      "title": "Voiceprint Mimicry Attack Towards Speaker Verification System in Smart Home",
      "abstract": "The advancement of voice controllable systems (VC-Ses) has dramatically affected our daily lifestyle and catalyzed the smart home's deployment. Currently, most VCSes exploit automatic speaker verification (ASV) to prevent various voice attacks (e.g., replay attack). In this study, we present VMask, a novel and practical voiceprint mimicry attack that could fool ASV in smart home and inject the malicious voice command disguised as a legitimate user. The key observation behind VMask is that the deep learning models utilized by ASV are vulnerable to the subtle perturbations in the voice input space. To generate these subtle perturbations, VMask leverages the idea of adversarial examples. Then by adding the subtle perturbations to the recordings from an arbitrary speaker, VMask can mislead the ASV into classifying the crafted speech samples, which mirror the former speaker for human, as the targeted victim. Moreover, psychoacoustic masking is employed to manipulate the adversarial perturbations under human perception threshold, thus making victim unaware of ongoing attacks. We validate the effectiveness of VMask by performing comprehensive experiments on both grey box (VGGVox) and black box (Microsoft Azure Speaker Verification) ASVs. Additionally, a real-world case study on Apple HomeKit proves the VMask's practicability on smart home platforms.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Lei Zhang",
        "Yan Meng",
        "Jiahao Yu",
        "Chong Xiang",
        "Brandon Falk",
        "Haojin Zhu"
      ],
      "url": "https://openalex.org/W3047561893",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3113092146",
      "title": "Composite Adversarial Attacks",
      "abstract": "Adversarial attack is a technique for deceiving Machine Learning (ML) models, which provides a way to evaluate the adversarial robustness. In practice, attack algorithms are artificially selected and tuned by human experts to break a ML system. However, manual selection of attackers tends to be sub-optimal, leading to a mistakenly assessment of model security. In this paper, a new procedure called Composite Adversarial Attack (CAA) is proposed for automatically searching the best combination of attack algorithms and their hyper-parameters from a candidate pool of 32 base attackers. We design a search space where attack policy is represented as an attacking sequence, i.e., the output of the previous attacker is used as the initialization input for successors. Multi-objective NSGA-II genetic algorithm is adopted for finding the strongest attack policy with minimum complexity. The experimental result shows CAA beats 10 top attackers on 11 diverse defenses with less elapsed time (6 \u00d7 faster than AutoAttack), and achieves the new state-of-the-art on linf, l2 and unrestricted adversarial attacks.",
      "year": 2021,
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaofeng Mao",
        "Yuefeng Chen",
        "Shuhui Wang",
        "Hang Su",
        "Yuan He",
        "Hui Xue"
      ],
      "url": "https://openalex.org/W3113092146",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    }
  ]
}