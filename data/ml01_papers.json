{
  "updated": "2026-01-06",
  "total": 49,
  "owasp_id": "ML01",
  "owasp_name": "Input Manipulation Attack",
  "description": "Adversarial attacks on machine learning model inputs. This includes\n        adversarial examples, evasion attacks, perturbation attacks, and any\n        technique that manipulates input data to cause misclassification or\n        incorrect model behavior. Covers both white-box and black-box adversarial\n        attacks, robustness evaluation, and input perturbation methods.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "title": "CipherSteal: Stealing Input Data from TEE-Shielded Neural Networks with Ciphertext Side Channels",
      "abstract": "Shielding neural networks (NNs) from untrusted hosts with Trusted Execution Environments (TEEs) has been increasingly adopted. Nevertheless, this paper shows that the confidentiality of NNs and user data is compromised by the recently disclosed ciphertext side channels in TEEs, which leak memory write patterns of TEE-shielded NNs to malicious hosts. While recent works have used ciphertext side channels to recover cryptographic key bits, the technique does not apply to NN inputs which are more complex and only have partial information leaked. We propose an automated input recovery framework, CipherSteal, and for the first time demonstrate the severe threat of ciphertext side channels to NN inputs. CipherSteal novelly recasts the input recovery as a two-step approach \u2014 information transformation and reconstruction \u2014 and proposes optimizations to fully utilize partial input information leaked in ciphertext side channels. We evaluate CipherSteal on diverse NNs (e.g., Transformer) and image/video inputs, and successfully recover visually identical inputs under different levels of attacker's pre-knowledge towards the target NNs and their inputs. We comprehensively evaluate two popular NN frameworks, TensorFlow and PyTorch, and NN executables generated by two recent NN compilers, TVM and Glow, and study their different attack surfaces. Moreover, we further steal the target NN's functionality by training a surrogate NN with our recovered inputs, and also leverage the surrogate NN to generate \u201cwhite-box\u201d adversarial examples, effectively manipulating the target NN's predictions.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yuanyuan Yuan",
        "Zhibo Liu",
        "Sen Deng",
        "Yanzuo Chen",
        "Shuai Wang",
        "Yinqian Zhang",
        "Zhendong Su"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/69458b5b67892ca281b96cbddd15fa9adcbbd524",
      "pdf_url": "",
      "publication_date": "2025-05-12",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "5bce864b579b376c028ec40a8fec0f999b005d0e",
      "title": "Attack and defense techniques in large language models: A survey and new perspectives",
      "abstract": "Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attacks, optimized attacks, model theft, as well as attacks on LLM applications, detailing their mechanisms and implications. Consequently, we analyze defense strategies, such as prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open issues, including the need for adaptive scalable defenses, adversarial attack detection, generalized defense mechanisms, and ethical and bias concerns. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",
      "year": 2025,
      "venue": "Neural Networks",
      "authors": [
        "Zhiyu Liao",
        "Kang Chen",
        "Y. Lin",
        "Kangkang Li",
        "Yunxuan Liu",
        "Hefeng Chen",
        "Xingwang Huang",
        "Yuanhui Yu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5bce864b579b376c028ec40a8fec0f999b005d0e",
      "pdf_url": "",
      "publication_date": "2025-05-02",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "be0d667c4b910dad74a39e135697d7d7957512e1",
      "title": "Side-Channel Analysis of Integrate-and-Fire Neurons Within Spiking Neural Networks",
      "abstract": "Spiking neural networks gain increasing attention in constraint edge devices due to event-based low-power operation and little resource usage. Such edge devices often allow physical access, opening the door for Side-Channel Analysis. In this work, we introduce a novel robust attack strategy on the neuron level to retrieve the trained parameters of an implemented spiking neural network. Utilizing horizontal correlation power analysis, we demonstrate how to recover the weights and thresholds of a feed-forward spiking neural network implementation. We verify our methodology with real-world measurements of localized electromagnetic emanations of an FPGA design. Additionally, we propose countermeasures against the introduced novel attack approach. We evaluate shuffling and masking as countermeasures to protect the implementation against our proposed attack and demonstrate their effectiveness and limitations.",
      "year": 2025,
      "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
      "authors": [
        "Matthias Probst",
        "Manuel Brosch",
        "G. Sigl"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/be0d667c4b910dad74a39e135697d7d7957512e1",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "title": "Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Siyu Luan",
        "Zhenyi Wang",
        "Li Shen",
        "Zonghua Gu",
        "Chao Wu",
        "Dacheng Tao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e550aeb894034ffacd2fcf2aa39cd744aea2f77c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction defense"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9173c0d483b2fdd5e99c4c39efa51b20b5e9792c",
      "title": "Uncertainty Estimation in Neural Network-enabled Side-channel Analysis and Links to Explainability",
      "abstract": null,
      "year": 2025,
      "venue": "IACR Cryptology ePrint Archive",
      "authors": [
        "Seyedmohammad Nouraniboosjin",
        "Fatameh Ganji"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9173c0d483b2fdd5e99c4c39efa51b20b5e9792c",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b04a1325dca9daa2e421877a206b6881d2a54abd",
      "title": "Data reduction for black-box adversarial attacks against deep neural networks based on side-channel attacks",
      "abstract": null,
      "year": 2025,
      "venue": "Computers & security",
      "authors": [
        "Hanxun Zhou",
        "Zhihui Liu",
        "Yufeng Hu",
        "Shuo Zhang",
        "Longyu Kang",
        "Yong Feng",
        "Yan Wang",
        "Wei Guo",
        "C. Zou"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b04a1325dca9daa2e421877a206b6881d2a54abd",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0a56e4103911d723020bb99bf63f5bb452b236e9",
      "title": "PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization",
      "abstract": "Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner",
      "year": 2025,
      "venue": "",
      "authors": [
        "Mingzhe Li",
        "Renhao Zhang",
        "Zhiyang Wen",
        "Siqi Pan",
        "Bruno Castro da Silva",
        "Juan Zhai",
        "Shiqing Ma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a56e4103911d723020bb99bf63f5bb452b236e9",
      "pdf_url": "",
      "publication_date": "2025-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-12-02"
    },
    {
      "paper_id": "d66b4910d4c16463e1859035cca94bbefbd76f92",
      "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection",
      "abstract": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhengchunmin Dai",
        "Jiaxiong Tang",
        "Peng Sun",
        "Honglong Chen",
        "Liantao Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d66b4910d4c16463e1859035cca94bbefbd76f92",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9d0f0fb82a339debb181382f725baa278bb202d7",
      "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors",
      "abstract": "In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access. We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Wenlong Meng",
        "Shuguo Fan",
        "Chengkun Wei",
        "Min Chen",
        "Yuwei Li",
        "Yuanchao Zhang",
        "Zhikun Zhang",
        "Wenzhi Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d0f0fb82a339debb181382f725baa278bb202d7",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "16b5db7894dde1b49960f03e68b280ac895d86fc",
      "title": "Securing Data From Side-Channel Attacks: A Graph Neural Network-Based Approach for Smartphone-Based Side Channel Attack Detection",
      "abstract": "The widespread use of smartphones has brought convenience and connectivity to the fingertips of the masses. As a result, this has paved the way for potential security vulnerabilities concerning sensitive data, particularly by exploiting side-channel attacks. When typing on a smartphone\u2019s keyboard, its vibrations can be misused to discern the entered characters, thus facilitating side-channel attacks. These smartphone hardware sensors can capture such information while users input sensitive data like personal details, names, email addresses, age, bank details and passwords. This study presents a novel Graph Neural Network (GNN) approach to predict side-channel attacks on smartphone keyboards; different GNN architectures were used, including GNN, DeepGraphNet, Gradient Boosting (GB)+DeepGraphNet, Extreme Gradient Boosting (XGB)+DeepGraphNet and K-Nearest Neighbor (KNN)+DeepGraphNet. The proposed approach detects the side channel attack using vibrations produced while typing on the smartphone soft keyboard. The data was collected from three smartphone sensors, an accelerometer, gyroscope, and magnetometer, and evaluated this data using common evaluation measures such as accuracy, precision, recall, F1-score, ROC curves, confusion matrix and accuracy and loss curves. This study demonstrated that GNN architectures can effectively capture complex relationships in data, making them well-suited for analyzing patterns in smartphone sensor data. Likewise, this research aims to fill a crucial gap by enhancing data privacy in the information entered through smartphone keyboards, shielding it from side-channel attacks by providing an accuracy of 98.26%. Subsequently, the primary objective of this study is to assess the effectiveness of GNN architectures in this precise context. Similarly, the GNN model exhibits compelling performance, achieving accuracy, precision, recall, and f1 score metrics that showcase the model\u2019s effectiveness, with the highest values of 0.98, 0.98, 0.98, and 0.98, respectively. Significantly, the metrics mentioned in the study outperform those documented in the previous literature. Overall, the study contributes to the detection of side-channel smartphone attacks, which advances secure data practices.INDEX TERMS Graph neural networks (GNN), keystroke inference, motion sensors, machine learning, smartphone security, side-channel attacks.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Sidra Abbas",
        "Stephn Ojo",
        "Imen Bouazzi",
        "Gabriel Avelino Sampedro",
        "Abdullah Al Hejaili",
        "Ahmad S. Almadhor",
        "Rastislav Kulh\u00e1nek"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/16b5db7894dde1b49960f03e68b280ac895d86fc",
      "pdf_url": "https://doi.org/10.1109/access.2024.3465662",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "title": "Adversarial Machine Learning In Network Security: A Systematic Review Of Threat Vectors And Defense Mechanisms",
      "abstract": "Adversarial Machine Learning (AML) has emerged as a critical area of research within network security, addressing the evolving challenge of adversaries exploiting machine learning (ML) models. This systematic review adopts the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology to comprehensively examine threat vectors and defense mechanisms in AML. The study identifies, categorizes, and evaluates existing research focused on adversarial attacks targeting ML algorithms in network security applications, including evasion, poisoning, and model extraction attacks. By rigorously following the PRISMA guidelines, a systematic search across multiple scholarly databases yielded a robust dataset of peer-reviewed articles that were screened, reviewed, and analyzed for inclusion. The review outlines key adversarial techniques employed against ML systems, such as gradient-based attack strategies and black-box attacks and explores the underlying vulnerabilities in network security architectures. Additionally, it highlights defense mechanisms, including adversarial training, input preprocessing, and robust model design, discussing their efficacy and limitations in mitigating adversarial threats. The study also identifies critical gaps in current research, such as the lack of standardized benchmarking for adversarial defenses and the need for scalable and real-time AML solutions.",
      "year": 2024,
      "venue": "Innovatech Engineering Journal",
      "authors": [
        "Abdul Awal Mintoo",
        "Ashrafur Rahman Nabil",
        "Md Ashraful Alam",
        "Imran Ahmad"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e1412b3c4eca9421bd6c53432d7b1524ddf17c50",
      "pdf_url": "",
      "publication_date": "2024-11-14",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "title": "Exploring Zero-Day Attacks on Machine Learning and Deep Learning Algorithms",
      "abstract": "In the rapidly evolving field of artificial intelligence, machine learning (ML) and deep learning (DL) algorithms have emerged as powerful tools for solving complex problems in various domains, including cyber security. However, as these algorithms become increasingly prevalent, they also face new security challenges. One of the most significant of these challenges is the threat of zero-day attacks, which exploit unknown and unpredictable vulnerabilities in the algorithms or the data they process. \nThis paper provides a comprehensive overview of zero-day attacks on ML/DL algorithms, exploring their types, causes, effects, and potential countermeasures. The paper begins by introducing the concept and definition of zero-day attacks, providing a clear understanding of this emerging threat. It then reviews the existing research on zero-day attacks on ML/DL algorithms, focusing on three main categories: data poisoning attacks, adversarial input attacks, and model stealing attacks. Each of these attack types poses unique challenges and requires specific countermeasures. \nThe paper also discusses the potential impacts and risks of these attacks on various application domains. For instance, in facial expression recognition, an adversarial input attack could lead to misclassification of emotions, with serious implications for user experience and system integrity. In object classification, a data poisoning attack could cause the algorithm to misidentify critical objects, potentially endangering human lives in applications like autonomous driving. In satellite intersection recognition, a model stealing attack could compromise national security by revealing sensitive information. \nFinally, the paper presents some possible protection methods against zero-day attacks on ML/DL algorithms. These include anomaly detection techniques to identify unusual patterns in the data or the algorithm\u2019s behaviour, model verification and validation methods to ensure the algorithm\u2019s correctness and robustness, federated learning approaches to protect the privacy of the training data, and differential privacy techniques to add noise to the data or the algorithm\u2019s outputs to prevent information leakage. \nThe paper concludes by highlighting some open issues and future directions for research in this area, emphasizing the need for ongoing efforts to secure ML/DL algorithms against zero-day attacks.",
      "year": 2024,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "Marie Kov\u00e1\u0159ov\u00e1"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/87004d053c0c2b0f91c293ef26d817d5a05e017c",
      "pdf_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/2310/2134",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "title": "Power Side-Channel Analysis and Mitigation for Neural Network Accelerators based on Memristive Crossbars",
      "abstract": "The modern trend of exploring Artificial Intelligence (AI) in various industries, such as big data, edge computing, automobile, and medical applications, has increased tremendously. As functionalities grow, energy-efficient hardware for AI devices becomes crucial. To address that, Computation-in-Memory (CiM) using Non-Volatile Memories (NVMs) offers a promising solution. However, security is also an important concern in this computation paradigm. In this work, we analyze the vulnerability for power side-channel attacks on Multiply-Accumulate (MAC) operations implemented in CiM architecture based on emerging NVMs. Our results show that peripheral devices such as Analog-to-Digital Converters (ADCs) leak much more sensitive information than the crossbar itself because of its significant power consumption. Therefore, we propose a circuit-level countermeasure based on hiding for the ADCs of memristive CiM architecture to mitigate the power attacks. The efficiency of our proposed countermeasure is shown by both attacks and leakage assessment methodologies using a maximum of one million measurement traces.",
      "year": 2024,
      "venue": "Asia and South Pacific Design Automation Conference",
      "authors": [
        "Brojogopal Sapui",
        "M. Tahoori"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f49f2057b73ae44fc44a116962435f8e18fcd5ba",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8d0fa6b361fc6319eac027da608a99f03b541111",
      "title": "Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters",
      "abstract": "Deep Neural Networks (DNNs) have made significant strides, but their susceptibility to backdoor attacks still remains a concern. Most defenses typically assume access to white-box models or poisoned data, requirements that are often not feasible in practice, especially for proprietary DNNs. Existing defenses in a black-box setting usually rely on confidence scores of DNN's predictions. However, this exposes DNNs to the risk of model stealing attacks, a significant concern for proprietary DNNs. In this paper, we introduce a novel strategy for detecting back-doors, focusing on a more realistic black-box scenario where only hard-label (i.e., without any prediction confidence) query access is available. Our strategy utilizes data flow dynamics in a computational environment during DNN inference to identify potential backdoor inputs and is agnostic of trigger types or their locations in the input. We observe that a clean image and its corresponding backdoor counterpart with a trigger induce distinct patterns across various microarchitectural activities during the inference phase. We exploit these variations captured by Hardware Performance Counters (HPCs) and use principles of the Gaussian Mixture Model to detect backdoor inputs. To the best of our knowledge, this is the first work that utilizes HPCs for detecting backdoors in DNNs. Extensive evaluation considering a range of benchmark datasets, DNN architectures, and trigger patterns shows the efficacy of the proposed method in distinguishing between clean and backdoor inputs using HPCs.",
      "year": 2024,
      "venue": "Design, Automation and Test in Europe",
      "authors": [
        "Manaar Alam",
        "Yue Wang",
        "Michail Maniatakos"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8d0fa6b361fc6319eac027da608a99f03b541111",
      "pdf_url": "",
      "publication_date": "2024-03-25",
      "keywords_matched": [
        "model stealing",
        "model stealing attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "704d6ace88e1f5cbea2a6465cc627e196a6fe440",
      "title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces",
      "abstract": "In the domain of black-box model extraction, conventional methods reliant on soft labels or surrogate datasets struggle with scaling to high-dimensional input spaces and managing the complexity of an extensive array of interrelated classes. In this work, we present a novel approach that utilizes SHAP (SHapley Additive exPlanations) to enhance synthetic data generation. SHAP quantifies the individual contributions of each input feature towards the victim model's output, facilitating the optimization of an energy-based GAN towards a desirable output. This method significantly boosts performance, achieving a 16.45% increase in the accuracy of image classification models and extending to video classification models with an average improvement of 26.11% and a maximum of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics 600, and Something-Something V2. We further demonstrate the effectiveness and practical utility of our method under various scenarios, including the availability of top-k prediction probabilities, top-k prediction labels, and top-1 labels.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Somnath Sendhil Kumar",
        "Yuvaraj Govindarajulu",
        "Pavan Kulkarni",
        "Manojkumar Parmar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/704d6ace88e1f5cbea2a6465cc627e196a6fe440",
      "pdf_url": "",
      "publication_date": "2024-08-04",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "title": "The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning",
      "abstract": "Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.",
      "year": 2023,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "M. Rigaki",
        "S. Garc\u00eda"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/a85c53617666fe896e1c5834d21dcb30c5de1b8b",
      "pdf_url": "",
      "publication_date": "2023-08-31",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "title": "Power2Picture: Using Generative CNNs for Input Recovery of Neural Network Accelerators through Power Side-Channels on FPGAs",
      "abstract": "Artificial neural networks pervade almost all areas of today's life, being used for both simple image classification tasks as well as highly complex decision making in mission-critical tasks. This makes artificial neural networks an attractive target for attackers to recover the model architecture or user inputs and outputs through either classical software vulnerabilities or hardware side-channel and fault attacks. With increasing complexity of the models, smaller companies now often opt for pre-trained public models, which are then used with potentially sensitive inputs, for instance, in medical applications. In this work, we present a novel remote side-channel attack methodology to steal neural network inputs using generative convolutional neural networks. After measuring voltage fluctuations using on-chip sensors, we are able to recover the original inputs to image classifiers on different FPGA platforms. Our results prove the effectiveness of our attack, as we are able to recover inputs from networks running on different devices, with different datasets, and under different operating conditions.",
      "year": 2023,
      "venue": "IEEE Symposium on Field-Programmable Custom Computing Machines",
      "authors": [
        "Lukas Huegle",
        "M. Gotthard",
        "Vincent Meyers",
        "Jonas Krautter",
        "Dennis R. E. Gnad",
        "M. Tahoori"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9809d3a7e1b49a659d86fbdf88b3e29f326d05e5",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "steal neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators with Adversarial Attacks",
      "abstract": "Artificial Intelligence (AI) hardware accelerators have been widely adopted to enhance the efficiency of deep learning applications. However, they also raise security concerns regarding their vulnerability to power side-channel attacks (SCA). In these attacks, the adversary exploits unintended communication channels to infer sensitive information processed by the accelerator, posing significant privacy and copyright risks to the models. Advanced machine learning algorithms are further employed to facilitate the side-channel analysis and exacerbate the privacy issue of AI accelerators. Traditional defense strategies naively inject execution noise to the runtime of AI models, which inevitably introduce large overheads. In this paper, we present AIAShield, a novel defense methodology to safeguard FPGA-based AI accelerators and mitigate model extraction threats via power-based SCAs. The key insight of AIAShield is to leverage the prominent adversarial attack technique from the machine learning community to craft delicate noise, which can significantly obfuscate the adversary's side-channel observation while incurring minimal overhead to the execution of the protected model. At the hardware level, we design a new module based on ring oscillators to achieve fine-grained noise generation. At the algorithm level, we repurpose Neural Architecture Search to worsen the adversary's extraction results. Extensive experiments on the Nvidia Deep Learning Accelerator (NVDLA) demonstrate that AIAShield outperforms existing solutions with excellent transferability.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xiaobei Yan",
        "Chip Hong Chang",
        "Tianwei Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c2e1a7c575e0a10c5493b5390b4c6ce321c6cf8d",
      "pdf_url": "",
      "publication_date": "2023-12-07",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "6587fab69696a41a5186226f87bf0f4552f794a6",
      "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Abdullah Caglar Oksuz",
        "Anisa Halimi",
        "Erman Ayday"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/6587fab69696a41a5186226f87bf0f4552f794a6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "8796b36ed1a74a1c9fdafe2409c981102983653f",
      "title": "On Function-Coupled Watermarks for Deep Neural Networks",
      "abstract": "Well-performed deep neural networks (DNNs) generally require massive labeled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers can claim IP ownership by retrieving their embedded watermarks. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning, model pruning, and model extraction. In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model\u2019s performance on normal inputs. Specifically, on one hand, we sample inputs from the original training dataset and fuse them as watermark images. On the other hand, we randomly mask model weights during training to distribute the watermark information in the network. Our method can successfully defend against common watermark removal attacks, watermark ambiguity attacks, and existing widely used backdoor detection methods, outperforming existing solutions as demonstrated by evaluation results on various benchmarks. Our code is available at: https://github.com/cure-lab/Function-Coupled-Watermark.",
      "year": 2023,
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems",
      "authors": [
        "Xiangyu Wen",
        "Yu Li",
        "Weizhen Jiang",
        "Qian-Lan Xu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8796b36ed1a74a1c9fdafe2409c981102983653f",
      "pdf_url": "https://doi.org/10.1109/jetcas.2024.3476386",
      "publication_date": "2023-02-08",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "title": "Robot Mimicry Attack on Keystroke-Dynamics User Identification and Authentication System",
      "abstract": "Future robots will be very advanced with high flexibility and accurate control performance. They will have the ability to mimic human behaviours or even perform better, which raises the significant risk of robot attack. In this work, we study the robot mimic attack on the current keystroke-dynamic user authentication system. Specifically, we proposed a robot mimicry attack framework for keystroke-dynamics systems. We collected keyboard logging data and acoustical signal data from real users and extracted the timing pattern of keystrokes to understand victim's behaviour for robot imitation attacks. Furthermore, we develop a deep Q-Network (DQN) algorithm to control the velocity of robot which is one of the key challenges of forging the human typing timing features. We tested and evaluated our approach on the real-life robotic testbed. We presented our results considering user identification and user authentication performance. We achieved a 90.3% user identification accuracy with genuine keyboard logging data samples and 89.6% accuracy with robot-forged data samples. Furthermore, we achieved 11.1%, and 36.6% EER for user authentication performance with zero-effort attack, and robot mimicry attack, respectively.",
      "year": 2023,
      "venue": "IEEE International Conference on Robotics and Automation",
      "authors": [
        "Rongyu Yu",
        "Burak Kizilkaya",
        "Zhen Meng",
        "Emma Li",
        "Guodong Zhao",
        "M. Imran"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/de9f8fe71c37741aa0ff999ca97d71cfabc37b52",
      "pdf_url": "https://eprints.gla.ac.uk/289833/2/289833.pdf",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers",
      "abstract": "We introduce the problem of model-extraction attacks in cyber-physical systems in which an attacker attempts to estimate (or extract) the feedback controller of the system. Extracting (or estimating) the controller provides an unmatched edge to attackers since it allows them to predict the future control actions of the system and plan their attack accordingly. Hence, it is important to understand the ability of the attackers to perform such an attack. In this paper, we focus on the setting when a Deep Neural Network (DNN) controller is trained using Reinforcement Learning (RL) algorithms and is used to control a stochastic system. We play the role of the attacker that aims to estimate such an unknown DNN controller, and we propose a two-phase algorithm. In the first phase, also called the offline phase, the attacker uses side-channel information about the RL-reward function and the system dynamics to identify a set of candidate estimates of the unknown DNN. In the second phase, also called the online phase, the attacker observes the behavior of the unknown DNN and uses these observations to shortlist the set of final policy estimates. We provide theoretical analysis of the error between the unknown DNN and the estimated one. We also provide numerical results showing the effectiveness of the proposed algorithm.",
      "year": 2023,
      "venue": "IEEE Conference on Decision and Control",
      "authors": [
        "Momina Sajid",
        "Yanning Shen",
        "Yasser Shoukry"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4612f541d3a548bc1b87e42b82e61d37bb8cd66b",
      "pdf_url": "http://arxiv.org/pdf/2304.13090",
      "publication_date": "2023-04-25",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "title": "Proof of Spacetime as a Defensive Technique Against Model Extraction Attacks",
      "abstract": "\u2014When providing a service that utilizes a machine learning model, the countermeasures against cyber-attacks are required. The model extraction attack is one of the attacks, in which an attacker attempts to replicate the model by obtaining a large number of input-output pairs. While a defense using Proof of Work has already been proposed, an attacker can still conduct model extraction attacks by increasing their computational power. Moreover, this approach leads to unnecessary energy consumption and might not be environmentally friendly. In this paper, the defense method using Proof of Spacetime instead of Proof of Work is proposed to reduce the energy consumption. The Proof of Spacetime is a method to impose spatial and temporal costs on the users of the service. While the Proof of Work makes a user to calculate until permission is granted, the Proof of Spacetime makes a user to keep a result of calculation, so the energy consumption is reduced. Through computer simulations, it was found that systems with Proof of Spacetime, compared to those with Proof of Work, impose 0.79 times the power consumption and 1.07 times the temporal cost on the attackers, while 0.73 times and 0.64 times on the non-attackers. Therefore, the system with Proof of Spacetime can prevent model extraction attacks with lower energy consumption.",
      "year": 2023,
      "venue": "International Journal of Advanced Computer Science and Applications",
      "authors": [
        "Tatsuki Fukuda"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e6c21c16e7b739ff19d90d3432caf01e701c1bf9",
      "pdf_url": "http://thesai.org/Downloads/Volume14No6/Paper_11-Proof_of_Spacetime_as_a_Defensive_Technique_Against_Model.pdf",
      "publication_date": null,
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "96d3cd0854085c21595fb8efec867416af44c8b5",
      "title": "NaturalFinger: Generating Natural Fingerprint with Generative Adversarial Networks",
      "abstract": "Deep neural network (DNN) models have become a critical asset of the model owner as training them requires a large amount of resource (i.e. labeled data). Therefore, many fingerprinting schemes have been proposed to safeguard the intellectual property (IP) of the model owner against model extraction and illegal redistribution. However, previous schemes adopt unnatural images as the fingerprint, such as adversarial examples and noisy images, which can be easily perceived and rejected by the adversary. In this paper, we propose NaturalFinger which generates natural fingerprint with generative adversarial networks (GANs). Besides, our proposed NaturalFinger fingerprints the decision difference areas rather than the decision boundary, which is more robust. The application of GAN not only allows us to generate more imperceptible samples, but also enables us to generate unrestricted samples to explore the decision boundary.To demonstrate the effectiveness of our fingerprint approach, we evaluate our approach against four model modification attacks including adversarial training and two model extraction attacks. Experiments show that our approach achieves 0.91 ARUC value on the FingerBench dataset (154 models), exceeding the optimal baseline (MetaV) over 17\\%.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kan Yang",
        "Kunhao Lai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96d3cd0854085c21595fb8efec867416af44c8b5",
      "pdf_url": "http://arxiv.org/pdf/2305.17868",
      "publication_date": "2023-05-29",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models",
      "abstract": "Neural text ranking models have witnessed significant advancement and are increasingly being deployed in practice. Unfortunately, they also inherit adversarial vulnerabilities of general neural models, which have been detected but remain underexplored by prior studies. Moreover, the inherit adversarial vulnerabilities might be leveraged by blackhat SEO to defeat better-protected search engines. In this study, we propose an imitation adversarial attack on black-box neural passage ranking models. We first show that the target passage ranking model can be transparentized and imitated by enumerating critical queries/candidates and then train a ranking imitation model. Leveraging the ranking imitation model, we can elaborately manipulate the ranking results and transfer the manipulation attack to the target ranking model. For this purpose, we propose an innovative gradient-based attack method, empowered by the pairwise objective function, to generate adversarial triggers, which causes premeditated disorderliness with very few tokens. To equip the trigger camouflages, we add the next sentence prediction loss and the language model fluency constraint to the objective function. Experimental results on passage ranking demonstrate the effectiveness of the ranking imitation attack model and adversarial triggers against various SOTA neural ranking models. Furthermore, various mitigation analyses and human evaluation show the effectiveness of camouflages when facing potential mitigation approaches. To motivate other scholars to further investigate this novel and important problem, we make the experiment data and code publicly available.",
      "year": 2022,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiawei Liu",
        "Yangyang Kang",
        "Di Tang",
        "Kaisong Song",
        "Changlong Sun",
        "Xiaofeng Wang",
        "Wei Lu",
        "Xiaozhong Liu"
      ],
      "citation_count": 52,
      "url": "https://www.semanticscholar.org/paper/d03f4ca6facd18b30ab4c6034350d430bca0bc33",
      "pdf_url": "",
      "publication_date": "2022-09-14",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
      "abstract": "Machine learning algorithms are used to construct a mathematical model for a system based on training data. Such a model is capable of making highly accurate predictions without being explicitly programmed to do so. These techniques have a great many applications in all areas of the modern digital economy and artificial intelligence. More importantly, these methods are essential for a rapidly increasing number of safety-critical applications such as autonomous vehicles and intelligent defense systems. However, emerging adversarial learning attacks pose a serious security threat that greatly undermines further such systems. The latter are classified into four types, evasion (manipulating data to avoid detection), poisoning (injection malicious training samples to disrupt retraining), model stealing (extraction), and inference (leveraging over-generalization on training data). Understanding this type of attacks is a crucial first step for the development of effective countermeasures. The paper provides a detailed tutorial on the principles of adversarial machining learning, explains the different attack scenarios, and gives an in-depth insight into the state-of-art defense mechanisms against this rising threat .",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Cato Pauling",
        "Michael Gimson",
        "Muhammed Qaid",
        "Ahmad Kida",
        "Basel Halak"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ee67b5e85769018e09d76345e648b29ec0cfa8b3",
      "pdf_url": "",
      "publication_date": "2022-02-21",
      "keywords_matched": [
        "model stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "title": "Protecting Deep Neural Network Intellectual Property with Architecture-Agnostic Input Obfuscation",
      "abstract": "Deep Convolutional Neural Networks (DCNNs) have revolutionized and improved many aspects of modern life. However, these models are increasingly more complex, and training them to perform at desirable levels is difficult undertaking; hence, the trained parameters represent a valuable intellectual property (IP) asset which a motivated attacker may wish to steal. To better protect the IP, we propose a method of lightweight input obfuscation that is undone prior to inference, where input data is obfuscated in order to use the model to specification. Without using the correct key and unlocking sequence, the accuracy of the classifier is reduced to a random guess, thus protecting the input/output interface and mitigating model extraction attacks which rely on such access. We evaluate the system using a VGG-16 network trained on CIFAR-10, and demonstrate that with an incorrect deobfuscation key or sequence, the classification accuracy drops to a random guess, with an inference timing overhead of 4.4% on an Nvidia-based evaluation platform. The system avoids the costs associated with retraining and has no impact on model accuracy for authorized users.",
      "year": 2022,
      "venue": "ACM Great Lakes Symposium on VLSI",
      "authors": [
        "Brooks Olney",
        "Robert Karam"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/0e9b66bd253a7fd0934a72e350aeed54e0a92df4",
      "pdf_url": "",
      "publication_date": "2022-06-06",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "ea411e6f457f45d59a74d79853a8d65ff7afdf22",
      "title": "Using Convolutional Neural Network to Redress Outliers in Clustering Based Side-Channel Analysis on Cryptosystem",
      "abstract": null,
      "year": 2022,
      "venue": "International Conference on Smart Computing and Communication",
      "authors": [
        "Anzhou Wang",
        "Shulin He",
        "Congming Wei",
        "Shaofei Sun",
        "Yaoling Ding",
        "Jiayao Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ea411e6f457f45d59a74d79853a8d65ff7afdf22",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "06992347fb403d457fb09063c528fb68a50547c3",
      "title": "Continuous Authentication against Collusion Attacks",
      "abstract": "As mobile devices become more and more popular, users gain many conveniences. It has also made smartphone makers install new software and prebuilt hardware on their products, including many kinds of sensors. With improved storage and computing power, users also become accustomed to storing and interacting with personally sensitive information. Due to convenience and efficiency, mobile devices use gait authentication widely. In recent years, protecting the information security of mobile devices has become increasingly important. It has become a hot research area because smartphones are vulnerable to theft or unauthorized access. This paper proposes a novel attack model called a collusion attack. Firstly, we study the imitation attack in the general state and its results and propose and verify the feasibility of our attack. We propose a collusion attack model and train participants with quantified action specifications. The results demonstrate that our attack increases the attacker\u2019s false match rate only using an acceleration sensor in some systems sensor. Furthermore, we propose a multi-cycle defense model based on acceleration direction changes to improve the robustness of smartphone-based gait authentication methods against such attacks. Experimental results show that our defense model can significantly reduce the attacker\u2019s success rate.",
      "year": 2022,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Pin Lyu",
        "Wandong Cai",
        "Yao Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/06992347fb403d457fb09063c528fb68a50547c3",
      "pdf_url": "https://www.mdpi.com/1424-8220/22/13/4711/pdf?version=1655896310",
      "publication_date": "2022-06-22",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "title": "Watermarking Graph Neural Networks based on Backdoor Attacks",
      "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary.This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. We also explore our watermarking mechanism against an adaptive attacker with access to partial knowledge of the watermarked data. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks.",
      "year": 2021,
      "venue": "European Symposium on Security and Privacy",
      "authors": [
        "Jing Xu",
        "S. Picek"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4b3eefba6fb0051ec4cecf30dce8b432c242f2b1",
      "pdf_url": "https://repository.ubn.ru.nl//bitstream/handle/2066/295585/295585.pdf",
      "publication_date": "2021-10-21",
      "keywords_matched": [
        "model stealing",
        "model extraction",
        "model stealing attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "fe139f3d17dc9460332c198be352080b964ea939",
      "title": "Yes We can: Watermarking Machine Learning Models beyond Classification",
      "abstract": "Since machine learning models have become a valuable asset for companies, watermarking techniques have been developed to protect the intellectual property of these models and prevent model theft. We observe that current watermarking frameworks solely target image classification tasks, neglecting a considerable part of machine learning techniques. In this paper, we propose to address this lack and study the watermarking process of various machine learning techniques such as machine translation, regression, binary image classification and reinforcement learning models. We adapt current definitions to each specific technique and we evaluate the main characteristics of the watermarking process, in particular the robustness of the models against a rational adversary. We show that watermarking models beyond classification is possible while preserving their overall performance. We further investigate various attacks and discuss the importance of the performance metric in the verification process and its impact on the success of the adversary.",
      "year": 2021,
      "venue": "IEEE Computer Security Foundations Symposium",
      "authors": [
        "Sofiane Lounici",
        "M. Njeh",
        "Orhan Ermis",
        "Melek \u00d6nen",
        "S. Trabelsi"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/fe139f3d17dc9460332c198be352080b964ea939",
      "pdf_url": "https://hal.archives-ouvertes.fr/hal-03220793/file/publi-6532.pdf",
      "publication_date": "2021-06-01",
      "keywords_matched": [
        "model theft",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "2c47435498b8e67873cce1cc86cfbf8397b18286",
      "title": "Timing Black-Box Attacks: Crafting Adversarial Examples through Timing Leaks against DNNs on Embedded Devices",
      "abstract": "Deep neural networks (DNNs) have been applied to various industries. In particular, DNNs on embedded devices have attracted considerable interest because they allow real-time and distributed processing on site. However, adversarial examples (AEs), which add small perturbations to the input data of DNNs to cause misclassification, are serious threats to DNNs. In this paper, a novel black-box attack is proposed to craft AEs based only on processing time, i.e., the side-channel leaks from DNNs on embedded devices. Unlike several existing black-box attacks that utilize output probability, the proposed attack exploits the relationship between the number of activated nodes and processing time without using training data, model architecture, parameters, substitute models, or output probability. The perturbations for AEs are determined by the differential processing time based on the input data of the DNNs in the proposed attack. The experimental results show that the AEs of the proposed attack effectively cause an increase in the number of activated nodes and the misclassification of one of the incorrect labels against the DNNs on a microcontroller unit. Moreover, these results indicate that the attack can evade gradient-masking and confidence reduction countermeasures, which conceal the output probability, to prevent the crafting of AEs against several black-box attacks. Finally, the countermeasures against the attack are implemented and evaluated to clarify that the implementation of an activation function with data-dependent timing leaks is the cause of the proposed attack.",
      "year": 2021,
      "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.",
      "authors": [
        "Tsunato Nakai",
        "Daisuke Suzuki",
        "T. Fujino"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/2c47435498b8e67873cce1cc86cfbf8397b18286",
      "pdf_url": "https://doi.org/10.46586/tches.v2021.i3.149-175",
      "publication_date": null,
      "keywords_matched": [
        "DNN weights leakage (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "e651f76459263a85e4123a6838b71aaf5c645358",
      "title": "A New Foe in GPUs: Power Side-Channel Attacks on Neural Network",
      "abstract": null,
      "year": 2021,
      "venue": "IEEE International Symposium on Quality Electronic Design",
      "authors": [
        "Hyeran Jeon",
        "Nima Karimian",
        "T. Lehman"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e651f76459263a85e4123a6838b71aaf5c645358",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "866dab5d9f5ec63ae4b323479266b2a20c7150b2",
      "title": "Parasite: Mitigating Physical Side-Channel Attacks Against Neural Networks",
      "abstract": null,
      "year": 2021,
      "venue": "SPACE",
      "authors": [
        "H. Chabanne",
        "J. Danger",
        "Linda Guiga",
        "U. K\u00fchne"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/866dab5d9f5ec63ae4b323479266b2a20c7150b2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "side-channel attack (title)",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "title": "Model Extraction and Adversarial Attacks on Neural Networks Model Extraction and Adversarial Attacks on Neural Networks Using Side-Channel Information Using Side-Channel Information",
      "abstract": null,
      "year": 2021,
      "venue": "",
      "authors": [
        "Tommy Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b9f14b52cf9c01e651a4c8e9b04344e9c3929ddc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "title": "Neural Network Model Extraction Based on Adversarial Examples",
      "abstract": "The neural network model has been applied to all walks of life. By detecting the internal information of a black-box model, the attacker can obtain potential commercial value of the model. At the same time, understanding the model structure helps the attacker customize the strategy to attack the model. We have improved a model detection method based on input and output pairs to detect the internal information of the trained neural network black-box model. On the one hand, our work proved that adversarial examples are very likely to carry architecture information of the neural network model. On the other hand, we added adversarial examples to the model pre-detection module, and verified the positive effects of adversarial examples on model detection through experiments, which improved the accuracy of the meta-model and reduced the cost of model detection.",
      "year": 2021,
      "venue": "International Conference on Advanced Information Science and System",
      "authors": [
        "Huiwen Fang",
        "Chunhua Wu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3f4a4c3d028bffe080b97fdf59a290b9d708955d",
      "pdf_url": "",
      "publication_date": "2021-11-26",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "title": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
      "abstract": "We consider an adversary looking to steal or attack a black-box machine translation (MT) system, either for financial gain or to exploit model errors. We first show that black-box MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their victims. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades imitation model BLEU and attack transfer rates at some cost in BLEU and inference speed.",
      "year": 2020,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Eric Wallace",
        "Mitchell Stern",
        "D. Song"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/d73561ab8318ce343f5cb15f96c74f210b6b24fa",
      "pdf_url": "https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf",
      "publication_date": "2020-04-30",
      "keywords_matched": [
        "model stealing",
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "82132f7abf204b13cc171980781f76a0d627a970",
      "title": "Towards Security Threats of Deep Learning Systems: A Survey",
      "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning\u2019s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches\u2019 merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",
      "year": 2020,
      "venue": "IEEE Transactions on Software Engineering",
      "authors": [
        "Yingzhe He",
        "Guozhu Meng",
        "Kai Chen",
        "Xingbo Hu",
        "Jinwen He"
      ],
      "citation_count": 103,
      "url": "https://www.semanticscholar.org/paper/82132f7abf204b13cc171980781f76a0d627a970",
      "pdf_url": "https://arxiv.org/pdf/1911.12562",
      "publication_date": "2020-10-27",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "title": "Stealing Deep Reinforcement Learning Models for Fun and Profit",
      "abstract": "This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions, and then leverages state-of-the-art imitation learning techniques to replicate the model from the identified algorithm family. Experimental results indicate that our methodology can effectively recover the DRL models with high fidelity and accuracy. We also demonstrate two use cases to show that our model extraction attack can (1) significantly improve the success rate of adversarial attacks, and (2) steal DRL models stealthily even they are protected by DNN watermarks. These pose a severe threat to the intellectual property and privacy protection of DRL applications.",
      "year": 2020,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Kangjie Chen",
        "Tianwei Zhang",
        "Xiaofei Xie",
        "Yang Liu"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/a7abac76ec8aea31ee595af45d733bd462d7f35b",
      "pdf_url": "https://ink.library.smu.edu.sg/sis_research/7110",
      "publication_date": "2020-06-09",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "title": "MARLeME: A Multi-Agent Reinforcement Learning Model Extraction Library",
      "abstract": "Multi-Agent Reinforcement Learning (MARL) encompasses a powerful class of methodologies that have been applied in a wide range of fields. An effective way to further empower these methodologies is to develop approaches and tools that could expand their interpretability and explainability. In this work, we introduce MARLeME: a MARL model extraction library, designed to improve explainability of MARL systems by approximating them with symbolic models. Symbolic models offer a high degree of interpretability, well-defined properties, and verifiable behaviour. Consequently, they can be used to inspect and better understand the underlying MARL systems and corresponding MARL agents, as well as to replace all/some of the agents that are particularly safety and security critical. In this work, we demonstrate how MARLeME can be applied to two well-known case studies (Cooperative Navigation and RoboCup Takeaway), using extracted models based on Abstract Argumentation.",
      "year": 2020,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Dmitry Kazhdan",
        "Z. Shams",
        "Pietro Lio"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4",
      "pdf_url": "http://arxiv.org/pdf/2004.07928",
      "publication_date": "2020-04-16",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "title": "SCNet: A Neural Network for Automated Side-Channel Attack",
      "abstract": "The side-channel attack is an attack method based on the information gained about implementations of computer systems, rather than weaknesses in algorithms. Information about system characteristics such as power consumption, electromagnetic leaks and sound can be exploited by the side-channel attack to compromise the system. Much research effort has been directed towards this field. However, such an attack still requires strong skills, thus can only be performed effectively by experts. Here, we propose SCNet, which automatically performs side-channel attacks. And we also design this network combining with side-channel domain knowledge and different deep learning model to improve the performance and better to explain the result. The results show that our model achieves good performance with fewer parameters. The proposed model is a useful tool for automatically testing the robustness of computer systems.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Guanlin Li",
        "Chang Liu",
        "Han Yu",
        "Yanhong Fan",
        "Libang Zhang",
        "Zongyue Wang",
        "Meiqin Wang"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/001db238123a7a2c08c4ba8186c9b1a4460d3fb2",
      "pdf_url": "",
      "publication_date": "2020-08-02",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "title": "Stealing Black-Box Functionality Using The Deep Neural Tree Architecture",
      "abstract": "This paper makes a substantial step towards cloning the functionality of black-box models by introducing a Machine learning (ML) architecture named Deep Neural Trees (DNTs). This new architecture can learn to separate different tasks of the black-box model, and clone its task-specific behavior. We propose to train the DNT using an active learning algorithm to obtain faster and more sample-efficient training. In contrast to prior work, we study a complex \"victim\" black-box model based solely on input-output interactions, while at the same time the attacker and the victim model may have completely different internal architectures. The attacker is a ML based algorithm whereas the victim is a generally unknown module, such as a multi-purpose digital chip, complex analog circuit, mechanical system, software logic or a hybrid of these. The trained DNT module not only can function as the attacked module, but also provides some level of explainability to the cloned model due to the tree-like nature of the proposed architecture.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Daniel Teitelman",
        "I. Naeh",
        "Shie Mannor"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a2923ace209e6193effa49a5c254b29d77b2ee0d",
      "pdf_url": "",
      "publication_date": "2020-02-23",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "title": "Leveraging Extracted Model Adversaries for Improved Black Box Attacks",
      "abstract": "We present a method for adversarial input generation against black box models for reading comprehension based question answering. Our approach is composed of two steps. First, we approximate a victim black box model via model extraction. Second, we use our own white box method to generate input perturbations that cause the approximate model to fail. These perturbed inputs are used against the victim. In experiments we find that our method improves on the efficacy of the ADDANY\u2014a white box attack\u2014performed on the approximate model by 25% F1, and the ADDSENT attack\u2014a black box attack\u2014by 11% F1.",
      "year": 2020,
      "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
      "authors": [
        "Naveen Jafer Nizar",
        "Ari Kobren"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/981c7b3cd70e23fce8f8877ccb6ee92810b65e85",
      "pdf_url": "https://www.aclweb.org/anthology/2020.blackboxnlp-1.6.pdf",
      "publication_date": "2020-10-30",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "title": "Open DNN Box by Power Side-Channel Attack",
      "abstract": "Deep neural networks are becoming popular and important assets of many AI companies. However, recent studies indicate that they are also vulnerable to adversarial attacks. Adversarial attacks can be either white-box or black-box. The white-box attacks assume full knowledge of the models while the black-box ones assume none. In general, revealing more internal information can enable much more powerful and efficient attacks. However, in most real-world applications, the internal information of embedded AI devices is unavailable. Therefore, in this brief, we propose a side-channel information based technique to reveal the internal information of black-box models. Specifically, we have made the following contributions: (1) different from previous works, we use side-channel information to reveal internal network architecture in embedded devices; (2) we construct models for internal parameter estimation that no research has been reached yet; and (3) we validate our methods on real-world devices and applications. The experimental results show that our method can achieve 96.50% accuracy on average. Such results suggest that we should pay strong attention to the security problem of many AI devices, and further propose corresponding defensive strategies in the future.",
      "year": 2019,
      "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
      "authors": [
        "Yun Xiang",
        "Zhuangzhi Chen",
        "Zuohui Chen",
        "Zebin Fang",
        "Haiyang Hao",
        "Jinyin Chen",
        "Yi Liu",
        "Zhefu Wu",
        "Qi Xuan",
        "Xiaoniu Yang"
      ],
      "citation_count": 101,
      "url": "https://www.semanticscholar.org/paper/1f25ad13444dcd9898a8500428f2c97bdb05369c",
      "pdf_url": "https://arxiv.org/pdf/1907.10406",
      "publication_date": "2019-07-21",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "f45975189d1f73b313dc65401724a5e21f635b3b",
      "title": "Adversarial Exploitation of Policy Imitation",
      "abstract": "This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",
      "year": 2019,
      "venue": "AISafety@IJCAI",
      "authors": [
        "Vahid Behzadan",
        "W. Hsu"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/f45975189d1f73b313dc65401724a5e21f635b3b",
      "pdf_url": "",
      "publication_date": "2019-06-03",
      "keywords_matched": [
        "model extraction",
        "model extraction attack",
        "(via citation)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "title": "Poster: Recovering the Input of Neural Networks via Single Shot Side-channel Attacks",
      "abstract": "The interplay between machine learning and security is becoming more prominent. New applications using machine learning also bring new security risks. Here, we show it is possible to reverse-engineer the inputs to a neural network with only a single-shot side-channel measurement assuming the attacker knows the neural network architecture being used.",
      "year": 2019,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "L. Batina",
        "S. Bhasin",
        "Dirmanto Jap",
        "S. Picek"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/6d3c97ec0e86b1291b33de4e9777df545770ff70",
      "pdf_url": "",
      "publication_date": "2019-11-06",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "65d21b754a788782f05e43d6786e342ba8284cff",
      "title": "Neutralizing BLE Beacon-Based Electronic Attendance System Using Signal Imitation Attack",
      "abstract": "Many emerging location- or proximity-based applications use Bluetooth low energy (BLE) beacons thanks to the increasing popularity of the technology in mobile systems. An outstanding example is the BLE beacon-based electronic attendance system (BEAS) used in many universities today to increase the efficiency of lectures. Despite its popularity and usefulness, however, BEAS has not been thoroughly analyzed for its potential vulnerabilities. In this paper, we neutralize a university\u2019s BEAS by maliciously cheating attendance (i.e., faking attendance while the subject is not physically present at the location) in various scenarios using signal imitation attack, and investigate its possible vulnerabilities. The BEAS exploited in this paper is a commercial system actually used in a well-known university. After the exploitation experiment, we analyze the system\u2019s weaknesses and present possible counter-measures. Furthermore, additional attack methods are shown to re-counteract those possible counter-measures and to discuss the fundamental challenges, deficiencies, and suggestions in electronic attendance systems using BLE beacons.",
      "year": 2018,
      "venue": "IEEE Access",
      "authors": [
        "Moonbeom Kim",
        "Jongho Lee",
        "Jeongyeup Paek"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/65d21b754a788782f05e43d6786e342ba8284cff",
      "pdf_url": "https://doi.org/10.1109/access.2018.2884488",
      "publication_date": "2018-12-03",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "68564f11e79c19195d0e82854a0aa156d7764922",
      "title": "Interpreting Blackbox Models via Model Extraction",
      "abstract": "Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 182,
      "url": "https://www.semanticscholar.org/paper/68564f11e79c19195d0e82854a0aa156d7764922",
      "pdf_url": "",
      "publication_date": "2017-05-23",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0c2a550b30a97abe3766d514098bc1590183a0b3",
      "title": "The Plateau: Imitation Attack Resistance of Gait Biometrics",
      "abstract": null,
      "year": 2010,
      "venue": "IFIP Conference on Policies and Research in Identity Management",
      "authors": [
        "B. Mjaaland"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/0c2a550b30a97abe3766d514098bc1590183a0b3",
      "pdf_url": "",
      "publication_date": "2010-11-18",
      "keywords_matched": [
        "imitation attack"
      ],
      "first_seen": "2025-12-05"
    }
  ]
}