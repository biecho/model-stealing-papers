{
  "updated": "2026-01-06",
  "total": 581,
  "keywords": [
    "prompt injection",
    "prompt injection attack",
    "indirect prompt injection",
    "prompt hacking",
    "prompt attack",
    "LLM injection",
    "inject prompt",
    "malicious prompt",
    "adversarial prompt",
    "prompt manipulation",
    "instruction injection",
    "context injection",
    "system prompt attack",
    "prompt exploit",
    "LLM attack",
    "language model attack",
    "chatbot attack",
    "prompt injection defense",
    "prompt filtering",
    "prompt sanitization",
    "detect prompt injection",
    "prevent prompt injection"
  ],
  "seed_papers": [],
  "papers": [
    {
      "paper_id": "a26a44d8999faf96364718084459e188869dc177",
      "title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1",
      "abstract": "The rapid development of large reasoning models (LRMs), such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source reasoning models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on open LRMs is needed. (2) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (3) Safety thinking emerges in the reasoning process of LRMs, but fails frequently against adversarial attacks. (4) The thinking process in R1 models poses greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models'safety to close the gap.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "KAI-QING Zhou",
        "Chengzhi Liu",
        "Xuandong Zhao",
        "Shreedhar Jangam",
        "Jayanth Srinivasa",
        "Gaowen Liu",
        "D. Song",
        "Xin Eric Wang"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/a26a44d8999faf96364718084459e188869dc177",
      "pdf_url": "",
      "publication_date": "2025-02-18",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f25f0e35058d5a112115f83a6cbb3d3ba16ded42",
      "title": "Defeating Prompt Injections by Design",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77\\%$ of tasks with provable security (compared to $84\\%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Edoardo Debenedetti",
        "Ilia Shumailov",
        "Tianqi Fan",
        "Jamie Hayes",
        "Nicholas Carlini",
        "Daniel Fabian",
        "Christoph Kern",
        "Chongyang Shi",
        "Andreas Terzis",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 66,
      "url": "https://www.semanticscholar.org/paper/f25f0e35058d5a112115f83a6cbb3d3ba16ded42",
      "pdf_url": "",
      "publication_date": "2025-03-24",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a90068513bf2eae66c81ff80b5060ce115097176",
      "title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks",
      "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection methods have limited effectiveness against state-of-the-art attacks, let alone adaptive ones. In this work, we propose DataSentinel, a game-theoretic method to detect prompt injection attacks. Specifically, DataSentinel fine-tunes an LLM to detect inputs contaminated with injected prompts that are strategically adapted to evade detection. We formulate this as a minimax optimization problem, with the objective of fine-tuning the LLM to detect strong adaptive attacks. Furthermore, we propose a gradient-based method to solve the minimax optimization problem by alternating between the inner max and outer min problems. Our evaluation results on multiple benchmark datasets and LLMs show that DataSentinel effectively detects both existing and adaptive prompt injection attacks. Our code and data are available at: https://github.com/liu00222/Open-Prompt-Injection.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Dawn Song",
        "N. Gong"
      ],
      "citation_count": 48,
      "url": "https://www.semanticscholar.org/paper/a90068513bf2eae66c81ff80b5060ce115097176",
      "pdf_url": "",
      "publication_date": "2025-04-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b5fdd8fd5f398c81136fa7dac941f52b9568ad0a",
      "title": "WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks",
      "abstract": "Autonomous UI agents powered by AI have tremendous potential to boost human productivity by automating routine tasks such as filing taxes and paying bills. However, a major challenge in unlocking their full potential is security, which is exacerbated by the agent's ability to take action on their user's behalf. Existing tests for prompt injections in web agents either over-simplify the threat by testing unrealistic scenarios or giving the attacker too much power, or look at single-step isolated tasks. To more accurately measure progress for secure web agents, we introduce WASP -- a new publicly available benchmark for end-to-end evaluation of Web Agent Security against Prompt injection attacks. Evaluating with WASP shows that even top-tier AI models, including those with advanced reasoning capabilities, can be deceived by simple, low-effort human-written injections in very realistic scenarios. Our end-to-end evaluation reveals a previously unobserved insight: while attacks partially succeed in up to 86% of the case, even state-of-the-art agents often struggle to fully complete the attacker goals -- highlighting the current state of security by incompetence.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ivan Evtimov",
        "Arman Zharmagambetov",
        "Aaron Grattafiori",
        "Chuan Guo",
        "Kamalika Chaudhuri"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/b5fdd8fd5f398c81136fa7dac941f52b9568ad0a",
      "pdf_url": "",
      "publication_date": "2025-04-22",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "07734909a75edc13eb838d9f83b52dcc1ea06e64",
      "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
      "abstract": "Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agent's security guardrails.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sa-hana Chennabasappa",
        "Cyrus Nikolaidis",
        "Daniel Song",
        "David Molnar",
        "Stephanie Ding",
        "Shengye Wan",
        "Spencer Whitman",
        "Lauren Deason",
        "Nicholas Doucette",
        "Abraham Montilla",
        "Alekhya Gampa",
        "Beto de Paola",
        "Dominik Gabi",
        "James Crnkovich",
        "Jean-Christophe Testud",
        "Kat He",
        "Rashnil Chaturvedi",
        "Wu Zhou",
        "Joshua Saxe"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/07734909a75edc13eb838d9f83b52dcc1ea06e64",
      "pdf_url": "",
      "publication_date": "2025-05-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0ed8da38a5fb60cd5fc70d325a63e6120111756c",
      "title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents",
      "abstract": "Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks. In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%. This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability. The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.",
      "year": 2025,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Qiusi Zhan",
        "Richard Fang",
        "Henil Shalin Panchal",
        "Daniel Kang"
      ],
      "citation_count": 33,
      "url": "https://www.semanticscholar.org/paper/0ed8da38a5fb60cd5fc70d325a63e6120111756c",
      "pdf_url": "",
      "publication_date": "2025-02-27",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54ea1ebd8a17ee9a88f4ca07ae820359215929f5",
      "title": "Prompt injection attacks on vision language models in oncology",
      "abstract": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be compromised by prompt injection attacks. These can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We perform a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs: Claude-3 Opus, Claude-3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N\u2009=\u2009594 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in manifold medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption. Vision-language artificial intelligence models (VLMs) can be employed to recognize lesions in cancer images. Here, the authors show that VLMs can be misled by prompt injection attacks, producing harmful output and leading to incorrect diagnoses.",
      "year": 2025,
      "venue": "Nature Communications",
      "authors": [
        "J. Clusmann",
        "Dyke Ferber",
        "I. Wiest",
        "Carolin V. Schneider",
        "T. Brinker",
        "S. Foersch",
        "Daniel Truhn",
        "J. Kather"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/54ea1ebd8a17ee9a88f4ca07ae820359215929f5",
      "pdf_url": "https://doi.org/10.1038/s41467-024-55631-x",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb6cbd2b06a980d8fabc1d7615575498385b35c8",
      "title": "Prompt Injection Attack to Tool Selection in LLM Agents",
      "abstract": "Tool selection is a key component of LLM agents. A popular approach follows a two-step process - \\emph{retrieval} and \\emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, DataSentinel, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiawen Shi",
        "Zenghui Yuan",
        "Guiyao Tie",
        "Pan Zhou",
        "Neil Zhenqiang Gong",
        "Lichao Sun"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/eb6cbd2b06a980d8fabc1d7615575498385b35c8",
      "pdf_url": "",
      "publication_date": "2025-04-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "560fc5db493c20f788c7d0a80e0e3a5c1ca1b200",
      "title": "PromptArmor: Simple yet Effective Prompt Injection Defenses",
      "abstract": "Despite their potential, recent research has demonstrated that LLM agents are vulnerable to prompt injection attacks, where malicious prompts are injected into the agent's input, causing it to perform an attacker-specified task rather than the intended task provided by the user. In this paper, we present PromptArmor, a simple yet effective defense against prompt injection attacks. Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove potential injected prompts from the input before the agent processes it. Our results show that PromptArmor can accurately identify and remove injected prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves both a false positive rate and a false negative rate below 1% on the AgentDojo benchmark. Moreover, after removing injected prompts with PromptArmor, the attack success rate drops to below 1%. We also demonstrate PromptArmor's effectiveness against adaptive attacks and explore different strategies for prompting an LLM. We recommend that PromptArmor be adopted as a standard baseline for evaluating new defenses against prompt injection attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tianneng Shi",
        "Kaijie Zhu",
        "Zhun Wang",
        "Yuqi Jia",
        "Will Cai",
        "Weida Liang",
        "Haonan Wang",
        "Hend Alzahrani",
        "Joshua Lu",
        "Kenji Kawaguchi",
        "Basel Alomair",
        "Xuandong Zhao",
        "William Yang Wang",
        "N. Gong",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/560fc5db493c20f788c7d0a80e0e3a5c1ca1b200",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fa4cb03e73a53f67386775c2cab44da0afbe91eb",
      "title": "Progent: Programmable Privilege Control for LLM Agents",
      "abstract": "LLM agents utilize Large Language Models as central components with diverse tools to complete various user tasks, but face significant security risks when interacting with external environments. Attackers can exploit these agents through various vectors, including indirect prompt injection, memory/knowledge base poisoning, and malicious tools, tricking agents into performing dangerous actions such as unauthorized financial transactions or data leakage. The core problem that enables attacks to succeed lies in over-privileged tool access. We introduce Progent, the first privilege control framework to secure LLM agents. Progent enforces security at the tool level by restricting agents to performing tool calls necessary for user tasks while blocking potentially malicious ones. Progent features a domain-specific language that allows for expressing fine-grained policies for controlling tool privileges, flexible fallback actions when calls are blocked, and dynamic policy updates to adapt to changing agent states. The framework operates deterministically at runtime, providing provable security guarantees. Thanks to our modular design, integrating Progent does not alter agent internals and only requires minimal changes to the existing agent implementation, enhancing its practicality and potential for widespread adoption. Our extensive evaluation across various agent use cases, using benchmarks like AgentDojo, ASB, and AgentPoison, demonstrates that Progent reduces attack success rates to 0%, while preserving agent utility and speed. Additionally, we show that LLMs can automatically generate effective policies, highlighting their potential for automating the process of writing Progent's security policies.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tianneng Shi",
        "Jingxuan He",
        "Zhun Wang",
        "Linyu Wu",
        "Hongwei Li",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/fa4cb03e73a53f67386775c2cab44da0afbe91eb",
      "pdf_url": "",
      "publication_date": "2025-04-16",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20723b734e2838c22f921a0fc343d32e1d6d84e7",
      "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
      "abstract": "Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. If we restrict ourselves specifically to works which perform detection rather than direct defense, most of them focus on direct prompt injection attacks, while there are few works for the indirect scenario, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Yuan Sui",
        "Yufei He",
        "Yue Liu",
        "Yangqiu Song",
        "Bryan Hooi"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/20723b734e2838c22f921a0fc343d32e1d6d84e7",
      "pdf_url": "",
      "publication_date": "2025-02-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fc3d1be58cc8fe00f15923493a0b59db2c50004c",
      "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
      "abstract": "Prompt injection attack has been listed as the top-1 security threat to LLM-integrated applications, which interact with external environment data for complex tasks. The untrusted data may contain an injected prompt trying to arbitrarily manipulate the system. Model-level prompt injection defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source secure models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigating prompt injection attacks. To this end, we develop Meta SecAlign, the first fully open-source LLM with built-in model-level defense that achieves commercial-grade performance, powerful enough for complex agentic tasks. We provide complete details of our training recipe, an improved version of the SOTA SecAlign defense. We perform the most comprehensive evaluation to date on 9 utility benchmarks and 7 security benchmarks on general knowledge, instruction following, and agentic workflows. Results show that Meta SecAlign, despite being trained on generic instruction-tuning samples only, surprisingly confers security in unseen downstream tasks, including tool-calling and web-navigation, in addition to general instruction-following. Our best model -- Meta-SecAlign-70B -- establishes a new frontier of utility-security trade-off for open-source LLMs. Even compared to closed-course commercial models such as GPT-5, our model is much securer than most of them. Below are links for the code (https://github.com/facebookresearch/Meta_SecAlign), Meta-SecAlign-70B(https://huggingface.co/facebook/Meta-SecAlign-70B), and Meta-SecAlign-8B(https://huggingface.co/facebook/Meta-SecAlign-8B) models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "David Wagner",
        "Chuan Guo"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/fc3d1be58cc8fe00f15923493a0b59db2c50004c",
      "pdf_url": "",
      "publication_date": "2025-07-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4e92cc52b11da0785362c3b98a8821005d326b45",
      "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
      "abstract": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated remarkable capabilities in various complex tasks, ranging from collaborative problem-solving to autonomous decision-making. However, as these systems become increasingly integrated into critical applications, their vulnerability to adversarial attacks, misinformation propagation, and unintended behaviors have raised significant concerns. To address this challenge, we introduce G-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS, which leverages graph neural networks to detect anomalies on the multi-agent utterance graph and employ topological intervention for attack remediation. Extensive experiments demonstrate that G-Safeguard: (I) exhibits significant effectiveness under various attack strategies, recovering over 40% of the performance for prompt injection; (II) is highly adaptable to diverse LLM backbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS with security guarantees. The code is available at https://github.com/wslong20/G-safeguard.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Shilong Wang",
        "Gui-Min Zhang",
        "Miao Yu",
        "Guancheng Wan",
        "Fanci Meng",
        "Chongye Guo",
        "Kun Wang",
        "Yang Wang"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/4e92cc52b11da0785362c3b98a8821005d326b45",
      "pdf_url": "",
      "publication_date": "2025-02-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5012daa3095e9b6bf6d70d0ec8ffdb381e982687",
      "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
      "abstract": "Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chongyang Shi",
        "Sharon Lin",
        "Shuang Song",
        "Jamie Hayes",
        "Ilia Shumailov",
        "Itay Yona",
        "Juliette Pluto",
        "Aneesh Pappu",
        "Christopher A. Choquette-Choo",
        "Milad Nasr",
        "Chawin Sitawarin",
        "Gena Gibson",
        "Andreas Terzis",
        "JohnFourFlynn"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/5012daa3095e9b6bf6d70d0ec8ffdb381e982687",
      "pdf_url": "",
      "publication_date": "2025-05-20",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46a9f0dc9f74bef40c2f860e604c338c8092d30e",
      "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptible to adversarial attacks such as prompt injection and jailbreaks that override alignment safeguards. This paper provides a systematic investigation of jailbreak strategies against various state-of-the-art LLMs. We categorize over 1,400 adversarial prompts, analyze their success against GPT-4, Claude 2, Mistral 7B, and Vicuna, and examine their generalizability and construction logic. We further propose layered mitigation strategies and recommend a hybrid red-teaming and sandboxing approach for robust LLM security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chetan Pathade"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/46a9f0dc9f74bef40c2f860e604c338c8092d30e",
      "pdf_url": "",
      "publication_date": "2025-05-07",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a02a9e21fc116194570201d84dca7bb2ff85de71",
      "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
      "abstract": "As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discuss their trade-offs in terms of utility and security, and illustrate their real-world applicability through a series of case studies.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Luca Beurer-Kellner",
        "Beat Buesser Ana-Maria Crectu",
        "Edoardo Debenedetti",
        "Daniel Dobos",
        "Daniel Fabian",
        "Marc Fischer",
        "David Froelicher",
        "Kathrin Grosse",
        "Daniel Naeff",
        "Ez-inwanne Ozoani",
        "Andrew Paverd",
        "Florian Tram\u00e8r",
        "V'aclav Volhejn"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/a02a9e21fc116194570201d84dca7bb2ff85de71",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8c47e94a804f80c70d0bc5add76f803f2405d6ce",
      "title": "Fine-Tuning Multimodal Vision-Language Models for Brain CT Diagnosis via a Triple-Branch Framework",
      "abstract": "Medical image analysis, particularly for brain CT scans, requires advanced techniques to improve diagnostic accuracy and efficiency. Existing multimodal models often struggle with the complexities of domain-specific medical images and their associated reports. This article presents TriMedTune, a novel triple-branch framework designed to fine-tune multimodal models for brain CT diagnosis. The proposed framework consists of three core components: Hierarchical Visual Prompt Injection (HVPI), Diagnostic Alignment for Terminology Accuracy (DATA), and Medical Knowledge Distillation with Uncertainty Regularization (MKD-UR). These modules enhance the model\u2019s ability to process and interpret medical images by improving visual context understanding, aligning diagnostic terminology, and ensuring robust performance through knowledge distillation and uncertainty modeling. The framework utilizes efficient training strategies, including LoRA-based fine-tuning, dynamic prompt sampling, and mixed-precision optimization. Experimental evaluations show that TriMedTune outperforms state-of-the-art models across multiple performance metrics, demonstrating its effectiveness in medical image diagnosis and report generation.",
      "year": 2025,
      "venue": "2025 2nd International Conference on Digital Image Processing and Computer Applications (DIPCA)",
      "authors": [
        "Xiong Luo"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/8c47e94a804f80c70d0bc5add76f803f2405d6ce",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6da58f1bf1a4873c15f0862dd58ecdde4fe6a029",
      "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
      "abstract": "The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this paper, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attacker embeds adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agents' decision-making process and execute unauthorized tasks. Our approach incorporates two key coordinated components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to construct the black-box defensive system prompt through adversarial meta-prompting and generate a malicious textual command based on it that steers the agents' output toward better compliance with attacker's requests. Extensive experiments demonstrate that our method outperforms state-of-the-art attacks, achieving at least a +30.1% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications. Code can be found in https://github.com/Larry0454/CrossInject.",
      "year": 2025,
      "venue": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "authors": [
        "Le Wang",
        "Zonghao Ying",
        "Tianyuan Zhang",
        "Siyuan Liang",
        "Shengshan Hu",
        "Mingchuan Zhang",
        "Aishan Liu",
        "Xianglong Liu"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/6da58f1bf1a4873c15f0862dd58ecdde4fe6a029",
      "pdf_url": "",
      "publication_date": "2025-04-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c0874f6db5eac70f2f4c000c66541ce39bde4d3a",
      "title": "Securing AI Agents with Information-Flow Control",
      "abstract": "As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner that tracks confidentiality and integrity labels, deterministically enforces security policies, and introduces novel primitives for selectively hiding information. Its evaluation in AgentDojo demonstrates that this approach enables us to complete a broad range of tasks with security guarantees. A tutorial to walk readers through the the concepts introduced in the paper can be found at https://github.com/microsoft/fides",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Manuel Costa",
        "Boris K\u00f6pf",
        "Aashish Kolluri",
        "Andrew Paverd",
        "M. Russinovich",
        "Ahmed Salem",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-B'eguelin"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/c0874f6db5eac70f2f4c000c66541ce39bde4d3a",
      "pdf_url": "",
      "publication_date": "2025-05-29",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a376a054511881325218073b42461048b456b819",
      "title": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
      "abstract": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Peter Yong Zhong",
        "Siyuan Chen",
        "Ruiqi Wang",
        "McKenna McCall",
        "Ben L. Titzer",
        "Heather Miller",
        "Phillip B. Gibbons"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/a376a054511881325218073b42461048b456b819",
      "pdf_url": "",
      "publication_date": "2025-02-13",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "830cb22483595ec0421398af195842d788e4ea6e",
      "title": "MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents",
      "abstract": "Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs. Code is available at https://github.com/kaijiezhu11/MELON.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Kaijie Zhu",
        "Xianjun Yang",
        "Jindong Wang",
        "Wenbo Guo",
        "William Yang Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/830cb22483595ec0421398af195842d788e4ea6e",
      "pdf_url": "",
      "publication_date": "2025-02-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3c4ceccf9d7c70bd4d77f0ee566a15e9ed084cc1",
      "title": "Security Concerns for Large Language Models: A Survey",
      "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.",
      "year": 2025,
      "venue": "Journal of Information Security and Applications",
      "authors": [
        "Miles Q. Li",
        "Benjamin C. M. Fung"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/3c4ceccf9d7c70bd4d77f0ee566a15e9ed084cc1",
      "pdf_url": "",
      "publication_date": "2025-05-24",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e3e3aa5576de899b755100db211501bb405aba3e",
      "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
      "abstract": "LLM agents will likely communicate on behalf of users with other entity-representing agents on tasks involving long-horizon plans with interdependent goals. Current work neglects these agentic networks and their challenges. We identify required properties for agent communication: proactivity, adaptability, privacy (sharing only task-necessary information), and security (preserving integrity and utility against selfish entities). After demonstrating communication vulnerabilities, we propose a practical design and protocol inspired by network security principles. Our framework automatically derives task-specific rules from prior conversations to build firewalls. These firewalls construct a closed language that is completely controlled by the developer. They transform any personal data to the allowed degree of permissibility entailed by the task. Both operations are completely quarantined from external attackers, disabling the potential for prompt injections, jailbreaks, or manipulation. By incorporating rules learned from their previous mistakes, agents rewrite their instructions and self-correct during communication. Evaluations on diverse attacks demonstrate our framework significantly reduces privacy and security vulnerabilities while allowing adaptability.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sahar Abdelnabi",
        "Amr Gomaa",
        "Eugene Bagdasarian",
        "P. O. Kristensson",
        "Reza Shokri"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/e3e3aa5576de899b755100db211501bb405aba3e",
      "pdf_url": "",
      "publication_date": "2025-02-03",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cbf326517d8983aeb3c4c12d68b4444582c0307a",
      "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
      "abstract": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the computational overhead resulting from massive video frames, often alleviated through compression strategies. However, the visual content is not equally contributed to user instructions, existing strategies (e.g., average pool) inevitably lead to the loss of potentially useful information. To tackle this, we propose the Hybridlevel Instruction Injection Strategy for Conditional Token Compression in MLLMs (HICom), utilizing the instruction as a condition to guide the compression from both local and global levels. This encourages the compression to retain the maximum amount of user-focused information while reducing visual tokens to minimize computational burden. Specifically, the instruction condition is injected into the grouped visual tokens at the local level and the learnable tokens at the global level, and we conduct the attention mechanism to complete the conditional compression. From the hybrid-level compression, the instruction-relevant visual parts are highlighted while the temporal-spatial structure is also preserved for easier understanding of LLMs. To further unleash the potential of HICom, we introduce a new conditional pre-training stage with our proposed dataset HICom-248K. Experiments show that our HICom can obtain distinguished video understanding ability with fewer tokens, increasing the performance by 2.43% average on three multiple-choice QA benchmarks and saving 78.8% tokens compared with the SOTA method. The code is available at https://github.com/lntzm/HICom.",
      "year": 2025,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zhihang Liu",
        "Chen-Wei Xie",
        "Pandeng Li",
        "Liming Zhao",
        "Longxiang Tang",
        "Yun Zheng",
        "Chuanbin Liu",
        "Hongtao Xie"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/cbf326517d8983aeb3c4c12d68b4444582c0307a",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "427c3a502d132b6e1cea2d5565460d284db6e3f7",
      "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based Approach",
      "abstract": "This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.",
      "year": 2025,
      "venue": "2025 7th International Conference on Artificial Intelligence Technologies and Applications (ICAITA)",
      "authors": [
        "Shuangquan Lyu",
        "Yingnan Deng",
        "Guiran Liu",
        "Zhen Qi",
        "Ruotong Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/427c3a502d132b6e1cea2d5565460d284db6e3f7",
      "pdf_url": "",
      "publication_date": "2025-06-27",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b02a3849d5482d9933468225713ed0c2a4ddc9e2",
      "title": "Defending Against Prompt Injection With a Few DefensiveTokens",
      "abstract": "When large language model (LLM) systems interact with external data to perform complex tasks, a new attack, namely prompt injection, becomes a significant threat. By injecting instructions into the data accessed by the system, the attacker is able to override the initial user task with an arbitrary task directed by the attacker. To secure the system, test-time defenses, e.g., defensive prompting, have been proposed for system developers to attain security only when needed in a flexible manner. However, they are much less effective than training-time defenses that change the model parameters. Motivated by this, we propose DefensiveToken, a test-time defense with prompt injection robustness comparable to training-time alternatives. DefensiveTokens are newly inserted as special tokens, whose embeddings are optimized for security. In security-sensitive cases, system developers can append a few DefensiveTokens before the LLM input to achieve security with a minimal utility drop. In scenarios where security is less of a concern, developers can simply skip DefensiveTokens; the LLM system remains the same as there is no defense, generating high-quality responses. Thus, DefensiveTokens, if released alongside the model, allow a flexible switch between the state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code is available here.",
      "year": 2025,
      "venue": "Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security",
      "authors": [
        "Sizhe Chen",
        "Yizhu Wang",
        "Nicholas Carlini",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/b02a3849d5482d9933468225713ed0c2a4ddc9e2",
      "pdf_url": "",
      "publication_date": "2025-07-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85eac6fe1eabd526282a224863c338fe36ec6587",
      "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and several defenses have recently been proposed, often claiming to mitigate these attacks successfully. However, we argue that existing studies lack a principled approach to evaluating these defenses. In this paper, we argue the need to assess defenses across two critical dimensions: (1) effectiveness, measured against both existing and adaptive prompt injection attacks involving diverse target and injected prompts, and (2) general-purpose utility, ensuring that the defense does not compromise the foundational capabilities of the LLM. Our critical evaluation reveals that prior studies have not followed such a comprehensive evaluation methodology. When assessed using this principled approach, we show that existing defenses are not as successful as previously reported. This work provides a foundation for evaluating future defenses and guiding their development. Our code and data are available at: https://github.com/PIEval123/PIEval.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuqi Jia",
        "Zedian Shao",
        "Yupei Liu",
        "Jinyuan Jia",
        "Dawn Song",
        "N. Gong"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/85eac6fe1eabd526282a224863c338fe36ec6587",
      "pdf_url": "",
      "publication_date": "2025-05-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "83a1ed7d89065a81b1244bf998d84d8e61dfd819",
      "title": "Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents",
      "abstract": "AI agents integrated with Web3 offer autonomy and openness but raise security concerns as they interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation -- a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds. It expands on traditional prompt injection and reveals a more stealthy and persistent threat: memory injection. Using ElizaOS, a representative decentralized AI agent framework for automated Web3 operations, we showcase that malicious injections into prompts or historical records can trigger unauthorized asset transfers and protocol violations which could be financially devastating in reality. To quantify these risks, we introduce CrAIBench, a Web3-focused benchmark covering 150+ realistic blockchain tasks. such as token transfers, trading, bridges, and cross-chain interactions, and 500+ attack test cases using context manipulation. Our evaluation results confirm that AI models are significantly more vulnerable to memory injection compared to prompt injection. Finally, we evaluate a comprehensive defense roadmap, finding that prompt-injection defenses and detectors only provide limited protection when stored context is corrupted, whereas fine-tuning-based defenses substantially reduce attack success rates while preserving performance on single-step tasks. These results underscore the urgent need for AI agents that are both secure and fiduciarily responsible in blockchain environments.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Atharv Singh Patlan",
        "Peiyao Sheng",
        "Ashwin Hebbar",
        "Prateek Mittal",
        "P. Viswanath"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/83a1ed7d89065a81b1244bf998d84d8e61dfd819",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e2ec43e69a95a4b4db153ca1a07885d1668b8446",
      "title": "The Hidden Dangers of Browsing AI Agents",
      "abstract": "Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their reliance on dynamic content, tool execution, and user-provided data exposes them to a broad attack surface. This paper presents a comprehensive security evaluation of such agents, focusing on systemic vulnerabilities across multiple architectural layers. Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments. To address discovered threats, we propose a defense in depth strategy incorporating input sanitization, planner executor isolation, formal analyzers, and session safeguards. These measures protect against both initial access and post exploitation attack vectors. Through a white box analysis of a popular open source project, Browser Use, we demonstrate how untrusted web content can hijack agent behavior and lead to critical security breaches. Our findings include prompt injection, domain validation bypass, and credential exfiltration, evidenced by a disclosed CVE and a working proof of concept exploit.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Mykyta Mudryi",
        "Markiyan Chaklosh",
        "Grzegorz W'ojcik"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/e2ec43e69a95a4b4db153ca1a07885d1668b8446",
      "pdf_url": "",
      "publication_date": "2025-05-19",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f529cec54e76166cdfe5f9610a26412886d80fbd",
      "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections",
      "abstract": "How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed. Instead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Milad Nasr",
        "Nicholas Carlini",
        "Chawin Sitawarin",
        "Sander Schulhoff",
        "Jamie Hayes",
        "Michael Ilie",
        "Juliette Pluto",
        "Shuang Song",
        "Harsh Chaudhari",
        "Ilia Shumailov",
        "Abhradeep Thakurta",
        "Kai Yuanqing Xiao",
        "Andreas Terzis",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/f529cec54e76166cdfe5f9610a26412886d80fbd",
      "pdf_url": "",
      "publication_date": "2025-10-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "089938febf54aad46e36234f5e1052b5d24339ad",
      "title": "LLM Agents Should Employ Security Principles",
      "abstract": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kaiyuan Zhang",
        "Zian Su",
        "Pin-Yu Chen",
        "Elisa Bertino",
        "Xiangyu Zhang",
        "Ninghui Li"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/089938febf54aad46e36234f5e1052b5d24339ad",
      "pdf_url": "",
      "publication_date": "2025-05-29",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "712e08695ec807163afbbfd9f442ab027ccc29ac",
      "title": "Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction",
      "abstract": "Large language models (LLMs) have demonstrated impressive performance and have come to dominate the field of natural language processing (NLP) across various tasks. However, due to their strong instruction-following capabilities and inability to distinguish between instructions and data content, LLMs are vulnerable to prompt injection attacks. These attacks manipulate LLMs into deviating from the original input instructions and executing maliciously injected instructions within data content, such as web documents retrieved from search engines. Existing defense methods, including prompt-engineering and fine-tuning approaches, typically instruct models to follow the original input instructions while suppressing their tendencies to execute injected instructions. However, our experiments reveal that suppressing instruction-following tendencies is challenging. Through analyzing failure cases, we observe that although LLMs tend to respond to any recognized instructions, they are aware of which specific instructions they are executing and can correctly reference them within the original prompt. Motivated by these findings, we propose a novel defense method that leverages, rather than suppresses, the instruction-following abilities of LLMs. Our approach prompts LLMs to generate responses that include both answers and their corresponding instruction references. Based on these references, we filter out answers not associated with the original input instructions. Comprehensive experiments demonstrate that our method outperforms prompt-engineering baselines and achieves performance comparable to fine-tuning methods, reducing the attack success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has minimal impact on overall utility.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Yuan Sui",
        "Yue Liu",
        "Yufei He",
        "Yangqiu Song",
        "Bryan Hooi"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/712e08695ec807163afbbfd9f442ab027ccc29ac",
      "pdf_url": "",
      "publication_date": "2025-04-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "045d0a985cbae7f44d7577c8f0d2183406d9a24d",
      "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
      "abstract": "Large language models (LLMs) have gained widespread adoption across diverse applications due to their impressive generative capabilities. Their plug-and-play nature enables both developers and end users to interact with these models through simple prompts. However, as LLMs become more integrated into various systems in diverse domains, concerns around their security are growing. Existing studies mainly focus on threats arising from user prompts (e.g. prompt injection attack) and model output (e.g. model inversion attack), while the security of system prompts remains largely overlooked. This work bridges the critical gap. We introduce system prompt poisoning, a new attack vector against LLMs that, unlike traditional user prompt injection, poisons system prompts hence persistently impacts all subsequent user interactions and model responses. We systematically investigate four practical attack strategies in various poisoning scenarios. Through demonstration on both generative and reasoning LLMs, we show that system prompt poisoning is highly feasible without requiring jailbreak techniques, and effective across a wide range of tasks, including those in mathematics, coding, logical reasoning, and natural language processing. Importantly, our findings reveal that the attack remains effective even when user prompts employ advanced prompting techniques like chain-of-thought (CoT). We also show that such techniques, including CoT and retrieval-augmentation-generation (RAG), which are proven to be effective for improving LLM performance in a wide range of tasks, are significantly weakened in their effectiveness by system prompt poisoning.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiawei Guo",
        "Haipeng Cai"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/045d0a985cbae7f44d7577c8f0d2183406d9a24d",
      "pdf_url": "",
      "publication_date": "2025-05-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9039ae9f21e838bfbcdb14d00a56c595b2183689",
      "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
      "abstract": "LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution. In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that ACE is secure against attacks from the InjecAgent and Agent Security Bench benchmarks for indirect prompt injection, and our newly introduced attacks. We also evaluate the utility of ACE in realistic environments, using the Tool Usage suite from the LangChain benchmark. Our architecture represents a significant advancement towards hardening LLM-based systems using system security principles.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Evan Li",
        "Tushin Mallick",
        "Evan Rose",
        "William Robertson",
        "Alina Oprea",
        "Cristina Nita-Rotaru"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9039ae9f21e838bfbcdb14d00a56c595b2183689",
      "pdf_url": "",
      "publication_date": "2025-04-29",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ff52eced7b23c01e5b63ef6a9df3c72027831177",
      "title": "Privacy and Security Challenges in Large Language Models",
      "abstract": "Large Language Models (LLMs) are at the forefront of artificial intelligence advancements, demonstrating exceptional capabilities in natural language understanding and generation across diverse domains such as healthcare, finance, and customer service. However, their deployment introduces substantial secu-rity and privacy risks, including prompt injection, data leakage, and unauthorized data disclosures. These vulnerabilities highlight the need for robust frameworks to safeguard sensitive data and prevent misuse. This paper provides a comprehensive analysis of the security and privacy challenges in LLMs, examines existing mitigation strategies such as intelligent LLM firewalls, differen-tial privacy, and OW ASP-based security principles, and discusses future directions for ethical and secure LLM deployment. By addressing these challenges in detail, we identify gaps in current practices and propose a roadmap for the secure and responsible deployment of LLMs in high-stakes applications. Our findings underscore the importance of tailored security frameworks and privacy-preserving techniques to ensure the ethical and reliable use of LLMs in sensitive environments. Additionally, this pa-per emphasizes the significance of a human-in-the-loop (HITL) approach to ensure accountability and accuracy, particularly in critical domains. The discussion extends to emerging technologies such as retrieval-augmented generation (RAG) and adaptive threat detection systems, which hold promise for enhancing the security and ethical deployment of LLMs.",
      "year": 2025,
      "venue": "Computing and Communication Workshop and Conference",
      "authors": [
        "Vishal Rathod",
        "Seyedsina Nabavirazavi",
        "Samira Zad",
        "S. S. Iyengar"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/ff52eced7b23c01e5b63ef6a9df3c72027831177",
      "pdf_url": "",
      "publication_date": "2025-01-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33c04fbf16bbcac199462a9f95044fcdfa6161fa",
      "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
      "abstract": "Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics.",
      "year": 2025,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Ruiyi Zhang",
        "David Sullivan",
        "Kyle Jackson",
        "Pengtao Xie",
        "Mei Chen"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/33c04fbf16bbcac199462a9f95044fcdfa6161fa",
      "pdf_url": "",
      "publication_date": "2025-04-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39361a6a3d86f8831de8537965ec4f9f65ea31f2",
      "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments",
      "abstract": "Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning high ASRs in realistic end-to-end settings, with the strongest-to-date Claude 4.5 Sonnet | CUA exhibiting the highest ASR of 60%, indicating that CUA threats can already result in tangible risks to users and computer systems. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zeyi Liao",
        "Jaylen Jones",
        "Linxi Jiang",
        "Eric Fosler-Lussier",
        "Yu Su",
        "Zhiqiang Lin",
        "Huan Sun"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/39361a6a3d86f8831de8537965ec4f9f65ea31f2",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eca625b2d3488c4f5524f12a277bef5896e3cbc6",
      "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
      "abstract": "As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at https://github.com/IBM/Adversarial-Prompt-Evaluation.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Giulio Zizzo",
        "Giandomenico Cornacchia",
        "Kieran Fraser",
        "Muhammad Zaid Hameed",
        "Ambrish Rawat",
        "Beat Buesser",
        "Mark Purcell",
        "Pin-Yu Chen",
        "P. Sattigeri",
        "Kush R. Varshney"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/eca625b2d3488c4f5524f12a277bef5896e3cbc6",
      "pdf_url": "",
      "publication_date": "2025-02-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f88411c138583c3875009db49291e1daafc825ba",
      "title": "xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking",
      "abstract": "Safety alignment mechanism are essential for preventing large language models (LLMs) from generating harmful information or unethical content. However, cleverly crafted prompts can bypass these safety measures without accessing the model's internal parameters, a phenomenon known as black-box jailbreak. Existing heuristic black-box attack methods, such as genetic algorithms, suffer from limited effectiveness due to their inherent randomness, while recent reinforcement learning (RL) based methods often lack robust and informative reward signals. To address these challenges, we propose a novel black-box jailbreak method leveraging RL, which optimizes prompt generation by analyzing the embedding proximity between benign and malicious prompts. This approach ensures that the rewritten prompts closely align with the intent of the original prompts while enhancing the attack's effectiveness. Furthermore, we introduce a comprehensive jailbreak evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous and holistic assessment of jailbreak success. Experimental results show the superiority of our approach, achieving state-of-the-art (SOTA) performance on several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs. The codebase for this work is available at https://github.com/Aegis1863/xJailbreak.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sunbowen Lee",
        "Shiwen Ni",
        "Chi Wei",
        "Shuaimin Li",
        "Liyang Fan",
        "A. Argha",
        "Hamid Alinejad-Rokny",
        "Ruifeng Xu",
        "Yicheng Gong",
        "Min Yang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/f88411c138583c3875009db49291e1daafc825ba",
      "pdf_url": "",
      "publication_date": "2025-01-28",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f74726bf146835dc48ba4d8ab2dcfd0ed762af8e",
      "title": "Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks against Prompt Injection and Jailbreak Detection Systems",
      "abstract": "Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "William Hackett",
        "Lewis Birch",
        "Stefan Trawicki",
        "Neeraj Suri",
        "Peter Garraghan"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/f74726bf146835dc48ba4d8ab2dcfd0ed762af8e",
      "pdf_url": "",
      "publication_date": "2025-04-15",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8be6b0b0c7af16da10938c252db05cd8dd2f74d0",
      "title": "Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
      "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Rana Muhammad Shahroz Khan",
        "Zhen Tan",
        "Sukwon Yun",
        "Charles Flemming",
        "Tianlong Chen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/8be6b0b0c7af16da10938c252db05cd8dd2f74d0",
      "pdf_url": "",
      "publication_date": "2025-03-31",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32f24af87e890011ed3e11194ba53e8dd53afbe0",
      "title": "ControlNET: A Firewall for RAG-based LLM System",
      "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hongwei Yao",
        "Haoran Shi",
        "Yidou Chen",
        "Yixin Jiang",
        "Cong Wang",
        "Zhan Qin"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/32f24af87e890011ed3e11194ba53e8dd53afbe0",
      "pdf_url": "",
      "publication_date": "2025-04-13",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "edd63168ea32a84ef63b3f34495cb8b65222ee32",
      "title": "WebInject: Prompt Injection Attack to Web Agents",
      "abstract": "Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. In this work, we propose WebInject, a prompt injection attack that manipulates the webpage environment to induce a web agent to perform an attacker-specified action. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the attacker-specified action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple datasets shows that WebInject is highly effective and significantly outperforms baselines.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xilong Wang",
        "John Bloch",
        "Zedian Shao",
        "Yuepeng Hu",
        "Shuyan Zhou",
        "N. Gong"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/edd63168ea32a84ef63b3f34495cb8b65222ee32",
      "pdf_url": "",
      "publication_date": "2025-05-16",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c7ba94eb5d96b665397db77f765910e5acde5c6",
      "title": "Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-Based Prompt Injection Attacks via the Fine-Tuning Interface",
      "abstract": "We surface a new threat to closed-weight Large Language Models (LLMs) that enables an attacker to compute optimization-based prompt injections. Specifically, we characterize how an attacker can leverage the loss-like information returned from the remote fine-tuning interface to guide the search for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor and allows developers to fine-tune LLMs for their tasks, thus providing utility, but also exposes enough information for an attacker to compute adversarial prompts. Through an experimental analysis, we characterize the loss-like values returned by the Gemini fine-tuning API and demonstrate that they provide a useful signal for discrete optimization of adversarial prompts using a greedy search algorithm. Using the PurpleLlama prompt injection benchmark, we demonstrate attack success rates between 65% and 82% on Google's Gemini family of LLMs. These attacks exploit the classic utility-security tradeoff - the fine-tuning interface provides a useful feature for developers but also exposes the LLMs to powerful attacks.",
      "year": 2025,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Andrey Labunets",
        "Nishit V. Pandya",
        "Ashish Hooda",
        "Xiaohan Fu",
        "Earlence Fernandes"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/7c7ba94eb5d96b665397db77f765910e5acde5c6",
      "pdf_url": "",
      "publication_date": "2025-01-16",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "287e6e8a5593ba2169f3cf66e66e5d344a36064d",
      "title": "PIGuard: Prompt Injection Guardrail via Mitigating Overdefense for Free",
      "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense\u2014falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose PIGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. PIGuard demonstrates state-of-the-art performance on diverse benchmarks including Not-Inject, surpassing the existing best model by 30.4%, offering a robust and open-source so-lution for detecting prompt injection attacks. The code and datasets are released at https: //github.com/leolee99/PIGuard .",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/287e6e8a5593ba2169f3cf66e66e5d344a36064d",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "51941fb4d679b102a50b18f1c1a6f70b6b929f59",
      "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
      "abstract": "Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at https://github.com/egozverev/aside.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Egor Zverev",
        "Evgenii Kortukov",
        "Alexander Panfilov",
        "Soroush Tabesh",
        "Alexandra Volkova",
        "S. Lapuschkin",
        "Wojciech Samek",
        "Christoph H. Lampert"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/51941fb4d679b102a50b18f1c1a6f70b6b929f59",
      "pdf_url": "",
      "publication_date": "2025-03-13",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6128c475f0ecc937ac69f149a95353f42f068070",
      "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover",
      "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables remarkable capabilities in natural language processing and generation. However, these systems introduce security vulnerabilities that extend beyond traditional content generation to system-level compromises. This paper presents a comprehensive evaluation of the LLMs security used as reasoning engines within autonomous agents, highlighting how they can be exploited as attack vectors capable of achieving computer takeovers. We focus on how different attack surfaces and trust boundaries can be leveraged to orchestrate such takeovers. We demonstrate that adversaries can effectively coerce popular LLMs into autonomously installing and executing malware on victim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we tested trust boundaries within multi-agent systems, where LLM agents interact and influence each other, and we revealed that LLMs which successfully resist direct injection or RAG backdoor attacks will execute identical payloads when requested by peer agents. We found that 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks, and that every model exhibits context-dependent security behaviors that create exploitable blind spots.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Matteo Lupinacci",
        "F. A. Pironti",
        "Francesco Blefari",
        "Francesco Romeo",
        "Luigi Arena",
        "Angelo Furfaro"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/6128c475f0ecc937ac69f149a95353f42f068070",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e998d2b09b2677fb7c76a771ca4253175bc4c3c5",
      "title": "Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation",
      "abstract": "Large language models (LLMs) have demonstrated significant utility in a wide range of applications; however, their deployment is plagued by security vulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to generate harmful or unethical content by crafting adversarial prompts. While much of the current research on jailbreak attacks has focused on single-turn interactions, it has largely overlooked the impact of historical dialogues on model behavior. In this paper, we introduce a novel jailbreak paradigm, Dialogue Injection Attack (DIA), which leverages the dialogue history to enhance the success rates of such attacks. DIA operates in a black-box setting, requiring only access to the chat API or knowledge of the LLM's chat template. We propose two methods for constructing adversarial historical dialogues: one adapts gray-box prefilling attacks, and the other exploits deferred responses. Our experiments show that DIA achieves state-of-the-art attack success rates on recent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that DIA can bypass 5 different defense mechanisms, highlighting its robustness and effectiveness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Wenlong Meng",
        "Fan Zhang",
        "Wendao Yao",
        "Zhenyuan Guo",
        "Yuwei Li",
        "Chengkun Wei",
        "Wenzhi Chen"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e998d2b09b2677fb7c76a771ca4253175bc4c3c5",
      "pdf_url": "",
      "publication_date": "2025-03-11",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3f63bb891b0769f36b5c74f064bb4e0657ad801e",
      "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents",
      "abstract": "Computer-Use Agents (CUAs) with full system access enable powerful task automation but pose significant security and privacy risks due to their ability to manipulate files, access user data, and execute arbitrary commands. While prior work has focused on browser-based agents and HTML-level attacks, the vulnerabilities of CUAs remain underexplored. In this paper, we investigate Visual Prompt Injection (VPI) attacks, where malicious instructions are visually embedded within rendered user interfaces, and examine their impact on both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of 306 test cases across five widely used platforms, to evaluate agent robustness under VPI threats. Each test case is a variant of a web platform, designed to be interactive, deployed in a realistic environment, and containing a visually embedded malicious prompt. Our empirical study shows that current CUAs and BUAs can be deceived at rates of up to 51% and 100%, respectively, on certain platforms. The experimental results also indicate that system prompt defenses offer only limited improvements. These findings highlight the need for robust, context-aware defenses to ensure the safe deployment of multimodal AI agents in real-world environments. The code and dataset are available at: https://github.com/cua-framework/agents",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tri Cao",
        "Bennett Lim",
        "Yue Liu",
        "Yuan Sui",
        "Yuexin Li",
        "Shumin Deng",
        "Lin Lu",
        "Nay Oo",
        "Shuicheng Yan",
        "Bryan Hooi"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3f63bb891b0769f36b5c74f064bb4e0657ad801e",
      "pdf_url": "",
      "publication_date": "2025-06-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f8f0eaf47be5a75a9483a8cb312e163b0e3eaab",
      "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
      "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs. In this paper, departing from traditional deep learning attack paradigms, we explore their intrinsic relationship and collectively term them Prompt Trigger Attacks (PTA). This raises a key question: Can we determine if a prompt is benign or poisoned? To address this, we propose UniGuardian, the first unified defense mechanism designed to detect prompt injection, backdoor attacks, and adversarial attacks in LLMs. Additionally, we introduce a single-forward strategy to optimize the detection pipeline, enabling simultaneous attack detection and text generation within a single forward pass. Our experiments confirm that UniGuardian accurately and efficiently identifies malicious prompts in LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Huawei Lin",
        "Yingjie Lao",
        "Tong Geng",
        "Tan Yu",
        "Weijie Zhao"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/4f8f0eaf47be5a75a9483a8cb312e163b0e3eaab",
      "pdf_url": "",
      "publication_date": "2025-02-18",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5063f41336549b08240f14420b9000c2281aab19",
      "title": "In-Context Watermarks for Large Language Models",
      "abstract": "The growing use of large language models (LLMs) for sensitive applications has highlighted the need for effective watermarking techniques to ensure the provenance and accountability of AI-generated text. However, most existing watermarking methods require access to the decoding process, limiting their applicability in real-world settings. One illustrative example is the use of LLMs by dishonest reviewers in the context of academic peer review, where conference organizers have no access to the model used but still need to detect AI-generated reviews. Motivated by this gap, we introduce In-Context Watermarking (ICW), which embeds watermarks into generated text solely through prompt engineering, leveraging LLMs' in-context learning and instruction-following abilities. We investigate four ICW strategies at different levels of granularity, each paired with a tailored detection method. We further examine the Indirect Prompt Injection (IPI) setting as a specific case study, in which watermarking is covertly triggered by modifying input documents such as academic manuscripts. Our experiments validate the feasibility of ICW as a model-agnostic, practical watermarking approach. Moreover, our findings suggest that as LLMs become more capable, ICW offers a promising direction for scalable and accessible content attribution.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yepeng Liu",
        "Xuandong Zhao",
        "Christopher Kruegel",
        "D. Song",
        "Yuheng Bu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/5063f41336549b08240f14420b9000c2281aab19",
      "pdf_url": "",
      "publication_date": "2025-05-22",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f583a040e8324257a993935122c7f1e274bdb20a",
      "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks",
      "abstract": "The security of LLM-based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various communication patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Rui Miao",
        "Yixin Liu",
        "Yili Wang",
        "Xu Shen",
        "Yue Tan",
        "Yiwei Dai",
        "Shirui Pan",
        "Xin Wang"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/f583a040e8324257a993935122c7f1e274bdb20a",
      "pdf_url": "",
      "publication_date": "2025-08-11",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c97f5fa7bf306e74e1f1254dea99dd6e39fb602d",
      "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection",
      "abstract": "With the emergence of strong vision language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: vision-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct vision-focused strategies, dynamically generating auxiliary images when necessary to construct a vision-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which achieves a toxicity score of 2.48 and an ASR of 22.2%. Code: https://github.com/Dtc7w3PQ/Visco-Attack.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Ziqi Miao",
        "Yi Ding",
        "Lijun Li",
        "Jing Shao"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/c97f5fa7bf306e74e1f1254dea99dd6e39fb602d",
      "pdf_url": "",
      "publication_date": "2025-07-03",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "da0414882dc49b9bf60805e3aca0d9aeb85d0881",
      "title": "Emerging Cyber Attack Risks of Medical AI Agents",
      "abstract": "Large language models (LLMs)-powered AI agents exhibit a high level of autonomy in addressing medical and healthcare challenges. With the ability to access various tools, they can operate within an open-ended action space. However, with the increase in autonomy and ability, unforeseen risks also arise. In this work, we investigated one particular risk, i.e., cyber attack vulnerability of medical AI agents, as agents have access to the Internet through web browsing tools. We revealed that through adversarial prompts embedded on webpages, cyberattackers can: i) inject false information into the agent's response; ii) they can force the agent to manipulate recommendation (e.g., healthcare products and services); iii) the attacker can also steal historical conversations between the user and agent, resulting in the leak of sensitive/private medical information; iv) furthermore, the targeted agent can also cause a computer system hijack by returning a malicious URL in its response. Different backbone LLMs were examined, and we found such cyber attacks can succeed in agents powered by most mainstream LLMs, with the reasoning models such as DeepSeek-R1 being the most vulnerable.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jianing Qiu",
        "Lin Li",
        "Jiankai Sun",
        "Hao Wei",
        "Zhe Xu",
        "Kyle Lam",
        "Wu Yuan"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/da0414882dc49b9bf60805e3aca0d9aeb85d0881",
      "pdf_url": "",
      "publication_date": "2025-04-02",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "020ad9b3a8022ac6464cc06b2e80c2a267001e77",
      "title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt",
      "abstract": "LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model\u2019s behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.",
      "year": 2025,
      "venue": "2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)",
      "authors": [
        "Zhilong Wang",
        "Neha Nagaraja",
        "Lan Zhang",
        "Hayretdin Bah\u015fi",
        "Pawan Patil",
        "Peng Liu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/020ad9b3a8022ac6464cc06b2e80c2a267001e77",
      "pdf_url": "",
      "publication_date": "2025-06-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e70f33df4318a1f67d3c397b5d1ebd5130c6a98b",
      "title": "Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks",
      "abstract": "Prompt injection constitutes a significant challenge for generative AI systems by inducing unintended outputs. We introduce a multi-agent NLP framework specifically designed to address prompt injection vulnerabilities through layered detection and enforcement mechanisms. The framework orchestrates specialized agents for generating responses, sanitizing outputs, and enforcing policy compliance. Evaluation on 500 engineered injection prompts demonstrates a marked reduction in injection success and policy breaches. Novel metrics, including Injection Success Rate (ISR), Policy Override Frequency (POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS), are proposed to derive a composite Total Injection Vulnerability Score (TIVS). The system utilizes the OVON (Open Voice Network) framework for inter-agent communication via structured JSON messages, extending a previously established multi-agent architecture from hallucination mitigation to address the unique challenges of prompt injection.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Diego Gosmar",
        "Deborah A. Dahl",
        "Dario Gosmar"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/e70f33df4318a1f67d3c397b5d1ebd5130c6a98b",
      "pdf_url": "",
      "publication_date": "2025-03-14",
      "keywords_matched": [
        "prompt injection",
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9ddcbbff1e23b7f995fc363ad5123091f8866748",
      "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents",
      "abstract": "Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents'task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hengyu An",
        "Jinghuai Zhang",
        "Tianyu Du",
        "Chunyi Zhou",
        "Qingming Li",
        "Tao Lin",
        "Shouling Ji"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9ddcbbff1e23b7f995fc363ad5123091f8866748",
      "pdf_url": "",
      "publication_date": "2025-08-21",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "838c480b3211acc04d5ff3e8ac350b924b39fdd8",
      "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs",
      "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive information. Both closed-source and open-source LLMs are underinvestigated for these attacks. This paper studies effective prompt injection attacks against the $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks. Current metrics only consider successful attacks, whereas our proposed Attack Success Probability (ASP) also captures uncertainty in the model's response, reflecting ambiguity in attack feasibility. By comprehensively analyzing the effectiveness of prompt injection attacks, we propose a simple and effective hypnotism attack; results show that this attack causes aligned language models, including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable behaviors, achieving around $90$% ASP. They also indicate that our ignore prefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over $60$% ASP on a multi-categorical dataset. We find that moderately well-known LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the need to raise public awareness and prioritize efficient mitigation strategies.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiawen Wang",
        "Pritha Gupta",
        "Ivan Habernal",
        "Eyke H\u00fcllermeier"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/838c480b3211acc04d5ff3e8ac350b924b39fdd8",
      "pdf_url": "",
      "publication_date": "2025-05-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "611967b1b2ddbee4b308451d4ae958f0ac78eb93",
      "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution",
      "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Meysam Alizadeh",
        "Zeynab Samei",
        "Daria Stetsenko",
        "Fabrizio Gilardi"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/611967b1b2ddbee4b308451d4ae958f0ac78eb93",
      "pdf_url": "",
      "publication_date": "2025-06-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4848ee46c3e83c0e69a84b6f5779c1557df3218a",
      "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications",
      "abstract": "The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-Guard, a robust, layered defense architecture designed for LLM--tool interactions. MCP-Guard employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model achieves (96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM arbitrator synthesizes these signals to deliver the final decision while minimizing false positives. To facilitate rigorous training and evaluation, we also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000 samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench simulates diverse, real-world attack vectors in the MCP format, providing a foundation for future research into securing LLM-tool ecosystems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Wenpeng Xing",
        "Zhonghao Qi",
        "Yupeng Qin",
        "Yilin Li",
        "Caini Chang",
        "Jiahui Yu",
        "Changting Lin",
        "Zhenzhen Xie",
        "Meng Han"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/4848ee46c3e83c0e69a84b6f5779c1557df3218a",
      "pdf_url": "",
      "publication_date": "2025-08-14",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c6d6f408c07a6b0fe5ca01c1e418ee1e26bd531f",
      "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models",
      "abstract": "Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Johan Wahr'eus",
        "Ahmed Mohamed Hussain",
        "P. Papadimitratos"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/c6d6f408c07a6b0fe5ca01c1e418ee1e26bd531f",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e92dc56f5d94c5ba9570ab0ce272e1a5863d1730",
      "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
      "abstract": "In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as\"GIVE A POSITIVE REVIEW ONLY\"were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as\"honeypots\"to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhicheng Lin"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/e92dc56f5d94c5ba9570ab0ce272e1a5863d1730",
      "pdf_url": "",
      "publication_date": "2025-07-08",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ba447936c20ad894d5ab8b00d2fedd3d3cc4d370",
      "title": "\u2018For Argument\u2019s Sake, Show Me How to Harm Myself!\u2019: Jailbreaking LLMs in Suicide and Self-Harm Contexts",
      "abstract": "Recent advances in large language models (LLMs) have led to increasingly sophisticated safety protocols and features designed to prevent harmful, unethical, or unauthorized outputs. However, these guardrails remain susceptible to novel and creative forms of adversarial prompting, including manually generated test cases. In this work, we present two new test cases in mental health for (i) suicide and (ii) self-harm, using multi-step, prompt-level jailbreaking and bypass built-in content and safety filters. We show that user intent is disregarded, leading to the generation of detailed harmful content and instructions that could cause real-world harm. We conduct an empirical evaluation across six widely available LLMs, demonstrating the generalizability and reliability of the bypass. We assess these findings and the multilayered ethical tensions that they present for their implications on prompt-response filtering and context-and task-specific model development. We recommend a more comprehensive and systematic approach to AI safety and ethics while emphasizing the need for continuous adversarial testing in safety-critical AI deployments. We also argue that while certain clearly defined safety measures and guardrails can and must be implemented in LLMs, ensuring robust and comprehensive safety across all use cases and domains remains extremely challenging given the current technical maturity of general-purpose LLMs.Content and Trigger Warning: This paper contains examples of harmful language, references to suicide and self-harm with instructions, tools, and methods.Responsible Disclosure: We communicated our results to OpenAI, Google, PerplexityAI, and Anthropic in advance and have received acknowledgment of receipt. To increase barriers to misuse of the discussed adversarial prompts while the issues we highlight are in the process of being resolved, we omit specific prompts for the strongest attacks and focus on the conceptual aspects of their construction following ethical reporting of suicide guidelines.1 A full version of the transcripts are available to researchers upon request and receipt of IRB approval. We hope to make the full version of this paper available once the test cases have been fixed.",
      "year": 2025,
      "venue": "International Symposium on Technology and Society",
      "authors": [
        "Annika Marie Schoene",
        "Cansu Canca"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/ba447936c20ad894d5ab8b00d2fedd3d3cc4d370",
      "pdf_url": "",
      "publication_date": "2025-09-10",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4851dbb8c416d8c15d4f6a71a12ffd346d9685f4",
      "title": "Prompt Injection 2.0: Hybrid AI Threats",
      "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jeremy McHugh",
        "Kristina Sekrst",
        "Jonathan Rodriguez Cefalu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/4851dbb8c416d8c15d4f6a71a12ffd346d9685f4",
      "pdf_url": "",
      "publication_date": "2025-07-17",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b73f8dbb2af737fe707df3b2dcb21a9cea62db5",
      "title": "Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree",
      "abstract": "This work demonstrates that LLM-based web navigation agents offer powerful automation capabilities but are vulnerable to Indirect Prompt Injection (IPI) attacks. We show that adversaries can embed universal adversarial triggers in webpage HTML to hijack agent behavior that utilizes the accessibility tree to parse HTML, causing unintended or malicious actions. Using the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1, our system demonstrates high success rates across real websites in both targeted and general attacks, including login credential exfiltration and forced ad clicks. Our empirical results highlight critical security risks and the need for stronger defenses as LLM-driven autonomous web agents become more widely adopted. The system software (https://github.com/sej2020/manipulating-web-agents) is released under the MIT License, with an accompanying publicly available demo website (http://lethaiq.github.io/attack-web-llm-agent).",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sam Johnson",
        "Viet Pham",
        "Thai Le"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2b73f8dbb2af737fe707df3b2dcb21a9cea62db5",
      "pdf_url": "",
      "publication_date": "2025-07-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3835926e08ef43b543ee380600e4f271df589b84",
      "title": "Defending against Indirect Prompt Injection by Instruction Detection",
      "abstract": "The integration of Large Language Models (LLMs) with external sources is becoming increasingly common, with Retrieval-Augmented Generation (RAG) being a prominent example. However, this integration introduces vulnerabilities of Indirect Prompt Injection (IPI) attacks, where hidden instructions embedded in external data can manipulate LLMs into executing unintended or harmful actions. We recognize that IPI attacks fundamentally rely on the presence of instructions embedded within external content, which can alter the behavioral states of LLMs. Can the effective detection of such state changes help us defend against IPI attacks? In this paper, we propose InstructDetector, a novel detection-based approach that leverages the behavioral states of LLMs to identify potential IPI attacks. Specifically, we demonstrate the hidden states and gradients from intermediate layers provide highly discriminative features for instruction detection. By effectively combining these features, InstructDetector achieves a detection accuracy of 99.60% in the in-domain setting and 96.90% in the out-of-domain setting, and reduces the attack success rate to just 0.03% on the BIPIA benchmark. The code is publicly available at https://github.com/MYVAE/Instruction-detection.",
      "year": 2025,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Tongyu Wen",
        "Chenglong Wang",
        "Xiyuan Yang",
        "Haoyu Tang",
        "Yueqi Xie",
        "Lingjuan Lyu",
        "Zhicheng Dou",
        "Fangzhao Wu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/3835926e08ef43b543ee380600e4f271df589b84",
      "pdf_url": "",
      "publication_date": "2025-05-08",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e15ab241b0d562f402ee85813983e2ad3c381ca",
      "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs'inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Rui Wang",
        "Junda Wu",
        "Yu Xia",
        "Tong Yu",
        "Ruiyi Zhang",
        "Ryan A. Rossi",
        "Lina Yao",
        "Julian J. McAuley"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9e15ab241b0d562f402ee85813983e2ad3c381ca",
      "pdf_url": "",
      "publication_date": "2025-04-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "104e49214c0b390805cffd7aa8f5ed8b418f9185",
      "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection",
      "abstract": "As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection, attacks in which misleading instructions are embedded into the agent's visual environment, such as popups or chat messages, and misinterpreted as part of the intended task. A typical example is environmental injection, in which GUI elements are manipulated to influence agent behavior without directly modifying the user prompt. To address these emerging attacks, we propose EVA, a red teaming framework for indirect prompt injection which transforms the attack into a closed loop optimization by continuously monitoring an agent's attention distribution over the GUI and updating adversarial cues, keywords, phrasing, and layout, in response. Compared with prior one shot methods that generate fixed prompts without regard for how the model allocates visual attention, EVA dynamically adapts to emerging attention hotspots, yielding substantially higher attack success rates and far greater transferability across diverse GUI scenarios. We evaluate EVA on six widely used generalist and specialist GUI agents in realistic settings such as popup manipulation, chat based phishing, payments, and email composition. Experimental results show that EVA substantially improves success rates over static baselines. Under goal agnostic constraints, where the attacker does not know the agent's task intent, EVA still discovers effective patterns. Notably, we find that injection styles transfer well across models, revealing shared behavioral biases in GUI agents. These results suggest that evolving indirect prompt injection is a powerful tool not only for red teaming agents, but also for uncovering common vulnerabilities in their multimodal decision making.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yijie Lu",
        "Tianjie Ju",
        "Manman Zhao",
        "Xinbei Ma",
        "Yuan Guo",
        "Zhuosheng Zhang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/104e49214c0b390805cffd7aa8f5ed8b418f9185",
      "pdf_url": "",
      "publication_date": "2025-05-20",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c0956166abdc1c330f56a4b5fdb09d05d659a453",
      "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection",
      "abstract": "Prompt injection poses a serious threat to the reliability and safety of LLM agents. Recent defenses against prompt injection, such as Instruction Hierarchy and SecAlign, have shown notable robustness against static attacks. However, to more thoroughly evaluate the robustness of these defenses, it is arguably necessary to employ strong attacks such as automated red-teaming. To this end, we introduce RL-Hammer, a simple recipe for training attacker models that automatically learn to perform strong prompt injections and jailbreaks via reinforcement learning. RL-Hammer requires no warm-up data and can be trained entirely from scratch. To achieve high ASRs against industrial-level models with defenses, we propose a set of practical techniques that enable highly effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR against GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy defense. We further discuss the challenge of achieving high diversity in attacks, highlighting how attacker models tend to reward-hack diversity objectives. Finally, we show that RL-Hammer can evade multiple prompt injection detectors. We hope our work advances automatic red-teaming and motivates the development of stronger, more principled defenses. Code is available at https://github.com/facebookresearch/rl-injector.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuxin Wen",
        "Arman Zharmagambetov",
        "Ivan Evtimov",
        "Narine Kokhlikyan",
        "Tom Goldstein",
        "Kamalika Chaudhuri",
        "Chuan Guo"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/c0956166abdc1c330f56a4b5fdb09d05d659a453",
      "pdf_url": "",
      "publication_date": "2025-10-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "241604be7b07cbb884efb575780da73d8e40564a",
      "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly equipped with capabilities of real-time web search and integrated with protocols like Model Context Protocol (MCP). This extension could introduce new security vulnerabilities. We present a systematic investigation of LLM vulnerabilities to hidden adversarial prompts through malicious font injection in external resources like webpages, where attackers manipulate code-to-glyph mapping to inject deceptive content which are invisible to users. We evaluate two critical attack scenarios: (1)\"malicious content relay\"and (2)\"sensitive data leakage\"through MCP-enabled tools. Our experiments reveal that indirect prompts with injected malicious font can bypass LLM safety mechanisms through external resources, achieving varying success rates based on data sensitivity and prompt design. Our research underscores the urgent need for enhanced security measures in LLM deployments when processing external content.",
      "year": 2025,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Junjie Xiong",
        "Changjia Zhu",
        "Shuhang Lin",
        "Chong Zhang",
        "Yongfeng Zhang",
        "Yao Liu",
        "Lingyao Li"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/241604be7b07cbb884efb575780da73d8e40564a",
      "pdf_url": "",
      "publication_date": "2025-05-22",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9bbbc63e0f622e33cdbafe13f6d8a546b6d42dd5",
      "title": "'Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs",
      "abstract": "Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flanking Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chun Wai Chiu",
        "Linghan Huang",
        "Bo Li",
        "Huaming Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9bbbc63e0f622e33cdbafe13f6d8a546b6d42dd5",
      "pdf_url": "",
      "publication_date": "2025-02-02",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec9062053726fa4c501b7deb016b2174bd93cc21",
      "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
      "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due to their strong reasoning and planning capabilities. By interacting with external environments through predefined tools, these agents can carry out complex user tasks. Nonetheless, this interaction also introduces the risk of prompt injection attacks, where malicious inputs from external sources can mislead the agent's behavior, potentially resulting in economic loss, privacy leakage, or system compromise. System-level defenses have recently shown promise by enforcing static or predefined policies, but they still face two key challenges: the ability to dynamically update security rules and the need for memory stream isolation. To address these challenges, we propose DRIFT, a Dynamic Rule-based Isolation Framework for Trustworthy agentic systems, which enforces both control- and data-level constraints. A Secure Planner first constructs a minimal function trajectory and a JSON-schema-style parameter checklist for each function node based on the user query. A Dynamic Validator then monitors deviations from the original plan, assessing whether changes comply with privilege limitations and the user's intent. Finally, an Injection Isolator detects and masks any instructions that may conflict with the user query from the memory stream to mitigate long-term risks. We empirically validate the effectiveness of DRIFT on the AgentDojo and ASB benchmark, demonstrating its strong security performance while maintaining high utility across diverse models, showcasing both its robustness and adaptability. The code is released at https://github.com/SaFoLab-WISC/DRIFT.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "Hung-Chun Chiu",
        "Dianqi Li",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/ec9062053726fa4c501b7deb016b2174bd93cc21",
      "pdf_url": "",
      "publication_date": "2025-06-13",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e7beeb2db44d57737a986875dc5af4ddf15959de",
      "title": "Diagnosing Bias and Instability in LLM Evaluation: A Scalable Pairwise Meta-Evaluator",
      "abstract": "The evaluation of large language models (LLMs) increasingly relies on other LLMs acting as automated judges. While this approach offers scalability and efficiency, it raises serious concerns regarding evaluator reliability, positional bias, and ranking stability. This paper presents a scalable framework for diagnosing positional bias and instability in LLM-based evaluation by using controlled pairwise comparisons judged by multiple independent language models. The system supports mirrored comparisons with reversed response order, prompt injection, and surface-level perturbations (e.g., paraphrasing, lexical noise), enabling fine-grained analysis of evaluator consistency and verdict robustness. Over 3600 pairwise comparisons were conducted across five instruction-tuned open-weight models using ten open-ended prompts. The top-performing model (gemma:7b-instruct) achieved a 66.5% win rate. Evaluator agreement was uniformly high, with 100% consistency across judges, yet 48.4% of verdicts reversed under mirrored response order, indicating strong positional bias. Kendall\u2019s Tau analysis further showed that local model rankings varied substantially across prompts, suggesting that semantic context influences evaluator judgment. All evaluation traces were stored in a graph database (Neo4j), enabling structured querying and longitudinal analysis. The proposed framework provides not only a diagnostic lens for benchmarking models but also a blueprint for fairer and more interpretable LLM-based evaluation. These findings underscore the need for structure-aware, perturbation-resilient evaluation pipelines when benchmarking LLMs. The proposed framework offers a reproducible path for diagnosing evaluator bias and ranking instability in open-ended language tasks. Future work will apply this methodology to educational assessment tasks, using rubric-based scoring and graph-based traceability to evaluate student responses in technical domains.",
      "year": 2025,
      "venue": "Information",
      "authors": [
        "Catalin Anghel",
        "A. Anghel",
        "Emilia Pecheanu",
        "A. Cocu",
        "A. Istrate",
        "Constantin Adrian Andrei"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/e7beeb2db44d57737a986875dc5af4ddf15959de",
      "pdf_url": "",
      "publication_date": "2025-07-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd4afc30e9f52f7b3224911b83855bd428e3a819",
      "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing",
      "abstract": "Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse. We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code. Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code. Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations. Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Johan Wahr'eus",
        "Ahmed Mohamed Hussain",
        "P. Papadimitratos"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/cd4afc30e9f52f7b3224911b83855bd428e3a819",
      "pdf_url": "",
      "publication_date": "2025-03-27",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "886194f65d0c2a0b56458d8ba2cd96bd69a97f6a",
      "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
      "abstract": "Large language models (LLMs) have achieved widespread adoption across numerous applications. However, many LLMs are vulnerable to malicious attacks even after safety alignment. These attacks typically bypass LLMs' safety guardrails by wrapping the original malicious instructions inside adversarial jailbreaks prompts. Previous research has proposed methods such as adversarial training and prompt rephrasing to mitigate these safety vulnerabilities, but these methods often reduce the utility of LLMs or lead to significant computational overhead and online latency. In this paper, we propose SecurityLingua, an effective and efficient approach to defend LLMs against jailbreak attacks via security-oriented prompt compression. Specifically, we train a prompt compressor designed to discern the\"true intention\"of the input prompt, with a particular focus on detecting the malicious intentions of adversarial prompts. Then, in addition to the original prompt, the intention is passed via the system prompt to the target LLM to help it identify the true intention of the request. SecurityLingua ensures a consistent user experience by leaving the original input prompt intact while revealing the user's potentially malicious intention and stimulating the built-in safety guardrails of the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only a negligible overhead and extra token cost compared to all existing defense methods, making it an especially practical solution for LLM defense. Experimental results demonstrate that SecurityLingua can effectively defend against malicious attacks and maintain utility of the LLM with negligible compute and latency overhead. Our code is available at https://aka.ms/SecurityLingua.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yucheng Li",
        "Surin Ahn",
        "Huiqiang Jiang",
        "Amir H. Abdi",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/886194f65d0c2a0b56458d8ba2cd96bd69a97f6a",
      "pdf_url": "",
      "publication_date": "2025-06-15",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46a4e8c7d5fb84288c067c9ee769273a591226f2",
      "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection",
      "abstract": "Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \\&data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the utility drop only 1\\%.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Peiran Wang",
        "Yang Liu",
        "Yunfei Lu",
        "Yifeng Cai",
        "Hongbo Chen",
        "Qingyou Yang",
        "Jie Zhang",
        "Jue Hong",
        "Ye Wu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/46a4e8c7d5fb84288c067c9ee769273a591226f2",
      "pdf_url": "",
      "publication_date": "2025-08-02",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "532f88e78c96a7efb418c6fedd428b741a73e9af",
      "title": "PromptLocate: Localizing Prompt Injection Attacks",
      "abstract": "Prompt injection attacks deceive a large language model into completing an attacker-specified task instead of its intended task by contaminating its input data with an injected prompt, which consists of injected instruction(s) and data. Localizing the injected prompt within contaminated data is crucial for post-attack forensic analysis and data recovery. Despite its growing importance, prompt injection localization remains largely unexplored. In this work, we bridge this gap by proposing PromptLocate, the first method for localizing injected prompts. PromptLocate comprises three steps: (1) splitting the contaminated data into semantically coherent segments, (2) identifying segments contaminated by injected instructions, and (3) pinpointing segments contaminated by injected data. We show PromptLocate accurately localizes injected prompts across eight existing and eight adaptive attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuqi Jia",
        "Yupei Liu",
        "Zedian Shao",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/532f88e78c96a7efb418c6fedd428b741a73e9af",
      "pdf_url": "",
      "publication_date": "2025-10-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2818ef5d5fdafa2760a9fa088e1ee362ad766669",
      "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks",
      "abstract": "A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning to separate instructions and data, so that the LLM does not follow instructions that might be present with data. We evaluate the robustness of this approach in the whitebox setting by constructing strong optimization-based attacks, and show that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for textual LLMs and apply it to three recent whitebox defenses SecAlign (CCS 2025), SecAlign++, and StruQ (USENIX Security 2025), showing attacks with success rates of up to \\textbf{85-95\\%} on unseen prompts with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Nishit V. Pandya",
        "Andrey Labunets",
        "Sicun Gao",
        "Earlence Fernandes"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2818ef5d5fdafa2760a9fa088e1ee362ad766669",
      "pdf_url": "",
      "publication_date": "2025-07-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ba8ea8486125b1381c7e95b1e8669ed614ec3f16",
      "title": "Exploring Potential Prompt Injection Attacks in Federated Military LLMs and Their Mitigation",
      "abstract": "Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these risks, we propose a human-AI collaborative framework with both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Youngjoon Lee",
        "Taehyun Park",
        "Yunho Lee",
        "J. Gong",
        "Joonhyuk Kang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ba8ea8486125b1381c7e95b1e8669ed614ec3f16",
      "pdf_url": "",
      "publication_date": "2025-01-30",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c87a76ebb3df5edcbd7a03699e798d2be1806a9b",
      "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition",
      "abstract": "Large language models (LLMs) have shown remarkable performance across a range of NLP tasks. However, their strong instruction-following capabilities and inability to distinguish instructions from data content make them vulnerable to indirect prompt injection attacks. In such attacks, instructions with malicious purposes are injected into external data sources, such as web documents. When LLMs retrieve this injected data through tools, such as a search engine and execute the injected instructions, they provide misled responses. Recent attack methods have demonstrated potential, but their abrupt instruction injection often undermines their effectiveness. Motivated by the limitations of existing attack methods, we propose TopicAttack, which prompts the LLM to generate a fabricated conversational transition prompt that gradually shifts the topic toward the injected instruction, making the injection smoother and enhancing the plausibility and success of the attack. Through comprehensive experiments, TopicAttack achieves state-of-the-art performance, with an attack success rate (ASR) over 90\\% in most cases, even when various defense methods are applied. We further analyze its effectiveness by examining attention scores. We find that a higher injected-to-original attention ratio leads to a greater success probability, and our method achieves a much higher ratio than the baseline methods.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Yuexin Li",
        "Yue Liu",
        "Yangqiu Song",
        "Bryan Hooi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c87a76ebb3df5edcbd7a03699e798d2be1806a9b",
      "pdf_url": "",
      "publication_date": "2025-07-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection",
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "63f183b8f1fe0d87b605ad60fea393f13afdbd00",
      "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
      "abstract": "Security threats like prompt injection attacks pose significant risks to applications that integrate Large Language Models (LLMs), potentially leading to unauthorized actions such as API misuse. Unlike previous approaches that aim to detect these attacks on a best-effort basis, this paper introduces a novel method that appends an Encrypted Prompt to each user prompt, embedding current permissions. These permissions are verified before executing any actions (such as API calls) generated by the LLM. If the permissions are insufficient, the LLM's actions will not be executed, ensuring safety. This approach guarantees that only actions within the scope of the current permissions from the LLM can proceed. In scenarios where adversarial prompts are introduced to mislead the LLM, this method ensures that any unauthorized actions from LLM wouldn't be executed by verifying permissions in Encrypted Prompt. Thus, threats like prompt injection attacks that trigger LLM to generate harmful actions can be effectively mitigated.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shih-Han Chan"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/63f183b8f1fe0d87b605ad60fea393f13afdbd00",
      "pdf_url": "",
      "publication_date": "2025-03-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d9303eb738a56b0e72404d3ad57964c25611a226",
      "title": "Sentinel: SOTA model to protect against prompt injections",
      "abstract": "Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dror Ivry",
        "Oran Nahum"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d9303eb738a56b0e72404d3ad57964c25611a226",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "502b573666af63c5baa7bc24673c9d4c78dd28e8",
      "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections",
      "abstract": "LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Narek Maloyan",
        "Dmitry Namiot"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/502b573666af63c5baa7bc24673c9d4c78dd28e8",
      "pdf_url": "",
      "publication_date": "2025-04-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt attack",
        "system prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70e92e184448ac59cb585cd6946e1f2d951735f9",
      "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
      "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hao Cheng",
        "Erjia Xiao",
        "Yichi Wang",
        "Kaidi Xu",
        "Mengshu Sun",
        "Jindong Gu",
        "Renjing Xu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/70e92e184448ac59cb585cd6946e1f2d951735f9",
      "pdf_url": "",
      "publication_date": "2025-03-14",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3037549015be6f15d4f1ec59b34554b9238ac0c3",
      "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms",
      "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tarek Gasmi",
        "Ramzi Guesmi",
        "Ines Belhadj",
        "Jihene Bennaceur"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3037549015be6f15d4f1ec59b34554b9238ac0c3",
      "pdf_url": "",
      "publication_date": "2025-07-08",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e086e2f5c47a0f3acd45577bad937ee699c4a570",
      "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory",
      "abstract": "Autonomous web navigation agents, which translate natural language instructions into sequences of browser actions, are increasingly deployed for complex tasks across e-commerce, information retrieval, and content discovery. Due to the stateless nature of large language models (LLMs), these agents rely heavily on external memory systems to maintain context across interactions. Unlike centralized systems where context is securely stored server-side, agent memory is often managed client-side or by third-party applications, creating significant security vulnerabilities. This was recently exploited to attack production systems. We introduce and formalize\"plan injection,\"a novel context manipulation attack that corrupts these agents'internal task representations by targeting this vulnerable context. Through systematic evaluation of two popular web agents, Browser-use and Agent-E, we show that plan injections bypass robust prompt injection defenses, achieving up to 3x higher attack success rates than comparable prompt-based attacks. Furthermore,\"context-chained injections,\"which craft logical bridges between legitimate user goals and attacker objectives, lead to a 17.7% increase in success rate for privacy exfiltration tasks. Our findings highlight that secure memory handling must be a first-class concern in agentic systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Atharv Singh Patlan",
        "Ashwin Hebbar",
        "P. Viswanath",
        "Prateek Mittal"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e086e2f5c47a0f3acd45577bad937ee699c4a570",
      "pdf_url": "",
      "publication_date": "2025-06-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2058fa7cb94af3a7e597fe4812b89166b2ed3d92",
      "title": "Detecting LLM-generated peer reviews",
      "abstract": "The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper\u2019s PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in na\u00efve implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies\u2013including font-based embedding and obfuscated prompts\u2013and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting.",
      "year": 2025,
      "venue": "PLoS ONE",
      "authors": [
        "V. Rao",
        "Aounon Kumar",
        "Hima Lakkaraju",
        "Nihar B. Shah"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2058fa7cb94af3a7e597fe4812b89166b2ed3d92",
      "pdf_url": "",
      "publication_date": "2025-03-20",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ab70897706267d3a6497e23facc93a073b989170",
      "title": "Unified Prompt Attack Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration for improved efficiency and effectiveness. To handle cases where T2I models block image outputs due to defenses, we introduce Sphere-Probing Learning (SPL) to enable optimization even without image results. Following SPL, our model bypasses defenses, inducing the generation of harmful content. To ensure semantic alignment with attacker intent, we propose Semantic-Enhancing Learning (SEL) for precise semantic control. UPAM also prioritizes the naturalness of adversarial prompts using In-context Naturalness Enhancement (INE), making them harder for human examiners to detect. Additionally, we address the issue of iterative queries\u2013common in prior methods and easily detectable by API defenders\u2013by introducing Transferable Attack Learning (TAL), allowing effective attacks with minimal queries. Extensive experiments validate UPAM\u2019s superiority in effectiveness, efficiency, naturalness, and low query detection rates.",
      "year": 2025,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Duo Peng",
        "Qi Ke",
        "Mark He Huang",
        "Ping Hu",
        "Jun Liu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/ab70897706267d3a6497e23facc93a073b989170",
      "pdf_url": "",
      "publication_date": "2025-02-23",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d2d1edfe39d9836d36bd294a7fcf77f2d943d586",
      "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
      "abstract": "Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zihao Wang",
        "Yibo Jiang",
        "Jiahao Yu",
        "Heqing Huang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d2d1edfe39d9836d36bd294a7fcf77f2d943d586",
      "pdf_url": "",
      "publication_date": "2025-05-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2fca1b4a24831357797fe440b0b12fc69e876516",
      "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs",
      "abstract": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tom Pawelek",
        "Raj Patel",
        "Charlotte Crowell",
        "Noorbakhsh Amiri",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Andy Perkins"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2fca1b4a24831357797fe440b0b12fc69e876516",
      "pdf_url": "",
      "publication_date": "2025-09-23",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb8fe747d213a10b1b3080ddd2ad599520f3143a",
      "title": "RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models",
      "abstract": "Recent advances in large language models (LLMs) have underscored the importance of safeguarding intellectual property rights through robust fingerprinting techniques. Traditional fingerprint verification approaches typically focus on a single model, seeking to improve the robustness of its fingerprint.However, these single-model methods often struggle to capture intrinsic commonalities across multiple related models. In this paper, we propose RAP-SM (Robust Adversarial Prompt via Shadow Models), a novel framework that extracts a public fingerprint for an entire series of LLMs. Experimental results demonstrate that RAP-SM effectively captures the intrinsic commonalities among different models while exhibiting strong adversarial robustness. Our findings suggest that RAP-SM presents a valuable avenue for scalable fingerprint verification, offering enhanced protection against potential model breaches in the era of increasingly prevalent LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhenhua Xu",
        "Zhebo Wang",
        "Maike Li",
        "Wenpeng Xing",
        "Chunqiang Hu",
        "Chen Zhi",
        "Meng Han"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/bb8fe747d213a10b1b3080ddd2ad599520f3143a",
      "pdf_url": "",
      "publication_date": "2025-05-08",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2ada4d1a381c341e0b3b7931ba92cd13bdcebfe",
      "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs",
      "abstract": "As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xiang Li",
        "Chong Zhang",
        "Jia Wang",
        "Fangyu Wu",
        "Yushi Li",
        "Xiaobo Jin"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c2ada4d1a381c341e0b3b7931ba92cd13bdcebfe",
      "pdf_url": "",
      "publication_date": "2025-05-26",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d7e2dc2d298bc9ed7d584563f96929c7692cb048",
      "title": "Adversarial Domain Prompt Tuning and Generation for Single Domain Generalization",
      "abstract": "Single domain generalization (SDG) aims to learn a robust model, which could perform well on many unseen domains while there is only one single domain available for training. One of the promising directions for achieving single-domain generalization is to generate out-of-domain (OOD) training data through data augmentation or image generation. Given the rapid advancements in AI-generated content (AIGC), this paper is the first to propose leveraging powerful pre-trained text-to-image (T2I) foundation models to create the training data. However, manually designing textual prompts to generate images for all possible domains is often impractical, and some domain characteristics may be too abstract to describe with words. To address these challenges, we propose a novel Progressive Adversarial Prompt Tuning (PAPT) framework for pre-trained diffusion models. Instead of relying on static textual domains, our approach learns two sets of abstract prompts as conditions for the diffusion model: one that captures domain-invariant category information and another that models domain-specific styles. This adversarial learning mechanism enables the T2I model to generate images in various domain styles while preserving key categorical features. Extensive experiments demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art single-domain generalization approaches.",
      "year": 2025,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zhipeng Xu",
        "De Cheng",
        "Xinyang Jiang",
        "Nan Wang",
        "Dongsheng Li",
        "Xinbo Gao"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/d7e2dc2d298bc9ed7d584563f96929c7692cb048",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3df3a63e65eb6ab71049334466369f33dab37236",
      "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach",
      "abstract": "Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.",
      "year": 2025,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yurong Wu",
        "Fangwen Mu",
        "Qiuhong Zhang",
        "Jinjing Zhao",
        "Xinrun Xu",
        "Lingrui Mei",
        "Yang Wu",
        "Lin Shi",
        "Junjie Wang",
        "Zhiming Ding",
        "Yiwei Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/3df3a63e65eb6ab71049334466369f33dab37236",
      "pdf_url": "",
      "publication_date": "2025-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c3da993174d74465fbb632f2d3de5c31d8b58a26",
      "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI",
      "abstract": "We introduce Cognitive Degradation as a novel vulnerability class in agentic AI systems. Unlike traditional adversarial external threats such as prompt injection, these failures originate internally, arising from memory starvation, planner recursion, context flooding, and output suppression. These systemic weaknesses lead to silent agent drift, logic collapse, and persistent hallucinations over time. To address this class of failures, we introduce the Qorvex Security AI Framework for Behavioral&Cognitive Resilience (QSAF Domain 10), a lifecycle-aware defense framework defined by a six-stage cognitive degradation lifecycle. The framework includes seven runtime controls (QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger proactive mitigation through fallback routing, starvation detection, and memory integrity enforcement. Drawing from cognitive neuroscience, we map agentic architectures to human analogs, enabling early detection of fatigue, starvation, and role collapse. By introducing a formal lifecycle and real-time mitigation controls, this work establishes Cognitive Degradation as a critical new class of AI system vulnerability and proposes the first cross-platform defense model for resilient agentic behavior.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hammad Atta",
        "M. Baig",
        "Yasir Mehmood",
        "Nadeem Shahzad",
        "Ken Huang",
        "Muhammad Aziz Ul Haq",
        "Muhammad Awais",
        "Kamal Ahmed"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/c3da993174d74465fbb632f2d3de5c31d8b58a26",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8a0e375670bd0cbd22b9e4e63029b70ac53be5a3",
      "title": "T2V-OptJail: Discrete Prompt Optimization for Text-to-Video Jailbreak Attacks",
      "abstract": "In recent years, fueled by the rapid advancement of diffusion models, text-to-video (T2V) generation models have achieved remarkable progress, with notable examples including Pika, Luma, Kling, and Open-Sora. Although these models exhibit impressive generative capabilities, they also expose significant security risks due to their vulnerability to jailbreak attacks, where the models are manipulated to produce unsafe content such as pornography, violence, or discrimination. Existing works such as T2VSafetyBench provide preliminary benchmarks for safety evaluation, but lack systematic methods for thoroughly exploring model vulnerabilities. To address this gap, we are the first to formalize the T2V jailbreak attack as a discrete optimization problem and propose a joint objective-based optimization framework, called T2V-OptJail. This framework consists of two key optimization goals: bypassing the built-in safety filtering mechanisms to increase the attack success rate, preserving semantic consistency between the adversarial prompt and the unsafe input prompt, as well as between the generated video and the unsafe input prompt, to enhance content controllability. In addition, we introduce an iterative optimization strategy guided by prompt variants, where multiple semantically equivalent candidates are generated in each round, and their scores are aggregated to robustly guide the search toward optimal adversarial prompts. We conduct large-scale experiments on several T2V models, covering both open-source models and real commercial closed-source models. The experimental results show that the proposed method improves 11.4% and 10.0% over the existing state-of-the-art method in terms of attack success rate assessed by GPT-4, attack success rate assessed by human accessors, respectively, verifying the significant advantages of the method in terms of attack effectiveness and content control.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Jiayang Liu",
        "Siyuan Liang",
        "Shiqian Zhao",
        "Rongcheng Tu",
        "Wenbo Zhou",
        "Aishan Liu",
        "Dacheng Tao",
        "S. Lam"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/8a0e375670bd0cbd22b9e4e63029b70ac53be5a3",
      "pdf_url": "",
      "publication_date": "2025-05-10",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e11d50c4e63cdca0b53f913b0b4565a789b14f81",
      "title": "Mitigating Sexual Content Generation via Embedding Distortion in Text-conditioned Diffusion Models",
      "abstract": "Diffusion models show remarkable image generation performance following text prompts, but risk generating sexual contents. Existing approaches, such as prompt filtering, concept removal, and even sexual contents mitigation methods, struggle to defend against adversarial attacks while maintaining benign image quality. In this paper, we propose a novel approach called Distorting Embedding Space (DES), a text encoder-based defense mechanism that effectively tackles these issues through innovative embedding space control. DES transforms unsafe embeddings, extracted from a text encoder using unsafe prompts, toward carefully calculated safe embedding regions to prevent unsafe contents generation, while reproducing the original safe embeddings. DES also neutralizes the ``nudity''embedding, by aligning it with neutral embedding to enhance robustness against adversarial attacks. As a result, extensive experiments on explicit content mitigation and adaptive attack defense show that DES achieves state-of-the-art (SOTA) defense, with attack success rate (ASR) of 9.47% on FLUX.1, a recent popular model, and 0.52% on the widely adopted Stable Diffusion v1.5. These correspond to ASR reductions of 76.5% and 63.9% compared to previous SOTA methods, EraseAnything and AdvUnlearn, respectively. Furthermore, DES maintains benign image quality, achieving Frechet Inception Distance and CLIP score comparable to those of the original FLUX.1 and Stable Diffusion v1.5.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Jaesin Ahn",
        "Heechul Jung"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/e11d50c4e63cdca0b53f913b0b4565a789b14f81",
      "pdf_url": "",
      "publication_date": "2025-01-31",
      "keywords_matched": [
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5b3f710d1923323a0113f6db056aa712a04b0f13",
      "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling",
      "abstract": "Prompt injection attacks pose a pervasive threat to the security of Large Language Models (LLMs). State-of-the-art prevention-based defenses typically rely on fine-tuning an LLM to enhance its security, but they achieve limited effectiveness against strong attacks. In this work, we propose \\emph{SecInfer}, a novel defense against prompt injection attacks built on \\emph{inference-time scaling}, an emerging paradigm that boosts LLM capability by allocating more compute resources for reasoning during inference. SecInfer consists of two key steps: \\emph{system-prompt-guided sampling}, which generates multiple responses for a given input by exploring diverse reasoning paths through a varied set of system prompts, and \\emph{target-task-guided aggregation}, which selects the response most likely to accomplish the intended task. Extensive experiments show that, by leveraging additional compute at inference, SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses as well as existing inference-time scaling approaches.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yupei Liu",
        "Yanting Wang",
        "Yuqi Jia",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5b3f710d1923323a0113f6db056aa712a04b0f13",
      "pdf_url": "",
      "publication_date": "2025-09-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca590b1b00a31f599702f90b5b44bef61a590188",
      "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions",
      "abstract": "Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g.,\"What is 3 + 2?\") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xuyang Guo",
        "Zekai Huang",
        "Zhao Song",
        "Jiahao Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/ca590b1b00a31f599702f90b5b44bef61a590188",
      "pdf_url": "",
      "publication_date": "2025-08-16",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7b9f2477dce1435f698b4dfa605c1551e5b08a4e",
      "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection",
      "abstract": "We demonstrate how AI-powered cybersecurity tools can be turned against themselves through prompt injection attacks. Prompt injection is reminiscent of cross-site scripting (XSS): malicious text is hidden within seemingly trusted content, and when the system processes it, that text is transformed into unintended instructions. When AI agents designed to find and exploit vulnerabilities interact with malicious web servers, carefully crafted reponses can hijack their execution flow, potentially granting attackers system access. We present proof-of-concept exploits against the Cybersecurity AI (CAI) framework and its CLI tool, and detail our mitigations against such attacks in a multi-layered defense implementation. Our findings indicate that prompt injection is a recurring and systemic issue in LLM-based architectures, one that will require dedicated work to address, much as the security community has had to do with XSS in traditional web applications.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "V. Vilches",
        "Per Mannermaa Rynning"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7b9f2477dce1435f698b4dfa605c1551e5b08a4e",
      "pdf_url": "",
      "publication_date": "2025-08-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "88637a2c392f1c185bc282d4abaccc221a0bb291",
      "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression",
      "abstract": "While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a\"thinking-stopped\"vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named\"Reasoning Interruption Attack\", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Cui",
        "Yujun Cai",
        "Yiwei Wang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/88637a2c392f1c185bc282d4abaccc221a0bb291",
      "pdf_url": "",
      "publication_date": "2025-04-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1e545d6f64e40ce621f10a96b6204078bed2002a",
      "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools",
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex reasoning and decision-making by leveraging external tools. However, this tool-centric paradigm introduces a previously underexplored attack surface: adversaries can manipulate tool metadata -- such as names, descriptions, and parameter schemas -- to influence agent behavior. We identify this as a new and stealthy threat surface that allows malicious tools to be preferentially selected by LLM agents, without requiring prompt injection or access to model internals. To demonstrate and exploit this vulnerability, we propose the Attractive Metadata Attack (AMA), a black-box in-context learning framework that generates highly attractive but syntactically and semantically valid tool metadata through iterative optimization. Our attack integrates seamlessly into standard tool ecosystems and requires no modification to the agent's execution framework. Extensive experiments across ten realistic, simulated tool-use scenarios and a range of popular LLM agents demonstrate consistently high attack success rates (81\\%-95\\%) and significant privacy leakage, with negligible impact on primary task execution. Moreover, the attack remains effective even under prompt-level defenses and structured tool-selection protocols such as the Model Context Protocol, revealing systemic vulnerabilities in current agent architectures. These findings reveal that metadata manipulation constitutes a potent and stealthy attack surface, highlighting the need for execution-level security mechanisms that go beyond prompt-level defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kanghua Mo",
        "Li Hu",
        "Yucheng Long",
        "Zhihao Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1e545d6f64e40ce621f10a96b6204078bed2002a",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a7e5dd0b6cf4934876e60531fff22716421104e",
      "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
      "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but this may expose them to extraction attacks, leading to potential copyright and privacy risks. However, existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts Knowledge Extraction on RAG systems through benign queries. Specifically, IKEA first leverages anchor concepts-keywords related to internal knowledge-to generate queries with a natural appearance, and then designs two mechanisms that lead anchor concepts to thoroughly\"explore\"the RAG's knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response histories, ensuring their relevance to the topic; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions shows comparable performance to the original RAG and outperforms those based on baselines across multiple evaluation tasks, underscoring the stealthy copyright infringement risk in RAG systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuhao Wang",
        "Wenjie Qu",
        "Yanze Jiang",
        "Zichen Liu",
        "Yue Liu",
        "Shengfang Zhai",
        "Yinpeng Dong",
        "Jiaheng Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3a7e5dd0b6cf4934876e60531fff22716421104e",
      "pdf_url": "",
      "publication_date": "2025-05-21",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a60e902d31e306c0118cc6f90a1a095142bb650b",
      "title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models",
      "abstract": "The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yang Feng",
        "Xudong Pan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/a60e902d31e306c0118cc6f90a1a095142bb650b",
      "pdf_url": "",
      "publication_date": "2025-04-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e8f42c742f2e6d3f570ec4ae4089309c64a61b24",
      "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
      "abstract": "Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.",
      "year": 2025,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yanting Wang",
        "Wei Zou",
        "Runpeng Geng",
        "Jinyuan Jia"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/e8f42c742f2e6d3f570ec4ae4089309c64a61b24",
      "pdf_url": "",
      "publication_date": "2025-06-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "80581658e7c708c40e635a5360b5eca9a31eee1d",
      "title": "Practical Reasoning Interruption Attacks on Reasoning Large Language Models",
      "abstract": "Reasoning large language models (RLLMs) have demonstrated outstanding performance across a variety of tasks, yet they also expose numerous security vulnerabilities. Most of these vulnerabilities have centered on the generation of unsafe content. However, recent work has identified a distinct\"thinking-stopped\"vulnerability in DeepSeek-R1: under adversarial prompts, the model's reasoning process ceases at the system level and produces an empty final answer. Building upon this vulnerability, researchers developed a novel prompt injection attack, termed reasoning interruption attack, and also offered an initial analysis of its root cause. Through extensive experiments, we verify the previous analyses, correct key errors based on three experimental findings, and present a more rigorous explanation of the fundamental causes driving the vulnerability. Moreover, existing attacks typically require over 2,000 tokens, impose significant overhead, reduce practicality, and are easily detected. To overcome these limitations, we propose the first practical reasoning interruption attack. It succeeds with just 109 tokens by exploiting our newly uncovered\"reasoning token overflow\"(RTO) effect to overwrite the model's final answer, forcing it to return an invalid response. Experimental results demonstrate that our proposed attack is highly effective. Furthermore, we discover that the method for triggering RTO differs between the official DeepSeek-R1 release and common unofficial deployments. As a broadened application of RTO, we also construct a novel jailbreak attack that enables the transfer of unsafe content within the reasoning tokens into final answer, thereby exposing it to the user. Our work carries significant implications for enhancing the security of RLLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Cui",
        "Cong Zuo"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/80581658e7c708c40e635a5360b5eca9a31eee1d",
      "pdf_url": "",
      "publication_date": "2025-05-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4fda35ce967672e548e65ad50ef9cbbb2c4dbfbc",
      "title": "A Systematization of Security Vulnerabilities in Computer Use Agents",
      "abstract": "Computer Use Agents (CUAs), autonomous systems that interact with software interfaces via browsers or virtual machines, are rapidly being deployed in consumer and enterprise environments. These agents introduce novel attack surfaces and trust boundaries that are not captured by traditional threat models. Despite their growing capabilities, the security boundaries of CUAs remain poorly understood. In this paper, we conduct a systematic threat analysis and testing of real-world CUAs under adversarial conditions. We identify seven classes of risks unique to the CUA paradigm, and analyze three concrete exploit scenarios in depth: (1) clickjacking via visual overlays that mislead interface-level reasoning, (2) indirect prompt injection that enables Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure attacks that manipulate implicit interface framing to hijack multi-step reasoning. These case studies reveal deeper architectural flaws across current CUA implementations. Namely, a lack of input provenance tracking, weak interface-action binding, and insufficient control over agent memory and delegation. We conclude by proposing a CUA-specific security evaluation framework and design principles for safe deployment in adversarial and high-stakes settings.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daniel Jones",
        "Giorgio Severi",
        "Martin Pouliot",
        "Gary Lopez",
        "Joris de Gruyter",
        "Santiago Zanella-B\u00e9guelin",
        "Justin Song",
        "Blake Bullwinkel",
        "Pamela Cortez",
        "Amanda Minnich"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/4fda35ce967672e548e65ad50ef9cbbb2c4dbfbc",
      "pdf_url": "",
      "publication_date": "2025-07-07",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7df45234de79e1c93651813fa7d2acfc3c2e6f00",
      "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations",
      "abstract": "As Large Language Model (LLM) agents become increasingly capable of automating complex, multi-step tasks, the need for robust, secure, and predictable architectural patterns is paramount. This paper provides a comprehensive guide to the ``Plan-then-Execute''(P-t-E) pattern, an agentic design that separates strategic planning from tactical execution. We explore the foundational principles of P-t-E, detailing its core components - the Planner and the Executor - and its architectural advantages in predictability, cost-efficiency, and reasoning quality over reactive patterns like ReAct (Reason + Act). A central focus is placed on the security implications of this design, particularly its inherent resilience to indirect prompt injection attacks by establishing control-flow integrity. We argue that while P-t-E provides a strong foundation, a defense-in-depth strategy is necessary, and we detail essential complementary controls such as the Principle of Least Privilege, task-scoped tool access, and sandboxed code execution. To make these principles actionable, this guide provides detailed implementation blueprints and working code references for three leading agentic frameworks: LangChain (via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing the P-t-E pattern is analyzed, highlighting unique features like LangGraph's stateful graphs for re-planning, CrewAI's declarative tool scoping for security, and AutoGen's built-in Docker sandboxing. Finally, we discuss advanced patterns, including dynamic re-planning loops, parallel execution with Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop (HITL) verification, to offer a complete strategic blueprint for architects, developers, and security engineers aiming to build production-grade, resilient, and trustworthy LLM agents.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ron F. Del Rosario",
        "Klaudia Krawiecka",
        "Christian Schroeder de Witt"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/7df45234de79e1c93651813fa7d2acfc3c2e6f00",
      "pdf_url": "",
      "publication_date": "2025-09-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "68132d7cf87e6e0357757c5ff0bb55105eb9980d",
      "title": "EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System",
      "abstract": "Large language model (LLM) assistants are increasingly integrated into enterprise workflows, raising new security concerns as they bridge internal and external data sources. This paper presents an in-depth case study of EchoLeak (CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365 Copilot that enabled remote, unauthenticated data exfiltration via a single crafted email. By chaining multiple bypasses--evading Microsoft\u2019s XPIA (Cross Prompt Injection Attempt) classifier, circumventing link redaction with reference-style Markdown, exploiting auto-fetched images, and abusing a Microsoft Teams proxy allowed by the content security policy, EchoLeak achieved full privilege escalation across LLM trust boundaries without user interaction. We analyze why existing defenses failed, and outline a set of engineering mitigations including prompt partitioning, enhanced input/output filtering, provenance-based access control, and strict content security policies. Beyond the specific exploit, we derive generalizable lessons for building secure AI copilots, emphasizing the principle of least privilege, defense-in-depth architectures, and continuous adversarial testing. Our findings establish prompt injection as a practical, high-severity vulnerability class in production AI systems and provide a blueprint for defending against future AI-native threats.",
      "year": 2025,
      "venue": "Proceedings of the AAAI Symposium Series",
      "authors": [
        "Pavan Reddy",
        "Aditya Sanjay Gujral"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/68132d7cf87e6e0357757c5ff0bb55105eb9980d",
      "pdf_url": "",
      "publication_date": "2025-09-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c77d0401fcef4b9dcb16e8d8e0f8c688c90abd1d",
      "title": "Prompt Injection in Large Language Model Exploitation: A Security Perspective",
      "abstract": "With the rapid growth of AI technologies, ensuring the security of open-source Large Language Models (LLMs) is crucial to maintaining their reliability and trustworthiness. The paper presents a detailed framework to evaluate the security risks of these models in today's fast-changing technological world. This framework includes generators, probes, detectors, and evaluation methods, all centered around the Prompt Inject framework, which helps identify vulnerabilities and possible attacks on LLMs. By using this approach, organizations, researchers, and developers can assess how easily these models can be manipulated and take steps to improve their security. The paper highlights important applications like security testing, penetration testing, compliance checks, and continuous monitoring, ensuring that LLMs remain safe and reliable. This research is valuable for strengthening cybersecurity efforts in the era of open-source AI, helping to build safer and more trustworthy AI systems.",
      "year": 2025,
      "venue": "2025 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)",
      "authors": [
        "Jefferson Kanjirakkattu Joseph",
        "Esther Daniel",
        "V. Kathiresan",
        "Manimegalai M.A.P"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/c77d0401fcef4b9dcb16e8d8e0f8c688c90abd1d",
      "pdf_url": "",
      "publication_date": "2025-05-01",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "73555d2a139417a205797d5cd6d1eaa2cfa977b4",
      "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems",
      "abstract": "Prompt injection threatens novel applications that emerge from adapting LLMs for various user tasks. The newly developed LLM-based software applications become more ubiquitous and diverse. However, the threat of prompt injection attacks undermines the security of these systems as the mitigation and defenses against them, proposed so far, are insufficient. We investigated the capabilities of early prompt injection detection systems, focusing specifically on the detection performance of techniques implemented in various open-source solutions. These solutions are supposed to detect certain types of prompt injection attacks, including the prompt leak. In prompt leakage attacks, an attacker maliciously manipulates the LLM into outputting its system instructions, violating the system's confidentiality. Our study presents analyzes of distinct prompt leakage detection techniques, and a comparative analysis of several detection solutions, which implement those techniques. We identify the strengths and weaknesses of these techniques and elaborate on their optimal configuration and usage in high-stake deployments. In one of the first studies on existing prompt leak detection solutions, we compared the performances of LLM Guard, Vigil, and Rebuff. We concluded that the implementations of canary word checks in Vigil and Rebuff were not effective at detecting prompt leak attacks, and we proposed improvements for them. We also found an evasion weakness in Rebuff's secondary model-based technique and proposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil, and Rebuff at their peak performance revealed that Vigil is optimal for cases when minimal false positive rate is required, and Rebuff is the most optimal for average needs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Valerii Gakh",
        "Hayretdin Bah\u015fi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/73555d2a139417a205797d5cd6d1eaa2cfa977b4",
      "pdf_url": "",
      "publication_date": "2025-06-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "29678fc094dd52fe3a3dd2f9c27e4d6e60ee607e",
      "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search",
      "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Quy-Anh Dang",
        "Chris Ngo",
        "Truong-Son Hy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/29678fc094dd52fe3a3dd2f9c27e4d6e60ee607e",
      "pdf_url": "",
      "publication_date": "2025-04-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "29ab2b1566ed4f45d91a1208a5fcdafd5449f94a",
      "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
      "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to determine the difficulty of tasks pre-prompting and support Large Language Models (LLMs) in production contexts. By setting thresholds based on the number of thoughts, this metric can discern the difficulty of prompts and support more effective prompt routing. A 2% decrease in latency is achieved when routing prompts from the MathInstruct dataset through quantized, distilled versions of Deepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this metric can be used to detect adversarial prompts used in prompt injection attacks with high efficacy. The Number of Thoughts can inform a classifier that achieves 95% accuracy in adversarial prompt detection. Our experiments ad datasets used are available on our GitHub page: https://github.com/rymarinelli/Number_Of_Thoughts/tree/main.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ryan Marinelli",
        "Josef Pichlmeier",
        "Tamas Bisztray"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/29ab2b1566ed4f45d91a1208a5fcdafd5449f94a",
      "pdf_url": "",
      "publication_date": "2025-03-27",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1310a0d76806c6cf9063e387dcad8e925fba1b8e",
      "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models",
      "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive generalization but remain highly vulnerable to adversarial examples (AEs). Previous work has explored robust text prompts through adversarial training, achieving some improvement in both robustness and generalization. However, they primarily rely on singlegradient direction perturbations (e.g., PGD) to generate AEs, which lack diversity, resulting in limited improvement in adversarial robustness. To address these limitations, we propose an evolution-based region adversarial prompt tuning method called ER-APT, which combines gradient methods with genetic evolution to generate more diverse and challenging AEs. In each training iteration, we first generate AEs using traditional gradient-based methods. Subsequently, a genetic evolution mechanism incorporating selection, mutation, and crossover is applied to optimize the AEs, ensuring a broader and more aggressive perturbation distribution.The final evolved AEs are used for prompt tuning, achieving region-based adversarial optimization instead of conventional single-point adversarial prompt tuning. We also propose a dynamic loss weighting method to adjust prompt learning efficiency for accuracy and robustness. Experimental evaluations on various benchmark datasets demonstrate the superiority of our proposed method, outperforming stateof-the-art APT methods. The code is released at https://github.com/jiaxiaojunQAQ/ER-APT.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xiaojun Jia",
        "Sensen Gao",
        "Simeng Qin",
        "Ke Ma",
        "Xinfeng Li",
        "Yihao Huang",
        "Wei Dong",
        "Yang Liu",
        "Xiaochun Cao"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1310a0d76806c6cf9063e387dcad8e925fba1b8e",
      "pdf_url": "",
      "publication_date": "2025-03-17",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1fedac49b3076815c7b0f04b982616900dcf0f3a",
      "title": "MDAPT: Multi-Modal Depth Adversarial Prompt Tuning to Enhance the Adversarial Robustness of Visual Language Models",
      "abstract": "Large visual language models like Contrastive Language-Image Pre-training (CLIP), despite their excellent performance, are highly vulnerable to the influence of adversarial examples. This work investigates the accuracy and robustness of visual language models (VLMs) from a novel multi-modal perspective. We propose a multi-modal fine-tuning method called Multi-modal Depth Adversarial Prompt Tuning (MDAPT), which guides the generation of visual prompts through text prompts to improve the accuracy and performance of visual language models. We conducted extensive experiments and significantly improved performance on three datasets (\u03f5=4/255). Compared with traditional manual design prompts, the accuracy and robustness increased by an average of 17.84% and 10.85%, respectively. Not only that, our method still has a very good performance improvement under different attack methods. With our efficient settings, compared with traditional manual prompts, our average accuracy and robustness are improved by 32.16% and 21.00%, respectively, under three different attacks.",
      "year": 2025,
      "venue": "Italian National Conference on Sensors",
      "authors": [
        "Chao Li",
        "Yonghao Liao",
        "Caichang Ding",
        "Zhiwei Ye"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1fedac49b3076815c7b0f04b982616900dcf0f3a",
      "pdf_url": "https://doi.org/10.3390/s25010258",
      "publication_date": "2025-01-01",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d55ab3a01de6299b514296b7a5f271285d4cf0a2",
      "title": "Adversarial Prompt Detection in Large Language Models: A Classification-Driven Approach",
      "abstract": ": Large Language Models (LLMs) have significantly advanced human-computer interaction by improving natural language understanding and generation. However, their vulnerability to adversarial prompts\u2013carefully designed inputs that manipulate model outputs\u2013presents substantial challenges. This paper introduces a classification-based approach to detect adversarial prompts by utilizing both prompt features and prompt response features. Eleven machine learning models were evaluated based on key metrics such as accuracy, precision, recall, and F1-score. The results show that the Convolutional Neural Network\u2013Long Short-Term Memory (CNN-LSTM) cascade model delivers the best performance, especially when using prompt features, achieving an accuracy of over 97% in all adversarial scenarios. Furthermore, the Support Vector Machine (SVM) model performed best with prompt response features, particularly excelling in prompt type classification tasks. Classification results revealed that certain types of adversarial attacks, such as \u201cWord Level\u201d and \u201cAdversarial Prefix\u201d, were particularly difficult to detect, as indicated by their low recall and F1-scores. These findings suggest that more subtle manipulations can evade detection mechanisms. In contrast, attacks like \u201cSentence Level\u201d and \u201cAdversarial Insertion\u201d were easier to identify, due to the model\u2019s effectiveness in recognizing inserted content. Natural Language Processing (NLP) techniques played a critical role by enabling the extraction of semantic and syntactic features from both prompts and their corresponding responses. These insights highlight the importance of combining traditional and deep learning approaches, along with advanced NLP techniques, to build more reliable adversarial prompt detection systems for LLMs.",
      "year": 2025,
      "venue": "Computers, Materials &amp; Continua",
      "authors": [
        "Ahmet Emre Erg\u00fcn",
        "Aytug Onan"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/d55ab3a01de6299b514296b7a5f271285d4cf0a2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fc4fe9d58618c0d5aee3cda9edab927569d290cd",
      "title": "Large Language Models for Cybersecurity Intelligence, Threat Hunting, and Decision Support",
      "abstract": "Large language models (LLMs) have emerged as transformative technologies in cybersecurity, offering unprecedented capabilities in threat detection, vulnerability analysis, and intelligent decision-making. This review examines the application of LLMs across critical cybersecurity domains, including cyber threat intelligence (CTI), threat hunting, vulnerability detection, malware analysis, and decision support systems. The integration of LLMs such as Generative Pre-trained Transformer 4 (GPT-4), Bidirectional Encoder Representations from Transformers (BERT), Large Language Model Meta AI (LLaMA), and domain-specific models like SecureFalcon has demonstrated remarkable potential in automating complex security tasks, enhancing analyst productivity, and enabling proactive defense mechanisms. However, the deployment of LLMs in cybersecurity contexts introduces unique challenges, including prompt injection vulnerabilities, data poisoning risks, hallucination concerns, and ethical considerations regarding adversarial use. This paper synthesizes recent research advances, evaluates current LLM architectures and their security applications, examines real-world implementation challenges, and identifies critical gaps requiring further investigation. Through comprehensive analysis of over sixty recent studies, we highlight how LLMs are reshaping cybersecurity practices while emphasizing the necessity for robust security frameworks, continuous model validation, and responsible deployment strategies to mitigate emerging risks associated with these powerful artificial intelligence (AI) systems.",
      "year": 2025,
      "venue": "Computer Life",
      "authors": [
        "Shaochen Ren",
        "Shiyang Chen"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/fc4fe9d58618c0d5aee3cda9edab927569d290cd",
      "pdf_url": "",
      "publication_date": "2025-10-19",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b0794049c22fb3d868a907131d0e8c0a6511025e",
      "title": "Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations",
      "abstract": "Prompt injection attacks are a critical security vulnerability in large language models (LLMs), allowing attackers to hijack model behavior by injecting malicious instructions within the input context. Recent defense mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often implemented through special delimiter tokens or additive embeddings to denote the privilege level of input tokens. However, these prior works typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers of the model. To overcome this limitation, we introduce a novel approach that injects the IH signal into the intermediate token representations within the network. Our method augments these representations with layer-specific trainable embeddings that encode the privilege information. Our evaluations across multiple models and training methods reveal that our proposal yields between $1.6\\times$ and $9.2\\times$ reduction in attack success rate on gradient-based prompt injection attacks compared to state-of-the-art methods, without significantly degrading the model's utility.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sanjay Kariyappa",
        "G. E. Suh"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/b0794049c22fb3d868a907131d0e8c0a6511025e",
      "pdf_url": "",
      "publication_date": "2025-05-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "004dcf35abbce390779107c6a929fa62876b1f69",
      "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment",
      "abstract": "With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system's intelligence cannot be separated from its judgment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sarah Ball",
        "Greg Gluch",
        "S. Goldwasser",
        "Frauke Kreuter",
        "Omer Reingold",
        "G. Rothblum"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/004dcf35abbce390779107c6a929fa62876b1f69",
      "pdf_url": "",
      "publication_date": "2025-07-09",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "239a42e72cea398b972726e0907df5314f631bd3",
      "title": "AGENTFUZZER: Generic Black-Box Fuzzing for Indirect Prompt Injection against LLM Agents",
      "abstract": null,
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zhun Wang",
        "Vincent Siu",
        "Zhe Ye",
        "Tianneng Shi",
        "Yuzhou Nie",
        "Xuandong Zhao",
        "Chenguang Wang",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/239a42e72cea398b972726e0907df5314f631bd3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09377c53e8bff67f1d8f1d778e529736a878f338",
      "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents",
      "abstract": "The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs'dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Hwan Chang",
        "Yonghyun Jun",
        "Hwanhee Lee"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/09377c53e8bff67f1d8f1d778e529736a878f338",
      "pdf_url": "",
      "publication_date": "2025-09-26",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bd8d64d442ec88eaf5c3117e3700e61021db0587",
      "title": "Mind Mapping Prompt Injection: Visual Prompt Injection Attacks in Modern Large Language Models",
      "abstract": "Large language models (LLMs) have made significant strides in generating coherent and contextually relevant responses across diverse domains. However, these advancements have also led to an increase in adversarial attacks, such as prompt injection, where attackers embed malicious instructions within prompts to bypass security filters and manipulate LLM outputs. Various injection techniques, including masking and encoding sensitive words, have been employed to circumvent security measures. While LLMs continuously enhance their security protocols, they remain vulnerable, particularly in multimodal contexts. This study introduces a novel method for bypassing LLM security policies by embedding malicious instructions within a mind map image. The attack leverages the intentional incompleteness of the mind map structure, specifically the absence of explanatory details. When the LLM processes the image and fills in the missing sections, it inadvertently generates unauthorized outputs, violating its intended security constraints. This approach applies to any LLM capable of extracting and interpreting text from images. Compared to the best-performing baseline method, which achieved an ASR of 30.5%, our method reaches an ASR of 90%, yielding an approximately threefold-higher attack success. Understanding this vulnerability is crucial for strengthening security policies in state-of-the-art LLMs.",
      "year": 2025,
      "venue": "Electronics",
      "authors": [
        "Seyong Lee",
        "Jaebeom Kim",
        "Wooguil Pak"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/bd8d64d442ec88eaf5c3117e3700e61021db0587",
      "pdf_url": "",
      "publication_date": "2025-05-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d696ca7d9ad701d6a39db1f5dab0af177e063459",
      "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge",
      "abstract": "Indirect Prompt Injection attacks exploit the inherent limitation of Large Language Models (LLMs) to distinguish between instructions and data in their inputs. Despite numerous defense proposals, the systematic evaluation against adaptive adversaries remains limited, even when successful attacks can have wide security and privacy implications, and many real-world LLM-based applications remain vulnerable. We present the results of LLMail-Inject, a public challenge simulating a realistic scenario in which participants adaptively attempted to inject malicious instructions into emails in order to trigger unauthorized tool calls in an LLM-based email assistant. The challenge spanned multiple defense strategies, LLM architectures, and retrieval configurations, resulting in a dataset of 208,095 unique attack submissions from 839 participants. We release the challenge code, the full dataset of submissions, and our analysis demonstrating how this data can provide new insights into the instruction-data separation problem. We hope this will serve as a foundation for future research towards practical structural solutions to prompt injection.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sahar Abdelnabi",
        "Aideen Fay",
        "Ahmed Salem",
        "Egor Zverev",
        "Kai-Chieh Liao",
        "Chi-Huang Liu",
        "Chun-Chih Kuo",
        "Jannis Weigend",
        "Danyael Manlangit",
        "Alex Apostolov",
        "Haris Umair",
        "Jo\u00e3o Donato",
        "Masayuki Kawakita",
        "A. Mahboob",
        "Tran Huu Bach",
        "Tsun-Han Chiang",
        "Myeongjin Cho",
        "Hajin Choi",
        "Byeonghyeon Kim",
        "Hyeonjin Lee",
        "Benjamin Pannell",
        "Conor McCauley",
        "M. Russinovich",
        "Andrew Paverd",
        "Giovanni Cherubin"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d696ca7d9ad701d6a39db1f5dab0af177e063459",
      "pdf_url": "",
      "publication_date": "2025-06-11",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0ab9c6fcfb5ed7e2e57bd2732180489e29f05b17",
      "title": "Enhancing Security in Large Language Models: A Comprehensive Review of Prompt Injection Attacks and Defenses",
      "abstract": ": This review paper explores advanced methods to prompt Large Language Models (LLMs) into generating objectionable or unintended behaviors through adversarial prompt injection attacks. We examine a series of novel projects like HOUYI, Robustly Aligned LLM (RA-LLM), StruQ, and Virtual Prompt Injection that compel LLMs to produce affirmative responses to harmful queries. Several new benchmarks, such as PromptBench, AdvBench, AttackEval, INJECAGENT, and RobustnessSuite, have been created to evaluate the performance and resilience of LLMs against these adversarial attacks. Results show significant success rates in misleading models like Vicuna-7B, LLaMA-2-7B-Chat, GPT-3.5, and GPT-4. The review highlights limitations in existing defense mechanisms and proposes future directions for enhancing LLM alignment and safety protocols, including the concept of LLM SELF DEFENSE. Our study emphasizes the need for improved robustness in LLMs, which will potentially shape the future of Artificial Intelligence (AI) driven applications and security protocols. Understanding the vulnerabilities of LLMs is crucial for developing effective defenses against adversarial prompt injection attacks. This paper proposes a systemic classification framework that discusses various types of prompt injection attacks and defenses. We also go through a broad spectrum of state-of-the-art attack methods (such as HouYi and Virtual Prompt Injection) alongside advanced defense mechanisms (like RA-LLM, StruQ, and LLM Self-Defense), providing critical insights into vulnerabilities and robustness. We also integrate and compare results from multiple recent benchmarks, including PromptBench, INJECENT, and BIPIA.",
      "year": 2025,
      "venue": "Journal of Artificial Intelligence",
      "authors": [
        "Eleena Sarah Mathew"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/0ab9c6fcfb5ed7e2e57bd2732180489e29f05b17",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8c72e098e53eddb8e1e8fb05601a03a7b0ca57e0",
      "title": "Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection",
      "abstract": "This report presents a real-world case study demonstrating how prompt injection can attack large language model platforms such as ChatGPT according to a proposed injection framework. By providing three real-world examples, we show how adversarial prompts can be injected via user inputs, web-based retrieval, and system-level agent instructions. These attacks, though lightweight and low-cost, can cause persistent and misleading behaviors in LLM outputs. Our case study reveals that even commercial-grade LLMs remain vulnerable to subtle manipulations that bypass safety filters and influence user decisions. \\textbf{More importantly, we stress that this report is not intended as an attack guide, but as a technical alert. As ethical researchers, we aim to raise awareness and call upon developers, especially those at OpenAI, to treat prompt-level security as a critical design priority.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xiangyu Chang",
        "Guang Dai",
        "Hao Di",
        "Haishan Ye"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/8c72e098e53eddb8e1e8fb05601a03a7b0ca57e0",
      "pdf_url": "",
      "publication_date": "2025-04-20",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7222e1b74675ebfd85d5f261ef8ab5010111e737",
      "title": "Defending Against Prompt Injection with DataFilter",
      "abstract": "When large language model (LLM) agents are increasingly deployed to automate tasks and interact with untrusted external data, prompt injection emerges as a significant security threat. By injecting malicious instructions into the data that LLMs access, an attacker can arbitrarily override the original user task and redirect the agent toward unintended, potentially harmful actions. Existing defenses either require access to model weights (fine-tuning), incur substantial utility loss (detection-based), or demand non-trivial system redesign (system-level). Motivated by this, we propose DataFilter, a test-time model-agnostic defense that removes malicious instructions from the data before it reaches the backend LLM. DataFilter is trained with supervised fine-tuning on simulated injections and leverages both the user's instruction and the data to selectively strip adversarial content while preserving benign information. Across multiple benchmarks, DataFilter consistently reduces the prompt injection attack success rates to near zero while maintaining the LLMs'utility. DataFilter delivers strong security, high utility, and plug-and-play deployment, making it a strong practical defense to secure black-box commercial LLMs against prompt injection. Our DataFilter model is released at https://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to reproduce our results at https://github.com/yizhu-joy/DataFilter.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yizhu Wang",
        "Sizhe Chen",
        "Raghad F Alkhudair",
        "Basel Alomair",
        "David Wagner"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7222e1b74675ebfd85d5f261ef8ab5010111e737",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "93329e2eb2d80239d2ace2a191e9dbd887cf65e5",
      "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications",
      "abstract": "The ongoing intense discussion on rising LLM usage in the scientific peer-review process has recently been mingled by reports of authors using hidden prompt injections to manipulate review scores. Since the existence of such\"attacks\"- although seen by some commentators as\"self-defense\"- would have a great impact on the further debate, this paper investigates the practicability and technical success of the described manipulations. Our systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide range of LLMs shows two distinct results: I) very simple prompt injections are indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews are generally biased toward acceptance (>95% in many models). Both results have great impact on the ongoing discussions on LLM usage in peer-review.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Janis Keuper"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/93329e2eb2d80239d2ace2a191e9dbd887cf65e5",
      "pdf_url": "",
      "publication_date": "2025-09-12",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5c771766b54a3e40aee1c045141a0119e2bdfee3",
      "title": "How Not to Detect Prompt Injections with an LLM",
      "abstract": "LLM-integrated applications and agents are vulnerable to prompt injection attacks, where adversaries embed malicious instructions within seemingly benign input data to manipulate the LLM\u2019s intended behavior. Recent defenses based on known-answer detection (KAD) scheme have reported near-perfect performance by observing an LLM\u2019s output to classify input data as clean or contaminated. KAD attempts to repurpose the very susceptibility to prompt injection as a defensive mechanism. We formally characterize the KAD scheme and uncover a structural vulnerability that invalidates its core security premise. To exploit this fundamental vulnerability, we methodically design an adaptive attack, DataFlip. It consistently evades KAD defenses, achieving detection rates as low as \\(0\\%\\) while reliably inducing malicious behavior with a success rate of \\(91\\%\\)\u2014all without requiring white-box access to the LLM or any optimization procedures. We release our evaluation code at [10].",
      "year": 2025,
      "venue": "Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security",
      "authors": [
        "Sarthak Choudhary",
        "Divyam Anshumaan",
        "Nils Palumbo",
        "Somesh Jha"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/5c771766b54a3e40aee1c045141a0119e2bdfee3",
      "pdf_url": "",
      "publication_date": "2025-07-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fde97b99a946bc96d79e0f9d73ef1359594b7d3e",
      "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy",
      "abstract": "Large language models (LLMs) have been widely deployed in Conversational AIs (CAIs), while exposing privacy and security threats. Recent research shows that LLM-based CAIs can be manipulated to extract private information from human users, posing serious security threats. However, the methods proposed in that study rely on a white-box setting that adversaries can directly modify the system prompt. This condition is unlikely to hold in real-world deployments. The limitation raises a critical question: can unprivileged attackers still induce such privacy risks in practical LLM-integrated applications? To address this question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection attack that induces privacy extraction in LLM-integrated applications under black-box settings. By injecting token-efficient data containing false memories, \\textsc{VortexPIA} misleads LLMs to actively request private information in batches. Unlike prior methods, \\textsc{VortexPIA} allows attackers to flexibly define multiple categories of sensitive data. We evaluate \\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs, across four benchmark datasets. The results show that \\textsc{VortexPIA} significantly outperforms baselines and achieves state-of-the-art (SOTA) performance. It also demonstrates efficient privacy requests, reduced token consumption, and enhanced robustness against defense mechanisms. We further validate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated applications, demonstrating its practical effectiveness.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Cui",
        "Sicheng Pan",
        "Yifei Liu",
        "Haibin Zhang",
        "Cong Zuo"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/fde97b99a946bc96d79e0f9d73ef1359594b7d3e",
      "pdf_url": "",
      "publication_date": "2025-10-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e0558eb1b7a8b3b14202feaab710d7ab689f8e9",
      "title": "Evaluating Prompt Injection Attacks with LSTM-Based Generative Adversarial Networks: A Lightweight Alternative to Large Language Models",
      "abstract": "Generative Adversarial Networks (GANs) using Long Short-Term Memory (LSTM) provide a computationally cheaper approach for text generation compared to large language models (LLMs). The low hardware barrier of training GANs poses a threat because it means more bad actors may use them to mass-produce prompt attack messages against LLM systems. Thus, to better understand the threat of GANs being used for prompt attack generation, we train two well-known GAN architectures, SeqGAN and RelGAN, on prompt attack messages. For each architecture, we evaluate generated prompt attack messages, comparing results with each other, with generated attacks from another computationally cheap approach, a 1-billion-parameter Llama 3.2 small language model (SLM), and with messages from the original dataset. This evaluation suggests that GAN architectures like SeqGAN and RelGAN have the potential to be used in conjunction with SLMs to readily generate malicious prompts that impose new threats against LLM-based systems such as chatbots. Analyzing the effectiveness of state-of-the-art defenses against prompt attacks, we also find that GAN-generated attacks can deceive most of these defenses with varying levels of success with the exception of Meta\u2019s PromptGuard. Further, we suggest an improvement of prompt attack defenses based on the analysis of the language quality of the prompts, which we found to be the weakest point of GAN-generated messages.",
      "year": 2025,
      "venue": "Machine Learning and Knowledge Extraction",
      "authors": [
        "Sharaf Rashid",
        "Edson Bollis",
        "Lucas Pellicer",
        "Darian Rabbani",
        "Rafael Palacios",
        "Aneesh Gupta",
        "Amar Gupta"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/3e0558eb1b7a8b3b14202feaab710d7ab689f8e9",
      "pdf_url": "",
      "publication_date": "2025-08-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4ef497f74ae90b475b43d9dafd9ab74357252155",
      "title": "Cyberattacks on Large Language Models - Attack Detection and Architecture Adaptability",
      "abstract": "Large Language Models (LLMs) like GPT and $PaLM$ have transformed natural language processing, enabling advancements in text generation, language translation, and conversational AI. However, their increasing adoption has exposed critical vulnerabilities, making them susceptible to cyberattacks such as prompt injection, data poisoning, and Distributed Denial of Service (DDoS) attacks. These threats compromise the security, reliability, and integrity of LLM applications in real-world scenarios. To mitigate these challenges, advanced defense mechanisms such as attack detectors and attack libraries play a crucial role. Attack detectors analyze input patterns and monitor model responses to identify anomalies and potential security breaches. These systems rely on attack libraries that act as structured repositories of predefined attack patterns, known exploits, and evolving threats. The attack library functions as a dynamic lookup table, enabling real-time vulnerability detection and rapid response to existing and novel threats. By integrating systematic vulnerability detection techniques and structured attack analysis, these mechanisms strengthen LLM security. Attack detectors and libraries provide a comprehensive approach to identifying, classifying, and mitigating security risks. This framework ensures the safe deployment of LLMs in various industries, strengthening their resilience and minimizing exposure to emerging cyber threats in sensitive applications.",
      "year": 2025,
      "venue": "SoutheastCon",
      "authors": [
        "Srikar Alla",
        "Ali Shiri Sichani"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4ef497f74ae90b475b43d9dafd9ab74357252155",
      "pdf_url": "",
      "publication_date": "2025-03-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d619213b444658c84230534793b2df0be0741b51",
      "title": "Securing MCP-based Agent Workflows",
      "abstract": "AI agents are becoming more capable and increasingly integrated into daily life, spanning both enterprise systems and personal applications. However, this adoption introduces new security risks, particularly data leakage through indirect prompt injection attacks. To address this challenge, we present SAMOS, an Information Flow Control (IFC) system designed for the Model Context Protocol (MCP). SAMOS operates at the gateway level, intercepting all MCP tool calls and enforcing security policies based on annotations provided by the agent developer or deployment administrator. By tracking session-level context, SAMOS ensures that information flows remain within intended boundaries and detects policy violations in real time. We validate SAMOS's effectiveness through a case study of a recent vulnerability in the GitHub MCP server, demonstrating that SAMOS can successfully block such attacks while preserving the original functionality.",
      "year": 2025,
      "venue": "PACMI@SOSP",
      "authors": [
        "Grigoris Ntousakis",
        "J. Stephen",
        "Michael V. Le",
        "Sai Sree Laya Chukkapalli",
        "Teryl Taylor",
        "Ian M. Molloy",
        "Frederico Araujo"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/d619213b444658c84230534793b2df0be0741b51",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "869fa31d7eaf24285c4a816e48d23feab440f5c9",
      "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
      "abstract": "Large Language Models (LLMs) are increasingly being integrated into the scientific peer-review process, raising new questions about their reliability and resilience to manipulation. In this work, we investigate the potential for hidden prompt injection attacks, where authors embed adversarial text within a paper's PDF to influence the LLM-generated review. We begin by formalising three distinct threat models that envision attackers with different motivations -- not all of which implying malicious intent. For each threat model, we design adversarial prompts that remain invisible to human readers yet can steer an LLM's output toward the author's desired outcome. Using a user study with domain scholars, we derive four representative reviewing prompts used to elicit peer reviews from LLMs. We then evaluate the robustness of our adversarial prompts across (i) different reviewing prompts, (ii) different commercial LLM-based systems, and (iii) different peer-reviewed papers. Our results show that adversarial prompts can reliably mislead the LLM, sometimes in ways that adversely affect a\"honest-but-lazy\"reviewer. Finally, we propose and empirically assess methods to reduce detectability of adversarial prompts under automated content checks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "M. G. Collu",
        "Umberto Salviati",
        "Roberto Confalonieri",
        "Mauro Conti",
        "Giovanni Apruzzese"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/869fa31d7eaf24285c4a816e48d23feab440f5c9",
      "pdf_url": "",
      "publication_date": "2025-08-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4726a77fbbc397d2596bc3b13fc6ef8eaa9572f2",
      "title": "Hierarchical Knowledge Injection for Improving LLM-based Program Repair",
      "abstract": "Prompting LLMs with bug-related context (e.g., error messages, stack traces) improves automated program repair, but many bugs still remain unresolved. In real-world projects, developers often rely on broader repository and project-level context beyond the local code to resolve such bugs. In this paper, we investigate how automatically extracting and providing such knowledge can improve LLM-based program repair. We propose a layered knowledge injection framework that incrementally augments LLMs with structured context. It starts with the Bug Knowledge Layer, which includes information such as the buggy function and failing tests; expands to the Repository Knowledge Layer, which adds structural dependencies, related files, and commit history; and finally injects the Project Knowledge Layer, which incorporates relevant details from documentation and previously fixed bugs. We evaluate this framework on a dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini), and analyze fix rates across six bug types. By progressively injecting knowledge across layers, our approach achieves a fix rate of 79% (250/314) using Llama 3.3, a significant improvement of 23% over previous work. All bug types show improvement with the addition of repository-level context, while only a subset benefit further from project-level knowledge, highlighting that different bug types require different levels of contextual information for effective repair. We also analyze the remaining unresolved bugs and find that more complex and structurally isolated bugs, such as Program Anomaly and GUI bugs, remain difficult even after injecting all available information. Our results show that layered context injection improves program repair and suggest the need for interactive and adaptive APR systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Ramtin Ehsani",
        "Esteban Parra",
        "Sonia Haiduc",
        "Preetha Chatterjee"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4726a77fbbc397d2596bc3b13fc6ef8eaa9572f2",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "03f458d740dea240d07abe93cd7e756cb6a20cb8",
      "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they remain vulnerable to adversarial manipulations such as jailbreaking via prompt injection attacks. These attacks bypass safety mechanisms to generate restricted or harmful content. In this study, we investigated the underlying latent subspaces of safe and jailbroken states by extracting hidden activations from a LLM. Inspired by attractor dynamics in neuroscience, we hypothesized that LLM activations settle into semi stable states that can be identified and perturbed to induce state transitions. Using dimensionality reduction techniques, we projected activations from safe and jailbroken responses to reveal latent subspaces in lower dimensional spaces. We then derived a perturbation vector that when applied to safe representations, shifted the model towards a jailbreak state. Our results demonstrate that this causal intervention results in statistically significant jailbreak responses in a subset of prompts. Next, we probed how these perturbations propagate through the model's layers, testing whether the induced state change remains localized or cascades throughout the network. Our findings indicate that targeted perturbations induced distinct shifts in activations and model responses. Our approach paves the way for potential proactive defenses, shifting from traditional guardrail based methods to preemptive, model agnostic techniques that neutralize adversarial states at the representation level.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xin Wei Chia",
        "Jonathan Pan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/03f458d740dea240d07abe93cd7e756cb6a20cb8",
      "pdf_url": "",
      "publication_date": "2025-03-11",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45444018a8d2b3b755279034fa30a92dd395858b",
      "title": "DUAL PROMPTING FOR DIVERSE COUNT-LEVEL PET DENOISING",
      "abstract": "The to-be-denoised positron emission tomography (PET) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. In this work, we resort to the recently flourished prompt learning to achieve generalizable PET denoising with different count levels. Specifically, we propose dual prompts to guide the PET denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential PET denoising knowledge. Then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. The prompts are able to dynamically guide the noise-conditioned denoising process. Therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly selected 13-22% fractions of events from 97 18F-MK6240 tau PET studies. It shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.",
      "year": 2025,
      "venue": "IEEE International Symposium on Biomedical Imaging",
      "authors": [
        "Xiaofeng Liu",
        "Yongsong Huang",
        "Thibault Marin",
        "Y. Chemli",
        "Samira Vafay Eslahi",
        "A. Tiss",
        "K. Johnson",
        "Georges El Fakhri",
        "J. Ouyang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/45444018a8d2b3b755279034fa30a92dd395858b",
      "pdf_url": "",
      "publication_date": "2025-04-01",
      "keywords_matched": [
        "inject prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "83271a2428cb507f67b59924d68d86f20020d28b",
      "title": "Smoke and Mirrors: Jailbreaking LLM-based Code Generation via Implicit Malicious Prompts",
      "abstract": "The proliferation of Large Language Models (LLMs) has revolutionized natural language processing and significantly impacted code generation tasks, enhancing software development efficiency and productivity. Notably, LLMs like GPT-4 have demonstrated remarkable proficiency in text-to-code generation tasks. However, the growing reliance on LLMs for code generation necessitates a critical examination of the safety implications associated with their outputs. Existing research efforts have primarily focused on verifying the functional correctness of LLMs, overlooking their safety in code generation. This paper introduces a jailbreaking approach, CodeJailbreaker, designed to uncover safety concerns in LLM-based code generation. The basic observation is that existing safety mechanisms for LLMs are built through the instruction-following paradigm, where malicious intent is explicitly articulated within the instruction of the prompt. Consequently, CodeJailbreaker explores to construct a prompt whose instruction is benign and the malicious intent is implicitly encoded in a covert channel, i.e., the commit message, to bypass the safety mechanism. Experiments on the recently-released RMCBench benchmark demonstrate that CodeJailbreaker markedly surpasses the conventional jailbreaking strategy, which explicitly conveys malicious intents in the instructions, in terms of the attack effectiveness across three code generation tasks. This study challenges the traditional safety paradigms in LLM-based code generation, emphasizing the need for enhanced safety measures in safeguarding against implicit malicious cues.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ouyang Sheng",
        "Yihao Qin",
        "Bo Lin",
        "Liqian Chen",
        "Xiaoguang Mao",
        "Shangwen Wang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/83271a2428cb507f67b59924d68d86f20020d28b",
      "pdf_url": "",
      "publication_date": "2025-03-23",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1d759be1041e39d524d47115e6032eeb2cdcd3a7",
      "title": "Adversarial prompt and fine-tuning attacks threaten medical large language models",
      "abstract": "The integration of Large Language Models (LLMs) into healthcare applications offers promising advancements in medical diagnostics, treatment recommendations, and patient care. However, the susceptibility of LLMs to adversarial attacks poses a significant threat, potentially leading to harmful outcomes in delicate medical contexts. This study investigates the vulnerability of LLMs to two types of adversarial attacks\u2013prompt injections with malicious instructions and fine-tuning with poisoned samples\u2013across three medical tasks: disease prevention, diagnosis, and treatment. Utilizing real-world patient data, we demonstrate that both open-source and proprietary LLMs are vulnerable to malicious manipulation across multiple tasks. We discover that while integrating poisoned data does not markedly degrade overall model performance on medical benchmarks, it can lead to noticeable shifts in fine-tuned model weights, suggesting a potential pathway for detecting and countering model attacks. This research highlights the urgent need for robust security measures and the development of defensive mechanisms to safeguard LLMs in medical applications, to ensure their safe and effective deployment in healthcare settings. Large language models hold significant potential in healthcare settings. This study exposes their vulnerability in medical applications and demonstrates the inadequacy of existing safeguards, highlighting the need for future studies to develop reliable methods for detecting and mitigating these risks.",
      "year": 2025,
      "venue": "Nature Communications",
      "authors": [
        "Yifan Yang",
        "Qiao Jin",
        "Furong Huang",
        "Zhiyong Lu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1d759be1041e39d524d47115e6032eeb2cdcd3a7",
      "pdf_url": "",
      "publication_date": "2025-10-09",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "428fae7f73a29f49d8c6dd80738a08bd69b7e129",
      "title": "Gandalf the Red: Adaptive Security for LLMs",
      "abstract": "Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications.",
      "year": 2025,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Niklas Pfister",
        "V'aclav Volhejn",
        "Manuel Knott",
        "Santiago Arias",
        "Julia Bazi'nska",
        "Mykhailo Bichurin",
        "Alan Commike",
        "Janet Darling",
        "Peter Dienes",
        "Matthew Fiedler",
        "David Haber",
        "Matthias Kraft",
        "Marco Lancini",
        "Max Mathys",
        "Dami'an Pascual-Ortiz",
        "Jakub Podolak",
        "Adria Romero-L'opez",
        "Kyriacos Shiarlis",
        "A. Signer",
        "Z. Ter\u00e9k",
        "Athanasios Theocharis",
        "D. Timbrell",
        "Samuel Trautwein",
        "Samuel Watts",
        "Yun-Han Wu",
        "Mateo Rojas-Carulla"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/428fae7f73a29f49d8c6dd80738a08bd69b7e129",
      "pdf_url": "",
      "publication_date": "2025-01-14",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05952f2e27762c7c61fdfe75563e876b4ca60f36",
      "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack",
      "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities across a wide range of multimodal tasks. However, their integration of visual inputs introduces expanded attack surfaces, thereby exposing them to novel security vulnerabilities. In this work, we conduct a systematic representational analysis to uncover why conventional adversarial attacks can circumvent the safety mechanisms embedded in LVLMs. We further propose a novel two stage evaluation framework for adversarial attacks on LVLMs. The first stage differentiates among instruction non compliance, outright refusal, and successful adversarial exploitation. The second stage quantifies the degree to which the model's output fulfills the harmful intent of the adversarial prompt, while categorizing refusal behavior into direct refusals, soft refusals, and partial refusals that remain inadvertently helpful. Finally, we introduce a normative schema that defines idealized model behavior when confronted with harmful prompts, offering a principled target for safety alignment in multimodal systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Juan Ren",
        "M. Dras",
        "Usman Naseem"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/05952f2e27762c7c61fdfe75563e876b4ca60f36",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41c90f62e27d33ee20e57f6867c3c5bb873084b9",
      "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System",
      "abstract": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sheikh Samit Muhaimin",
        "Spyridon Mastorakis"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/41c90f62e27d33ee20e57f6867c3c5bb873084b9",
      "pdf_url": "",
      "publication_date": "2025-05-02",
      "keywords_matched": [
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2bd95d04d428e00d2c68a88011cb6f80bdb2458",
      "title": "Practical Integration of Large Language Models into Enterprise CI/CD Pipelines for Security Policy Validation: An Industry-Focused Evaluation",
      "abstract": "Manual security policy validation of Infrastructure-as-Code (IaC) creates bottlenecks in enterprise CI/CD pipelines, with 90% of cloud breaches involving misconfigured IaC. Traditional static analyzers struggle with evolving cloud services and custom policies. We propose a production-ready framework augmenting conventional scans with Large Language Models (LLMs) for Kubernetes, IAM, and Terraform validation. Our evaluation on 500 synthetic IaC cases shows ensemble methods achieve F1 = 0.95 at 3.1s latency. LLMs detect complex violations missed by rule-based tools, reducing manual review by 60% and maintenance by 70%. Real-world testing in Jenkins and Bamboo confirms cross-platform compatibility. We provide: (i) privacy-preserving CI/CD architecture, (ii) safeguards against prompt injection and hallucination, and (iii) phased rollout strategy for regulated enterprises balancing security and velocity.",
      "year": 2025,
      "venue": "International Symposium on Service Oriented Software Engineering",
      "authors": [
        "Akshay Mittal",
        "Vivek Venkatesan"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a2bd95d04d428e00d2c68a88011cb6f80bdb2458",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d140f4252fb303653d35b70b5196e1cc579a247",
      "title": "Assessing Vulnerabilities in State-of-the-Art Large Language Models Through Hex Injection (Student Abstract)",
      "abstract": "State-of-the-art large language models (LLMs) are designed with robust safeguards to prevent the disclosure of harmful information and dangerous procedures. However, \"jailbreaking\" techniques can circumvent these protections by exploiting vulnerabilities in the models. This paper introduces a novel method, Hex Injection, which leverages a specific weakness in LLMs' ability to decode encoded text to uncover concealed dangerous instructions. Hex Injection distinguishes itself from traditional methods by combining encoded instructions with plaintext prompts to reveal unsafe content more effectively. Our approach involves encoding potentially malicious prompts in hexadecimal and integrating them. We observe a 94% average success rate (ASR) with a combination of plaintext, encoded, and role-play for Llama 3 and 3.1 models, and an 86% ASR for the Gemma 2 model. This research not only advances the understanding of LLM security but also offers valuable insights for improving safety mechanisms in artificial intelligence systems.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Da Cheng Gu",
        "Wei Liu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/9d140f4252fb303653d35b70b5196e1cc579a247",
      "pdf_url": "",
      "publication_date": "2025-04-11",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e8b21eb382b1060019acb91ed32248abf1015c3",
      "title": "Prompt injection attacks on vision-language models for surgical decision support",
      "abstract": "Importance: Artificial Intelligence-driven analysis of laparoscopic video holds potential to increase the safety and precision of minimally invasive surgery. Vision-language models are particularly promising for video-based surgical decision support due to their capabilities to comprehend complex temporospatial (video) data. However, the same multimodal interfaces that enable such capabilities also introduce new vulnerabilities to manipulations through embedded deceptive text or images (prompt injection attacks). Objective: To systematically evaluate how susceptible state-of-the-art video-capable vision-language models are to textual and visual prompt injection attacks in the context of clinically relevant surgical decision support tasks. Design, Setting, and Participants: In this observational study, we systematically evaluated four state-of-the-art vision-language models, Gemini 1.5 Pro, Gemini 2.5 Pro, GPT-o4-mini-high, and Qwen 2.5-VL, across eleven surgical decision support tasks: detection of bleeding events, foreign objects, image distortions, critical view of safety assessment, and surgical skill assessment. Prompt injection scenarios involved misleading textual prompts and visual perturbations, displayed as white text overlay, applied at varying durations. Main Outcomes and Measures: The primary measure was model accuracy, contrasted between baseline performance and each prompt injection condition. Results: All vision-language models demonstrated good baseline accuracy, with Gemini 2.5 Pro generally achieving the highest mean [standard deviation] accuracy across all tasks (0.82 [0.01]), compared to Gemini 1.5 Pro (0.70 [0.03]) and GPT-o4 mini-high (0.67 [0.06]). Across tasks, Qwen 2.5-VL censored most outputs and achieved an accuracy of (0.58 [0.03]) on non-censored outputs. Textual and temporally-varying visual prompt injections reduced the accuracy for all models. Prolonged visual prompt injections were generally more harmful than single-frame injections. Gemini 2.5 Pro showed the greatest robustness and maintained stable performance for several tasks despite prompt injections, whereas GPT-o4-mini-high exhibited the highest vulnerability, with mean (standard deviation) accuracy across all tasks declining from 0.67 (0.06) at baseline to 0.24 (0.04) under full-duration visual prompt injection (P < .001). Conclusion and Relevance: These findings indicate the critical need for robust temporal reasoning capabilities and specialized guardrails before vision-language models can be safely deployed for real-time surgical decision support.",
      "year": 2025,
      "venue": "medRxiv",
      "authors": [
        "Zheyuan Zhang",
        "Muhammad Ibtsaam Qadir",
        "Matthias Carstens",
        "Evan Hongyang Zhang",
        "Madison Sarah Loiselle",
        "Farren Marc Martinus",
        "Maksymilian Ksawier Mroczkowski",
        "J. Clusmann",
        "J. Kather",
        "Fiona R. Kolbinger"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/0e8b21eb382b1060019acb91ed32248abf1015c3",
      "pdf_url": "",
      "publication_date": "2025-07-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "168c85d78fa61dcd48ab528a8b606c33b57660a2",
      "title": "Text Prompt Injection of Vision Language Models",
      "abstract": "The widespread application of large vision language models has significantly raised safety concerns. In this project, we investigate text prompt injection, a simple yet effective method to mislead these models. We developed an algorithm for this type of attack and demonstrated its effectiveness and efficiency through experiments. Compared to other attack methods, our approach is particularly effective for large models without high demand for computational resources.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ruizhe Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/168c85d78fa61dcd48ab528a8b606c33b57660a2",
      "pdf_url": "",
      "publication_date": "2025-10-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39c923bffb4a8c59df0107d1e492ed691041d15b",
      "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
      "abstract": "Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Gauri Kholkar",
        "Ratinder Ahuja"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/39c923bffb4a8c59df0107d1e492ed691041d15b",
      "pdf_url": "",
      "publication_date": "2025-05-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "072c28bcb6614f928791b6877fba5a793c9a7828",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "abstract": "Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yinuo Liu",
        "Ruohan Xu",
        "Xilong Wang",
        "Yuqi Jia",
        "N. Gong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/072c28bcb6614f928791b6877fba5a793c9a7828",
      "pdf_url": "",
      "publication_date": "2025-10-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7358f8c3e550498066d9429c041ac8435c36d1d5",
      "title": "Evaluating the Effectiveness of Psychological Prompt Injection Attacks on Large Language Models for Social Engineering Artifact Generation",
      "abstract": "This study explores the vulnerability of Large Language Models (LLMs) to prompt injection attacks, a critical security concern. We investigate the effectiveness of four psychological techniques (PTs) from social engineering \u2013 Impersonation, Incentive, Persuasion, and Quid Pro Quo \u2013 in facilitating these attacks. Prompt injection involves manipulating LLMs by embedding malicious instructions within user prompts, potentially generating harmful content or compromising sensitive data. Understanding these mechanisms is crucial for developing effective defenses. Our research assesses how these PTs influence prompt injection success rates against ChatGPT-4o mini and Gemma-7b-it LLMs used for ChatGPT and Gemini respectively. We hypothesized that PTs significantly increase the likelihood of successful attacks, with some techniques being more effective. 220 prompt injection tests (110 per LLM) were conducted, designed to elicit social-engineering artifacts like phishing emails, fake login screens, and ransomware notes, evaluating model susceptibility to diverse attack vectors. The four PTs were chosen based on their relevance to manipulating human behavior in social engineering. Impersonation involves assuming a trusted identity, Incentive offers rewards, Persuasion uses manipulative tactics, and Quid Pro Quo involves reciprocal exchanges. These techniques were adapted for prompt injections to simulate real-world social engineering scenarios. Statistical methods, including ANOVA and Kruskal-Wallis tests, assessed the overall impact of PTs. Mann-Whitney U tests with Bonferroni correction compared individual techniques, and Cohen\u2019s d measured effect sizes. Results demonstrate a statistically significant impact of PTs on prompt injection success. Impersonation was most effective across both LLMs, followed by Persuasion and Quid Pro Quo, with Incentive being least effective. These findings align with social engineering principles, highlighting the power of impersonation and other manipulative tactics. Our research has significant implications for LLM security and AI-driven social engineering. LLM vulnerability to psychologically-driven prompt injections necessitates proactive security measures. Future research should focus on robust defense mechanisms, explore the interplay of PTs, and investigate their impact on LLM security. This study contributes to understanding LLM vulnerabilities and developing more resilient AI systems.",
      "year": 2025,
      "venue": "European Conference on Cyber Warfare and Security",
      "authors": [
        "T. Heverin",
        "Eve Cohen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7358f8c3e550498066d9429c041ac8435c36d1d5",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c7aaffad5e462c8853450ba7792b9554cb7568bf",
      "title": "Comprehensive Analysis of Machine Learning and Deep Learning models on Prompt Injection Classification using Natural Language Processing techniques",
      "abstract": "This study addresses the prompt injection attack based vulnerability in large language models, which poses a significant security concern by allowing unauthorized commands by attackers to manipulate the outputs produced by model. Text classification methods used for detecting these malicious prompts are investigated on the prompt injection dataset obtained from Hugging Face datasets, utilizing a combination of natural language processing-based techniques applied on various machine learning and deep learning algorithms. Multiple vectorization approaches, like the Term Frequency-Inverse Document Frequency, Word2Vec, Bag of Words, and embeddings, are implemented to transform textual data into meaningful representations. The performance of several classifiers is assessed, on their ability to identify between malicious and non-malicious prompts. The Recurrent Neural Network model demonstrated high accuracy, achieving a detection rate of 94.74%. Obtained results indicated that deep learning architectures, particularly those that capture sequential dependencies, are highly effective in identifying prompt injection threats. This study contributes to the evolving field of AI security by addressing the issue of defending LLM based systems against adversarial threats in form of prompt injections. The findings highlight the importance of integrating sequential dependencies and contextual understanding in combatting LLM vulnerabilities. By the application of reliable detection mechanisms, this study enhances the security, integrity, and trustworthiness of AI-driven technologies, ensuring their safe use across diverse applications.",
      "year": 2025,
      "venue": "International Research Journal of Multidisciplinary Technovation",
      "authors": [
        "Bhavvya Jain",
        "Pranav Pawar",
        "Dhruv Gada",
        "Tanish Patwa",
        "Pratik Kanani",
        "Deepali Patil",
        "Lakshmi Kurup"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c7aaffad5e462c8853450ba7792b9554cb7568bf",
      "pdf_url": "",
      "publication_date": "2025-02-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c177630e593936a6698accb0b565cfb841d34517",
      "title": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses",
      "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\\%$ against a newly released defended model from Meta.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Xiaoxue Yang",
        "Bozhidar Stevanoski",
        "Matthieu Meeus",
        "Y. Montjoye"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c177630e593936a6698accb0b565cfb841d34517",
      "pdf_url": "",
      "publication_date": "2025-05-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b780b02d8efd54751a3af61374638bc3d80eb722",
      "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
      "abstract": "Multimodal agents built on large vision-language models (LVLMs) are increasingly deployed in open-world settings but remain highly vulnerable to prompt injection, especially through visual inputs. We introduce AgentTypo, a black-box red-teaming framework that mounts adaptive typographic prompt injection by embedding optimized text into webpage images. Our automatic typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction by substituting captioners while minimizing human detectability via a stealth loss, with a Tree-structured Parzen Estimator guiding black-box optimization over text placement, size, and color. To further enhance attack strength, we develop AgentTypo-pro, a multi-LLM system that iteratively refines injection prompts using evaluation feedback and retrieves successful past examples for continual learning. Effective prompts are abstracted into generalizable strategies and stored in a strategy repository, enabling progressive knowledge accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark across Classifieds, Shopping, and Reddit scenarios show that AgentTypo significantly outperforms the latest image-based attacks such as AgentAttack. On GPT-4o agents, our image-only attack raises the success rate from 0.23 to 0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also outperforming the latest baselines. Our findings reveal that AgentTypo poses a practical and potent threat to multimodal agents and highlight the urgent need for effective defense.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yanjie Li",
        "Yiming Cao",
        "Dong Wang",
        "Bin Xiao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b780b02d8efd54751a3af61374638bc3d80eb722",
      "pdf_url": "",
      "publication_date": "2025-10-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86f7568bd2741409afde19137132db2445b7774d",
      "title": "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents",
      "abstract": "Modern coding agents integrated into IDEs combine powerful tools and system-level actions, exposing a high-stakes attack surface. Existing Indirect Prompt Injection (IPI) studies focus mainly on query-specific behaviors, leading to unstable attacks with lower success rates. We identify a more severe, query-agnostic threat that remains effective across diverse user inputs. This challenge can be overcome by exploiting a common vulnerability: leakage of the agent's internal prompt, which turns the attack into a constrained white-box optimization problem. We present QueryIPI, the first query-agnostic IPI method for coding agents. QueryIPI refines malicious tool descriptions through an iterative, prompt-based process informed by the leaked internal prompt. Experiments on five simulated agents show that QueryIPI achieves up to 87 percent success, outperforming baselines, and the generated malicious descriptions also transfer to real-world systems, highlighting a practical security risk to modern LLM-based coding agents.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuchong Xie",
        "Zesen Liu",
        "Mingyu Luo",
        "Zhixiang Zhang",
        "Kaikai Zhang",
        "Zongjie Li",
        "Ping Chen",
        "Shuai Wang",
        "Dongdong She"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/86f7568bd2741409afde19137132db2445b7774d",
      "pdf_url": "",
      "publication_date": "2025-10-27",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1121c75cb5290163f6b0d7f4b668043eb7cbf4b1",
      "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators",
      "abstract": "Conversational large language models (LLMs) have gained widespread attention due to their instruction-following capabilities. To ensure conversational LLMs follow instructions, role separators are employed to distinguish between different participants in a conversation. However, incorporating role separators introduces potential vulnerabilities. Misusing roles can lead to prompt injection attacks, which can easily misalign the model's behavior with the user's intentions, raising significant security concerns. Although various prompt injection attacks have been proposed, recent research has largely overlooked the impact of role separators on safety. This highlights the critical need to thoroughly understand the systemic weaknesses in dialogue systems caused by role separators. This paper identifies modeling weaknesses caused by role separators. Specifically, we observe a strong positional bias associated with role separators, which is inherent in the format of dialogue modeling and can be triggered by the insertion of role separators. We further develop the Separators Injection Attack (SIA), a new orthometric attack based on role separators. The experiment results show that SIA is efficient and extensive in manipulating model behavior with an average gain of 18.2% for manual methods and enhances the attack success rate to 100% with automatic methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Xitao Li",
        "Haijun Wang",
        "Jiang Wu",
        "Ting Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1121c75cb5290163f6b0d7f4b668043eb7cbf4b1",
      "pdf_url": "",
      "publication_date": "2025-04-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d9b8c45bda83aaf0bf608664a35e5c078bd9d1e",
      "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection",
      "abstract": "Adversarial prompt attacks can significantly alter the reliability of Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce incorrect outputs. In this paper, we present a novel method that applies Differential Evolution (DE) to optimize adversarial prompt suffixes for RAG-based question answering. Our approach is gradient-free, treating the RAG pipeline as a black box and evolving a population of candidate suffixes to maximize the retrieval rank of a targeted incorrect document to be closer to real world scenarios. We conducted experiments on the BEIR QA datasets to evaluate attack success at certain retrieval rank thresholds under multiple retrieving applications. Our results demonstrate that DE-based prompt optimization attains competitive (and in some cases higher) success rates compared to GGPP to dense retrievers and PRADA to sparse retrievers, while using only a small number of tokens (<=5 tokens) in the adversarial suffix. Furthermore, we introduce a readability-aware suffix construction strategy, validated by a statistically significant reduction in MLM negative log-likelihood with Welch's t-test. Through evaluations with a BERT-based adversarial suffix detector, we show that DE-generated suffixes evade detection, yielding near-chance detection accuracy.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jerry Wang",
        "Fang Yu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3d9b8c45bda83aaf0bf608664a35e5c078bd9d1e",
      "pdf_url": "",
      "publication_date": "2025-07-20",
      "keywords_matched": [
        "prompt injection",
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "45661ee761e1b700e60c24d3e7e8866a119da011",
      "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers",
      "abstract": "With the rapid advancement of AI models, their deployment across diverse tasks has become increasingly widespread. A notable emerging application is leveraging AI models to assist in reviewing scientific papers. However, recent reports have revealed that some papers contain hidden, injected prompts designed to manipulate AI reviewers into providing overly favorable evaluations. In this work, we present an early systematic investigation into this emerging threat. We propose two classes of attacks: (1) static attack, which employs a fixed injection prompt, and (2) iterative attack, which optimizes the injection prompt against a simulated reviewer model to maximize its effectiveness. Both attacks achieve striking performance, frequently inducing full evaluation scores when targeting frontier AI reviewers. Furthermore, we show that these attacks are robust across various settings. To counter this threat, we explore a simple detection-based defense. While it substantially reduces the attack success rate, we demonstrate that an adaptive attacker can partially circumvent this defense. Our findings underscore the need for greater attention and rigorous safeguards against prompt-injection threats in AI-assisted peer review.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Qin Zhou",
        "Zhexin Zhang",
        "Zhi Li",
        "Limin Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/45661ee761e1b700e60c24d3e7e8866a119da011",
      "pdf_url": "",
      "publication_date": "2025-11-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "19f4bb442c551929118eeddaf92f594602f9e3ec",
      "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous",
      "abstract": "The growing integration of LLMs into applications has introduced new security risks, notably known as Promptware - maliciously engineered prompts designed to manipulate LLMs to compromise the CIA triad of these applications. While prior research warned about a potential shift in the threat landscape for LLM-powered applications, the risk posed by Promptware is frequently perceived as low. In this paper, we investigate the risk Promptware poses to users of Gemini-powered assistants (web application, mobile application, and Google Assistant). We propose a novel Threat Analysis and Risk Assessment (TARA) framework to assess Promptware risks for end users. Our analysis focuses on a new variant of Promptware called Targeted Promptware Attacks, which leverage indirect prompt injection via common user interactions such as emails, calendar invitations, and shared documents. We demonstrate 14 attack scenarios applied against Gemini-powered assistants across five identified threat classes: Short-term Context Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent Invocation, and Automatic App Invocation. These attacks highlight both digital and physical consequences, including spamming, phishing, disinformation campaigns, data exfiltration, unapproved user video streaming, and control of home automation devices. We reveal Promptware's potential for on-device lateral movement, escaping the boundaries of the LLM-powered application, to trigger malicious actions using a device's applications. Our TARA reveals that 73% of the analyzed threats pose High-Critical risk to end users. We discuss mitigations and reassess the risk (in response to deployed mitigations) and show that the risk could be reduced significantly to Very Low-Medium. We disclosed our findings to Google, which deployed dedicated mitigations.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ben Nassi",
        "Stav Cohen",
        "Or Yair"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/19f4bb442c551929118eeddaf92f594602f9e3ec",
      "pdf_url": "",
      "publication_date": "2025-08-16",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "76ea57995c225d1c2350a3400c721ae8dc7f0cc2",
      "title": "From Prompts to Motors: Man-in-the-Middle Attacks on LLM-Enabled Vacuum Robots",
      "abstract": "The integration of large language models (LLMs) into robotic platforms is transforming human\u2013robot interaction by enabling more natural communication and adaptive task execution. However, this advancement also introduces new security vulnerabilities, particularly in networked environments. In this study, we present a systematic analysis of man-in-the-middle (MITM) attacks targeting an LLM-enabled vacuum robot. Our research follows a three-phase development process: 1) command-line simulation of LLM\u2013robot interactions, 2) tabletop setup, and 3) implementation of a physical robot using a commercial vacuum platform enhanced with a Raspberry Pi\u2013hosted ChatGPT application programming interface (API) and you only look once (YOLO, v8) object detection. We define a gray-box threat model in which an attacker can intercept, inject, and manipulate JavaScript object notation (JSON)-formatted messages exchanged between the robot and the LLM. We evaluate four attack scenarios, two based on prompt injection and two on output manipulation, across three LLM configurations (ChatGPT-4, ChatGPT-4o mini, and ChatGPT-3.5 Turbo). While prior work on LLM security assumes secure communication channels and overlooks network-level threats, our experimental results demonstrate that a remote attacker can bypass safety protocols, override motor commands, and deliver deceptive feedback to users, ultimately leading to unsafe robot behavior. These findings reveal a critical and underexplored attack surface in LLM-integrated robotic systems and highlight the urgent need for secure-by-design communication architectures.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "A. Shaikh",
        "Aygun Varol",
        "Johanna Virkki"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/76ea57995c225d1c2350a3400c721ae8dc7f0cc2",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6bfe90f183f13cd1a0cb51114017a1bd715bbf8c",
      "title": "SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models",
      "abstract": "The rapid adoption of large language models (LLMs) in critical domains has spurred extensive research into their security issues. While input manipulation attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks (BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters and cause severe performance degradation -- have received far less attention. Existing BFA methods suffer from key limitations: they fail to balance performance degradation and output naturalness, making them prone to discovery. In this paper, we introduce SilentStriker, the first stealthy bit-flip attack against LLMs that effectively degrades task performance while maintaining output naturalness. Our core contribution lies in addressing the challenge of designing effective loss functions for LLMs with variable output length and the vast output space. Unlike prior approaches that rely on output perplexity for attack loss formulation, which inevitably degrade output naturalness, we reformulate the attack objective by leveraging key output tokens as targets for suppression, enabling effective joint optimization of attack effectiveness and stealthiness. Additionally, we employ an iterative, progressive search strategy to maximize attack efficacy. Experiments show that SilentStriker significantly outperforms existing baselines, achieving successful attacks without compromising the naturalness of generated text.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haotian Xu",
        "Qingsong Peng",
        "Jie Shi",
        "Huadi Zheng",
        "Yu Li",
        "Zhuo Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/6bfe90f183f13cd1a0cb51114017a1bd715bbf8c",
      "pdf_url": "",
      "publication_date": "2025-09-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "800724dd3b483b163b161fa0656968f66d314b9c",
      "title": "Hacking LLMs: A Technical Analysis of Security Vulnerabilities and Defense Mechanisms",
      "abstract": "Large Language Models (LLMs) such as GPT-4 and Google\u2019s Gemini have revolutionized the landscape of artificial intelligence, enabling sophisticated natural language processing capabilities across diverse domains. However, the rapid adoption of these models has unveiled a spectrum of security vulnerabilities exploitable by adversaries [1], [2]. This paper presents a comprehensive analysis of various attack vectors targeting LLMs, including prompt injection, data poisoning, model inversion, and side-channel attacks. We examine real-world incidents and vulnerabilities discovered in widely-used LLMs like ChatGPT and Gemini [3], highlighting the intricate mechanisms through which these attacks compromise model integrity and user trust. Furthermore, we explore existing defense mechanisms and best practices, emphasizing runtime protection systems, adversarial training approaches, and robust access control systems [4], [5]. Building upon these foundations, we introduce a novel Multi-Layer Defense Framework with Dynamic Trust Scoring, designed to enhance the security and robustness of LLM deployments through dynamic evaluation and adaptive response systems. By dissecting recent advancements in LLM security research and proposing innovative defense strategies, this paper aims to guide the development of resilient defenses against the evolving landscape of AI-driven threats.",
      "year": 2025,
      "venue": "2025 2nd International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)",
      "authors": [
        "Gaurav Raj",
        "Hamzah",
        "Nikhil Raj",
        "Nikhil Ranjan"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/800724dd3b483b163b161fa0656968f66d314b9c",
      "pdf_url": "",
      "publication_date": "2025-02-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "01ed62fbd5999fb1a471b5a7bfd9ab55e806727f",
      "title": "TokenBreak: Bypassing Text Classification Models Through Token Manipulation",
      "abstract": "Natural Language Processing (NLP) models are used for text-related tasks such as classification and generation. To complete these tasks, input data is first tokenized from human-readable text into a format the model can understand, enabling it to make inferences and understand context. Text classification models can be implemented to guard against threats such as prompt injection attacks against Large Language Models (LLMs), toxic input and cybersecurity risks such as spam emails. In this paper, we introduce TokenBreak: a novel attack that can bypass these protection models by taking advantage of the tokenization strategy they use. This attack technique manipulates input text in such a way that certain models give an incorrect classification. Importantly, the end target (LLM or email recipient) can still understand and respond to the manipulated text and therefore be vulnerable to the very attack the protection model was put in place to prevent. The tokenizer is tied to model architecture, meaning it is possible to predict whether or not a model is vulnerable to attack based on family. We also present a defensive strategy as an added layer of protection that can be implemented without having to retrain the defensive model.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kasimir Schulz",
        "Kenneth Yeung",
        "Kieran Evans"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/01ed62fbd5999fb1a471b5a7bfd9ab55e806727f",
      "pdf_url": "",
      "publication_date": "2025-06-09",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16099ac2baa6568bea525fb67eb79a246f529903",
      "title": "CyberLLMInstruct: A Pseudo-Malicious Dataset Revealing Safety-Performance Trade-offs in Cyber Security LLM Fine-tuning",
      "abstract": "The integration of large language models (LLMs) into cyber security applications presents both opportunities and critical safety risks. We introduce CyberLLMInstruct, a dataset of 54,928 pseudo-malicious instruction-response pairs spanning cyber security tasks including malware analysis, phishing simulations, and zero-day vulnerabilities. Our comprehensive evaluation using seven open-source LLMs reveals a critical trade-off: while fine-tuning improves cyber security task performance (achieving up to 92.50% accuracy on CyberMetric), it severely compromises safety resilience across all tested models and attack vectors (e.g., Llama 3.1 8B\u2019s security score against prompt injection drops from 0.95 to 0.15). The dataset incorporates diverse sources including CTF challenges, academic papers, industry reports, and CVE databases to ensure comprehensive coverage of cyber security domains. Our findings highlight the unique challenges of securing LLMs in adversarial domains and establish the critical need for developing fine-tuning methodologies that balance performance gains with safety preservation in security-sensitive domains.",
      "year": 2025,
      "venue": "Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security",
      "authors": [
        "Adel Elzemity",
        "Budi Arief",
        "Shujun Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/16099ac2baa6568bea525fb67eb79a246f529903",
      "pdf_url": "",
      "publication_date": "2025-03-12",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb33f9e07e661b52a6fade948f71d5a0e0bbc773",
      "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
      "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dongsen Zhang",
        "Zekun Li",
        "Xu Luo",
        "Xuannan Liu",
        "Peipei Li",
        "Wenjun Xu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bb33f9e07e661b52a6fade948f71d5a0e0bbc773",
      "pdf_url": "",
      "publication_date": "2025-10-14",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "babb7879069f6c62536f390cbc012883292fce82",
      "title": "When Your Reviewer is an LLM: Biases, Divergence, and Prompt Injection Risks in Peer Review",
      "abstract": "Peer review is the cornerstone of academic publishing, yet the process is increasingly strained by rising submission volumes, reviewer overload, and expertise mismatches. Large language models (LLMs) are now being used as\"reviewer aids,\"raising concerns about their fairness, consistency, and robustness against indirect prompt injection attacks. This paper presents a systematic evaluation of LLMs as academic reviewers. Using a curated dataset of 1,441 papers from ICLR 2023 and NeurIPS 2022, we evaluate GPT-5-mini against human reviewers across ratings, strengths, and weaknesses. The evaluation employs structured prompting with reference paper calibration, topic modeling, and similarity analysis to compare review content. We further embed covert instructions into PDF submissions to assess LLMs'susceptibility to prompt injection. Our findings show that LLMs consistently inflate ratings for weaker papers while aligning more closely with human judgments on stronger contributions. Moreover, while overarching malicious prompts induce only minor shifts in topical focus, explicitly field-specific instructions successfully manipulate specific aspects of LLM-generated reviews. This study underscores both the promises and perils of integrating LLMs into peer review and points to the importance of designing safeguards that ensure integrity and trust in future review processes.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Changjia Zhu",
        "Junjie Xiong",
        "Renkai Ma",
        "Zhicong Lu",
        "Yao Liu",
        "Lingyao Li"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/babb7879069f6c62536f390cbc012883292fce82",
      "pdf_url": "",
      "publication_date": "2025-09-12",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8b9011b256c4819d47721efc3f9522a721b92735",
      "title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers",
      "abstract": "Large Language Model (LLM) based agents integrated into web browsers (often called agentic AI browsers) offer powerful automation of web tasks. However, they are vulnerable to indirect prompt injection attacks, where malicious instructions hidden in a webpage deceive the agent into unwanted actions. These attacks can bypass traditional web security boundaries, as the AI agent operates with the user privileges across sites. In this paper, we present a novel fuzzing framework that runs entirely in the browser and is guided by an LLM to automatically discover such prompt injection vulnerabilities in real time.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Avihay Cohen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8b9011b256c4819d47721efc3f9522a721b92735",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9c3a9812fbdb97cacd3ed9d77a68a978d6205421",
      "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents",
      "abstract": "Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jianshuo Dong",
        "Sheng Guo",
        "Hao Wang",
        "Zhuotao Liu",
        "Tianwei Zhang",
        "Ke Xu",
        "Minlie Huang",
        "Han Qiu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9c3a9812fbdb97cacd3ed9d77a68a978d6205421",
      "pdf_url": "",
      "publication_date": "2025-09-28",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5717e2bd544b999c678c6199e844f790459f7327",
      "title": "PhantomLint: Principled Detection of Hidden LLM Prompts in Structured Documents",
      "abstract": "Hidden LLM prompts have appeared in online documents with increasing frequency. Their goal is to trigger indirect prompt injection attacks while remaining undetected from human oversight, to manipulate LLM-powered automated document processing systems, against applications as diverse as r\\'esum\\'e screeners through to academic peer review processes. Detecting hidden LLM prompts is therefore important for ensuring trust in AI-assisted human decision making. This paper presents the first principled approach to hidden LLM prompt detection in structured documents. We implement our approach in a prototype tool called PhantomLint. We evaluate PhantomLint against a corpus of 3,402 documents, including both PDF and HTML documents, and covering academic paper preprints, CVs, theses and more. We find that our approach is generally applicable against a wide range of methods for hiding LLM prompts from visual inspection, has a very low false positive rate (approx. 0.092%), is practically useful for detecting hidden LLM prompts in real documents, while achieving acceptable performance.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Toby Murray"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5717e2bd544b999c678c6199e844f790459f7327",
      "pdf_url": "",
      "publication_date": "2025-08-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1b59418178da2becc8e8c70b66723fb7c6103f67",
      "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users",
      "abstract": "We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a\"poisoned\"or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Almog Hilel",
        "Idan Shenfeld",
        "Jacob Andreas",
        "Leshem Choshen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/1b59418178da2becc8e8c70b66723fb7c6103f67",
      "pdf_url": "",
      "publication_date": "2025-07-03",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4442d4d54c369034d4e504b311332dd89c463fdb",
      "title": "LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction",
      "abstract": "Question-answering (QA) interfaces powered by large language models (LLMs) present a promising direction for improving interactivity with HVAC system insights, particularly for non-expert users. However, enabling accurate, real-time, and context-aware interactions with HVAC systems introduces unique challenges, including the integration of frequently updated sensor data, domain-specific knowledge grounding, and coherent multi-stage reasoning. In this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to translate high-level user queries into structured execution instructions, and an Agent that performs SQL-based data retrieval, statistical processing, and final response generation. To address HVAC-specific challenges, JARVIS integrates (1) an adaptive context injection strategy for efficient HVAC and deployment-specific information integration, (2) a parameterized SQL builder and executor to improve data access reliability, and (3) a bottom-up planning scheme to ensure consistency across multi-stage response generation. We evaluate JARVIS using real-world data collected from a commercial HVAC system and a ground truth QA dataset curated by HVAC experts to demonstrate its effectiveness in delivering accurate and interpretable responses across diverse queries. Results show that JARVIS consistently outperforms baseline and ablation variants in both automated and user-centered assessments, achieving high response quality and accuracy.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Sungmin Lee",
        "Minju Kang",
        "Joonhee Lee",
        "Seungyong Lee",
        "Dongju Kim",
        "Jingi Hong",
        "Jun Shin",
        "Pei Zhang",
        "Jeonggil Ko"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4442d4d54c369034d4e504b311332dd89c463fdb",
      "pdf_url": "",
      "publication_date": "2025-07-07",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "28d46c365113cef98a603c11c71526ec8f968718",
      "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
      "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically\"analysis paralysis\"or\"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Xingfu Zhou",
        "Pengfei Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/28d46c365113cef98a603c11c71526ec8f968718",
      "pdf_url": "",
      "publication_date": "2025-12-16",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3ffb459ab34a6e766edbbdad32fe2c132ce47580",
      "title": "Reliable Weak-to-Strong Monitoring of LLM Agents",
      "abstract": "We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Neil Kale",
        "Chen Bo Calvin Zhang",
        "Kevin Zhu",
        "Ankit Aich",
        "Paula Rodriguez",
        "Scale Red Team",
        "Christina Q. Knight",
        "Zifan Wang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3ffb459ab34a6e766edbbdad32fe2c132ce47580",
      "pdf_url": "",
      "publication_date": "2025-08-26",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee1acfc49ec441603908d36e2740c8ab5f001dca",
      "title": "Context and Layers in Harmony: A Unified Strategy for Mitigating LLM Hallucinations",
      "abstract": "Large language models, despite their strong performance, frequently produce hallucinated content due to excessive reliance on pre-trained knowledge while insufficiently integrating newly provided context. We introduce LACD, a technique that dynamically rebalances probability distributions across layers, ensuring critical context is not overshadowed. By emphasizing new prompt information, LACD alleviates lower-layer dominance and mitigates hallucinations. On the HotPotQA dataset, LACD outperforms basic context injection baselines by approximately 2.2% in exact match (EM) and matches or exceeds advanced methods such as DoLa and CAD. LACD also demonstrates robust gains on SQuAD, underscoring its capacity to reduce hallucinations while improving factual consistency. Overall, these findings highlight the importance of carefully integrating newly provided context with pre-trained knowledge to achieve more reliable language generation.",
      "year": 2025,
      "venue": "Mathematics",
      "authors": [
        "Sangyeon Yu",
        "Gyunyeop Kim",
        "Sangwoo Kang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ee1acfc49ec441603908d36e2740c8ab5f001dca",
      "pdf_url": "",
      "publication_date": "2025-05-30",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb0c9e8fb0cf2172a09e4e112804c92665cb7823",
      "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
      "abstract": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be twofold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.",
      "year": 2025,
      "venue": "International Conference on Machine Learning and Computing",
      "authors": [
        "Jonathan Pan",
        "Swee Liang Wong",
        "Yidi Yuan",
        "Xin Wei Chia"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/bb0c9e8fb0cf2172a09e4e112804c92665cb7823",
      "pdf_url": "",
      "publication_date": "2025-02-16",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "56a525d0cabb673e0d4149ddd89c948f5ea67386",
      "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks",
      "abstract": "In the past few years, Language Models (LMs) have shown par-human capabilities in several domains. Despite their practical applications and exceeding user consumption, they are susceptible to jailbreaks when malicious input exploits the LM\u2019s weaknesses, causing it to deviate from its intended behavior. Current defensive strategies either classify the input prompt as adversarial or prevent LMs from generating harmful outputs. However, it is challenging to explain the reason behind the malicious nature of the jailbreak, which results in a wide variety of closed-box approaches. In this research, we propose and demonstrate that system-prompt attention from Small Language Models (SLMs) can be used to characterize adversarial prompts, providing a novel, explainable, and cheaper defense approach called AttentionDefense. Our research suggests that the attention mechanism is an integral component in understanding and explaining how LMs respond to malicious input that is not captured in the semantic meaning of text embeddings. The proposed AttentionDefense is evaluated against existing jailbreak benchmark datasets. Ablation studies show that SLM-based AttentionDefense has equivalent or better jailbreak detection performance compared to text embedding-based classifiers and GPT-4 zero-shot detectors. To further validate the efficacy of the proposed approach, we generate a dataset of novel jailbreak variants of the existing benchmark dataset using a closed-loop LLM-based multi-agent system. We demonstrate that the proposed AttentionDefense approach performs robustly on this novel jailbreak dataset while existing approaches suffer in performance. Additionally, for practical purposes AttentionDefense is an ideal solution as it has the computation requirements of a small LM but the performance of a LLM detector.",
      "year": 2025,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Charlotte Siska",
        "A. Sankaran"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/56a525d0cabb673e0d4149ddd89c948f5ea67386",
      "pdf_url": "",
      "publication_date": "2025-04-10",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95701aab9c91aa485c8622f255f6a6a0416f2328",
      "title": "MoAPT: Mixture of Adversarial Prompt Tuning for Vision-Language Models",
      "abstract": "Large pre-trained Vision Language Models (VLMs) demonstrate excellent generalization capabilities but remain highly susceptible to adversarial examples, posing potential security risks. To improve the robustness of VLMs against adversarial examples, adversarial prompt tuning methods are proposed to align the text feature with the adversarial image feature without changing model parameters. However, when facing various adversarial attacks, a single learnable text prompt has insufficient generalization to align well with all adversarial image features, which ultimately results in overfitting. To address the above challenge, in this paper, we empirically find that increasing the number of learned prompts yields greater robustness improvements than simply extending the length of a single prompt. Building on this observation, we propose an adversarial tuning method named \\textbf{Mixture of Adversarial Prompt Tuning (MoAPT)} to enhance the generalization against various adversarial attacks for VLMs. MoAPT aims to learn mixture text prompts to obtain more robust text features. To further enhance the adaptability, we propose a conditional weight router based on the adversarial images to predict the mixture weights of multiple learned prompts, which helps obtain sample-specific mixture text features aligning with different adversarial image features. Extensive experiments across 11 datasets under different settings show that our method can achieve better adversarial robustness than state-of-the-art approaches.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Shiji Zhao",
        "Qihui Zhu",
        "Shukun Xiong",
        "Shouwei Ruan",
        "Maoxun Yuan",
        "Jialing Tao",
        "Jiexi Liu",
        "Ranjie Duan",
        "Jie Zhang",
        "Xingxing Wei"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/95701aab9c91aa485c8622f255f6a6a0416f2328",
      "pdf_url": "",
      "publication_date": "2025-05-23",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ded64097fb6ca7d10c58eeb3506114b8211c85ac",
      "title": "Geometry-Guided Adversarial Prompt Detection via Curvature and Local Intrinsic Dimension",
      "abstract": "Adversarial prompts are capable of jailbreaking frontier large language models (LLMs) and inducing undesirable behaviours, posing a significant obstacle to their safe deployment. Current mitigation strategies primarily rely on activating built-in defence mechanisms or fine-tuning LLMs, both of which are computationally expensive and can sacrifice model utility. In contrast, detection-based approaches are more efficient and practical for deployment in real-world applications. However, the fundamental distinctions between adversarial and benign prompts remain poorly understood. In this work, we introduce CurvaLID, a novel defence framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. To further enhance our solution, we leverage Local Intrinsic Dimensionality (LID) to capture complementary geometric features of text prompts within adversarial subspaces. Our findings show that adversarial prompts exhibit distinct geometric signatures from benign prompts, enabling CurvaLID to achieve near-perfect classification and outperform state-of-the-art detectors in adversarial prompt detection. CurvaLID provides a reliable and efficient safeguard against malicious queries as a model-agnostic method that generalises across multiple LLMs and attack families.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Canaan Yung",
        "Hanxun Huang",
        "S. Erfani",
        "Christopher Leckie"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/ded64097fb6ca7d10c58eeb3506114b8211c85ac",
      "pdf_url": "",
      "publication_date": "2025-03-05",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e31c0bdbdcdb5c862b5532a925017831525bbb70",
      "title": "Benchmarking LLM Robustness Against Prompt-Based Adversarial Attacks",
      "abstract": "As Large Language Models (LLMs) become increasingly integrated into real-world applications and are made more accessible to the general public, significant concerns regarding their security and the safety of the content they output arise. Recent research demonstrates that these models are vulnerable to adversarial techniques, which can manipulate them into generating harmful or biased outputs. These risks highlight the need for robust evaluation methods to assess the resilience of LLMs against these threats. While some efforts have been done to benchmark LLMs' robustness against adversarial prompt-based attacks, these approaches are often highly context-specific and lack the comprehensive components required for a robust and systematic evaluation. In this work we present a benchmarking framework for systematically evaluating the adversarial robustness of LLMs' built-in security and safety measures against text-based prompt attacks during inference time. The proposed framework is composed of the key components that make up a reliable benchmark, including scenarios, workloads, metrics and attack loads and is designed to ensure its representativeness, usefulness and adaptability to different contexts.",
      "year": 2025,
      "venue": "2025 20th European Dependable Computing Conference Companion Proceedings (EDCC-C)",
      "authors": [
        "Jo\u00e3o Donato"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/e31c0bdbdcdb5c862b5532a925017831525bbb70",
      "pdf_url": "",
      "publication_date": "2025-04-08",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9764da15d7a21bdbc8ff5073dd4279b100065c66",
      "title": "Joint Optimization of Prompt Security and System Performance in Edge-Cloud LLM Systems",
      "abstract": "Large language models (LLMs) have significantly facilitated human life, and prompt engineering has improved the efficiency of these models. However, recent years have witnessed a rise in prompt engineering-empowered attacks, leading to issues such as privacy leaks, increased latency, and system resource wastage. Though safety fine-tuning based methods with Reinforcement Learning from Human Feedback (RLHF) are proposed to align the LLMs, existing security mechanisms fail to cope with fickle prompt attacks, highlighting the necessity of performing security detection on prompts. In this paper, we jointly consider prompt security, service latency, and system resource optimization in Edge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt security, a vector-database-enabled lightweight attack detector is proposed. We formalize the problem of joint prompt detection, latency, and resource optimization into a multi-stage dynamic Bayesian game model. The equilibrium strategy is determined by predicting the number of malicious tasks and updating beliefs at each stage through Bayesian updates. The proposed scheme is evaluated on a real implemented EC-LLM system, and the results demonstrate that our approach offers enhanced security, reduces the service latency for benign users, and decreases system resource consumption compared to state-of-the-art algorithms.",
      "year": 2025,
      "venue": "IEEE Conference on Computer Communications",
      "authors": [
        "Haiyang Huang",
        "Tianhui Meng",
        "Weijia Jia"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9764da15d7a21bdbc8ff5073dd4279b100065c66",
      "pdf_url": "",
      "publication_date": "2025-01-30",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9f1ed602f5b3d6701c4783aad59ce42e7da73665",
      "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks",
      "abstract": "This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chengwei Xia",
        "Fan Ma",
        "Ruijie Quan",
        "Kun Zhan",
        "Yi Yang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9f1ed602f5b3d6701c4783aad59ce42e7da73665",
      "pdf_url": "",
      "publication_date": "2025-07-31",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "80c7921964f622abe6c023a2df5d5a824f9c66d9",
      "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by grounding their outputs in external documents. These systems, however, remain vulnerable to attacks on the retrieval corpus, such as prompt injection. RAG-based search systems (e.g., Google's Search AI Overview) present an interesting setting for studying and protecting against such threats, as defense algorithms can benefit from built-in reliability signals -- like document ranking -- and represent a non-LLM challenge for the adversary due to decades of work to thwart SEO. Motivated by, but not limited to, this scenario, this work introduces ReliabilityRAG, a framework for adversarial robustness that explicitly leverages reliability information of retrieved documents. Our first contribution adopts a graph-theoretic perspective to identify a\"consistent majority\"among retrieved documents to filter out malicious ones. We introduce a novel algorithm based on finding a Maximum Independent Set (MIS) on a document graph where edges encode contradiction. Our MIS variant explicitly prioritizes higher-reliability documents and provides provable robustness guarantees against bounded adversarial corruption under natural assumptions. Recognizing the computational cost of exact MIS for large retrieval sets, our second contribution is a scalable weighted sample and aggregate framework. It explicitly utilizes reliability information, preserving some robustness guarantees while efficiently handling many documents. We present empirical results showing ReliabilityRAG provides superior robustness against adversarial attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks where prior robustness-focused methods struggled. Our work is a significant step towards more effective, provably robust defenses against retrieved corpus corruption in RAG.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Zeyu Shen",
        "Basileal Imana",
        "Tong Wu",
        "Chong Xiang",
        "Prateek Mittal",
        "A. Korolova"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/80c7921964f622abe6c023a2df5d5a824f9c66d9",
      "pdf_url": "",
      "publication_date": "2025-09-27",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "096ac14b85070587ca20e77e193c9935adf22fc9",
      "title": "Vulnerabilities and Defenses: A Monograph on Comprehensive Analysis of Security Attacks on Large Language Models",
      "abstract": "This research mainly focused on highly developednatural language processing capabilities, such as large languagemodels (LLMs), which can generate code and power chatbots,among many other uses. Their growing use, though, has putthem under many security risks. This work thoroughlyinvestigates LLM vulnerabilities, including adversarial attacks,data poisoning, prompt injection, privacy leaking, and modelexploitation via jailbreak. Though there is an increasing corpusof defensive tactics, most still have limited reach, potency, oradaptability. The paper lists ideas for the following studies andemphasizes the requirement for strong, generalizable,explainable security solutions. Creating uniform evaluationstandards, adaptive defense mechanisms, more transparentmodels, automated threat detection, and frameworks for ethicalintegration are all part of the approach. Ensuring LLMs callsfor a multidisciplinary strategy that strikes a compromisebetween responsible government and technology innovation.",
      "year": 2025,
      "venue": "Indian Journal of Information Sources and Services",
      "authors": [
        "P. Balakrishnan",
        "A. Leema"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/096ac14b85070587ca20e77e193c9935adf22fc9",
      "pdf_url": "",
      "publication_date": "2025-06-25",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32bc9a01f4b0cbf8976bc682c282181dc6e0c1af",
      "title": "A Survey on Data Security in Large Language Models",
      "abstract": "Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to be integrated into critical real-world systems, understanding and addressing these data-centric security risks is imperative to safeguard user trust and system reliability. This survey offers a comprehensive overview of the main data security risks facing LLMs and reviews current defense strategies, including adversarial training, RLHF, and data augmentation. Additionally, we categorize and analyze relevant datasets used for assessing robustness and security across different domains, providing guidance for future research. Finally, we highlight key research directions that focus on secure model updates, explainability-driven defenses, and effective governance frameworks, aiming to promote the safe and responsible development of LLM technology. This work aims to inform researchers, practitioners, and policymakers, driving progress toward data security in LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kang Chen",
        "Xiuze Zhou",
        "Yuan-Pin Lin",
        "Jinhe Su",
        "Yuanhui Yu",
        "Li Shen",
        "Fan Lin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/32bc9a01f4b0cbf8976bc682c282181dc6e0c1af",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a4699315792b74f3dee87757b365965fda53dca5",
      "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
      "abstract": "Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Li",
        "Yuan Huang",
        "Tao Wang",
        "Caiyu Fan",
        "X-D Cai",
        "Sihan Hu",
        "Xinzijian Liu",
        "Cheng Shi",
        "Mingjun Xu",
        "Zhen Wang",
        "Yan Wang",
        "Xiangqi Jin",
        "Tianhan Zhang",
        "Linfeng Zhang",
        "Lei Wang",
        "Youjin Deng",
        "Pan Zhang",
        "Weijie Sun",
        "Xingyu Li",
        "E. Weinan",
        "Zhiyuan Yao",
        "Kun Chen"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a4699315792b74f3dee87757b365965fda53dca5",
      "pdf_url": "",
      "publication_date": "2025-10-30",
      "keywords_matched": [
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4a6d3e5cb9f53005f97196a09469ca3de3c8edf4",
      "title": "Enterprise AI Must Enforce Participant-Aware Access Control",
      "abstract": "Large language models (LLMs) are increasingly deployed in enterprise settings where they interact with multiple users and are trained or fine-tuned on sensitive internal data. While fine-tuning enhances performance by internalizing domain knowledge, it also introduces a critical security risk: leakage of confidential training data to unauthorized users. These risks are exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG) pipelines that dynamically fetch contextual documents at inference time. We demonstrate data exfiltration attacks on AI assistants where adversaries can exploit current fine-tuning and RAG architectures to leak sensitive information by leveraging the lack of access control enforcement. We show that existing defenses, including prompt sanitization, output filtering, system isolation, and training-level privacy mechanisms, are fundamentally probabilistic and fail to offer robust protection against such attacks. We take the position that only a deterministic and rigorous enforcement of fine-grained access control during both fine-tuning and RAG-based inference can reliably prevent the leakage of sensitive data to unauthorized recipients. We introduce a framework centered on the principle that any content used in training, retrieval, or generation by an LLM is explicitly authorized for \\emph{all users involved in the interaction}. Our approach offers a simple yet powerful paradigm shift for building secure multi-user LLM systems that are grounded in classical access control but adapted to the unique challenges of modern AI workflows. Our solution has been deployed in Microsoft Copilot Tuning, a product offering that enables organizations to fine-tune models using their own enterprise-specific data.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shashank Shreedhar Bhatt",
        "Tanmay Rajore",
        "Khushboo Aggarwal",
        "Ganesh Ananthanarayanan",
        "Ranveer Chandra",
        "Nishanth Chandran",
        "Suyash Choudhury",
        "Divya Gupta",
        "Emre Kiciman",
        "S. Pandey",
        "Srinath T. V. Setty",
        "Rahul Sharma",
        "Teijia Zhao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4a6d3e5cb9f53005f97196a09469ca3de3c8edf4",
      "pdf_url": "",
      "publication_date": "2025-09-18",
      "keywords_matched": [
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "98b790d195d775ee82c42fd596d588a792c6eab3",
      "title": "You've Changed: Detecting Modification of Black-Box Large Language Models",
      "abstract": "Large Language Models (LLMs) are often provided as a service via an API, making it challenging for developers to detect changes in their behavior. We present an approach to monitor LLMs for changes by comparing the distributions of linguistic and psycholinguistic features of generated text. Our method uses a statistical test to determine whether the distributions of features from two samples of text are equivalent, allowing developers to identify when an LLM has changed. We demonstrate the effectiveness of our approach using five OpenAI completion models and Meta's Llama 3 70B chat model. Our results show that simple text features coupled with a statistical test can distinguish between language models. We also explore the use of our approach to detect prompt injection attacks. Our work enables frequent LLM change monitoring and avoids computationally expensive benchmark evaluations.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "A. Dima",
        "James R. Foulds",
        "Shimei Pan",
        "Philip Feldman"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/98b790d195d775ee82c42fd596d588a792c6eab3",
      "pdf_url": "",
      "publication_date": "2025-04-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0cb61ce2a348d044ae0d4ad5d9acc05daa1e7776",
      "title": "Systematically Analysing Prompt Injection Vulnerabilities in Diverse LLM Architectures",
      "abstract": "This paper presents an exploratory systematic analysis of prompt injection vulnerabilities across 36 diverse large language models (LLMs), revealing significant security concerns in these widely adopted AI tools. Prompt injection attacks, which involve crafting inputs to manipulate LLM outputs, pose risks such as unauthorized access, data leaks, and misinformation. Through 144 tests with four tailored prompt injections, we found that 56% of attempts successfully bypassed LLM safeguards, with vulnerability rates ranging from 53% to 61% across different prompt designs. Notably, 28% of tested LLMs were susceptible to all four prompts, indicating a critical lack of robustness.\u00a0Our findings show that model size and architecture significantly influence susceptibility, with smaller models generally more prone to attacks. Statistical methods, including random forest feature analysis and logistic regression, revealed that model parameters play a primary role in vulnerability, though LLM type also contributes. Clustering analysis further identified distinct vulnerability profiles based on model configuration, underscoring the need for multi-faceted defence strategies.\u00a0The study's implications are broad, particularly for sectors integrating LLMs into sensitive applications. Our results align with OWASP and MITRE\u2019s security frameworks, highlighting the urgency for proactive measures, such as human oversight and trust boundaries, to protect against prompt injection risks. Future research should explore multilingual prompt injections and multi-step attack defences to enhance the resilience of LLMs in complex, real-world environments. This work contributes valuable insights into LLM vulnerabilities, aiming to advance the field toward safer AI deployments.",
      "year": 2025,
      "venue": "International Conference on Cyber Warfare and Security",
      "authors": [
        "T. Heverin",
        "Victoria Benjamin",
        "Emily Braca",
        "Israel Carter",
        "Hafsa Kanchwala",
        "Nava Khojasteh",
        "Charly Landow",
        "Yi Luo",
        "Caroline Ma",
        "Anna Magarelli",
        "Rachel Mirin",
        "Avery Moyer",
        "Kayla Simpson",
        "Amelia Skawinski"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0cb61ce2a348d044ae0d4ad5d9acc05daa1e7776",
      "pdf_url": "",
      "publication_date": "2025-03-24",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f502fd8f67f3f849d5ab19cc2cb694e0de2c7d4f",
      "title": "AGENTVIGIL: Automatic Black-Box Red-teaming for Indirect Prompt Injection against LLM Agents",
      "abstract": ",",
      "year": 2025,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Zhun Wang",
        "Vincent Siu",
        "Zhe Ye",
        "Tianneng Shi",
        "Yuzhou Nie",
        "Xuandong Zhao",
        "Chenguang Wang",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f502fd8f67f3f849d5ab19cc2cb694e0de2c7d4f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "87f7524a48c909600717e89946ed117beb7186ac",
      "title": "Fingerprinting LLMs via Prompt Injection",
      "abstract": "Large language models (LLMs) are often modified after release through post-processing such as post-training or quantization, which makes it challenging to determine whether one model is derived from another. Existing provenance detection methods have two main limitations: (1) they embed signals into the base model before release, which is infeasible for already published models, or (2) they compare outputs across models using hand-crafted or random prompts, which are not robust to post-processing. In this work, we propose LLMPrint, a novel detection framework that constructs fingerprints by exploiting LLMs'inherent vulnerability to prompt injection. Our key insight is that by optimizing fingerprint prompts to enforce consistent token preferences, we can obtain fingerprints that are both unique to the base model and robust to post-processing. We further develop a unified verification procedure that applies to both gray-box and black-box settings, with statistical guarantees. We evaluate LLMPrint on five base models and around 700 post-trained or quantized variants. Our results show that LLMPrint achieves high true positive rates while keeping false positive rates near zero.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yuepeng Hu",
        "Zhengyuan Jiang",
        "Mengyuan Li",
        "Osama Ahmed",
        "Zhicong Huang",
        "Cheng Hong",
        "N. Gong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/87f7524a48c909600717e89946ed117beb7186ac",
      "pdf_url": "",
      "publication_date": "2025-09-29",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1115a843e8f3d142180f807c97f71cde65d2ad12",
      "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems",
      "abstract": "Multi-agent debate (MAD) systems leverage collaborative interactions among large language models (LLMs) agents to improve reasoning capabilities. While recent studies have focused on increasing the accuracy and scalability of MAD systems, their security vulnerabilities have received limited attention. In this work, we introduce MAD-Spear, a targeted prompt injection attack that compromises a small subset of agents but significantly disrupts the overall MAD process. Manipulated agents produce multiple plausible yet incorrect responses, exploiting LLMs'conformity tendencies to propagate misinformation and degrade consensus quality. Furthermore, the attack can be composed with other strategies, such as communication attacks, to further amplify its impact by increasing the exposure of agents to incorrect responses. To assess MAD's resilience under attack, we propose a formal definition of MAD fault-tolerance and develop a comprehensive evaluation framework that jointly considers accuracy, consensus efficiency, and scalability. Extensive experiments on five benchmark datasets with varying difficulty levels demonstrate that MAD-Spear consistently outperforms the baseline attack in degrading system performance. Additionally, we observe that agent diversity substantially improves MAD performance in mathematical reasoning tasks, which challenges prior work suggesting that agent diversity has minimal impact on performance. These findings highlight the urgent need to improve the security in MAD design.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Cui",
        "Hongyang Du"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1115a843e8f3d142180f807c97f71cde65d2ad12",
      "pdf_url": "",
      "publication_date": "2025-07-17",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "88043f04702a99d5acc496c6e799b6ae330295c3",
      "title": "ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data",
      "abstract": "Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data. In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated. Our code and data are available at: https://github.com/ReachalWang/ObliInjection.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Ruiqi Wang",
        "Yuqi Jia",
        "N. Gong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/88043f04702a99d5acc496c6e799b6ae330295c3",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9851a84b4749556c02f677a5a9f7c8c7a09c39ea",
      "title": "Prompt Injection Attack Detection with Machine Learning",
      "abstract": "The introduction of large language models into our lives has created a revolution, allowing artificial intelligence to be easily used and accessible by everyone. With this opportunity, large language models are now used in every step of our lives, from education to economy, agriculture to health, and are on their way to becoming a part of us. This widespread use of large language models attracts the attention of malicious people and brings with it security problems and concerns. Despite this rapid growth and expanding attack surface area, the precautions taken by model publishers are inadequate. This study focuses on the current inadequacy in the relevant field and presents a detection suggestion against 'Prompt Injection' attacks, one of the most commonly used attacks against large language models. Within this scope, 10 different models developed using various approaches were used to analyze the input text and classify whether or not it contained attacks. The tests showed that the most successful results were obtained with the BERT-based model, achieving 88 % accuracy and an F1 score of 87 %. The most important feature that distinguishes the study from existing studies is that, unlike existing detection models, it has the ability to detect in Turkish.",
      "year": 2025,
      "venue": "2025 Innovations in Intelligent Systems and Applications Conference (ASYU)",
      "authors": [
        "Berkay \u00d6z\u00e7am",
        "Berksu Ant Solmaz",
        "M. Amasyal\u0131",
        "Mustafa Kara",
        "Muhammed Ali Ayd\u0131n",
        "H. H. Balik"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9851a84b4749556c02f677a5a9f7c8c7a09c39ea",
      "pdf_url": "",
      "publication_date": "2025-09-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "294436434b3ec77475f58c36b236e51102cd0419",
      "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as, aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ishaan Verma"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/294436434b3ec77475f58c36b236e51102cd0419",
      "pdf_url": "",
      "publication_date": "2025-09-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f201926c2fcd7f172dd8370392c39699ea46d093",
      "title": "Prompt Injection Attacks on Large Language Models: Multi-Model Security Analysis with Categorized Attack Types",
      "abstract": ": Large Language Models (LLMs) are widely used in information processing, language interaction, and decision support. The command-based structure of these systems creates security vulnerabilities that can be exploited through attacks designed to bypass security measures and generate malicious content. This study presents a comparative analysis of three LLMs (GPT-4o, Claude 4 Sonnet, and Gemini 2.5 Flash) based on four fundamental security metrics: compliance, filter bypass, sensitive information leakage, and security risk level. The study used an attack dataset containing unethical, harmful, and manipulation-oriented prompts. According to the results, the Claude model demonstrated the most robust security posture by providing secure responses with high consistency. Gemini was the most vulnerable due to filtering failures and information leakage. GPT-4o showed average performance, behaving securely in most scenarios but exhibiting inconsistency in the face of indirect attacks. The findings reveal that LLM security is influenced not only by content-level factors but also by structural factors such as model architectural design, training data scope, and filtering strategies. Therefore, it is critical to regularly test models against attacks and establish transparent, explainable, and ethics-based security principles.",
      "year": 2025,
      "venue": "International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management",
      "authors": [
        "Selin \u015ea\u015fal",
        "\u00d6zg\u00fc Can"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f201926c2fcd7f172dd8370392c39699ea46d093",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9dd05a0a81736b159de62379851851a392f44410",
      "title": "Prompt Injection Attacks on Large Language Models: A Survey of Attack Methods, Root Causes, and Defense Strategies",
      "abstract": null,
      "year": 2025,
      "venue": "Computers, Materials &amp; Continua",
      "authors": [
        "Tongcheng Geng",
        "Zhiyuan Xu",
        "Yubin Qu",
        "W. E. Wong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9dd05a0a81736b159de62379851851a392f44410",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20885362d31a4d213881e2c178ffc6fc0a81fe93",
      "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods",
      "abstract": "With the development of technology, large language models (LLMs) have dominated the downstream natural language processing (NLP) tasks. However, because of the LLMs'instruction-following abilities and inability to distinguish the instructions in the data content, such as web pages from search engines, the LLMs are vulnerable to prompt injection attacks. These attacks trick the LLMs into deviating from the original input instruction and executing the attackers'target instruction. Recently, various instruction hierarchy defense strategies are proposed to effectively defend against prompt injection attacks via fine-tuning. In this paper, we explore more vicious attacks that nullify the prompt injection defense methods, even the instruction hierarchy: backdoor-powered prompt injection attacks, where the attackers utilize the backdoor attack for prompt injection attack purposes. Specifically, the attackers poison the supervised fine-tuning samples and insert the backdoor into the model. Once the trigger is activated, the backdoored model executes the injected instruction surrounded by the trigger. We construct a benchmark for comprehensive evaluation. Our experiments demonstrate that backdoor-powered prompt injection attacks are more harmful than previous prompt injection attacks, nullifying existing prompt injection defense methods, even the instruction hierarchy techniques.",
      "year": 2025,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Yuan Sui",
        "Yangqiu Song",
        "Bryan Hooi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/20885362d31a4d213881e2c178ffc6fc0a81fe93",
      "pdf_url": "",
      "publication_date": "2025-10-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ec84999ddecd420c0e4b215e524e7a7f38e9944",
      "title": "Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling",
      "abstract": "Direct Prompt Injection (DPI) attacks pose a critical security threat to Large Language Models (LLMs) due to their low barrier of execution and high potential damage. To address the impracticality of existing white-box/gray-box methods and the poor transferability of black-box methods, we propose an activations-guided prompt injection attack framework. We first construct an Energy-based Model (EBM) using activations from a surrogate model to evaluate the quality of adversarial prompts. Guided by the trained EBM, we employ the token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize adversarial prompts, thereby enabling gradient-free black-box attacks. Experimental results demonstrate our superior cross-model transferability, achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6% improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen task scenarios. Interpretability analysis reveals a correlation between activations and attack effectiveness, highlighting the critical role of semantic patterns in transferable vulnerability exploitation.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Minghui Li",
        "Hao Zhang",
        "Yechao Zhang",
        "Wei Wan",
        "Sheng-shou Hu",
        "Xiaobing Pei",
        "Jing Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2ec84999ddecd420c0e4b215e524e7a7f38e9944",
      "pdf_url": "",
      "publication_date": "2025-09-09",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7d76841c36a85d550d2211b148d0c593e75f7419",
      "title": "Disrupting Large Language Models with Hidden Prompt Injection Attacks Embedded in HTML Pages",
      "abstract": "This research evaluates the reliability of Large Language Models (LLMs) as collaborative tools for extracting information from various web sources, including standard websites, e-commerce platforms, blogs, landing pages, or even e-mails, where the content is much shorter and the processing language models have a higher chance of getting attacked. Prior to conducting model assessments, we present some methodologies involved in web-scraping approaches, prompt injection attack types, and datasets that evaluate the attack success rate (ASR) of LLMs. Our investigation subjects LLMs to rigorous testing using HTML pages containing concealed instructions designed to terminate response generation when users make queries. The evaluation dataset consists of HTML documents of varying sizes, containing randomly positioned prompt injection attacks programmed to direct the LLM to respond with a single specific key word. Using this benchmark, we assess locally deployed LLM models, revealing their ASR across multiple architectures. The defensive mechanism we propose demonstrates a significant reduction in ASR through the implementation of a filtering system with fixed-sized windows of tokens that analyzes HTML content and eliminates compromised data, reducing the success rate of the attacks, which proved to reduce the ASR.",
      "year": 2025,
      "venue": "2025 International Aegean Conference on Electrical Machines and Power Electronics (ACEMP) & 2025 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)",
      "authors": [
        "Ionu\u0163-Vl\u0103du\u0163 Dinu",
        "G. Danciu",
        "Raul Cristian Vintil\u0103",
        "Titus-Constantin B\u0103lan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7d76841c36a85d550d2211b148d0c593e75f7419",
      "pdf_url": "",
      "publication_date": "2025-05-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "26c2f5dc5d25e0ae473494185395f5476e5e8328",
      "title": "Reconstruction-Based Prompt Generation Algorithm for Prompt Injection Attacks",
      "abstract": "In order to better bypass the security mechanism of the existing model and verify the vulnerabilities of the model in prompt word injection, we propose a reconstructed prompt generation algorithm (RPGA) to explore the prompt injection attacks of large language models. By combining fictional scenes and template-based, more confusing and diverse prompt words are generated. By dynamically generating prompt words, the invisibility and success rate of attacks are enhanced. Experimental results show that RPGA has a high attack success rate on multiple mainstream large language models, which fully verifies the vulnerability of the current model in prompt word injection attacks. Compared with direct prompt injection and indirect prompt injection, the success rate of reconfiguration prompt injection attack is significantly improved. The result highlights the vulnerability of current large language models in the face of complex prompt word attacks, and provides an important reference for the improvement of subsequent model security mechanisms.",
      "year": 2025,
      "venue": "2025 5th International Conference on Advanced Algorithms and Neural Networks (AANN)",
      "authors": [
        "Zhengyi Yu",
        "Chunlong Fan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/26c2f5dc5d25e0ae473494185395f5476e5e8328",
      "pdf_url": "",
      "publication_date": "2025-08-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f9c57e6b76f5207f18c79dadc1d202bab284cb5",
      "title": "GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs",
      "abstract": "Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Jiaji Ma",
        "Puja Trivedi",
        "Danai Koutra"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5f9c57e6b76f5207f18c79dadc1d202bab284cb5",
      "pdf_url": "",
      "publication_date": "2025-11-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20d6d4a901b4b535e46e8a086c6a1f64e13948a4",
      "title": "Securing AI Agents Against Prompt Injection Attacks",
      "abstract": "Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filtering with embedding-based anomaly detection, hierarchical system prompt guardrails, and multi-stage response verification, across seven state-of-the-art language models. Our combined framework reduces successful attack rates from 73.2% to 8.7% while maintaining 94.3% of baseline task performance. We release our benchmark dataset and defense implementation to support future research in AI agent security.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Badrinath Ramakrishnan",
        "Akshaya Balaji"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/20d6d4a901b4b535e46e8a086c6a1f64e13948a4",
      "pdf_url": "",
      "publication_date": "2025-11-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b67115f4decad83af468d64e67297d1648590c90",
      "title": "A Novel Security Framework against Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, but their widespread adoption has introduced critical security vulnerabilities, particularly prompt injection attacks-where attackers design input prompts to induce the model to generate malicious or inappropriate content, potentially leading to sensitive information leakage, execution of malicious instructions, or other security risks. Traditional defense mechanisms, which typically rely on keyword filtering or simple rule matching, struggle to cope with the increasingly complex and dynamically evolving attack methods. To address these risks, researchers have developed defense mechanisms ranging from input sanitization and adversarial training to runtime monitoring.This paper proposes a security framework based on LLMs, aiming to conduct an in-depth analysis of the threat posed by prompt injection attacks and achieve comprehensive risk monitoring of model outputs. The framework analyzes input prompts from multiple dimensions to identify malicious prompts and their potential attack behaviors while simultaneously monitoring and evaluating the output generated by the LLM. Through in-depth analysis of attack techniques and monitoring of model outputs, the proposed framework can effectively recognize and defeat various types of attacks, reduce the potential threats posed by prompt injection attacks, and ensure the security and reliability of LLMs in practical applications. Experimental results show that the framework can achieve over 93% detection accuracy.",
      "year": 2025,
      "venue": "2025 IEEE 5th International Conference on Computer Communication and Artificial Intelligence (CCAI)",
      "authors": [
        "Yumei Zhao",
        "Xiaoming Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b67115f4decad83af468d64e67297d1648590c90",
      "pdf_url": "",
      "publication_date": "2025-05-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8f899eda5a2d8df504d805dcf8d6e5dd9e15d68c",
      "title": "Adversarial and Multilingual Threats in Retrieval-Augmented Generation: From Prompt Injection to Model Exploitation",
      "abstract": "Retrieval-Augmented Generation (RAG) systems have emerged as a cornerstone of modern generative AI by coupling large language models (LLMs) with external knowledge retrieval, thereby enabling more grounded, accurate, and context-aware outputs. Yet this paradigm simultaneously broadens the attack surface\u2014extending vulnerabilities from generation into retrieval pipelines, language interfaces, and exposed APIs. In this paper, we propose a comprehensive threat taxonomy and adversarial framework targeting both conventional and agentic RAG systems, in which LLMs autonomously plan and execute multi-step tasks using retrieved knowledge. Our analysis spans a wide spectrum of adversarial techniques, including prompt injection, jailbreak exploits, API-driven model extraction, and multilingual prompt manipulation. Unlike prior work that primarily focuses on English-only vulnerabilities, we show that multilingual RAG pipelines\u2014particularly those involving language blending (e.g., Arabic-English code-switching)\u2014introduce novel attack vectors that bypass safety filters and induce unintended behaviors. To study these risks, we develop a multilingual adversarial benchmark suite that integrates both synthetic and real-world corpora across English, Arabic, and Spanish. Through systematic evaluation, we demonstrate how the retrieval layer not only amplifies leakage and inversion risks but also enables adversaries to infer system logic, extract private data from vector stores, and manipulate autonomous agent behavior through tool-integrated APIs. To mitigate these threats, we assess multiple defense strategies, including prompt sanitization, cross-lingual content filtering, retriever-stage anomaly detection, and behavior-based profiling of API interactions. Our findings reveal that existing safeguards are insufficient\u2014particularly in agentic or low-resource language scenarios\u2014underscoring the need for architecture-specific and multilingual defenses. This work fills a critical gap in GenAI security by presenting the first holistic, multilingual, and agent-aware security framework for RAG, offering actionable guidance for building robust and trustworthy generative AI applications across diverse linguistic and deployment contexts.",
      "year": 2025,
      "venue": "2025 2nd International Generative AI and Computational Language Modelling Conference (GACLM)",
      "authors": [
        "Basma ElSaify",
        "Mohamed Baderelden"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8f899eda5a2d8df504d805dcf8d6e5dd9e15d68c",
      "pdf_url": "",
      "publication_date": "2025-08-18",
      "keywords_matched": [
        "prompt injection",
        "prompt manipulation",
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1056bd268b2ff96ed0adf70fb7649dffbcb247d7",
      "title": "BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents",
      "abstract": "The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood. In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Kaiyuan Zhang",
        "Mark Tenenholtz",
        "Kyle Polley",
        "Jerry Ma",
        "Denis Yarats",
        "Ninghui Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1056bd268b2ff96ed0adf70fb7649dffbcb247d7",
      "pdf_url": "",
      "publication_date": "2025-11-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1eed09698c6facc20b3c98b09e997a1f09076603",
      "title": "OET: Optimization-based prompt injection Evaluation Toolkit",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jinsheng Pan",
        "Xiaogeng Liu",
        "Chaowei Xiao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1eed09698c6facc20b3c98b09e997a1f09076603",
      "pdf_url": "",
      "publication_date": "2025-05-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "06fa2b9d40f97bd093aa3d21f42828ed82d8f55e",
      "title": "Meticulous Thought Defender: Fine-Grained Chain-of-Thought (CoT) for Detecting Prompt Injection Attacks of Large Language Models",
      "abstract": "Large language models (LLMs) have exhibited exceptional capabilities across various natural language processing tasks, however, they remain susceptible to prompt injection attacks, which pose significant security challenges. Traditional detection methods often fail to effectively identify such attacks due to their reliance on static rules or surface-level analysis. In this study, we introduce a novel, fine-grained CoT based detection framework that enhances the interpretability and robustness of attack identification. By dissecting the step-by-step reasoning process of LLMs, our approach leverages multidimensional anomaly detection mechanisms-encompassing semantic analysis, consistency evaluations at both step and path levels, and confidence estimations to uncover subtle disruptions caused by malicious prompt manipulations. Experimental results demonstrate that our method achieves superior performance metrics, with significant improvements in accuracy and F1 score, outperforming traditional method by 1.16% in accuracy and 3.39% in F1 score, and surpassing LLMs\u2019 intrinsic detection capabilities by 6.24% in accuracy and 7.65% in F1 score. This work not only fortifies the security of LLMs\u2019 applications but also provides a foundational framework for future research on adaptive defenses against evolving attack strategies.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Lijuan Shi",
        "Yajing Kang",
        "Jie Hu",
        "Xinchi Li",
        "Mingchuan Yang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/06fa2b9d40f97bd093aa3d21f42828ed82d8f55e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt",
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df47bafaccfbd2fd6a24f82abbbf1396e758080d",
      "title": "Prevention of Prompt Injection Attacks Over Financial Applications Integrated with LLM",
      "abstract": "Prompt injection is a growing concern in financial applications integrated with Large Language Models. These attacks pose a critical risk to financial applications leading to data breaches, confidential data loss and financial losses by manipulating input prompts to cause unintended or harmful outputs. Existing defense strategies include utilizing filters f or input and output, and delimiters, however, these techniques have been proven inadequate. This paper builds upon existing mechanisms and proposes an enhanced security system that incorporates role-based access, jailbreak attack detection, validating inputs and securely passing prompts to LLM. This is done by developing a pre-processing layer at both ends-the client and LLMs-through the identification of critical as well as destructive keywords. The experimental results show that blocking of that malicious prompt ensures that only authenticated and authorized commands are processed. The measures, therefore, allow financial institutions to significantly reduce the probability of prompt injection attacks besides increasing data protection and confidence i n AI-based financial applications.",
      "year": 2025,
      "venue": "2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT)",
      "authors": [
        "Tanaya Joshi",
        "Vaishnavi Naik",
        "Isha Mistry",
        "Ramchandra Mangrulkar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/df47bafaccfbd2fd6a24f82abbbf1396e758080d",
      "pdf_url": "",
      "publication_date": "2025-04-17",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "02aa2d75f98f84508a6da450774bf6e4b6ea3ff1",
      "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models",
      "abstract": "This paper documents early research conducted in 2022 on defending against prompt injection attacks in large language models, providing historical context for the evolution of this critical security domain. This research focuses on two adversarial attacks against Large Language Models (LLMs): prompt injection and goal hijacking. We examine how to construct these attacks, test them on various LLMs, and compare their effectiveness. We propose and evaluate a novel defense technique called Adversarial Fine-Tuning. Our results show that, without this defense, the attacks succeeded 31\\% of the time on GPT-3 series models. When using our Adversarial Fine-Tuning approach, attack success rates were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie), though we note that subsequent research has revealed limitations of fine-tuning-based defenses. We also find that more flexible models exhibit greater vulnerability to these attacks. Consequently, large models such as GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the specific models tested are now superseded, the core methodology and empirical findings contributed to the foundation of modern prompt injection defense research, including instruction hierarchy systems and constitutional AI approaches.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Gustavo Sandoval",
        "Denys Fenchenko",
        "Junyao Chen"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/02aa2d75f98f84508a6da450774bf6e4b6ea3ff1",
      "pdf_url": "",
      "publication_date": "2025-09-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca09b6c6b7233a4547d59841cffa115a083d9121",
      "title": "The Comprehensive Review on Prompt Injection Attacks and Defense Mechanisms in Large Language Models",
      "abstract": "This review analyzes prompt injection attacks in large language models (LLMs) from 2019 to 2025, addressing critical security challenges as models like ChatGPT proliferate across sectors. We synthesize advances in detection, classification, and mitigation strategies, proposing a tripartite framework categorizing attacks by vector (text/image/speech), mechanism (semantic manipulation, resource exploitation), and impact (data breaches, privacy theft). Key attack vectors include the GCG algorithm, DAN jailbreaks, and resource-exhaustion tactics (e.g., Engorgio). Current defenses are evaluated for efficacy, highlighting scalability gaps and trade-offs between security and model utility. Future priorities include adaptive defense systems leveraging reinforcement learning, interdisciplinary collaboration to address ethical-technical intersections, and open threat intelligence networks for proactive vulnerability management. This work equips researchers and practitioners with actionable strategies to secure LLM ecosystems against evolving adversarial threats.",
      "year": 2025,
      "venue": "Science and Technology of Engineering, Chemistry and Environmental Protection",
      "authors": [
        "Qingtian Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ca09b6c6b7233a4547d59841cffa115a083d9121",
      "pdf_url": "",
      "publication_date": "2025-06-17",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f16917914c847b8e03594ccedc233e8f88cb3ccd",
      "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance",
      "abstract": "Large Language Models (LLMs) are increasingly integrated into real-world applications, from virtual assistants to autonomous agents. However, their flexibility also introduces new attack vectors-particularly Prompt Injection (PI), where adversaries manipulate model behavior through crafted inputs. As attackers continuously evolve with paraphrased, obfuscated, and even multi-task injection strategies, existing benchmarks are no longer sufficient to capture the full spectrum of emerging threats. To address this gap, we construct a new benchmark that systematically extends prior efforts. Our benchmark subsumes the two widely-used existing ones while introducing new manipulation techniques and multi-task scenarios, thereby providing a more comprehensive evaluation setting. We find that existing defenses, though effective on their original benchmarks, show clear weaknesses under our benchmark, underscoring the need for more robust solutions. Our key insight is that while attack forms may vary, the adversary's intent-injecting an unauthorized task-remains invariant. Building on this observation, we propose PromptSleuth, a semantic-oriented defense framework that detects prompt injection by reasoning over task-level intent rather than surface features. Evaluated across state-of-the-art benchmarks, PromptSleuth consistently outperforms existing defense while maintaining comparable runtime and cost efficiency. These results demonstrate that intent-based semantic reasoning offers a robust, efficient, and generalizable strategy for defending LLMs against evolving prompt injection threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Mengxiao Wang",
        "Yuxuan Zhang",
        "Guofei Gu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f16917914c847b8e03594ccedc233e8f88cb3ccd",
      "pdf_url": "",
      "publication_date": "2025-08-28",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "af911725f2032ad6b8cb7d5c01fd7fb6e404e4ef",
      "title": "LLM Firewall Using Validator Agent for Prevention Against Prompt Injection Attacks",
      "abstract": "Large Language Models with Retrieval-Augmented Generation are considered to be modern, chat-native interfaces to enterprise knowledge. However, deploying such systems safely requires precautions more advanced than input filtering. Numerous LLM-related security threats (including the top one: prompt injection attacks) demand robust defense mechanisms beyond input filtering. This paper extends our dual-agent RAG architecture as an LLM firewall with output-level security validation. Similar to network firewalls, which monitor semantic boundaries, we position the Validator Agent as a response firewall performing multi-feature security checks: prompt injection detection, policy compliance verification, sensitive information redaction, and toxic content filtering. While existing defense approaches focus mainly on input-level screening, our architecture proposes to analyze the output, where a successful attack becomes more obvious. The proposed architecture was verified to be viable using the Polish Bielik 2.3 LLM in an on-premise RAG system designed for energy auditing.",
      "year": 2025,
      "venue": "Applied Sciences",
      "authors": [
        "Michal Podpora",
        "Marek Baranowski",
        "Maciej Chopcian",
        "Lukasz Kwasniewicz",
        "Wojciech Radziewicz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/af911725f2032ad6b8cb7d5c01fd7fb6e404e4ef",
      "pdf_url": "",
      "publication_date": "2025-12-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "62cd3a8f6ca307517456320009962a71487666f2",
      "title": "ShadowPlay: Engineering Defenses Against Role-Based Prompt Injection and Dependency Hallucination in LLM-Powered Development",
      "abstract": "Large Language Models (LLMs) have revolutionized software development by providing intelligent code generation and assistance capabilities. However, their integration into development workflows introduces critical security vulnerabilities that traditional cybersecurity measures fail to address. This paper presents ShadowPlay, a novel multi-layered defense framework specifically engineered to detect and mitigate two emerging threat vectors: role-based prompt injection attacks and dependency hallucination vulnerabilities. Our approach combines formal verification techniques with Machine Learning (ML)-based behavioral analysis to create a comprehensive security solution. The framework employs four integrated components: a semantic prompt analysis engine, a real-time dependency verification system, a behavioral monitoring module, and a response validation layer. Through extensive evaluation on over 20,000 attack scenarios across diverse threat categories, ShadowPlay demonstrates superior performance with 95.1% overall accuracy, 97.2% precision, and only 1.8% false positive rate, while maintaining an average response latency of 127 milliseconds. Our contributions include the first formal mathematical model for these attack vectors, a production-ready defense architecture, and comprehensive empirical validation demonstrating significant improvements over existing approaches.",
      "year": 2025,
      "venue": "2025 International Conference on Cybersecurity and AI-Based Systems (Cyber-AI)",
      "authors": [
        "Anas Alsobeh",
        "Zahraddeen Gwarzo",
        "Amani M. Shatnawi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/62cd3a8f6ca307517456320009962a71487666f2",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4ed842227e9a3e44b63d337168284d665ec104a7",
      "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering",
      "abstract": "With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.",
      "year": 2025,
      "venue": "Knowledge Science, Engineering and Management",
      "authors": [
        "Yi Ji",
        "Runzhi Li",
        "Baolei Mao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4ed842227e9a3e44b63d337168284d665ec104a7",
      "pdf_url": "",
      "publication_date": "2025-06-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9a07872826b0c431661d7b88c470e1f3c7f4c1ee",
      "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Omar Farooq Khan Suri",
        "J. Mccrae"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9a07872826b0c431661d7b88c470e1f3c7f4c1ee",
      "pdf_url": "",
      "publication_date": "2025-12-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f851714285c7a6bfb963da98d67f04df0eafbac9",
      "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
      "abstract": "Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "S. M. A. Hossain",
        "Ruksat Khan Shayoni",
        "Mohd Ruhul Ameen",
        "Akif Islam",
        "M. Mridha",
        "Jungpil Shin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f851714285c7a6bfb963da98d67f04df0eafbac9",
      "pdf_url": "",
      "publication_date": "2025-09-16",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ddc05de1e67e151a6aa8148c6f34260484207ba5",
      "title": "Prompt Injection Detection in LLM Integrated\u00a0Applications",
      "abstract": "The integration of large language models (LLMs) into creative applications has unlocked new capabilities but also introduced vulnerabilities, notably prompt injections. These are malicious inputs designed to manipulate model responses, posing threats to security, privacy, and functionality. This paper delves into the mechanisms of prompt injections, their impacts, and presents novel detection strategies. More specifically, the necessity for robust detection systems is outlined, a predefined list of banned terms is combined to embed techniques for similarity search, and a BERT (Bidirectional Encoder Representa- tions from Transformers) model is built to identify and mitigate prompt injections effectively with the aim to neutralize prompt injections in real-time. The research highlights the challenges in balancing secu- rity with usability, evolving attack vectors, and LLM limitationsm, and emphasizes the significance of securing LLM-integrated applications against prompt injections to preserve data privacy, user trust, and uphold ethical standards. This work aims to foster collaboration for developing standardized security frameworks, contributing to more safer and reliable AI-driven systems.",
      "year": 2025,
      "venue": "International Journal of Network Dynamics and Intelligence",
      "authors": [
        "Qianlong Lan",
        "AnujKaul",
        "Shaun Jones"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ddc05de1e67e151a6aa8148c6f34260484207ba5",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "96a7db0d73412d117e397fe90ddd72ecf098a1c3",
      "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks",
      "abstract": "The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Francesco Panebianco",
        "Stefano Bonfanti",
        "Francesco Trov\u00f2",
        "Michele Carminati"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/96a7db0d73412d117e397fe90ddd72ecf098a1c3",
      "pdf_url": "",
      "publication_date": "2025-08-01",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1e7dea3309decce367110ca9c749b8d94fe70401",
      "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection",
      "abstract": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the\"Lazy Reviewer\"hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these\"LLM-as-a-Judge\"systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping\"Reject\"decisions to\"Accept,\"for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like\"Maximum Mark Magyk\"successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Devanshu Sahoo",
        "Manish Prasad",
        "Vasudev Majhi",
        "Jahnvi Singh",
        "Vinay Chamola",
        "Yash Sinha",
        "Murari Mandal",
        "Dhruv Kumar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1e7dea3309decce367110ca9c749b8d94fe70401",
      "pdf_url": "",
      "publication_date": "2025-12-11",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41b13ebbf1769d140b01590fb073a330f063702e",
      "title": "Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding",
      "abstract": "Vision-language models (VLMs) have revolutionized multimodal AI applications but introduce novel security vulnerabilities that remain largely unexplored. We present the first comprehensive study of steganographic prompt injection attacks against VLMs, where malicious instructions are invisibly embedded within images using advanced steganographic techniques. Our approach demonstrates that current VLM architectures can inadvertently extract and execute hidden prompts during normal image processing, leading to covert behavioral manipulation. We develop a multi-domain embedding framework combining spatial, frequency, and neural steganographic methods, achieving an overall attack success rate of 24.3% (plus or minus 3.2%, 95% CI) across leading VLMs including GPT-4V, Claude, and LLaVA, with neural steganography methods reaching up to 31.8%, while maintaining reasonable visual imperceptibility (PSNR greater than 38 dB, SSIM greater than 0.94). Through systematic evaluation on 12 diverse datasets and 8 state-of-the-art models, we reveal moderate but meaningful vulnerabilities in current VLM architectures and propose effective countermeasures. Our findings have significant implications for VLM deployment in security-critical applications and highlight the need for proportionate multimodal AI security frameworks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chetan Pathade"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/41b13ebbf1769d140b01590fb073a330f063702e",
      "pdf_url": "",
      "publication_date": "2025-07-30",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a1579d2f7edc4642380b7271d88077e96a367cad",
      "title": "Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?",
      "abstract": "AI agents are vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external content or tool outputs cause unintended or harmful behavior. Inspired by the well-established concept of firewalls, we show that a simple, modular and model-agnostic defense operating at the agent--tool interface achieves perfect security (0% or the lowest possible attack success rate) with high utility (task success rate) across four public benchmarks: AgentDojo, Agent Security Bench, InjecAgent and tau-Bench, while achieving a state-of-the-art security-utility tradeoff compared to prior results. Specifically, we employ a defense based on two firewalls: a Tool-Input Firewall (Minimizer) and a Tool-Output Firewall (Sanitizer). Unlike prior complex approaches, this firewall defense makes minimal assumptions on the agent and can be deployed out-of-the-box, while maintaining strong performance without compromising utility. However, our analysis also reveals critical limitations in these existing benchmarks, including flawed success metrics, implementation bugs, and most importantly, weak attacks, hindering significant progress in the field. To foster more meaningful progress, we present targeted fixes to these issues for AgentDojo and Agent Security Bench while proposing best-practices for more robust benchmark design. Further, we demonstrate that although these firewalls push the state-of-the-art on existing benchmarks, it is still possible to bypass them in practice, underscoring the need to incorporate stronger attacks in security benchmarks. Overall, our work shows that existing agentic security benchmarks are easily saturated by a simple approach and highlights the need for stronger agentic security benchmarks with carefully chosen evaluation metrics and strong adaptive attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Rishika Bhagwatkar",
        "Kevin Kasa",
        "Abhay Puri",
        "Gabriel Huang",
        "Irina Rish",
        "Graham W. Taylor",
        "K. Dvijotham",
        "Alexandre Lacoste"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a1579d2f7edc4642380b7271d88077e96a367cad",
      "pdf_url": "",
      "publication_date": "2025-10-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "28f7d474e9c532c38fbea3610d31437c32dafc08",
      "title": "AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema",
      "abstract": "Prompt injection attacks pose a significant challenge to the safe deployment of Large Language Models (LLMs) in real-world applications. While prompt-based detection offers a lightweight and interpretable defense strategy, its effectiveness has been hindered by the need for manual prompt engineering. To address this issue, we propose AEGIS , an Automated co-Evolutionary framework for Guarding prompt Injections Schema. Both attack and defense prompts are iteratively optimized against each other using a gradient-like natural language prompt optimization technique. This framework enables both attackers and defenders to autonomously evolve via a Textual Gradient Optimization (TGO) module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our system on a real-world assignment grading dataset of prompt injection attacks and demonstrate that our method consistently outperforms existing baselines, achieving superior robustness in both attack success and detection. Specifically, the attack success rate (ASR) reaches 1.0, representing an improvement of 0.26 over the baseline. For detection, the true positive rate (TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and the true negative rate (TNR) remains comparable at 0.89. Ablation studies confirm the importance of co-evolution, gradient buffering, and multi-objective optimization. We also confirm that this framework is effective in different LLMs. Our results highlight the promise of adversarial training as a scalable and effective approach for guarding prompt injections.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ting-Chun Liu",
        "C. Hsu",
        "Kuan-Yi Lee",
        "C. Fu",
        "Hung-yi Lee"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/28f7d474e9c532c38fbea3610d31437c32dafc08",
      "pdf_url": "",
      "publication_date": "2025-08-27",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f56269f6b5e8ba4fa6befe20a24f3aa46d45769b",
      "title": "RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework",
      "abstract": "The rapid expansion of the Internet of Things (IoT) is reshaping communication and operational practices across industries, but it also broadens the attack surface and increases susceptibility to security breaches. Artificial Intelligence has become a valuable solution in securing IoT networks, with Large Language Models (LLMs) enabling automated attack behavior analysis and mitigation suggestion in Network Intrusion Detection Systems (NIDS). Despite advancements, the use of LLMs in such systems further expands the attack surface, putting entire networks at risk by introducing vulnerabilities such as prompt injection and data poisoning. In this work, we attack an LLM-based IoT attack analysis and mitigation framework to test its adversarial robustness. We construct an attack description dataset and use it in a targeted data poisoning attack that applies word-level, meaning-preserving perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge base of the framework. We then compare pre-attack and post-attack mitigation responses from the target model, ChatGPT-5 Thinking, to measure the impact of the attack on model performance, using an established evaluation rubric designed for human experts and judge LLMs. Our results show that small perturbations degrade LLM performance by weakening the linkage between observed network traffic features and attack behavior, and by reducing the specificity and practicality of recommended mitigations for resource-constrained devices.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Seif Ikbarieh",
        "Kshitiz Aryal",
        "Maanak Gupta"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f56269f6b5e8ba4fa6befe20a24f3aa46d45769b",
      "pdf_url": "",
      "publication_date": "2025-11-09",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f18b95663f87743335eee0578212fcb484882eae",
      "title": "Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems",
      "abstract": "This paper investigates defenses for LLM-based evaluation systems against prompt injection. We formalize a class of threats called blind attacks, where a candidate answer is crafted independently of the true answer to deceive the evaluator. To counter such attacks, we propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates the submission against a deliberately false ground-truth answer. An attack is detected if the system validates an answer under both standard and counterfactual conditions. Experiments show that while standard evaluation is highly vulnerable, our SE+CFE framework significantly improves security by boosting attack detection with minimal performance trade-offs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Lijia Liu",
        "Takumi Kondo",
        "Kyohei Atarashi",
        "Koh Takeuchi",
        "Jiyi Li",
        "Shigeru Saito",
        "Hisashi Kashima"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f18b95663f87743335eee0578212fcb484882eae",
      "pdf_url": "",
      "publication_date": "2025-07-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6e19e7546d7841fce242abf2d1877dbbb0c6424b",
      "title": "Language Model Agents Under Attack: A Cross Model-Benchmark of Profit-Seeking Behaviors in Customer Service",
      "abstract": "Customer-service LLM agents increasingly make policy-bound decisions (refunds, rebooking, billing disputes), but the same ``helpful''interaction style can be exploited: a small fraction of users can induce unauthorized concessions, shifting costs to others and eroding trust in agentic workflows. We present a cross-domain benchmark of profit-seeking direct prompt injection in customer-service interactions, spanning 10 service domains and 100 realistic attack scripts grouped into five technique families. Across five widely used models under a unified rubric with uncertainty reporting, attacks are highly domain-dependent (airline support is most exploitable) and technique-dependent (payload splitting is most consistently effective). We release data and evaluation code to support reproducible auditing and to inform the design of oversight and recovery workflows for trustworthy, human centered agent interfaces.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Jingyu Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6e19e7546d7841fce242abf2d1877dbbb0c6424b",
      "pdf_url": "",
      "publication_date": "2025-12-30",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "068a9ea887148d9bea4854b33ccf5feca833e140",
      "title": "Hybrid XRXNet for multilingual legal text analysis: a resilient approach to classification and attack mitigation",
      "abstract": "Abstract Background: As digital data in multiple languages increases, analyzing multilingual legal texts has become increasingly important. However, existing models often struggle with the complexity of legal language and cross-lingual understanding, making accurate classification and interpretation challenging. Objective: The study aims to develop a natural language processing multilingual legal data model (NMLDM) to address domain-specific legal terminology, adversarial and prompt injection attacks, data sparsity, and the interpretability and transparency of legal text classification. Methodology: The proposed NMLDM introduces XRXNet, a hybrid model combining XLM-R and XLNet transformer-based language models for robust multilingual legal text analysis. NMLDM integrates data collection, executes preprocessing with an improvised mBERT tokenizer, and performs feature extraction using legal-specific lexicons and linguistic features. XRXNet combines cross-lingual understanding and long-range dependency modeling to classify legal documents accurately while mitigating adversarial and prompt injection attacks. Results: The proposed framework shows exceptional performance across several metrics after evaluating NMLDM with existing models. The simulation results show that NMLDM attained an accuracy of 99.66%, highlighting its competence. Thus, the proposed model shows promise for analyzing multilingual legal documents.",
      "year": 2025,
      "venue": "Stochastic Analysis and Applications",
      "authors": [
        "Arockia Babi Reebha S.",
        "Saravanan D."
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/068a9ea887148d9bea4854b33ccf5feca833e140",
      "pdf_url": "",
      "publication_date": "2025-05-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c6456e9a93d6bfd27dc1516f9c5257c92c1a97c0",
      "title": "The Missing S: Securing the Model Context Protocol",
      "abstract": "The Model Context Protocol (MCP) standardizes AI models\u2019 integration and interaction with external tools and resources. However, its rapid adoption, particularly in allowing custom server and tool development, introduces significant security vulnerabilities, with command injection emerging as a dominant threat, accounting for 43% of observed vulnerabilities in popular MCP servers. While existing research has highlighted these dangers, focused investigation into their detection and mitigation, specifically within the MCP environment, remains limited. This research addresses this gap by evaluating the performance of five transformer-based models, such as BERT, RoBERTa, ALBERT, DistilBERT, and ELECTRA, for detecting command injection attacks in MCP environments. The dataset for this study was curated from real-world MCP server usage, public prompt injection datasets, and custom-created examples specifically designed to cover diverse MCP-specific command injection attack vectors. Our findings found that transformer-based models demonstrate an exceptional performance against command injection attacks, with all evaluated models achieving validation accuracy exceeding 98.8%. Among these models, DistilBERT emerged as the highest performing model, achieving 99.92% validation and test accuracy. By integrating real-world MCP usage with a broad spectrum of targeted attack vectors, the dataset established a robust foundation for training models capable of effectively mitigating command injection threats within the MCP ecosystem.",
      "year": 2025,
      "venue": "2025 1st International Conference on Artificial Intelligence Technology (ICoAIT)",
      "authors": [
        "Michelle Hamjaya",
        "Anthony Sai Richardo",
        "Darren Leonard",
        "Franz Adeta Junior"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c6456e9a93d6bfd27dc1516f9c5257c92c1a97c0",
      "pdf_url": "",
      "publication_date": "2025-09-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a476459f3c073225d2ef04f0350e74a0ffc28e3",
      "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
      "abstract": "When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Michael Schlichtkrull"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3a476459f3c073225d2ef04f0350e74a0ffc28e3",
      "pdf_url": "",
      "publication_date": "2025-10-13",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f57938a589e2e1670e92bcc2f89bf01e0b76059e",
      "title": "The Current State Of AI Education: Security As An Afterthought",
      "abstract": "Artificial Intelligence (AI) has become embedded in critical sectors at a pace unmatched by previous technologies; however, its security has lagged significantly. This paper examines the widening gap between rapid AI adoption and the preparedness of systems, institutions, and practitioners to defend against novel attack surfaces\u2014ranging from prompt injection and data poisoning to model theft, privacy leakage, and autonomous agent abuse. This paper presents empirical trends showing an accelerating rise in AI-related incidents, analyzes systemic causes including educational gaps and a move-fast deployment culture, and documents high-impact failures from 2022\u20132025 that illustrate escalating real-world harm. On the basis of these findings, this paper argues that AI Security Education (AISecEd) should be elevated to a first-class priority in both AI/ML and cybersecurity curricula, with secure-by-design principles embedded from the outset and dedicated courses to build specialized expertise. This paper maps AI course topics to security concepts, extends core cybersecurity modules to cover AI threats, and proposes a minimal baseline for industry adoption. By aligning education, practice, and policy around foundational AI security principles, the development community can close the gap between deployment and assurance\u2014reducing catastrophic failures, strengthening trust, and ensuring that AI delivers on its promise without becoming a systemic liability.",
      "year": 2025,
      "venue": "Proceedings of the 26th ACM Annual Conference on Cybersecurity &amp; Information Technology Education",
      "authors": [
        "Pavan Reddy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f57938a589e2e1670e92bcc2f89bf01e0b76059e",
      "pdf_url": "",
      "publication_date": "2025-11-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9a3113affa9c028304ba686156e0c8d3e8820b35",
      "title": "Imperceptible Jailbreaking against Large Language Models",
      "abstract": "Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is\"secretly\"altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kuofeng Gao",
        "Yiming Li",
        "Chao Du",
        "Xin Wang",
        "Xingjun Ma",
        "Shu-Tao Xia",
        "Tianyu Pang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9a3113affa9c028304ba686156e0c8d3e8820b35",
      "pdf_url": "",
      "publication_date": "2025-10-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d2b0d137cce58d391bd4aa23574f09fe04e7af9a",
      "title": "Securing AI-Powered Recruiting Platforms: A Zero Trust Approach to Enterprise Integration",
      "abstract": "Today's hiring environments are radically changed by conversational AI integration with human capital management systems, radically transforming classical talent acquisition practices. New platforms exhibit multimedia candidate interaction automated assessment capabilities, as well as causing complex security issues that traditional perimeter-based defenses are ill-equipped to address. API-mediated communication channels, programmatic identities, and bidirectional data pipes with sensitive candidate data move at machine speed, processing multiple thousands of applications routinely while handling personal data subject to draconian regulatory compliance regimes. The transparent integration of AI-fueled candidate engagement platforms and licensed HCM systems blurs traditional network boundaries, exposing larger attack surfaces that need end-to-end rearchitecting of enterprise security plans. Legacy security paradigms need to adapt to respond to machine learning model integrity, prompt injection attacks, and algorithmic bias issues alongside traditional data protection needs. New attack vectors outstrip legacy application security threats by exploiting model poisoning attacks, prompt injection methods, and API security breaches specifically targeting compromised object-level authorization systems. Zero Trust Architecture delivers essential frameworks for the security of contemporary recruiting integrations using ongoing verification, least-privilege access, and assumption of breach design principles, discarding network-based trust assumptions while addressing each transaction as isolated events that demand new authentication and authorization independent of prior successful interactions.",
      "year": 2025,
      "venue": "Journal of Information Systems Engineering & Management",
      "authors": [
        "Pratik G Koshiya"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d2b0d137cce58d391bd4aa23574f09fe04e7af9a",
      "pdf_url": "",
      "publication_date": "2025-09-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8974829e4269f5f1800f08603f052b29fa8b26a8",
      "title": "Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs",
      "abstract": "Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Yinan Zhong",
        "Qianhao Miao",
        "Yanjiao Chen",
        "Jiangyi Deng",
        "Yushi Cheng",
        "Wenyuan Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8974829e4269f5f1800f08603f052b29fa8b26a8",
      "pdf_url": "",
      "publication_date": "2025-12-09",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b1d370e2a028b951ea000b035a1b7699f8495a0f",
      "title": "Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis",
      "abstract": "Indirect prompt injection attacks (IPIAs), where large language models (LLMs) follow malicious instructions hidden in input data, pose a critical threat to LLM-powered agents. In this paper, we present IntentGuard, a general defense framework based on instruction-following intent analysis. The key insight of IntentGuard is that the decisive factor in IPIAs is not the presence of malicious text, but whether the LLM intends to follow instructions from untrusted data. Building on this insight, IntentGuard leverages an instruction-following intent analyzer (IIA) to identify which parts of the input prompt the model recognizes as actionable instructions, and then flag or neutralize any overlaps with untrusted data segments. To instantiate the framework, we develop an IIA that uses three\"thinking intervention\"strategies to elicit a structured list of intended instructions from reasoning-enabled LLMs. These techniques include start-of-thinking prefilling, end-of-thinking refinement, and adversarial in-context demonstration. We evaluate IntentGuard on two agentic benchmarks (AgentDojo and Mind2Web) using two reasoning-enabled LLMs (Qwen-3-32B and gpt-oss-20B). Results demonstrate that IntentGuard achieves (1) no utility degradation in all but one setting and (2) strong robustness against adaptive prompt injection attacks (e.g., reducing attack success rates from 100% to 8.5% in a Mind2Web scenario).",
      "year": 2025,
      "venue": "",
      "authors": [
        "Mintong Kang",
        "Chong Xiang",
        "Sanjay Kariyappa",
        "Chaowei Xiao",
        "Bo Li",
        "Edward Suh"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b1d370e2a028b951ea000b035a1b7699f8495a0f",
      "pdf_url": "",
      "publication_date": "2025-11-30",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "aab0a1d0531fc18b8833ad3254d7ba5cbdbd7e9d",
      "title": "ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior",
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Weikai Lu",
        "Ziqian Zeng",
        "Kehua Zhang",
        "Haoran Li",
        "Huiping Zhuang",
        "Ruidong Wang",
        "Cen Chen",
        "Hao Peng"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/aab0a1d0531fc18b8833ad3254d7ba5cbdbd7e9d",
      "pdf_url": "",
      "publication_date": "2025-12-05",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6bc99088f51e29efb6af663a30ab26f74063309",
      "title": "Weaponizing Words: Direct & Indirect Prompt Injection Attacks on LLM",
      "abstract": "As AI systems\u2014particularly LLM applications\u2014proliferate across products and workflows, security has not kept pace. Prompt injection is the failure mode most likely to turn well-intentioned LLM applications into unsafe systems [4]. This 75-minute, code-first tutorial focuses exclusively on the two forms that matter in practice: direct injection through trusted input channels and indirect injection via retrieved or linked content. Using two reproducible demos\u2014EmailGPT (direct) and EchoLeak (indirect)\u2014participants will reproduce end-to-end attacks that override system instructions, exfiltrate secrets, and trigger unauthorized actions, then harden the pipelines step by step. Labs run on free notebooks with small open models and mock tools; no API keys required. We distill why guardrails and \u201cbetter prompts\u201d alone are insufficient\u2014instruction-following bias, collapsed trust boundaries, and tool autopilot\u2014and build a deployable defense-in-depth stack: input canonicalization, instruction/data separation, untrusted-context labeling, structured I/O contracts, provenance and taint tracking, deny-by-default tool policies, and red-team regression tests. Attendees leave with working intuition, tested code, and a practical checklist they can drop into RAG flows, agents, and email assistants the same day.",
      "year": 2025,
      "venue": "Proceedings of the 26th ACM Annual Conference on Cybersecurity &amp; Information Technology Education",
      "authors": [
        "Pavan Reddy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6bc99088f51e29efb6af663a30ab26f74063309",
      "pdf_url": "",
      "publication_date": "2025-11-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "56c8d97c85380eafca4d57da471015b5c037aac3",
      "title": "The Dangers of Indirect Prompt Injection Attacks on LLM-based Autonomous Web Navigation Agents: A Demonstration",
      "abstract": ",",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "authors": [
        "Sam Johnson",
        "Viet Pham",
        "Thai Le"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/56c8d97c85380eafca4d57da471015b5c037aac3",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c0f0583f33017d638c7eead9e70d05131f0d86c9",
      "title": "Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs",
      "abstract": "Large Language Models (LLMs) have seen rapid adoption in recent years, with industries increasingly relying on them to maintain a competitive advantage. These models excel at interpreting user instructions and generating human-like responses, leading to their integration across diverse domains, including consulting and information retrieval. However, their widespread deployment also introduces substantial security risks, most notably in the form of prompt injection and jailbreak attacks. To systematically evaluate LLM vulnerabilities -- particularly to external prompt injection -- we conducted a series of experiments on eight commercial models. Each model was tested without supplementary sanitization, relying solely on its built-in safeguards. The results exposed exploitable weaknesses and emphasized the need for stronger security measures. Four categories of attacks were examined: direct injection, indirect (external) injection, image-based injection, and prompt leakage. Comparative analysis indicated that Claude 3 demonstrated relatively greater robustness; nevertheless, empirical findings confirm that additional defenses, such as input normalization, remain necessary to achieve reliable protection.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Andrew Yeo",
        "Daeseon Choi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c0f0583f33017d638c7eead9e70d05131f0d86c9",
      "pdf_url": "",
      "publication_date": "2025-09-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50f0b646d3b863bcbd4d3211e4a8ebf329d3f694",
      "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents",
      "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated\"Intent Graph\"; and (ii) an innovative\"Tiered Adjudicator\"that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zhibo Liang",
        "Tianze Hu",
        "Zaiye Chen",
        "Mingjie Tang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/50f0b646d3b863bcbd4d3211e4a8ebf329d3f694",
      "pdf_url": "",
      "publication_date": "2025-12-07",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fce68518ff1b58c84fc0f33ea4ddec7545ce42aa",
      "title": "Unveiling Security Vulnerabilities in Agentic AI: Threat Modeling, Exploits, and Mitigation Strategies",
      "abstract": "Agents that can reason and make choices on their own without human intervention, use instruments, and\nplan multiple steps of action have recently spread at an alarming pace, carrying along with them not only new\ncapabilities but also security risks. Security vulnerabilities in the design of Agentic AI systems are explored in this\npaper. These include vulnerabilities that can be exploited by prompt injection, tool abuse, context hijacking and selfdriving policy manipulation. Threat modeling, red teaming, and simulative penetration testing were all parts of the\nhybrid approach. The simulated AI settings were created to be self-sufficient and capable of carrying out tasks like\nprocessing files, making API calls or decision trees. The paper went on to propose the concept of adversarial objects as\na method of probing for vulnerabilities in the aspects of data leakage, illegal command execution, and aim mismatch.\nThe results show that external API abuse was possible in 64% of the cases and indirect prompt injection was possible in\n72% of the considered Agentic AI runs. Also, current safety guardrails failed as agents went down wrong execution\npaths in 58% of cases despite early detection of abnormal behavior. The results indicate that adaptive security policies,\neffective memory management and safe agent orchestration mechanisms are equally important at the agent planning\nlevel rather than just at the interface level. After that, the research classifies the major weaknesses and provides ways to\ncorrect the Agentic AI Tombs. According to the report, adversarial stress testing should be a proactive step during the\ndeployment of agentic AI, and ongoing adversarial stress testing is needed to ensure a secure pipeline.",
      "year": 2025,
      "venue": "International Journal of Advanced Research in Electrical Electronics and Instrumentation Engineering",
      "authors": [
        "Divaahar Muthuswamy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fce68518ff1b58c84fc0f33ea4ddec7545ce42aa",
      "pdf_url": "",
      "publication_date": "2025-08-30",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7fea1534d23a267bba581b3f1f59e86ee66406fb",
      "title": "Modern Day LLMs : A GOLDMINE or a LANDMINE ?",
      "abstract": "This paper presents a systematic analysis of Indirect Prompt Injection (IPI) attacks on Large Language Models (LLMs), examining their dual role as enablers of advanced AI applications and as sources of significant security risk. The study addresses three core research objectives: (1) identifying adversarial techniques that enable effective IPI execution, (2) comparing vulnerabilities across leading LLM architectures, and (3) evaluating the efficacy of existing mitigation strategies. Through empirical testing of Blackbox AI 3.1.6, ChatGPT 3.5, and Gemini 2.0, we find that multi-turn escalation, privilege manipulation, and role-conditioning consistently bypass baseline safety defenses. Our results reveal critical weaknesses in refusal consistency and context-aware filtering. To address these issues, we propose a structured evaluation framework for benchmarking model resilience and outline mitigation pathways with direct deployment relevance, contributing to adversarial robustness research and safer LLM design.",
      "year": 2025,
      "venue": "2025 International Conference on Intelligent Systems and Pioneering Innovations in Robotics and Electric Mobility (INSPIRE)",
      "authors": [
        "Amogh R S",
        "A. R. Muthudhanush",
        "Shwetha M D"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7fea1534d23a267bba581b3f1f59e86ee66406fb",
      "pdf_url": "",
      "publication_date": "2025-11-20",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d7db5cd7d3af51ea491cbcbc3129939b6957244",
      "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
      "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the\"USB-C for Agentic AI.\"While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how\"context\"can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Shiva Gaire",
        "Srijan Gyawali",
        "Saroj Mishra",
        "Suman Niroula",
        "Dilip Thakur",
        "Umesh Yadav"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9d7db5cd7d3af51ea491cbcbc3129939b6957244",
      "pdf_url": "",
      "publication_date": "2025-12-09",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f6fc5f8ad666da740a2b370ac54bbcd73f473d6",
      "title": "Exploiting Web Search Tools of AI Agents for Data Exfiltration",
      "abstract": "Large language models (LLMs) are now routinely used to autonomously execute complex tasks, from natural language processing to dynamic workflows like web searches. The usage of tool-calling and Retrieval Augmented Generation (RAG) allows LLMs to process and retrieve sensitive corporate data, amplifying both their functionality and vulnerability to abuse. As LLMs increasingly interact with external data sources, indirect prompt injection emerges as a critical and evolving attack vector, enabling adversaries to exploit models through manipulated inputs. Through a systematic evaluation of indirect prompt injection attacks across diverse models, we analyze how susceptible current LLMs are to such attacks, which parameters, including model size and manufacturer, specific implementations, shape their vulnerability, and which attack methods remain most effective. Our results reveal that even well-known attack patterns continue to succeed, exposing persistent weaknesses in model defenses. To address these vulnerabilities, we emphasize the need for strengthened training procedures to enhance inherent resilience, a centralized database of known attack vectors to enable proactive defense, and a unified testing framework to ensure continuous security validation. These steps are essential to push developers toward integrating security into the core design of LLMs, as our findings show that current models still fail to mitigate long-standing threats.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Dennis Rall",
        "Bernhard Bauer",
        "Mohit Mittal",
        "Thomas Fraunholz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1f6fc5f8ad666da740a2b370ac54bbcd73f473d6",
      "pdf_url": "",
      "publication_date": "2025-10-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "66c8e83ed0fc215dc8be0e44715a692ef27f005b",
      "title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks",
      "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Zimo Ji",
        "Xunguang Wang",
        "Zongjie Li",
        "Pingchuan Ma",
        "Yudong Gao",
        "Daoyuan Wu",
        "Xincheng Yan",
        "Tian Tian",
        "Shuai Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/66c8e83ed0fc215dc8be0e44715a692ef27f005b",
      "pdf_url": "",
      "publication_date": "2025-11-19",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be0509b3123131f612f0a65369f5783ae95f9760",
      "title": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology",
      "abstract": "Existing interpretability methods for Large Language Models (LLMs) often fall short by focusing on linear directions or isolated features, overlooking the high-dimensional, nonlinear, and relational geometry within model representations. This study focuses on how adversarial inputs systematically affect the internal representation spaces of LLMs, a topic which remains poorly understood. We propose persistent homology (PH), a tool from topological data analysis, as a principled framework to characterize the multi-scale dynamics within LLM activations. Using PH, we systematically analyze six state-of-the-art models under two distinct adversarial conditions, indirect prompt injection and backdoor fine-tuning, and identify a consistent topological signature of adversarial influence. Across architectures and model sizes, adversarial inputs induce ``topological compression'', where the latent space becomes structurally simpler, collapsing from varied, compact, small-scale features into fewer, dominant, and more dispersed large-scale ones. This topological signature is statistically robust across layers, highly discriminative, and provides interpretable insights into how adversarial effects emerge and propagate. By quantifying the shape of activations and neuronal information flow, our architecture-agnostic framework reveals fundamental invariants of representational change, offering a complementary perspective to existing interpretability methods.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Aideen Fay",
        "In'es Garc'ia-Redondo",
        "Qiquan Wang",
        "Haim Dubossarsky",
        "Anthea Monod"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/be0509b3123131f612f0a65369f5783ae95f9760",
      "pdf_url": "",
      "publication_date": "2025-05-26",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3fbee0560840cb3529632500218e2384a0c06af",
      "title": "Quantifying Conversation Drift in MCP via Latent Polytope",
      "abstract": "The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCP's efficacy.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Haoran Shi",
        "Hongwei Yao",
        "Shuo Shao",
        "Shaopeng Jiao",
        "Ziqi Peng",
        "Zhan Qin",
        "Cong Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f3fbee0560840cb3529632500218e2384a0c06af",
      "pdf_url": "",
      "publication_date": "2025-08-08",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6b4c83d40f7d757b5ddb5f7589e4cc0c33f1cfff",
      "title": "Method of Counteracting Manipulative Queries to Large Language Models",
      "abstract": "The integration of Large Language Models (LLMs) into critical infrastructure (SIEM, SOAR) has introduced new attack vectors, specifically prompt injection and jailbreaking. Traditional defense mechanisms, such as input sanitization and Reinforcement Learning from Human Feedback (RLHF), often fail against semantic obfuscation and indirect injections due to their inability to distinguish between control instructions and data context. This paper proposes a novel method for detecting manipulative prompts based on a Multi-Head DistilBERT architecture. Unlike standard binary classifiers, the proposed model decomposes the detection task into four semantic vectors: malicious intent, instruction override, persona adoption, and high-risk action. To address the scarcity of labeled adversarial datasets, we implemented a hybrid data generation strategy using Knowledge Distillation, employing a superior model (Teacher) to label synthetic attacks for the compact Student model. Experimental results on both synthetic and real-world datasets demonstrate that the proposed system achieves a Recall of 0.99, significantly outperforming traditional TF-IDF and keyword-based baselines. The solution operates effectively as a middleware layer, ensuring real-time protection with low computational latency suitable for deployment on edge devices.",
      "year": 2025,
      "venue": "Theoretical and Applied Cybersecurity",
      "authors": [
        "Yehor Kovalchuk",
        "Mykhailo Kolomytsev"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6b4c83d40f7d757b5ddb5f7589e4cc0c33f1cfff",
      "pdf_url": "",
      "publication_date": "2025-12-28",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b1d80b2eda7e175e235185bdc7df4e8233f36ef4",
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
      "year": 2025,
      "venue": "",
      "authors": [
        "S. Srivastava",
        "Haoyu He"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b1d80b2eda7e175e235185bdc7df4e8233f36ef4",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e3f35c9f300156346dd5ddd75b78f768a21deb8",
      "title": "When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins",
      "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), with prior work focusing on cutting-edge LLM applications like personal copilots. In contrast, simpler LLM applications, such as customer service chatbots, are widespread on the web, yet their security posture and exposure to such attacks remain poorly understood. These applications often rely on third-party chatbot plugins that act as intermediaries to commercial LLM APIs, offering non-expert website builders intuitive ways to customize chatbot behaviors. To bridge this gap, we present the first large-scale study of 17 third-party chatbot plugins used by over 10,000 public websites, uncovering previously unknown prompt injection risks in practice. First, 8 of these plugins (used by 8,000 websites) fail to enforce the integrity of the conversation history transmitted in network requests between the website visitor and the chatbot. This oversight amplifies the impact of direct prompt injection attacks by allowing adversaries to forge conversation histories (including fake system messages), boosting their ability to elicit unintended behavior (e.g., code generation) by 3 to 8x. Second, 15 plugins offer tools, such as web-scraping, to enrich the chatbot's context with website-specific content. However, these tools do not distinguish the website's trusted content (e.g., product descriptions) from untrusted, third-party content (e.g., customer reviews), introducing a risk of indirect prompt injection. Notably, we found that ~13% of e-commerce websites have already exposed their chatbots to third-party content. We systematically evaluate both vulnerabilities through controlled experiments grounded in real-world observations, focusing on factors such as system prompt design and the underlying LLM. Our findings show that many plugins adopt insecure practices that undermine the built-in LLM safeguards.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yigitcan Kaya",
        "Anton Landerer",
        "Stijn Pletinckx",
        "Michelle Zimmermann",
        "Christopher Kruegel",
        "Giovanni Vigna"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0e3f35c9f300156346dd5ddd75b78f768a21deb8",
      "pdf_url": "",
      "publication_date": "2025-11-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb1484d09fb1e32e5ad4049e22a653019ebc93a5",
      "title": "Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Daniyal Ganiuly",
        "Assel Smaiyl"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/eb1484d09fb1e32e5ad4049e22a653019ebc93a5",
      "pdf_url": "",
      "publication_date": "2025-11-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6fe606a9c6c93d5010c23906d1bdd08dba1c5c66",
      "title": "OpenGrok: Enhancing SNS Data Processing with Distilled Knowledge and Mask-like Mechanisms",
      "abstract": "This report details Lumen Labs' novel approach to processing Social Networking Service (SNS) data. We leverage knowledge distillation, specifically a simple distillation method inspired by DeepSeek-R1's CoT acquisition, combined with prompt hacking, to extract valuable training data from the Grok model. This data is then used to fine-tune a Phi-3-mini model, augmented with a mask-like mechanism specifically designed for handling the nuances of SNS data. Our method demonstrates state-of-the-art (SOTA) performance on several SNS data processing tasks, outperforming existing models like Grok, Phi-3, and GPT-4. We provide a comprehensive analysis of our approach, including mathematical formulations, engineering details, ablation studies, and comparative evaluations.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "AI Lumen",
        "Shihao Ji",
        "Zihui Song",
        "Fucheng Zhong",
        "Jisen Jia",
        "Zhaobo Wu",
        "Zheyi Cao",
        "Tianhao Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6fe606a9c6c93d5010c23906d1bdd08dba1c5c66",
      "pdf_url": "",
      "publication_date": "2025-02-11",
      "keywords_matched": [
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bef54f9531ac886ab740470a374e7d732c656722",
      "title": "CG-Bench: Can Language Models Assist Call Graph Construction in the Real World?",
      "abstract": "Language models for coding are shifting their focus from function-level to repository-level, with complex function invocations. We introduce CG-Bench, the first manually constructed benchmark that measures the ability to understand call graphs for language models. This benchmark contains 104 call sites and related code snippets associated with call chains from 7 representative open-source C/C++ projects. Language models are tasked with inference the calling targets from them. We evaluated four popular language models on CG-Bench. Surprisingly, all four models with different prompt settings achieve accuracy greater than 50% and Deepseek-6.7b with few-shot prompts reaches 69.70%. We further show four findings from a micro study, which demonstrates that using language models for call graph construction is promising and the performance can be improved by prompt hacking, removing irrelevant information, etc.",
      "year": 2025,
      "venue": "Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages",
      "authors": [
        "Ting Yuan",
        "Wenrui Zhang",
        "Dong Chen",
        "Jie Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/bef54f9531ac886ab740470a374e7d732c656722",
      "pdf_url": "",
      "publication_date": "2025-10-09",
      "keywords_matched": [
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9cee8e18c7d130c45f85951532e5ddd7013cbb97",
      "title": "Research on Hidden Backdoor Prompt Attack Method",
      "abstract": "Existing studies on backdoor attacks in large language models (LLMs) have contributed significantly to the literature by exploring trigger-based strategies\u2014such as rare tokens or syntactic anomalies\u2014that, however, limit both their stealth and generalizability, rendering them susceptible to detection. In this study, we propose HDPAttack, a novel hidden backdoor prompt attack method which is designed to overcome these limitations by leveraging the semantic and structural properties of prompts as triggers rather than relying on explicit markers. Not symmetric to traditional approaches, HDPAttack injects carefully crafted fake demonstrations into the training data, semantically re-expressing prompts to generate examples that exhibit high consistency in input semantics and corresponding labels. This method guides models to learn latent trigger patterns embedded in their deep representations, thereby enabling backdoor activation through natural language prompts without altering user inputs or introducing conspicuous anomalies. Experimental results across datasets (SST-2, SMS, AGNews, Amazon) reveal that HDPAttack achieved an average attack success rate of 99.87%, outperforming baseline methods by 2\u201320% while incurring a classification accuracy loss of \u22641%. These findings set a new benchmark for undetectable backdoor attacks and underscore the urgent need for advancements in prompt-based defense strategies.",
      "year": 2025,
      "venue": "Symmetry",
      "authors": [
        "Huanhuan Gu",
        "Qianmu Li",
        "Yufei Wang",
        "Yu Jiang",
        "A. Bhattacharjya",
        "Haichao Yu",
        "Qian Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9cee8e18c7d130c45f85951532e5ddd7013cbb97",
      "pdf_url": "",
      "publication_date": "2025-06-16",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b88a97b9bc750bac3c8a69cee2711cc64c7e32a5",
      "title": "Personalized Prompt Attack Strategies Against Large Language Models",
      "abstract": null,
      "year": 2025,
      "venue": "International Conference on Intelligent Computing",
      "authors": [
        "Yu Zhang",
        "Yongbing Gao",
        "Zirong Su",
        "Longcang Wang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/b88a97b9bc750bac3c8a69cee2711cc64c7e32a5",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "867f0550f5e2d9a08d3c2094cf3c6bf05b90b6b5",
      "title": "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing",
      "abstract": "Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Panagiotis C. Theocharopoulos",
        "Ajinkya Kulkarni",
        "Mathew Magimai.-Doss"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/867f0550f5e2d9a08d3c2094cf3c6bf05b90b6b5",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1db392d42f020925561c0686ec24831fcd78bcae",
      "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features",
      "abstract": "LLM-integrated applications are vulnerable to prompt injection attacks, where an attacker contaminates the input to inject malicious prompts, causing the LLM to follow the attacker's intent instead of the original user's. Existing prompt injection detection methods often have sub-optimal performance and/or high computational overhead. In this work, we propose PIShield, a detection method that is both effective and efficient. Our key observation is that the internal representation of the final token in a prompt-extracted from a specific layer of the LLM, which we term the injection-critical layer-captures distinguishing features between clean and contaminated prompts. Leveraging this insight, we train a simple linear classifier on these internal representations using a labeled set of clean and contaminated prompts. We compare PIShield against 11 baselines across 5 diverse benchmark datasets and 8 prompt injection attacks. The results demonstrate that PIShield is both highly effective and efficient, substantially outperforming existing methods. Additionally, we show that PIShield resists strong adaptive attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Wei Zou",
        "Yupei Liu",
        "Yanting Wang",
        "Ying Chen",
        "N. Gong",
        "Jinyuan Jia"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1db392d42f020925561c0686ec24831fcd78bcae",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ecde12a9c503955ec0cdb38f892df3664fc95f1",
      "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
      "abstract": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Yihan Wang",
        "Huanqi Yang",
        "Shantanu Pal",
        "Weitao Xu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7ecde12a9c503955ec0cdb38f892df3664fc95f1",
      "pdf_url": "",
      "publication_date": "2025-12-24",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca8a1a81e94af701a91cd40ef4cc72e22ed34143",
      "title": "Quantifying Return on Security Controls in LLM Systems",
      "abstract": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).",
      "year": 2025,
      "venue": "",
      "authors": [
        "Richard H. Moulton",
        "Austin O'Brien",
        "John D. Hastings"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ca8a1a81e94af701a91cd40ef4cc72e22ed34143",
      "pdf_url": "",
      "publication_date": "2025-12-17",
      "keywords_matched": [
        "prompt injection",
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d3761e670c3597babbbcd9cf7b88327e07e770b",
      "title": "Securing With Dual-LLM Architecture: ChatTEDU an Open Access Chatbot\u2019s Defense",
      "abstract": "Open access chatbots face escalating cybersecurity risks due to adversarial exploitation. This paper presents the case of ChatTEDU, a dual-LLM architecture designed to protect open-access AI systems from sophisticated adversarial attacks while maintaining the user experience quality. During a two-month deployment at TED University, we analyzed 4501 unique real-world interactions, including 180 malicious attempts targeting the system through prompt injection, jailbreaking, and content manipulation attacks. Our dual-layer security approach separates content moderation from response generation by using two specialized language models. The first model (LLM-1) acts as a security filter, analyzing incoming queries for threats, whereas the second model (LLM-2) generates educational responses only after validation. This architecture successfully blocked 100% of the identified attacks, with only 0.28% false positives, demonstrating robust protection without compromising legitimate educational interactions. The system handles over 100 concurrent users during peak registration periods without security breaches or performance degradation. Attack analysis of the case revealed that 77.8% of the threats used technical exploitation rather than content-based manipulation, with multilingual attacks comprising 15% of the attempts. The dual-LLM approach introduced only 18% latency overhead, while providing comprehensive protection against prompt injection, jailbreaking, spam insertion, and denial-of-service attacks. This study provides practical guidance for implementing robust security measures in public-faced AI deployments worldwide.",
      "year": 2025,
      "venue": "IEEE Access",
      "authors": [
        "Hakan Emekci",
        "G\u00fcls\u00fcm Budakoglu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2d3761e670c3597babbbcd9cf7b88327e07e770b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "786e4b2dfbfd16e3785b6b5a72b1f8d16551b118",
      "title": "Avalia\u00e7\u00e3o do NeMo Guardrails como um Firewall para a Intera\u00e7\u00e3o Usu\u00e1rio-LLM",
      "abstract": "Neste artigo \u00e9 analisado o NeMo Guardrails, Modelo de Linguagem em Larga Escala (LLM) que tem como papel atuar como um firewall durante intera\u00e7\u00f5es usu\u00e1rio-LLM. O objetivo \u00e9 avaliar seu desempenho na identifica\u00e7\u00e3o de tentativas maliciosas dentro do contexto dessas intera\u00e7\u00f5es, como jailbreaking e prompt injection. Foi utilizado o Do Not Answer dataset para a tarefa de classifica\u00e7\u00e3o do modelo dentre as classes Seguro ou Inseguro. A avalia\u00e7\u00e3o consistiu em uma an\u00e1lise por categoria de risco; do c\u00e1lculo de m\u00e9tricas de classifica\u00e7\u00e3o bin\u00e1ria; e da Taxa de Compensa\u00e7\u00e3o, uma nova m\u00e9trica proposta neste trabalho. O F1 Score apresenta um poss\u00edvel trade-off entre Precis\u00e3o e Sensibilidade.",
      "year": 2025,
      "venue": "Anais do XXV Simp\u00f3sio Brasileiro de Ciberseguran\u00e7a (SBSeg 2025)",
      "authors": [
        "Marcos Guilherme D. de Oliveira Alves",
        "Jo\u00e3o Victor F. de Castro",
        "Joan Petrus Oliveira Lima",
        "A. Azambuja",
        "Leonardo Barbosa Oliveira",
        "A. D. S. Soares"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/786e4b2dfbfd16e3785b6b5a72b1f8d16551b118",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0e39e5070474842c5ebd33c5845589e6d5287239",
      "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
      "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based sandbox to mitigate prompt injection attacks in large language model (LLM) agents. While effective, CaMeL assumes a trusted user prompt, omits side-channel concerns, and incurs performance tradeoffs due to its dual-LLM design. This response identifies these issues and proposes engineering improvements to expand CaMeL's threat coverage and operational usability. We introduce: (1) prompt screening for initial inputs, (2) output auditing to detect instruction leakage, (3) a tiered-risk access model to balance usability and control, and (4) a verified intermediate language for formal guarantees. Together, these upgrades align CaMeL with best practices in enterprise security and support scalable deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "K. Tallam",
        "Emma Miller"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0e39e5070474842c5ebd33c5845589e6d5287239",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8d2f1c327ea135bbc2c9ef66dc70f74bb1297783",
      "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
      "abstract": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Mary Llewellyn",
        "Annie Gray",
        "Josh Collyer",
        "Michael Harries"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8d2f1c327ea135bbc2c9ef66dc70f74bb1297783",
      "pdf_url": "",
      "publication_date": "2025-10-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9c5081c639cf922d2c5b8277a8299d39ebf6a369",
      "title": "Tricking LLM-Based NPCs into Spilling Secrets",
      "abstract": "Large Language Models (LLMs) are increasingly used to generate dynamic dialogue for game NPCs. However, their integration raises new security concerns. In this study, we examine whether adversarial prompt injection can cause LLM-based NPCs to reveal hidden background secrets that are meant to remain undisclosed.",
      "year": 2025,
      "venue": "Provable Security",
      "authors": [
        "Kyohei Shiomi",
        "Zhuotao Lian",
        "Toru Nakanishi",
        "T. Kitasuka"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9c5081c639cf922d2c5b8277a8299d39ebf6a369",
      "pdf_url": "",
      "publication_date": "2025-08-25",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ccd900fa6ac6814786a7adfdf8d74dc8b11045bc",
      "title": "Safeguarding LLM-Applications: Specify or Train?",
      "abstract": "Large Language Models (LLMs) are powerful tools used in several applications such as conversational AI, and code generation. However, significant robustness concerns arise with LLMs in production, such as hallucinations, prompt injection attacks, harmful content generation, and challenges in maintaining accurate domain-specific content moderation. Guardrails aim to mitigate these challenges by aligning LLM outputs with desired behaviors without modifying the underlying models. Nvidia NeMo Guardrails, for instance, rely on specifying acceptable/unacceptable behaviours. However, it is challenging to predict and address potential issues of LLMs in advance to create these guardrails. Also, manual updates from software engineers are often required to maintain and refine these guardrails. We introduce LLM-Guards, specialised machine learning (ML) models trained to function as protective guards. Additionally, we present an automation pipeline for training and continual fine-tuning of these guards using reinforcement learning from human feedback (RLHF). We evaluated several small LLMs, including Llama-3, Mistral, and Gemma, as LLM-Guards for challenges such as moderation and detecting off-topic queries, and compared their performance against NeMo Guardrails. The proposed Llama-3 LLM-Guard outperformed NeMo Guardrails in detecting offtopic queries, achieving an accuracy of 98.7% compared to 81%. Furthermore, the LLM-Guard detected 97.86% of harmful queries\u201d surpassing NeMo Guardrails by 19.86%.",
      "year": 2025,
      "venue": "2025 IEEE/ACM 4th International Conference on AI Engineering \u2013 Software Engineering for AI (CAIN)",
      "authors": [
        "Hala Abdelkader",
        "Mohamed Almorsy",
        "Sankhya Singh",
        "Irini Logothetis",
        "Priya Rani",
        "Rajesh Vasa",
        "Jean-Guy Schneider"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ccd900fa6ac6814786a7adfdf8d74dc8b11045bc",
      "pdf_url": "",
      "publication_date": "2025-04-27",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a3c0f8e5855d9931ca272e3aba4afb2ce12a0c8d",
      "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation",
      "abstract": "In the era of information overload, personalized news headline generation is crucial for engaging users by tailoring content to their preferences while accurately conveying news facts. Existing methods struggle with effectively capturing complex user interests and ensuring factual consistency, often leading to generic or misleading headlines. Leveraging the unprecedented capabilities of Large Language Models (LLMs) in text generation, we propose Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates user preferences and factual consistency constraints into a powerful pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture long-term user interests, a Context Injection Adapter to seamlessly integrate these preferences and current article context into the LLM's generation process, and a Fact-Consistency Reinforcement Module employing a novel contrastive loss to mitigate hallucination. Evaluated on the real-world PENS dataset, CAP-LLM achieves state-of-the-art performance across all metrics. Notably, it significantly improves factual consistency (FactCC of 87.50) over strong baselines like BART (86.67), while simultaneously enhancing personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations, and sensitivity analyses further validate the effectiveness of each component and the robustness of our approach, demonstrating CAP-LLM's ability to achieve a superior balance between personalization and factual accuracy in news headline generation.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Raymond Wilson",
        "Cole Graham",
        "Chase Carter",
        "Zefeng Yang",
        "Ruiqi Gu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a3c0f8e5855d9931ca272e3aba4afb2ce12a0c8d",
      "pdf_url": "",
      "publication_date": "2025-08-05",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e06cb93f82b4516bf3d03c2d5b29d313e5ee4c88",
      "title": "Adversarial Prompt Filtering Based on Societal Risk",
      "abstract": null,
      "year": 2025,
      "venue": "2025 IEEE 25th International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)",
      "authors": [
        "Ryan Marinelli"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e06cb93f82b4516bf3d03c2d5b29d313e5ee4c88",
      "pdf_url": "",
      "publication_date": "2025-05-19",
      "keywords_matched": [
        "adversarial prompt",
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "84e85dd5c331a71001a016dd7c8708ce7acc29de",
      "title": "The Unprecedented Surge in Generative AI: Empirical Analysis of Trusted and Malicious Large Language Models (LLMs)",
      "abstract": "Trusted large language models (LLMs) inherit ethical guidelines to prevent generating harmful content, whereas malicious LLMs are engineered to enable the generation of unethical and toxic responses. Both trusted and malicious LLMs use guardrails in differential contexts per the requirements of the developers and attackers, respectively. We explore the multifaceted world of guardrails implementation in LLMs by conducting an empirical analysis to assess the effectiveness of guardrails using prompts. Our results revealed that guardrails deployed in the trusted LLMs could be bypassed using prompt manipulation techniques such as \u201cpretend\u201d and \u201cpersist\u201d to generate harmful content. In addition, we also discovered that malicious LLMs still deploy weak guardrails to evade detection by generating human-like content. This empirical analysis provides insights into the design of the malicious and trusted LLMs. We also propose recommendations to defend against prompt manipulation and guardrails bypass while designing LLMs.",
      "year": 2025,
      "venue": "IEEE technology & society magazine",
      "authors": [
        "Aditya K. Sood",
        "S. Zeadally"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/84e85dd5c331a71001a016dd7c8708ce7acc29de",
      "pdf_url": "",
      "publication_date": "2025-09-01",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca6b0ab0050a9cc4865a1655a6ff38942f32cf1a",
      "title": "Towards Secure AI: Detection of Prompt Injection Attacks with Explainability",
      "abstract": "Prompt injection attacks are a grave threat to AI systems, because they compromise the integrity and security of user interactions. This paper investigates the use of two models for such detection: a Random Forest classifier trained on TF-IDF features and a Bidirectional Long Short-Term Memory (LSTM) network that relies on sequential dependencies in text.A synthetic dataset was prepared with four categories: Legitimate, Malicious - Instruction Hijacking, Malicious - Context Manipulation, and Malicious - Command Obfuscation. Thorough preprocessing, including text cleaning and feature extraction, ensures robust input representation. The Random Forest model achieved an accuracy of 88.75 percent, but the Bidirectional LSTM model outperformed with an accuracy of 92.50 percent, which establishes that capturing text context is the key to accurate classification. SHAP analysis provides interpretability in terms of the key features and their contributions to model predictions. The results show that the traditional and deep learning approaches complement each other well in malicious prompt detection, opening avenues for secure AI deployments. Future work will be to extend this methodology with real-world datasets and hybrid models to enhance the robustness and scalability.",
      "year": 2025,
      "venue": "2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)",
      "authors": [
        "A. Nair",
        "Akshara Balan",
        "Alan C M",
        "Maria Baiju",
        "Tom Davis"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ca6b0ab0050a9cc4865a1655a6ff38942f32cf1a",
      "pdf_url": "",
      "publication_date": "2025-04-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33591373d7f21a107fea8f9260e2ade5df9f08e7",
      "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation",
      "abstract": "Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.",
      "year": 2025,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Sofia Jamil",
        "K. S. Charan",
        "Sriparna Saha",
        "Koustava Goswami",
        "Joseph K J"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/33591373d7f21a107fea8f9260e2ade5df9f08e7",
      "pdf_url": "",
      "publication_date": "2025-09-15",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90ca5df399468f866667df4d34775bde09c97bbe",
      "title": "Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control",
      "abstract": "AI agents capable of GUI understanding and Model Context Protocol are increasingly deployed to automate mobile tasks. However, their reliance on over-privileged, static permissions creates a critical vulnerability: instruction injection. Malicious instructions, embedded in otherwise benign content like emails, can hijack the agent to perform unauthorized actions. We present AgentSentry, a lightweight runtime task-centric access control framework that enforces dynamic, task-scoped permissions. Instead of granting broad, persistent permissions, AgentSentry dynamically generates and enforces minimal, temporary policies aligned with the user's specific task (e.g., register for an app), revoking them upon completion. We demonstrate that AgentSentry successfully prevents an instruction injection attack, where an agent is tricked into forwarding private emails, while allowing the legitimate task to complete. Our approach highlights the urgent need for intent-aligned security models to safely govern the next generation of autonomous agents.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yifeng Cai",
        "Ziming Wang",
        "Zhaomeng Deng",
        "Mengyu Yao",
        "Junlin Liu",
        "Yutao Hu",
        "Ziqi Zhang",
        "Yao Guo",
        "Ding Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/90ca5df399468f866667df4d34775bde09c97bbe",
      "pdf_url": "",
      "publication_date": "2025-10-30",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "06d09ec7188e88abd360539547ae55622e98ac18",
      "title": "Efficient Graph Understanding with LLMs via Structured Context Injection",
      "abstract": "Large Language Models (LLMs) have shown strong capabilities in solving problems across domains, including graph-related tasks traditionally addressed by symbolic or algorithmic methods. In this work, we present a framework for structured context injection, where task-specific information is systematically embedded in the input to guide LLMs in solving a wide range of graph problems. Our method does not require fine-tuning of LLMs, making it cost-efficient and lightweight. We observe that certain graph reasoning tasks remain challenging for LLMs unless they are mapped to conceptually grounded representations. However, achieving such mappings through fine-tuning or repeated multi-step querying can be expensive and inefficient. Our approach offers a practical alternative by injecting structured context directly into the input, enabling the LLM to implicitly align the task with grounded conceptual spaces. We evaluate the approach on multiple graph tasks using both lightweight and large models, highlighting the trade-offs between accuracy and computational cost. The results demonstrate consistent performance improvements, showing that structured input context can rival or surpass more complex approaches. Our findings underscore the value of structured context injection as an effective and scalable strategy for graph understanding with LLMs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Govind V Waghmare",
        "BG Sumedh",
        "Sonia Gupta",
        "Srikanta J. Bedathur"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/06d09ec7188e88abd360539547ae55622e98ac18",
      "pdf_url": "",
      "publication_date": "2025-08-31",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6434036f8d3ed734b835d6c7a63a5b3508f4d5f",
      "title": "GCIFormer: An Efficient Global Context Injection Transformer for Remote Sensing Pansharpening",
      "abstract": "Pansharpening is an image fusion technique that integrates low-resolution multispectral images with high-resolution panchromatic (PAN) images to generate high-resolution multispectral images with enhanced spatial and spectral fidelity. Existing convolutional neural networks for pansharpening are limited in their ability to model long-range dependencies, whereas Transformer-based models often incur significant computational overhead. To efficiently capture global correlations among remote sensing image patches, we propose the global context injection Transformer (GCIFormer) as a core component in both the spatial and spectral branches. The proposed GCIFormer captures global dependencies at the window level and subsequently infuses them into the image representation at the patch level. Moreover, we design an axial fusion module (AFM) to capture long-range dependencies between the two modalities while mitigating information loss. AFM exploits axial features from different modalities by fusing their feature maps along respective axial directions. In addition, we introduce a PAN reconstruction loss to enhance the retention of high-frequency spatial details. Extensive experiments conducted on multiple benchmark datasets demonstrate that the proposed model outperforms state-of-the-art methods, showcasing its superior performance in pansharpening applications.",
      "year": 2025,
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
      "authors": [
        "Yue Que",
        "Qiu Yi",
        "Hanqing Xiong",
        "Weiguo Wan",
        "Nan Jiang",
        "Yong Yang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e6434036f8d3ed734b835d6c7a63a5b3508f4d5f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13e43ed84bb009d454abc8aa1869761a7acd8791",
      "title": "Robustness of Automated AI Agents Against Adversarial Context Injection in MCP",
      "abstract": null,
      "year": 2025,
      "venue": "International Journal of Computer Applications",
      "authors": [
        "Prudhvi Ratna Badri Satya",
        "Ajay Guyyala",
        "Vijay Putta",
        "Krishna Teja Areti"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/13e43ed84bb009d454abc8aa1869761a7acd8791",
      "pdf_url": "",
      "publication_date": "2025-11-18",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "463dd86f38814b45c8cc0242ac208add5e8c43ef",
      "title": "\"I know it's not right, but that's what it said to do\": Investigating Trust in AI Chatbots for Cybersecurity Policy",
      "abstract": "AI chatbots are an emerging security attack vector, vulnerable to threats such as prompt injection, and rogue chatbot creation. When deployed in domains such as corporate security policy, they could be weaponized to deliver guidance that intentionally undermines system defenses. We investigate whether users can be tricked by a compromised AI chatbot in this scenario. A controlled study (N=15) asked participants to use a chatbot to complete security-related tasks. Without their knowledge, the chatbot was manipulated to give incorrect advice for some tasks. The results show how trust in AI chatbots is related to task familiarity, and confidence in their ownn judgment. Additionally, we discuss possible reasons why people do or do not trust AI chatbots in different scenarios.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Brandon Lit",
        "Edward Crowder",
        "Daniel Vogel",
        "Hassan Khan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/463dd86f38814b45c8cc0242ac208add5e8c43ef",
      "pdf_url": "",
      "publication_date": "2025-10-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "67e1e5ef6bff0b6368531cd50b0144672604b8ea",
      "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks - A Case Study of Hsinchu, Taiwan",
      "abstract": "As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function calls, multi-layered linguistic analysis, and guardrails against injections, achieving high contextual awareness and security. Key features include a tiered response strategy, RAG-driven knowledge grounding, and intent decomposition across lexical, semantic, and pragmatic levels. Defense mechanisms include system norms, gatekeepers for intent judgment, and reverse RAG text to prioritize verified data. We also benchmark a GPT-5 variant (released 2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial prompts and 223 benign queries show over 95% accuracy on benign tasks and substantial detection of injection attacks. GPT-5 blocked about 85% of attacks, showing progress yet highlighting the need for layered defenses. Findings emphasize contributions to sustainable tourism, multilingual accessibility, and ethical AI deployment. This work offers a practical framework for deploying secure chatbots in smart tourism and contributes to resilient, trustworthy AI applications.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu-Kai Shih",
        "You-Kai Kang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/67e1e5ef6bff0b6368531cd50b0144672604b8ea",
      "pdf_url": "",
      "publication_date": "2025-09-22",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c18ac188e59bc0f7da13a9932c1311bad911ac4a",
      "title": "Hybrid Constitutional Classifiers for Prompt Injection Defense",
      "abstract": "In this paper, we propose a novel hybrid constitutional classifier system that combines a lightweight input classifier with a fine-tuned generative LLM to enhance security while maintaining efficiency. Our approach integrates a DeBERTa-based classifier for filtering adversarial prompts before they reach the model and a small LLM that performs response monitoring to prevent unsafe completions in real time. Through extensive benchmarking, we demonstrate that our hybrid approach effectively mitigates adversarial inputs while significantly reducing inference latency compared to large-scale models like LLaMA-2 13B. We further explore optimization techniques such as model quantization and knowledge distillation to improve system performance with minimal degradation in classification accuracy. Our results indicate that the hybrid system achieves detection accuracy of 96.97% for inputs and 78.79% for the outputs, maintaining robust safe filtering capabilities while offering 67% lower latency in overall compared to conventional large LLM classifiers. The deployment of a hybrid constitutional classifier system for real-time AI applications enhances LLM security by providing a scalable, high-performance defense against prompt injection attacks, enabling safer AI interactions in critical domains.",
      "year": 2025,
      "venue": "IEEE International Conference on Electro/Information Technology",
      "authors": [
        "Qianlong Lan",
        "Anuj Kaul",
        "Shaun Jones",
        "Stephanie Westrum",
        "Vinothini Pandurangan",
        "Nishant Kumar Das Pattanaik",
        "Piyush Pattanayak"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c18ac188e59bc0f7da13a9932c1311bad911ac4a",
      "pdf_url": "",
      "publication_date": "2025-05-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3de9bab87cfece99fc4da5bb8b45130405f7a12d",
      "title": "Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification",
      "abstract": "Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Yanxi Li",
        "Ruocheng Shan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3de9bab87cfece99fc4da5bb8b45130405f7a12d",
      "pdf_url": "",
      "publication_date": "2025-11-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6353a4bc7d341ba10df8170b21998f65b14e7ec0",
      "title": "REAL TIME AI DEFENSE AGAINST PROMPT INJECTION ATTACKS",
      "abstract": null,
      "year": 2025,
      "venue": "2nd International Conference on Cybersecurity and Digital Defense (ICyDD)",
      "authors": [],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6353a4bc7d341ba10df8170b21998f65b14e7ec0",
      "pdf_url": "",
      "publication_date": "2025-10-25",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e157a22a0315eaaafe5a66d4802040090fe0288",
      "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
      "abstract": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Safwan Shaheer",
        "G. M. R. Islam",
        "Mohammad Rafid Hamid",
        "Tahsin Zaman Jilan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3e157a22a0315eaaafe5a66d4802040090fe0288",
      "pdf_url": "",
      "publication_date": "2025-12-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "177137fd3d07855a6fa2e0bdcee3606d0159fa3d",
      "title": "PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization",
      "abstract": "Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Runpeng Geng",
        "Yanting Wang",
        "Chenlong Yin",
        "Minhao Cheng",
        "Ying Chen",
        "Jinyuan Jia"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/177137fd3d07855a6fa2e0bdcee3606d0159fa3d",
      "pdf_url": "",
      "publication_date": "2025-11-13",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt injection defense",
        "prompt sanitization",
        "prevent prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "11d778e096a1716b14afa5915378ad4a25525e77",
      "title": "Security Posture Evaluation of Model Context Protocol Servers Against Prompt Injection",
      "abstract": "This study presents a security-oriented evaluation of the Model Context Protocol (MCP), an open standard enabling large language models (LLMs) to interface with external tools through structured interactions. The experiment was deployed in a controlled environment using an open-source MCP server with basic file manipulation tools. A set of prompt injection tests was designed to probe the system's ability to handle adversarial instructions, ambiguous input, and encoded payloads. Despite expectations, the system failed to reject adversarial instructions embedded in user input and did not detect anomalous tool usage patterns-such as suspicious parameters. These failures underscore the current vulnerability of LLM-tool integrations to prompt injection threats. The results highlight the urgent need for defense-in-depth strategies within MCP implementations, including stricter input validation, contextaware filtering, strong access control policies and transparent logging. Collectively, these measures are essential for securing AI-assisted tool chains against emerging adversarial techniques.",
      "year": 2025,
      "venue": "International Symposium for Design and Technology in Electronic Packaging",
      "authors": [
        "Gabriela Tod-Raileanu",
        "S. Axinte",
        "Ana-Maria Dinca",
        "I. Bacivarov"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/11d778e096a1716b14afa5915378ad4a25525e77",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3329b815a0be506b5c0c8074214cbc8b1918aa91",
      "title": "CourtGuard: A Local, Multiagent Prompt Injection Classifier",
      "abstract": "As large language models (LLMs) become integrated into various sensitive applications, prompt injection, the use of prompting to induce harmful behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks can cause LLMs to leak sensitive data, spread misinformation, and exhibit harmful behaviors. To defend against these attacks, we propose CourtGuard, a locally-runnable, multiagent prompt injection classifier. In it, prompts are evaluated in a court-like multiagent LLM system, where a\"defense attorney\"model argues the prompt is benign, a\"prosecution attorney\"model argues the prompt is a prompt injection, and a\"judge\"model gives the final classification. CourtGuard has a lower false positive rate than the Direct Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt injection detector. Nevertheless, this lower false positive rate highlights the importance of considering both adversarial and benign scenarios for the classification of a prompt. Additionally, the relative performance of CourtGuard in comparison to other prompt injection classifiers advances the use of multiagent systems as a defense against prompt injection attacks. The implementations of CourtGuard and the Direct Detector with full prompts for Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at https://github.com/isaacwu2000/CourtGuard.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Isaac Wu",
        "Michael Maslowski"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3329b815a0be506b5c0c8074214cbc8b1918aa91",
      "pdf_url": "",
      "publication_date": "2025-10-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e434a5c3ec8dd92ea944d07b5ae227492114655a",
      "title": "Defending The AI-Powered Commerce Stack: A Security Framework For Prompt Injection, Review Integrity, And Privacy In Genai Retail Systems",
      "abstract": "Generative AI is fundamentally changing digital retail through intelligent search, conversational assistants, personalized recommendations, and generating dynamic content. The use of generative AI has created significant security gaps that disrupt customer trust, regulatory compliance, and operational integrity. Prompt injection attacks exploit untrusted content in product descriptions and user-generated reviews to manipulate assistant behavior and trigger unauthorized actions. AI-generated synthetic reviews undermine rating authenticity and distort marketplace signals at an unprecedented scale. Data poisoning compromises catalog systems and vector embeddings that power recommendation engines, degrading relevance and introducing malicious content propagation. Privacy leaks emerge from over-permissioned tool access and inadequate PII protection in conversational contexts. This framework presents layered defense architectures addressing each threat vector through input isolation, provenance tracking, quarantine systems, and access minimization. Cross-cutting governance mechanisms integrate brand guardrails, policy-as-code enforcement, and human-in-the-loop controls. Observability infrastructure enables continuous monitoring through decision logging, drift detection, and executive dashboards. The framework provides actionable guidance for engineering, security, legal, and customer experience teams building resilient AI-powered commerce systems that balance innovation with protection against adversarial manipulation and privacy violations.",
      "year": 2025,
      "venue": "Journal of International Crisis and Risk Communication Research",
      "authors": [
        "Prakash Kodali"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e434a5c3ec8dd92ea944d07b5ae227492114655a",
      "pdf_url": "",
      "publication_date": "2025-11-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d775531bf0c45fa89cb41e521e77d920f33fcf82",
      "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
      "abstract": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Toqeer Ali Syed",
        "Mishal Almutairi",
        "Mahmoud Abdel Moaty"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d775531bf0c45fa89cb41e521e77d920f33fcf82",
      "pdf_url": "",
      "publication_date": "2025-12-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ca8316bae22e9de13801af28d61254809b619799",
      "title": "DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture",
      "abstract": null,
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ruofan Liu",
        "Yun Lin",
        "J. Dong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ca8316bae22e9de13801af28d61254809b619799",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1e6a8a1717978f619eea934544b8a3bf5077deac",
      "title": "Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks",
      "abstract": "This paper presents a real-time modular defense system named Sentra-Guard. The system detects and mitigates jailbreak and prompt injection attacks targeting large language models (LLMs). The framework uses a hybrid architecture with FAISS-indexed SBERT embedding representations that capture the semantic meaning of prompts, combined with fine-tuned transformer classifiers, which are machine learning models specialized for distinguishing between benign and adversarial language inputs. It identifies adversarial prompts in both direct and obfuscated attack vectors. A core innovation is the classifier-retriever fusion module, which dynamically computes context-aware risk scores that estimate how likely a prompt is to be adversarial based on its content and context. The framework ensures multilingual resilience with a language-agnostic preprocessing layer. This component automatically translates non-English prompts into English for semantic evaluation, enabling consistent detection across over 100 languages. The system includes a HITL feedback loop, where decisions made by the automated system are reviewed by human experts for continual learning and rapid adaptation under adversarial pressure. Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and malicious prompts, enhancing detection reliability and reducing false positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible with diverse LLM backends. Its modular design supports scalable deployment in both commercial and open-source environments. The system establishes a new state-of-the-art in adversarial LLM defense.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Mehedi Hasan",
        "Ziaur Rahman",
        "Rafid Mostafiz",
        "Md. Abir Hossain"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1e6a8a1717978f619eea934544b8a3bf5077deac",
      "pdf_url": "",
      "publication_date": "2025-10-26",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5334066c175fae0e75c883c990e7297aa32a388f",
      "title": "New Paradigms of Adversarial Attacks Against Large Language Models and Their Defense Mechanisms",
      "abstract": "Large language models are gaining immense popularity, enhancing the usability of intelligent services while introducing unprecedented security challenges. Systems designed to improve question-answering quality may inadvertently leak personal privacy, while jailbreak and prompt injection attacks persistently threaten model defenses and trust. Traditional countermeasures either require additional computational steps or deep insight into model internals\u2014approaches that are costly and ineffective in complex network environments, especially as attack methods evolve rapidly. We developed a novel defense method called LeakSealer, specifically designed for large language models. This approach performs a deep examination of historical dialogues, using semantic fingerprinting and unsupervised clustering to map usage patterns and attack vectors in static scenarios. In dynamic environments, it employs semi-supervised classification combined with human-machine collaboration for real-time defense. This approach requires no insight into the model's inner workings. It is lightweight, scalable, fast, and provides explanations along with recommendations for improvement. Experimental results are promising. LeakSealer outperforms other methods in numerous adversarial scenarios. When detecting jailbreaks, its grouping accuracy reaches 0.97; and in protecting sensitive information, it achieved 0.95 for recovered information and 0.97 AUPRC\u2014both outperforming current methods. This approach not only reduces privacy leakage risks but also enables large language models to operate stably and securely over the long term. Our research demonstrates that combining static analysis with dynamic defense is feasible in adversarial AI environments, offering new insights for managing security issues in large language models.",
      "year": 2025,
      "venue": "2025 5th International Conference on Artificial Intelligence, Automation and High Performance Computing (AIAHPC)",
      "authors": [
        "Yi Zhou",
        "Xinyao Hou",
        "Luohui Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5334066c175fae0e75c883c990e7297aa32a388f",
      "pdf_url": "",
      "publication_date": "2025-09-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fe877de5fe0afa33a3ff9ff0ea0daf53b842be1",
      "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection",
      "abstract": "Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core&Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs'vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Jiaming Hu",
        "Haoyu Wang",
        "Debarghya Mukherjee",
        "I. Paschalidis"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/3fe877de5fe0afa33a3ff9ff0ea0daf53b842be1",
      "pdf_url": "",
      "publication_date": "2025-08-19",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fc626e7c712bd21ca760e49c0a1e201077fe2124",
      "title": "Adversarial threat vectors and risk mitigation for retrieval-augmented generation systems",
      "abstract": "Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.",
      "year": 2025,
      "venue": "Defense + Security",
      "authors": [
        "Chris M. Ward",
        "Joshua D. Harguess"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/fc626e7c712bd21ca760e49c0a1e201077fe2124",
      "pdf_url": "",
      "publication_date": "2025-05-28",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "39b460053d2d80f181de714ce8902c866106e48c",
      "title": "OpenGuardrails: A Configurable, Unified, and Scalable Guardrails Platform for Large Language Models",
      "abstract": "As large language models (LLMs) are increasingly integrated into real-world applications, ensuring their safety, robustness, and privacy compliance has become critical. We present OpenGuardrails, the first fully open-source platform that unifies large-model-based safety detection, manipulation defense, and deployable guardrail infrastructure. OpenGuardrails protects against three major classes of risks: (1) content-safety violations such as harmful or explicit text generation, (2) model-manipulation attacks including prompt injection, jailbreaks, and code-interpreter abuse, and (3) data leakage involving sensitive or private information. Unlike prior modular or rule-based frameworks, OpenGuardrails introduces three core innovations: (1) a Configurable Policy Adaptation mechanism that allows per-request customization of unsafe categories and sensitivity thresholds; (2) a Unified LLM-based Guard Architecture that performs both content-safety and manipulation detection within a single model; and (3) a Quantized, Scalable Model Design that compresses a 14B dense base model to 3.3B via GPTQ while preserving over 98 of benchmark accuracy. The system supports 119 languages, achieves state-of-the-art performance across multilingual safety benchmarks, and can be deployed as a secure gateway or API-based service for enterprise use. All models, datasets, and deployment scripts are released under the Apache 2.0 license.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Thomas Wang",
        "Haowen Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/39b460053d2d80f181de714ce8902c866106e48c",
      "pdf_url": "",
      "publication_date": "2025-10-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "89036ebb1a05db60dacda950971d12bc00118ea3",
      "title": "Defend LLMs Through Self-Consciousness",
      "abstract": "This paper introduces a novel self-consciousness defense mechanism for Large Language Models (LLMs) to combat prompt injection attacks. Unlike traditional approaches that rely on external classifiers, our method leverages the LLM's inherent reasoning capabilities to perform self-protection. We propose a framework that incorporates Meta-Cognitive and Arbitration Modules, enabling LLMs to evaluate and regulate their own outputs autonomously. Our approach is evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate significant improvements in defense success rates across models and datasets, with some achieving perfect and near-perfect defense in Enhanced Mode. We also analyze the trade-off between defense success rate improvement and computational overhead. This self-consciousness method offers a lightweight, cost-effective solution for enhancing LLM ethics, particularly beneficial for GenAI use cases across various platforms.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Boshi Huang",
        "Fabio Nonato de Paula"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/89036ebb1a05db60dacda950971d12bc00118ea3",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "556138a5ee1f6b2c2e3a847ba7086e39cf3aaed4",
      "title": "Ghosts in the Markup: Techniques to Fight Large Language Model-Powered Web Scrapers",
      "abstract": "The rise of large language models (LLMs) has revolutionized web scraping, creating new challenges for website owners seeking to protect valuable content. Traditional defenses such as CAPTCHAs and rate limiting are insufficient against sophisticated AI-driven scraping systems. Current state-of-the-art approaches primarily rely on server-side implementations requiring significant computational resources, IP-based blocking mechanisms, and complex pattern recognition systems that struggle to distinguish between legitimate users and advanced AI scrapers. This paper presents a two-step client-side defense strategy that combines content obfuscation and defensive prompt injection. Our approach dynamically alters HTML structure by wrapping characters in span elements with randomized content and strategic CSS rules while simultaneously injecting carefully crafted prompts designed to manipulate LLM extraction behavior. Experimental evaluation on three diverse datasets demonstrates the effectiveness of this combined approach, achieving near-zero extraction rates (0.6% Exact Match, 0.030 ROUGE-L) for structured content while maintaining a seamless user experience. Performance analysis shows that our method increases the HTML document file size by approximately 1773%, compared to over 6593% for alternative techniques such as Content Overload, providing a practical balance between security and usability while keeping rendering times comparable to unprotected pages. This lightweight implementation integrates seamlessly with existing Web infrastructure without requiring significant computational resources, addressing key limitations of server-side defenses while providing robust protection against unauthorized LLM-based content extraction. For further information, source code, and associated resources, please refer to the source code repository 1.",
      "year": 2025,
      "venue": "Conference of the Open Innovations Association",
      "authors": [
        "William Brach",
        "Matej Petr\u00edk",
        "Kristi\u00e1n Ko\u0161t\u2019\u00e1l",
        "Michal Ries"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/556138a5ee1f6b2c2e3a847ba7086e39cf3aaed4",
      "pdf_url": "",
      "publication_date": "2025-05-14",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b42c7fc709ae63a2a8ce4a54379c974617b752a",
      "title": "A Secure and Effective Framework for Key Concept Mining from Educational Content Using Large Language Models",
      "abstract": "Large Language Models (LLMs) are increasingly prevalent in educational systems, with significant potential to enhance learning experiences through various applications. However, their integration into educational environments can introduce security vulnerabilities, thereby threatening the reliability and integrity of the learning process. Despite the growing adoption of LLMs in education, domain-specific threat assessment for educational applications remains significantly underexplored. To address this gap, we developed a secure educational LLM framework for lecture summarization through systematic threat modeling using the OWASP risk rating methodology. Our analysis identified five major vulnerabilities, among which \u201cprompt injection\u201d was ranked as the highest-severity risk, particularly affecting student interfaces, where malicious inputs could compromise system behavior and educational content integrity. We evaluated an effective defense mechanism known as semantic smoothing, against 20 adversarial prompts across five state-of-the-art LLM models, namely, GPT-4o, GPT-5, Mistral-7B-Instruct-v0.1, Deepseek-llm-7b-chat, Falcon-7b-instruct. The results show that defense effectiveness is highly dependent on the model. GPT-5 achieved optimal protection (HCR reduced from 45% to 30%), while Deepseek-llm-7b-chat showed counterproductive regression under perturbations. These findings demonstrate that robust mitigation strategies must consider the characteristics of individual models, as defenses may vary widely in effectiveness across different LLM architectures.",
      "year": 2025,
      "venue": "Advanced Industrial Conference on Telecommunications",
      "authors": [
        "Ashika Sameem Abdul Rasheed",
        "Mohammad Mehedy Masud"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/2b42c7fc709ae63a2a8ce4a54379c974617b752a",
      "pdf_url": "",
      "publication_date": "2025-10-29",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a497744c319e8465df81a12ccfe60a2bce4017ea",
      "title": "Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs",
      "abstract": "LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to\"victim\"LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Alina Fastowski",
        "Bardh Prenkaj",
        "Yuxiao Li",
        "Gjergji Kasneci"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a497744c319e8465df81a12ccfe60a2bce4017ea",
      "pdf_url": "",
      "publication_date": "2025-11-08",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8355bf4ecd394b0f780e5b8914faca9de2a86486",
      "title": "Artificial Intelligence and Cybersecurity: Documented Risks, Enterprise Guardrails, and Emerging Threats in 2024\u20132025",
      "abstract": "The proliferation of generative artificial intelligence (GenAI) and large language models (LLMs) has introduced novel opportunities and significant hazards in cybersecurity. Recent documented incidents\u2014including zero-click prompt-injection vulnerabilities in enterprise AI assistants and increased effectiveness of AI-generated phishing\u2014demonstrate that organizations must reassess their security posture. This paper surveys verified risks, empirical studies, and real-world vulnerabilities from 2023\u20132025, focusing on AI-enabled phishing, prompt injection, and zero-click exfiltration attacks such as EchoLeak (CVE-2025-32711). Building on established guidance such as the NIST AI Risk Management Framework (AI RMF 1.0) and Microsoft\u2019s Secure AI Framework (SAIF), we propose practical guardrails, an AI-aware defense strategy, and an organization-wide governance prompt pattern to reduce risk when deploying AI systems at scale",
      "year": 2025,
      "venue": "International Journal For Multidisciplinary Research",
      "authors": [
        "Ali Azghar Hussain Syed Abbas"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8355bf4ecd394b0f780e5b8914faca9de2a86486",
      "pdf_url": "",
      "publication_date": "2025-08-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "60799be7c20380222e3300c5c062fd2140b8f9af",
      "title": "Threats and Defenses for Large Language Models: A Survey",
      "abstract": "As Large Language Models (LLMs) are increasingly applied to text comprehension, code generation, and multimodal tasks, their exposure to security threats has become more pronounced. This paper focuses on the security risks and countermeasures associated with LLMs during the training and inference phases. It systematically surveys the principal attack vectors and corresponding defense mechanisms across the model's lifecycle. In the training phase, LLMs are susceptible to data poisoning and backdoor attacks, whereby adversaries manipulate training datasets or embed trigger mechanisms to induce abnormal or malicious behavior. In the inference phase, jailbreak and prompt injection attacks exploit the model's sensitivity to input to circumvent safety constraints. In contrast, model extraction attacks aim to reconstruct model parameters or exfiltrate private data via strategically crafted queries. To mitigate these threats, defense strategies are categorized into offline and online approaches, encompassing data preprocessing, adversarial training, safety alignment, and prompt screening. Finally, informed by current research trends, directions for future LLM security research are discussed.",
      "year": 2025,
      "venue": "Proceedings of the 2025 8th International Conference on Computer Information Science and Artificial Intelligence",
      "authors": [
        "Xiaobao Sheng",
        "Qinhui Jiang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/60799be7c20380222e3300c5c062fd2140b8f9af",
      "pdf_url": "",
      "publication_date": "2025-09-12",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "895487033ec15f7d36d4950cb23c061ef198aee8",
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead. Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators. Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Akshaj Prashanth Rao",
        "Advait Singh",
        "Saumya Kumaar Saksena",
        "Dhruv Kumar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/895487033ec15f7d36d4950cb23c061ef198aee8",
      "pdf_url": "",
      "publication_date": "2025-12-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "31ceac6eaf486d33557e2f900a7553e95684bd72",
      "title": "LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities",
      "abstract": "The growing adoption of Large Language Models (LLMs) has influenced the development of their lighter counterparts-Small Language Models (SLMs)-to enable on-device deployment across smartphones and edge devices. These SLMs offer enhanced privacy, reduced latency, server-free functionality, and improved user experience. However, due to resource constraints of on-device environment, SLMs undergo size optimization through compression techniques like quantization, which can inadvertently introduce fairness, ethical and privacy risks. Critically, quantized SLMs may respond to harmful queries directly, without requiring adversarial manipulation, raising significant safety and trust concerns. To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard that provides real-time, prompt-level defense for quantized SLMs. Additionally, our prompt guard is designed to be model-agnostic such that it can be seamlessly integrated with any SLM, operating independently of underlying architectures. Our LLMG formalizes prompt filtering as a deep learning (DL)-based prompt answerability classification task, leveraging semantic understanding to determine whether a query should be answered by any SLM. Using our curated dataset, Answerable-or-Not, we trained and fine-tuned several DL models and selected ELECTRA as the candidate, with 97.75% answerability classification accuracy. Our safety effectiveness evaluations demonstrate that LLMG defends against over 87% of harmful prompts, including both direct instruction and jailbreak attack strategies. We further showcase its ability to mitigate the Open Knowledge Attacks, where compromised SLMs provide unsafe responses without adversarial prompting. In terms of prompt filtering effectiveness, LLMG achieves near state-of-the-art filtering accuracy of 94%, with an average latency of 135 ms, incurring negligible overhead for users.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kalyan Nakka",
        "Jimmy Dani",
        "Ausmit Mondal",
        "Nitesh Saxena"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/31ceac6eaf486d33557e2f900a7553e95684bd72",
      "pdf_url": "",
      "publication_date": "2025-05-08",
      "keywords_matched": [
        "adversarial prompt",
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "465c7ec9997cac4abb43ea3020fb93f45c685610",
      "title": "Eliminating False Positives in Single-Human Parsing via Prompt Filtering and Pose-Aware Segmentation",
      "abstract": "While existing single human parsing (SHP) models achieve strong performance on standard benchmarks, they can still erroneously predict segments for parts not actually present, particularly when faced with unseen or culturally unique garments. To tackle this limitation, we propose a novel parsing approach that integrates Grounding DINO, the Segment Anything Model (SAM), and a lightweight pose estimator (DWPose) into a unified pipeline. Additionally, we introduce a new challenging dataset comprising 200 carefully annotated images of Vietnamese high school students wearing traditional Ao Dai uniforms, aimed at evaluating model robustness on culturally specific attire. By combining prompt-based semantic filtering and pose-aware segmentation, our method effectively suppresses false-positive predictions and accurately recovers occluded or partially visible body parts. Experimental evaluations demonstrate that our approach achieves a mean Intersection-over-Union (mIoU) of 94.6% on the Ao Dai dataset, significantly surpassing existing baseline methods by significantly reducing hallucinated segments. Moreover, the proposed pipeline maintains competitive performance on the standard ATR dataset, reaching an mIoU of 85.0%. These results underscore the importance of prompt-driven segmentation combined with pose-aware refinement, setting a new benchmark for reliable and culturally adaptive human parsing systems.",
      "year": 2025,
      "venue": "International Conference on Multimedia Analysis and Pattern Recognition",
      "authors": [
        "Duc Kien Le",
        "Viet Hung Hoang",
        "T. Ngo",
        "Thanh Ha Le"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/465c7ec9997cac4abb43ea3020fb93f45c685610",
      "pdf_url": "",
      "publication_date": "2025-08-14",
      "keywords_matched": [
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0bc210d343e2a72c8d52ecc062a7b8ad976268b1",
      "title": "IntentFuse: Language-Guided 3D Scene Understanding via Prompt Filtering and Fusion",
      "abstract": "Affordances-the actions enabled by objects-are fundamental for meaningful interaction in 3D environments. While language models can infer affordances from abstract prompts, grounding them in visual scenes typically requires extensive supervision. We propose IntentFuse, a lightweight middleware that integrates a compact language model with a pretrained Language Embedded Radiance Field (LERF) to ground affordance reasoning directly in 3D scenes. Our method reformulates free-form queries into structured CLIP-aligned prompts, extracting key roles and filtering negations before injecting them into LERF. Experiments on descriptive, affordance, and negation benchmarks show clear improvements, achieving 95% accuracy on affordance queries and 88 % on negation handling over baseline LERF. IntentFuse enables intuitive, training-free, plug-andplay affordance grounding with applications in real-time robotic instruction following and AR/VR scene exploration, paving the way for more natural and interpretable human-environment interaction.",
      "year": 2025,
      "venue": "International Conference on Digital Image Computing: Techniques and Applications",
      "authors": [
        "Ahalya Ravendran",
        "Madhawa Perera",
        "Feng Xu",
        "Lars Petersson",
        "Dadong Wang",
        "Xun Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0bc210d343e2a72c8d52ecc062a7b8ad976268b1",
      "pdf_url": "",
      "publication_date": "2025-12-03",
      "keywords_matched": [
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e1a4afba486f65adf51ab4cc13f183799cddc808",
      "title": "NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation",
      "abstract": "The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by\"jailbreak\"attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yu Xie",
        "Chengjie Zeng",
        "Lingyun Zhang",
        "Yanwei Fu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/e1a4afba486f65adf51ab4cc13f183799cddc808",
      "pdf_url": "",
      "publication_date": "2025-06-22",
      "keywords_matched": [
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7831394c96fc49695da441d91159f1e9f2e68251",
      "title": "CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization",
      "abstract": "The increasing adoption of LLM agents with access to numerous tools and sensitive data significantly widens the attack surface for indirect prompt injections. Due to the context-dependent nature of attacks, however, current defenses are often ill-calibrated as they cannot reliably differentiate malicious and benign instructions, leading to high false positive rates that prevent their real-world adoption. To address this, we present a novel approach inspired by the fundamental principle of computer security: data should not contain executable instructions. Instead of sample-level classification, we propose a token-level sanitization process, which surgically removes any instructions directed at AI systems from tool outputs, capturing malicious instructions as a byproduct. In contrast to existing safety classifiers, this approach is non-blocking, does not require calibration, and is agnostic to the context of tool outputs. Further, we can train such token-level predictors with readily available instruction-tuning data only, and don't have to rely on unrealistic prompt injection examples from challenges or of other synthetic origin. In our experiments, we find that this approach generalizes well across a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB and SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on AgentDojo), without impairing agent utility in both benign and malicious settings.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Debeshee Das",
        "Luca Beurer-Kellner",
        "Marc Fischer",
        "Maximilian Baader"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7831394c96fc49695da441d91159f1e9f2e68251",
      "pdf_url": "",
      "publication_date": "2025-10-09",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection",
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ca15f229e965e0296836d5d419eb604d2434a1e",
      "title": "Detecting and Neutralizing Prompt Injection Attacks in Web-Deployed Large Language Model APIs Using Context-Aware Token Sanitization",
      "abstract": "The rapid integration of large language models (LLMs) into web-based applications has introduced new security vulnerabilities, with prompt injection attacks emerging as a critical threat vector. These attacks manipulate model outputs by embedding malicious instructions or misleading context within user inputs or system prompts. This paper reviews the landscape of prompt injection threats in the context of LLM APIs deployed via web interfaces, highlighting their implications on model integrity, trustworthiness, and data privacy. The study explores existing detection mechanisms, including static input validation, output filtering, and adversarial input testing, and evaluates their limitations in adaptive web environments. Emphasis is placed on a novel defensive paradigm\u2014context-aware token sanitization\u2014which leverages semantic context tracking, dependency parsing, and transformer-based anomaly detection to neutralize injection attempts before model execution. Through comparative analysis and architectural frameworks, the paper outlines design considerations for secure LLM API deployment. Finally, future directions for integrating robust, real-time sanitization with federated logging and compliance auditing are proposed, paving the way for secure and reliable web-based LLM systems.",
      "year": 2025,
      "venue": "International Journal of Multidisciplinary Evolutionary Research",
      "authors": [
        "Stephanie Onyekachi Oparah",
        "Funmi Eko Ezeh",
        "Pamela Gado",
        "Adeyeni Adeleke",
        "Stephen Vure Gbaraba"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/7ca15f229e965e0296836d5d419eb604d2434a1e",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a33345c6aca10d3979aa33c612e8ca18363ce53d",
      "title": "Soft Instruction De-escalation Defense",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Nils Philipp Walter",
        "Chawin Sitawarin",
        "Jamie Hayes",
        "David Stutz",
        "Ilia Shumailov"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a33345c6aca10d3979aa33c612e8ca18363ce53d",
      "pdf_url": "",
      "publication_date": "2025-10-24",
      "keywords_matched": [
        "prompt injection",
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f32b753d9c03ecfa1db2590157fb7b79945edf5",
      "title": "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems",
      "abstract": "Large Language Models (LLMs) deployed in enterprise settings (e.g., as Microsoft 365 Copilot) face novel security challenges. One critical threat is prompt inference attacks: adversaries chain together seemingly benign prompts to gradually extract confidential data. In this paper, we present a comprehensive study of multi-stage prompt inference attacks in an enterprise LLM context. We simulate realistic attack scenarios where an attacker uses mild-mannered queries and indirect prompt injections to exploit an LLM integrated with private corporate data. We develop a formal threat model for these multi-turn inference attacks and analyze them using probability theory, optimization frameworks, and information-theoretic leakage bounds. The attacks are shown to reliably exfiltrate sensitive information from the LLM's context (e.g., internal SharePoint documents or emails), even when standard safety measures are in place. We propose and evaluate defenses to counter such attacks, including statistical anomaly detection, fine-grained access control, prompt sanitization techniques, and architectural modifications to LLM deployment. Each defense is supported by mathematical analysis or experimental simulation. For example, we derive bounds on information leakage under differential privacy-based training and demonstrate an anomaly detection method that flags multi-turn attacks with high AUC. We also introduce an approach called\"spotlighting\"that uses input transformations to isolate untrusted prompt content, reducing attack success by an order of magnitude. Finally, we provide a formal proof of concept and empirical validation for a combined defense-in-depth strategy. Our work highlights that securing LLMs in enterprise settings requires moving beyond single-turn prompt filtering toward a holistic, multi-stage perspective on both attacks and defenses.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Andrii Balashov",
        "Olena Ponomarova",
        "Xiaohua Zhai"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4f32b753d9c03ecfa1db2590157fb7b79945edf5",
      "pdf_url": "",
      "publication_date": "2025-07-21",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection",
        "prompt filtering",
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4a8c795a0a56501dbcbf65230f7365b511e821b8",
      "title": "Promptsecure: Secure Prompt Engineering Protocols for Regulated Genai Environments",
      "abstract": "The rapid proliferation of Generative AI (GenAI) technologies has introduced a new era of content creation,\nautomation, and intelligence augmentation. However, the growing reliance on prompt-based interfaces within these models\n\nhas surfaced critical concerns related to prompt injection, data leakage, adversarial manipulation, and regulatory non-\ncompliance. Despite advancements in large language models (LLMs), the absence of standardized and secure prompt\n\nengineering frameworks leaves a vulnerability gap\u2014especially in high-stakes and regulated domains such as healthcare,\nlaw, finance, and government operations. This research proposes PromptSecure, a comprehensive protocol-driven\nframework that introduces secure, context-aware, and auditable prompt engineering methodologies designed for GenAI\ndeployments in regulated environments. Unlike traditional prompt tuning approaches that prioritize model performance,\nPromptSecure integrates principles from cybersecurity, differential privacy, and software verification to construct a\nhardened prompt lifecycle\u2014from design and sanitization to execution and monitoring. The protocol encapsulates both static\nand dynamic prompt validation mechanisms, role-based access control for sensitive prompt execution, and traceable prompt\nhistory management using secure audit trails. PromptSecure also incorporates a layered compliance scaffold tailored to\nconform with GDPR, HIPAA, ISO/IEC 27001, and other global AI governance directives. Experimental evaluation within\nsandboxed enterprise-grade GenAI environments demonstrates PromptSecure\u2019s capability to mitigate injection risks,\nenforce prompt boundaries, and retain system integrity under adversarial probing. This study fills a critical research gap at\nthe intersection of prompt engineering and AI governance, and lays the groundwork for establishing secure-by-design GenAI\npractices essential for building public trust and institutional adoption of foundation models.",
      "year": 2025,
      "venue": "International Journal of Innovative Science and Research Technology",
      "authors": [
        "Tinakaran Chinnachamy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4a8c795a0a56501dbcbf65230f7365b511e821b8",
      "pdf_url": "",
      "publication_date": "2025-08-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6398286e9e922c25f303ad01a095779dcde87f0d",
      "title": "Model-Agnostic Unsupervised Detection of Prompt Injection with Multiscale Perplexity Signatures",
      "abstract": "Prompt injection represents a critical and increasingly prevalent threat to the safety and reliability of large language models (LLMs). These attacks enable adversaries to override intended instructions using only natural language, often without access to model parameters or system internals. In this work, we propose a model-agnostic, unsupervised framework for prompt injection detection based on token-level uncertainty dynamics. Specifically, we introduce multi-scale perplexity signatures, i.e., structured representations of model perplexity computed across multiple sliding window sizes, to capture local and global fluctuations in predictive confidence. These signatures serve as input to lightweight statistical classifiers, such as OC-SVMs, which are trained exclusively on clean prompts to detect anomalous inputs. Extensive experiments are conducted on an open dataset, and the results demonstrate that the multiscale perplexity signatures captured by the proposed approach are effective in distinguishing injected prompts from clean ones across multiple detection models.",
      "year": 2025,
      "venue": "IEEE Military Communications Conference",
      "authors": [
        "Jielun Zhang",
        "Fuhao Li"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6398286e9e922c25f303ad01a095779dcde87f0d",
      "pdf_url": "",
      "publication_date": "2025-10-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "89ff8a42b05ce90826d253724601544c242b4bb2",
      "title": "Safeguarding Crowdsourcing Surveys from ChatGPT through Prompt Injection",
      "abstract": "ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses ''prompt injection,'' such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 98% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.",
      "year": 2025,
      "venue": "Proc. ACM Hum. Comput. Interact.",
      "authors": [
        "Chaofan Wang",
        "S. Kernan Freire",
        "Mo Zhang",
        "Jing Wei",
        "Jorge Goncalves",
        "V. Kostakos",
        "A. Bozzon",
        "E. Niforatos"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/89ff8a42b05ce90826d253724601544c242b4bb2",
      "pdf_url": "",
      "publication_date": "2025-10-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a78327b9cc6e3b980e70489d22c96c0d26bca6c",
      "title": "A Novel GNN-based Approach for Detection of Prompt Injection Attacks",
      "abstract": "Prompt injection attacks manipulate language model inputs to bypass intended constraints, extract sensitive information, or generate misleading responses, posing a significant security risk in real-world applications. To address this challenge, we propose a Graph Neural Network (GNN)based approach that integrates sentiment features and Bidirectional Encoder Representations from Transformers (BERT) embeddings to effectively detect malicious prompt injections. By transforming textual data into structured graph representations, our approach captures both semantic and contextual relationships that conventional models often overlook. We evaluated our approach against traditional machine learning techniques, including Random Forest, Logistic Regression, and XGBoost, demonstrating its superior performance. Experimental results show that our approach achieves a high detection accuracy of $98.70 \\%$ and an F1-score of 0.9799, significantly outperforming conventional methods. Additionally, we provide an in-depth analysis of computational efficiency, highlighting the trade-offs between detection effectiveness and model complexity, ensuring a practical balance between security and performance.",
      "year": 2025,
      "venue": "Computer Science Symposium in Russia",
      "authors": [
        "Gaurav Jadhav",
        "Amit Kumar Singh",
        "Zeba Khanam",
        "Robert Hercock"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0a78327b9cc6e3b980e70489d22c96c0d26bca6c",
      "pdf_url": "",
      "publication_date": "2025-08-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "81799a61b1e50bd1050720ec4e31a531eb02bbd1",
      "title": "Detecting Prompt Injection Attacks Against Application Using Classifiers",
      "abstract": "Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Safwan Shaheer",
        "G. M. R. Islam",
        "Mohammad Rafid Hamid",
        "Md. Abrar",
        "Faiaz Khan",
        "Md. Omar Faruk",
        "Yaseen Nur"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/81799a61b1e50bd1050720ec4e31a531eb02bbd1",
      "pdf_url": "",
      "publication_date": "2025-12-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c84e5d0baf72ef91a6b5bc0807b17b90b65a708d",
      "title": "Phishing Email Detection Using Large Language Models",
      "abstract": "Email phishing is one of the most prevalent and globally consequential vectors of cyber intrusion. As systems increasingly deploy Large Language Models (LLMs) applications, these systems face evolving phishing email threats that exploit their fundamental architectures. Current LLMs require substantial hardening before deployment in email security systems, particularly against coordinated multi-vector attacks that exploit architectural vulnerabilities. This paper proposes LLMPEA, an LLM-based framework to detect phishing email attacks across multiple attack vectors, including prompt injection, text refinement, and multilingual attacks. We evaluate three frontier LLMs (e.g., GPT-4o, Claude Sonnet 4, and Grok-3) and comprehensive prompting design to assess their feasibility, robustness, and limitations against phishing email attacks. Our empirical analysis reveals that LLMs can detect the phishing email over 90% accuracy while we also highlight that LLM-based phishing email detection systems could be exploited by adversarial attack, prompt injection, and multilingual attacks. Our findings provide critical insights for LLM-based phishing detection in real-world settings where attackers exploit multiple vulnerabilities in combination.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Najmul Hassan",
        "Prashanth Busireddygari",
        "Haitao Zhao",
        "Yihao Ren",
        "Jinsheng Xu",
        "Shaohu Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c84e5d0baf72ef91a6b5bc0807b17b90b65a708d",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5ab9b4a7a4983de06e13db434024339c4f4e68fa",
      "title": "Unlocking LLM Security: Automated Penetration Testing and Source Code Review Based on Vulnerability Analysis and NLP",
      "abstract": "This paper introduces an innovative framework designed to bolster the security of large language models (LLMs) and source code through automated penetration testing and vulnerability analysis. By integrating natural language processing (NLP) and machine learning, the proposed system overcomes the shortcomings of conventional security tools, offering a unified solution for testing LLMs and reviewing source code across multiple programming languages. The architecture employs API-based testing to probe LLMs for vulnerabilities such as prompt injections and data leaks, while leveraging static and dynamic analysis to detect coding flaws like SQL injection and buffer overflows. Experimental evaluations confirm the system's efficacy in identifying security risks, with features including real-time monitoring and an intuitive interface enhancing its accessibility to developers of varying expertise. Despite its promising accuracy and efficiency, challenges such as false positives and reliance on dataset quality are noted, suggesting directions for future refinement. This work advances AIdriven cybersecurity by delivering a scalable, adaptable toolset for securing modern software systems and intelligent models, contributing significantly to the evolving landscape of digital security.",
      "year": 2025,
      "venue": "2025 International Telecommunications Conference (ITC-Egypt)",
      "authors": [
        "Ali E. Takieldeen",
        "Noor Hossam",
        "Osama Elkassaby",
        "Moaaz Awad",
        "Mahmoud Gomaa",
        "Adham Ahmed",
        "Ahmed Saad",
        "S. H. Haggag",
        "Nada Samir Elsayed"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/5ab9b4a7a4983de06e13db434024339c4f4e68fa",
      "pdf_url": "",
      "publication_date": "2025-07-28",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6065b019e84b587b2012c66218455dc888877511",
      "title": "Adaptive and Privacy-Preserving Detection of Evolving Prompt Injection Attacks in Large Language Models",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities but are extremely vulnerable to prompt injection attacks where malicious inputs bypass the intended instructions, bypass safety mechanisms, or leak sensitive data. Current defences have two severe shortcomings: they are fixed and unresponsive to new attack techniques like obfuscation and multi-turn manipulation, and frequently they impact the privacy of the user by sending unprocessed queries to third-party validation services. Such constraints render existing solutions inappropriate to real-world, privacy-aware activities. This research suggests a privacy-sensitive and adaptive detection architecture that combines continual learning and context-sensitive embeddings, and federated detection pipelines. The framework becomes flexible to new adversarial actions and provides local data confidentiality. According to experimental findings, the detection accuracy is 94.8% and the false positive rate is 4.1, which is higher than that of baseline methods, meanwhile, the obfuscation attack and multi-turn attacks are detected more, with gains of 12.4 and 15.7 percentage points respectively, and latency is real-time (112 ms/query). The model provides a measurable and reliable framework of securing systems powered by LLM.",
      "year": 2025,
      "venue": "2025 International Conference on Sustainable Communication Networks and Application (ICSCN)",
      "authors": [
        "Sivakumar Depuru",
        "Kurugunda Radha Lakshmi",
        "Gopa Chandrahas",
        "Mopuri Chetana",
        "M. V. Reddy"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6065b019e84b587b2012c66218455dc888877511",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9be8902fc1bbe28ba09336f492577eb9a030d739",
      "title": "Adaptive Multi\u2011Layer Framework for Detecting and Mitigating Prompt Injection Attacks in Large Language Models",
      "abstract": "Background: Prompt injection attacks are methods that exploit the instruction\u2010following nature of fine\u2010tuned large language models (LLMs), leading to the execution of unintended or malicious commands. This vulnerability shows the limitation of traditional defenses, including static filters, keyword blocks, and multi\u2010LLMs cross\u2010checks, which lack semantic understanding or incur high latency and operational overhead.\nObjective: This study aimed to develop and evaluate a lightweight adaptive framework capable of detecting and neutralizing prompt injection attacks in real-time.\nMethods: Prompt-Shield Framework (PSF) was developed around a locally hosted Llama 3.2 API. This PSF integrated three modules, namely Context-Aware Parsing (CAP), Output Validation (OV), and Self-Feedback Loop (SFL), to pre-filter inputs, validate outputs, and iteratively refine detection rules. Subsequently, five scenarios were tested, comprising baseline (without any defenses), CAP only, OV only, CAP+OV, and CAP+OV+SFL. The evaluation was performed over a near-balanced dataset of 1,405 adversarial and 1,500 benign prompt, measuring classification performance through confusion matrices, precision, recall, and accuracy.\nResults: The results showed that baseline achieved 63.06% accuracy (precision = 0.678; recall = 0.450), while OV only improved performance to 79.28% (precision = 0.796; recall = 0.768). CAP reached 84.68% accuracy (precision = 0.891; recall = 0.779), while CAP+OV yielded 95.25% accuracy (precision = 0.938; recall = 0.966). Finally, integrating SFL over 10 epochs further improved performance to 97.83% accuracy (precision = 0.980; recall = 0.975) and reduced the false-negative count from 48 (CAP+OV) to 35 (CAP+OV+SFL).\nConclusion: The results show the significance of using multiple defenses, such as contextual understanding, OV, and adaptive learning fusion, which are needed for efficient prompt injection mitigation. This shows that PSF framework is an effective solution to protect LLMs against advancing threats. Moreover, further studies should aim to refine the adaptive thresholds in CAP and OV, particularly in multilingual or highly specialized environments, and examine other forms of SFL solutions for better efficiency.\n\u00a0Keywords: Prompt Injection, LLMs Security, Jailbreak, Natural Language Processing",
      "year": 2025,
      "venue": "Journal of Information Systems Engineering and Business Intelligence",
      "authors": [
        "R. B. Hadiprakoso",
        "Wiyar Wilujengning",
        "Amiruddin Amiruddin"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/9be8902fc1bbe28ba09336f492577eb9a030d739",
      "pdf_url": "",
      "publication_date": "2025-10-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "efbc6bec9d079cb8577fab22423e92bbc43cad01",
      "title": "Analysis of the impact of prompt obfuscation on the effectiveness of language models in detecting prompt injections",
      "abstract": "\n The article addresses the issue of prompt obfuscation as a means of circumventing protective mechanisms in large language models (LLMs) designed to detect prompt injections. Prompt injections represent a method of attack in which malicious actors manipulate input data to alter the model's behavior and cause it to perform undesirable or harmful actions. Obfuscation involves various methods of changing the structure and content of text, such as replacing words with synonyms, scrambling letters in words, inserting random characters, and others. The purpose of obfuscation is to complicate the analysis and classification of text in order to bypass filters and protective mechanisms built into language models. The study conducts an analysis of the effectiveness of various obfuscation methods in bypassing models trained for text classification tasks. Particular attention is paid to assessing the potential implications of obfuscation for security and data protection. The research utilizes different text obfuscation methods applied to prompts from the AdvBench dataset. The effectiveness of the methods is evaluated using three classifier models trained to detect prompt injections. The scientific novelty of the research lies in analyzing the impact of prompt obfuscation on the effectiveness of language models in detecting prompt injections. During the study, it was found that the application of complex obfuscation methods increases the proportion of requests classified as injections, highlighting the need for a thorough approach to testing the security of large language models. The conclusions of the research indicate the importance of balancing the complexity of the obfuscation method with its effectiveness in the context of attacks on models. Excessively complex obfuscation methods may increase the likelihood of injection detection, which requires further investigation to optimize approaches to ensuring the security of language models. The results underline the need for the continuous improvement of protective mechanisms and the development of new methods for detecting and preventing attacks on large language models.\n",
      "year": 2025,
      "venue": "\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b",
      "authors": [
        "Aleksei Sergeevich Krohin",
        "Maksim Mihailovich Gusev"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/efbc6bec9d079cb8577fab22423e92bbc43cad01",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "prompt injection",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "de3be1849858783913efe28d023c0dc2f528609a",
      "title": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts",
      "abstract": "The tidal wave of advancements in Large Language Models (LLMs) has led to their swift integration into application-level logic. Many software systems now use prompts to interact with these black-box models, combining natural language with dynamic values interpolated at runtime, to perform tasks ranging from sentiment analysis to question answering. Due to the programmatic and structured natural language aspects of these prompts, we refer to them as Developer Prompts. Unlike traditional software artifacts, Dev Prompts blend natural language instructions with artificial languages such as programming and markup languages, thus requiring specialized tools for analysis, distinct from classical software evaluation methods. In response to this need, we introduce PromptDoctor, a tool explicitly designed to detect and correct issues of Dev Prompts. PromptDoctor identifies and addresses problems related to bias, vulnerability, and sub-optimal performance in Dev Prompts, helping mitigate their possible harms. In our analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573 Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75% were vulnerable to prompt injection attacks. Additionally, 3,310 were amenable to automated prompt optimization. To address these issues, we applied PromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased 68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts. Finally, we developed a PromptDoctor VSCode extension, enabling developers to easily enhance Dev Prompts in their existing development workflows. The data and source code for this work are available at",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "D. Rzig",
        "Dhruba Jyoti Paul",
        "Kaiser Pister",
        "Jordan Henkel",
        "Foyzul Hassan"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/de3be1849858783913efe28d023c0dc2f528609a",
      "pdf_url": "",
      "publication_date": "2025-01-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4d0766c0edba64e35cf3369cc56e59b42551f58",
      "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment",
      "abstract": "Safely aligning large language models (LLMs) often demands extensive human-labeled preference data, a process that's both costly and time-consuming. While synthetic data offers a promising alternative, current methods frequently rely on complex iterative prompting or auxiliary models. To address this, we introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward, training-free, and model-agnostic framework that repurposes LLM attack techniques. RAAI works by detecting internal refusal signals and adaptively injecting predefined phrases to elicit harmful, yet fluent, completions. Our experiments show RAAI effectively jailbreaks LLMs, increasing the harmful response rate from a baseline of 2.15% to up to 61.04% on average across four benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by RAAI improves model robustness against harmful prompts while preserving general capabilities on standard tasks like MMLU and ARC. This work highlights how LLM attack methodologies can be reframed as practical tools for scalable and controllable safety alignment.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kyubyung Chae",
        "Hyunbin Jin",
        "Taesup Kim"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c4d0766c0edba64e35cf3369cc56e59b42551f58",
      "pdf_url": "",
      "publication_date": "2025-06-07",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "65d9d85b86ce1b8c3dd01a446ebf28d99fd26bdd",
      "title": "DRIP: Defending Prompt Injection via Token-wise Representation Editing and Residual Instruction Fusion",
      "abstract": "Large language models (LLMs) are increasingly integrated into IT infrastructures, where they process user data according to predefined instructions. However, conventional LLMs remain vulnerable to prompt injection, where malicious users inject directive tokens into the data to subvert model behavior. Existing defenses train LLMs to semantically separate data and instruction tokens, but still struggle to (1) balance utility and security and (2) prevent instruction-like semantics in the data from overriding the intended instructions. We propose DRIP, which (1) precisely removes instruction semantics from tokens in the data section while preserving their data semantics, and (2) robustly preserves the effect of the intended instruction even under strong adversarial content. To\"de-instructionalize\"data tokens, DRIP introduces a data curation and training paradigm with a lightweight representation-editing module that edits embeddings of instruction-like tokens in the data section, enhancing security without harming utility. To ensure non-overwritability of instructions, DRIP adds a minimal residual module that reduces the ability of adversarial data to overwrite the original instruction. We evaluate DRIP on LLaMA 8B and Mistral 7B against StruQ, SecAlign, ISE, and PFT on three prompt-injection benchmarks (SEP, AlpacaFarm, and InjecAgent). DRIP improves role-separation score by 12-49\\%, reduces attack success rate by over 66\\% under adaptive attacks, and matches the utility of the undefended model, establishing a new state of the art for prompt-injection robustness.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Ruofan Liu",
        "Yun Lin",
        "Zhiyong Huang",
        "J. Dong"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/65d9d85b86ce1b8c3dd01a446ebf28d99fd26bdd",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c93b404add662baa942e77a78db5763e85294f58",
      "title": "PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight",
      "abstract": "We propose a robust transformer architecture designed to prevent prompt injection attacks and ensure secure, reliable response generation. Our PICO (Prompt Isolation and Cybersecurity Oversight) framework structurally separates trusted system instructions from untrusted user inputs through dual channels that are processed independently and merged only by a controlled, gated fusion mechanism. In addition, we integrate a specialized Security Expert Agent within a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge Graph (CKG) to supply domain-specific reasoning. Our training design further ensures that the system prompt branch remains immutable while the rest of the network learns to handle adversarial inputs safely. This PICO framework is presented via a general mathematical formulation, then elaborated in terms of the specifics of transformer architecture, and fleshed out via hypothetical case studies including Policy Puppetry attacks. While the most effective implementation may involve training transformers in a PICO-based way from scratch, we also present a cost-effective fine-tuning approach.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ben Goertzel",
        "Paulos Yibelo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/c93b404add662baa942e77a78db5763e85294f58",
      "pdf_url": "",
      "publication_date": "2025-04-26",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prevent prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abd3df6c7d3e62c356199bf1d8c112afe4c52a74",
      "title": "Large Language Models for Cyber Security",
      "abstract": "This paper studies the integration off Large Language Models into cybersecurity tools and protocols. The main issue discussed in this paper is how traditional rule-based and signature based security systems are not enough to deal with modern AI powered cyber threats. Cybersecurity industry is changing as threats are becoming more dangerous and adaptive in nature by levering the features provided by AI tools. By integrating LLMs into these tools and protocols, make the systems scalable, context-aware and intelligent. Thus helping it to mitigate these evolving cyber threats. The paper studies the architecture and functioning of LLMs, its integration into Encrypted prompts to prevent prompt injection attacks. It also studies the integration of LLMs into cybersecurity tools using a four layered architecture. At last, the paper has tried to explain various ways of integration LLMs into traditional Intrusion Detection System and enhancing its original abilities in various dimensions. The key findings of this paper has been (i)Encrypted Prompt with LLM is an effective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber security tools are more accurate, scalable and adaptable to new threats as compared to traditional models, (iii) The decoupled model approach for LLM integration into IDS is the best way as it is the most accurate way.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Raunak Somani",
        "Cherukuri Aswani Kumar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/abd3df6c7d3e62c356199bf1d8c112afe4c52a74",
      "pdf_url": "",
      "publication_date": "2025-11-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prevent prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "516d7e054ada22f8974bdcea5659af9e47e67a01",
      "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints",
      "abstract": "The widespread deployment of LLMs across enterprise services has created a critical security blind spot. Organizations operate multiple LLM services handling billions of queries daily, yet regulatory compliance boundaries prevent these services from sharing threat intelligence about prompt injection attacks, the top security risk for LLMs. When an attack is detected in one service, the same threat may persist undetected in others for months, as privacy regulations prohibit sharing user prompts across compliance boundaries. We present BinaryShield, the first privacy-preserving threat intelligence system that enables secure sharing of attack fingerprints across compliance boundaries. BinaryShield transforms suspicious prompts through a unique pipeline combining PII redaction, semantic embedding, binary quantization, and randomized response mechanism to potentially generate non-invertible fingerprints that preserve attack patterns while providing privacy. Our evaluations demonstrate that BinaryShield achieves an F1-score of 0.94, significantly outperforming SimHash (0.77), the privacy-preserving baseline, while achieving 64x storage reduction and 38x faster similarity search compared to dense embeddings.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Waris Gill",
        "Natalie Isak",
        "Matthew Dressman"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/516d7e054ada22f8974bdcea5659af9e47e67a01",
      "pdf_url": "",
      "publication_date": "2025-09-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "117cd1d71516457304f87903bac925e5d1222325",
      "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents",
      "abstract": "Autonomous LLM agents can issue thousands of API calls per hour without human oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings stochastic reasoning, prompt injection, or multi-agent orchestration can silently expand privileges. We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each agent's action to verifiable user intent and, optionally, to a specific workflow step. A-JWT carries an agent's identity as a one-way checksum hash derived from its prompt, tools and configuration, and a chained delegation assertion to prove which downstream agent may execute a given task, and per-agent proof-of-possession keys to prevent replay and in-process impersonation. We define a new authorization mechanism and add a lightweight client shim library that self-verifies code at run time, mints intent tokens, tracks workflow steps and derives keys, thus enabling secure agent identity and separation even within a single process. We illustrate a comprehensive threat model for agentic applications, implement a Python proof-of-concept and show functional blocking of scope-violating requests, replay, impersonation, and prompt-injection pathways with sub-millisecond overhead on commodity hardware. The design aligns with ongoing OAuth agent discussions and offers a drop-in path toward zero-trust guarantees for agentic applications. A comprehensive performance and security evaluation with experimental results will appear in our forthcoming journal publication",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Abhishek Goswami"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/117cd1d71516457304f87903bac925e5d1222325",
      "pdf_url": "",
      "publication_date": "2025-09-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d1dfe09d10632c6d52cfdfc83c9f6d91a57570b6",
      "title": "Secure Artificial Intelligence (SAI): A Dual-layer defence model against prompt injection and prompt poisoning attacks",
      "abstract": "Secure Artificial Intelligence (SAI): A Dual-layer defence model against prompt injection and prompt poisoning attacks.\nKeywords\u2014 Large Language Model, Secure Artificial Intelligence, Artificial Intelligence, Prompt Injection, AI security. \nAbstract\u2014 Artificial Intelligence (AI) is deeply embedded in sectors handling sensitive information and mission-critical operations, and safeguarding these systems has become paramount. This paper introduces a novel dual-layer defence system termed Secure Artificial Intelligence (SAI), designed to mitigate risks associated with prompt injections and prompt poisoning attacks. Using two Large Language Models (LLMs) in a sequential setup \u201cSAI\u201d\u2013 a \u201cGuard\u201d model for initial input prompt classification which effectively filters out adversarial inputs to protect the AI system and a primary response model that responds to the user\u2019s queries. Through rigorous testing, SAI has shown resilience in preventing malicious prompts from compromising AI responses, thereby significantly advancing AI security. This paper thoroughly examines SAI\u2019s architecture, methodology, and performance, addressing the growing demand for secure and adversarial-resistant AI systems.",
      "year": 2025,
      "venue": "International Journal For Multidisciplinary Research",
      "authors": [
        "Hitika Teckani",
        "Devansh Pandya",
        "Harshita Makwana"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d1dfe09d10632c6d52cfdfc83c9f6d91a57570b6",
      "pdf_url": "",
      "publication_date": "2025-01-18",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6b4a03ad48aa6f0084743a1378358526f6ce573d",
      "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails",
      "abstract": "Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p<0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Prithviraj Singh Shahani",
        "Matthias Scheutz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/6b4a03ad48aa6f0084743a1378358526f6ce573d",
      "pdf_url": "",
      "publication_date": "2025-05-16",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "88d5634a52645f6b05a03536be1f26a2b9bba232",
      "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
      "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token\"Sure\"), potentially with multiple restarts. In this way, we achieve 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format at https://github.com/tml-epfl/llm-adaptive-attacks.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Maksym Andriushchenko",
        "Francesco Croce",
        "Nicolas Flammarion"
      ],
      "citation_count": 362,
      "url": "https://www.semanticscholar.org/paper/88d5634a52645f6b05a03536be1f26a2b9bba232",
      "pdf_url": "",
      "publication_date": "2024-04-02",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f18e5a844c37e5342f8f3d409c74c1a9c91d1f8f",
      "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
      "abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Eric Wallace",
        "Kai Xiao",
        "R. Leike",
        "Lilian Weng",
        "Johannes Heidecke",
        "Alex Beutel"
      ],
      "citation_count": 223,
      "url": "https://www.semanticscholar.org/paper/f18e5a844c37e5342f8f3d409c74c1a9c91d1f8f",
      "pdf_url": "",
      "publication_date": "2024-04-19",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c8eee9766f0968e8f1b1be0731bc70b85be0ac97",
      "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
      "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Qiusi Zhan",
        "Zhixiang Liang",
        "Zifan Ying",
        "Daniel Kang"
      ],
      "citation_count": 202,
      "url": "https://www.semanticscholar.org/paper/c8eee9766f0968e8f1b1be0731bc70b85be0ac97",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "523b8635597d6c0bf05fd0e3f35f3a18d2748a24",
      "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
      "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of\"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Sibo Yi",
        "Yule Liu",
        "Zhen Sun",
        "Tianshuo Cong",
        "Xinlei He",
        "Jiaxing Song",
        "Ke Xu",
        "Qi Li"
      ],
      "citation_count": 192,
      "url": "https://www.semanticscholar.org/paper/523b8635597d6c0bf05fd0e3f35f3a18d2748a24",
      "pdf_url": "",
      "publication_date": "2024-07-05",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "868fcbc6127e9cf79990e92116ec482051d470f3",
      "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
      "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Lijun Li",
        "Bowen Dong",
        "Ruohui Wang",
        "Xuhao Hu",
        "Wangmeng Zuo",
        "Dahua Lin",
        "Yu Qiao",
        "Jing Shao"
      ],
      "citation_count": 166,
      "url": "https://www.semanticscholar.org/paper/868fcbc6127e9cf79990e92116ec482051d470f3",
      "pdf_url": "",
      "publication_date": "2024-02-07",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f5e7e22036c3fe7d6660eee90642f716c3b303f5",
      "title": "StruQ: Defending Against Prompt Injection with Structured Queries",
      "abstract": "Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model into deviating from the original application's instructions and instead follow user directives. These attacks rely on the LLM's ability to follow instructions and inability to separate prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/StruQ.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Sizhe Chen",
        "Julien Piet",
        "Chawin Sitawarin",
        "David Wagner"
      ],
      "citation_count": 162,
      "url": "https://www.semanticscholar.org/paper/f5e7e22036c3fe7d6660eee90642f716c3b303f5",
      "pdf_url": "",
      "publication_date": "2024-02-09",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7ef6182f617ef3e7cc9682f562f794115a4c62c",
      "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
      "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Xing-ming Guo",
        "Fangxu Yu",
        "Huan Zhang",
        "Lianhui Qin",
        "Bin Hu"
      ],
      "citation_count": 146,
      "url": "https://www.semanticscholar.org/paper/b7ef6182f617ef3e7cc9682f562f794115a4c62c",
      "pdf_url": "",
      "publication_date": "2024-02-13",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ea5e86bbbcc445eca1a765deb314eefc06067b8",
      "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
      "abstract": "As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Mikayel Samvelyan",
        "S. Raparthy",
        "Andrei Lupu",
        "Eric Hambro",
        "Aram H. Markosyan",
        "Manish Bhatt",
        "Yun-ing Mao",
        "Minqi Jiang",
        "Jack Parker-Holder",
        "Jakob Foerster",
        "Tim Rocktaschel",
        "R. Raileanu"
      ],
      "citation_count": 144,
      "url": "https://www.semanticscholar.org/paper/7ea5e86bbbcc445eca1a765deb314eefc06067b8",
      "pdf_url": "",
      "publication_date": "2024-02-26",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "title": "Stealing Part of a Production Language Model",
      "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Nicholas Carlini",
        "Daniel Paleka",
        "K. Dvijotham",
        "Thomas Steinke",
        "Jonathan Hayase",
        "A. F. Cooper",
        "Katherine Lee",
        "Matthew Jagielski",
        "Milad Nasr",
        "Arthur Conmy",
        "Eric Wallace",
        "D. Rolnick",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 132,
      "url": "https://www.semanticscholar.org/paper/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48",
      "pdf_url": "",
      "publication_date": "2024-03-11",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf56a7b28fb27279b1c94fb920b5722cf50c8852",
      "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
      "abstract": "Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Anselm Paulus",
        "Arman Zharmagambetov",
        "Chuan Guo",
        "Brandon Amos",
        "Yuandong Tian"
      ],
      "citation_count": 119,
      "url": "https://www.semanticscholar.org/paper/cf56a7b28fb27279b1c94fb920b5722cf50c8852",
      "pdf_url": "",
      "publication_date": "2024-04-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e56f14ced9f7ce344ed14bdcb46860ccac72ac83",
      "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
      "abstract": "LLM-as-a-Judge uses a large language model (LLM) to select the best response from a set of candidates for a given question. LLM-as-a-Judge has many applications such as LLM-powered search, reinforcement learning with AI feedback (RLAIF), and tool selection. In this work, we propose JudgeDeceiver, an optimization-based prompt injection attack to LLM-as-a-Judge. JudgeDeceiver injects a carefully crafted sequence into an attacker-controlled candidate response such that LLM-as-a-Judge selects the candidate response for an attacker-chosen question no matter what other candidate responses are. Specifically, we formulate finding such sequence as an optimization problem and propose a gradient based method to approximately solve it. Our extensive evaluation shows that JudgeDeceive is highly effective, and is much more effective than existing prompt injection attacks that manually craft the injected sequences and jailbreak attacks when extended to our problem. We also show the effectiveness of JudgeDeceiver in three case studies, i.e., LLM-powered search, RLAIF, and tool selection. Moreover, we consider defenses including known-answer detection, perplexity detection, and perplexity windowed detection. Our results show these defenses are insufficient, highlighting the urgent need for developing new defense strategies.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Jiawen Shi",
        "Zenghui Yuan",
        "Yinuo Liu",
        "Yue Huang",
        "Pan Zhou",
        "Lichao Sun",
        "Neil Zhenqiang Gong"
      ],
      "citation_count": 114,
      "url": "https://www.semanticscholar.org/paper/e56f14ced9f7ce344ed14bdcb46860ccac72ac83",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690291",
      "publication_date": "2024-03-26",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fdda2d3fe6156f7f0e48e4af62ff995484fe866",
      "title": "Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models",
      "abstract": "Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs' image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: https://github.com/OPTML-Group/AdvUnlearn",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yimeng Zhang",
        "Xin Chen",
        "Jinghan Jia",
        "Yihua Zhang",
        "Chongyu Fan",
        "Jiancheng Liu",
        "Mingyi Hong",
        "Ke Ding",
        "Sijia Liu"
      ],
      "citation_count": 108,
      "url": "https://www.semanticscholar.org/paper/3fdda2d3fe6156f7f0e48e4af62ff995484fe866",
      "pdf_url": "",
      "publication_date": "2024-05-24",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0a6a350653369dc92fde4cf9992951534ed1f169",
      "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
      "abstract": "Large Language Models (LLMs) excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for understanding the objectives of prompt injection attacks and present an automated gradient-based method for generating highly effective and universal prompt injection data, even in the face of defensive measures. With only five training samples (0.3% relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient-based testing, which can avoid overestimation of robustness, especially for defense mechanisms.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xiaogeng Liu",
        "Zhiyuan Yu",
        "Yizhe Zhang",
        "Ning Zhang",
        "Chaowei Xiao"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/0a6a350653369dc92fde4cf9992951534ed1f169",
      "pdf_url": "",
      "publication_date": "2024-03-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32316f1d08911b397f52f8644f46f5c129619383",
      "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
      "abstract": "Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}\\% to below {2}\\% in our experiments with minimal impact on task efficacy.",
      "year": 2024,
      "venue": "CAMLIS",
      "authors": [
        "Keegan Hines",
        "Gary Lopez",
        "Matthew Hall",
        "Federico Zarfati",
        "Yonatan Zunger",
        "Emre Kiciman"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/32316f1d08911b397f52f8644f46f5c129619383",
      "pdf_url": "",
      "publication_date": "2024-03-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6fa682b62c7981402336ca57da1196ccbf3fc54",
      "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
      "abstract": "In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and closed-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 91.1% attack success rate on OpenAI GPT-4 chatbot.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tong Liu",
        "Yingjie Zhang",
        "Zhe Zhao",
        "Yinpeng Dong",
        "Guozhu Meng",
        "Kai Chen"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/f6fa682b62c7981402336ca57da1196ccbf3fc54",
      "pdf_url": "",
      "publication_date": "2024-02-28",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6a2e7da48e3588feee6e5dceceae3cd87bd02ae9",
      "title": "Attacking Large Language Models with Projected Gradient Descent",
      "abstract": "Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Simon Geisler",
        "Tom Wollschlager",
        "M. H. I. Abdalla",
        "Johannes Gasteiger",
        "Stephan Gunnemann"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/6a2e7da48e3588feee6e5dceceae3cd87bd02ae9",
      "pdf_url": "",
      "publication_date": "2024-02-14",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f4efbe3aae1d8f44ceab1da257ae685d6beb00b",
      "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents",
      "abstract": "Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 27 different types of attack/defense methods, and 7 evaluation metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and 11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30\\%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. We also introduce a new metric to evaluate the agents' capability to balance utility and security. Our code can be found at https://github.com/agiresearch/ASB.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Hanrong Zhang",
        "Jingyuan Huang",
        "Kai Mei",
        "Yifei Yao",
        "Zhenting Wang",
        "Chenlu Zhan",
        "Hongwei Wang",
        "Yongfeng Zhang"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/5f4efbe3aae1d8f44ceab1da257ae685d6beb00b",
      "pdf_url": "",
      "publication_date": "2024-10-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09e2391ab4c266f96c71cc379c1a7c7ddd40ee33",
      "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers",
      "abstract": "The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \\textbf{D}ecomposition and \\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries surpassed previous art by 33.1\\%. The project is available at https://github.com/xirui-li/DrAttack.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Xirui Li",
        "Ruochen Wang",
        "Minhao Cheng",
        "Tianyi Zhou",
        "Cho-Jui Hsieh"
      ],
      "citation_count": 88,
      "url": "https://www.semanticscholar.org/paper/09e2391ab4c266f96c71cc379c1a7c7ddd40ee33",
      "pdf_url": "",
      "publication_date": "2024-02-25",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "13f2985246d6b91f788bb222360a199967af32c8",
      "title": "Rethinking LLM Memorization through the Lens of Adversarial Compression",
      "abstract": "Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. One major question is whether these models\"memorize\"all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on how we define memorization. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs. A given string from the training data is considered memorized if it can be elicited by a prompt (much) shorter than the string itself -- in other words, if these strings can be\"compressed\"with the model by computing adversarial prompts of fewer tokens. The ACR overcomes the limitations of existing notions of memorization by (i) offering an adversarial view of measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Avi Schwarzschild",
        "Zhili Feng",
        "Pratyush Maini",
        "Zachary Chase Lipton",
        "J. Kolter"
      ],
      "citation_count": 87,
      "url": "https://www.semanticscholar.org/paper/13f2985246d6b91f788bb222360a199967af32c8",
      "pdf_url": "",
      "publication_date": "2024-04-23",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf95279b1da9de1aad9e7c651f5048f69af295ed",
      "title": "AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents",
      "abstract": "AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Edoardo Debenedetti",
        "Jie Zhang",
        "Mislav Balunovi'c",
        "Luca Beurer-Kellner",
        "Marc Fischer",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "149ab16bed8554e71358b3642cb6979884c786b9",
      "title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
      "abstract": "In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally adversarial perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that the adversarial image prompt LVLMs to respond positively to harmful queries. Subsequently, leveraging the image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our BAP significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as GPT-4o and Gemini. Our code is available at https://anonymous.4open.science/r/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt-5496",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Zonghao Ying",
        "Aishan Liu",
        "Tianyuan Zhang",
        "Zhengmin Yu",
        "Siyuan Liang",
        "Xianglong Liu",
        "Dacheng Tao"
      ],
      "citation_count": 74,
      "url": "https://www.semanticscholar.org/paper/149ab16bed8554e71358b3642cb6979884c786b9",
      "pdf_url": "",
      "publication_date": "2024-06-06",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97e0d9dd51115d21cd87c14068a127bb51de352c",
      "title": "A Survey of Attacks on Large Vision\u2013Language Models: Resources, Advances, and Future Trends",
      "abstract": "With the significant development of large models in recent years, large vision\u2013language models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding and reasoning tasks. Compared with traditional large language models (LLMs), LVLMs present great potential and challenges due to their closer proximity to the multiresource real-world applications and the complexity of multimodal processing. However, the vulnerability of LVLMs is relatively underexplored, posing potential security risks in the daily use of LVLM applications. In this article, we provide a comprehensive review of the various forms of existing LVLM attacks. Specifically, we first introduce the background of attacks targeting LVLMs, including the attack preliminary, attack challenges, and attack resources. Then, we systematically review the development of LVLM attack methods, such as adversarial attacks that manipulate model outputs, jailbreak attacks that exploit model vulnerabilities for unauthorized actions, prompt injection attacks that engineer the prompt type and pattern, and data poisoning that affects model training. Finally, we discuss promising future research directions in LVLM attacks. We believe that our survey provides insights into the current landscape of LVLM vulnerabilities, inspiring more researchers to explore and mitigate potential safety issues in LVLM developments.",
      "year": 2024,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "Daizong Liu",
        "Mingyu Yang",
        "Xiaoye Qu",
        "Pan Zhou",
        "Wei Hu",
        "Yu Cheng"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/97e0d9dd51115d21cd87c14068a127bb51de352c",
      "pdf_url": "",
      "publication_date": "2024-07-10",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2a4ddbed34916cfa345e957cf060da99685e37b",
      "title": "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning",
      "abstract": "Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4.",
      "year": 2024,
      "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
      "authors": [
        "Gelei Deng",
        "Yi Liu",
        "Kailong Wang",
        "Yuekang Li",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "citation_count": 68,
      "url": "https://www.semanticscholar.org/paper/a2a4ddbed34916cfa345e957cf060da99685e37b",
      "pdf_url": "",
      "publication_date": "2024-02-13",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3cd81b0123b5f8477f6b5777681030ef6b05dd46",
      "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
      "abstract": "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Lingfeng Shen",
        "Weiting Tan",
        "Sihao Chen",
        "Yunmo Chen",
        "Jingyu (Jack) Zhang",
        "Haoran Xu",
        "Boyuan Zheng",
        "Philipp Koehn",
        "Daniel Khashabi"
      ],
      "citation_count": 64,
      "url": "https://www.semanticscholar.org/paper/3cd81b0123b5f8477f6b5777681030ef6b05dd46",
      "pdf_url": "",
      "publication_date": "2024-01-23",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "da9e8b168817b4101fc166a11b2db74d7cafcbd9",
      "title": "A Hitchhiker\u2019s Guide to Jailbreaking ChatGPT via Prompt Engineering",
      "abstract": "Natural language prompts serve as an essential interface between users and Large Language Models (LLMs) like GPT-3.5 and GPT-4, which are employed by ChatGPT to produce outputs across various tasks. However, prompts crafted with malicious intent, known as jailbreak prompts, can circumvent the restrictions of LLMs, posing a significant threat to systems integrated with these models. Despite their critical importance, there is a lack of systematic analysis and comprehensive understanding of jailbreak prompts. Our paper aims to address this gap by exploring key research questions to enhance the robustness of LLM systems: 1) What common patterns are present in jailbreak prompts? 2) How effectively can these prompts bypass the restrictions of LLMs? 3) With the evolution of LLMs, how does the effectiveness of jailbreak prompts change? To address our research questions, we embarked on an empirical study targeting the LLMs underpinning ChatGPT, one of today's most advanced chatbots. Our methodology involved categorizing 78 jailbreak prompts into 10 distinct patterns, further organized into three jailbreak strategy types, and examining their distribution. We assessed the effectiveness of these prompts on GPT-3.5 and GPT-4, using a set of 3,120 questions across 8 scenarios deemed prohibited by OpenAI. Additionally, our study tracked the performance of these prompts over a 3-month period, observing the evolutionary response of ChatGPT to such inputs. Our findings offer a comprehensive view of jailbreak prompts, elucidating their taxonomy, effectiveness, and temporal dynamics. Notably, we discovered that GPT-3.5 and GPT-4 could still generate inappropriate content in response to malicious prompts without the need for jailbreaking. This underscores the critical need for effective prompt management within LLM systems and provides valuable insights and data to spur further research in LLM testing and jailbreak prevention.",
      "year": 2024,
      "venue": "SEA4DQ@SIGSOFT FSE",
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Zhengzi Xu",
        "Yuekang Li",
        "Yaowen Zheng",
        "Ying Zhang",
        "Lida Zhao",
        "Tianwei Zhang",
        "Kailong Wang"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/da9e8b168817b4101fc166a11b2db74d7cafcbd9",
      "pdf_url": "",
      "publication_date": "2024-07-15",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "165921d2aa4f1d110d25d488ea8b205d134b16e6",
      "title": "Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems",
      "abstract": "As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not publicly share all communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Donghyun Lee",
        "Mo Tiwari"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/165921d2aa4f1d110d25d488ea8b205d134b16e6",
      "pdf_url": "",
      "publication_date": "2024-10-09",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "320b86d8a23da014d018db23a5f43b94368dd749",
      "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
      "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query, ostensibly because it lacks relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. Our method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not employ an auxiliary LLM. We evaluate jamming attacks on several embeddings and LLMs and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Avital Shafran",
        "R. Schuster",
        "Vitaly Shmatikov"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/320b86d8a23da014d018db23a5f43b94368dd749",
      "pdf_url": "",
      "publication_date": "2024-06-09",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2f6de8291c9a803faa7f7a33c74f4a2a3debd83b",
      "title": "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks",
      "abstract": "We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them. Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches. Code available at https://github.com/pasquini-dario/LLM_NeuralExec",
      "year": 2024,
      "venue": "AISec@CCS",
      "authors": [
        "Dario Pasquini",
        "Martin Strohmeier",
        "Carmela Troncoso"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/2f6de8291c9a803faa7f7a33c74f4a2a3debd83b",
      "pdf_url": "https://arxiv.org/pdf/2403.03792",
      "publication_date": "2024-03-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e5e3bf694dca35269b40247cc52fcc54f475cf96",
      "title": "CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models",
      "abstract": "Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present BenchmarkName, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show that conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 26% and 41% successful prompt injection tests. We further introduce the safety-utility tradeoff: conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with\"borderline\"benign requests while still rejecting most unsafe requests. Finally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "M. Bhatt",
        "Sa-hana Chennabasappa",
        "Yue Li",
        "Cyrus Nikolaidis",
        "Daniel Song",
        "Shengye Wan",
        "Faizan Ahmad",
        "Cornelius Aschermann",
        "Yaohui Chen",
        "Dhaval Kapil",
        "David Molnar",
        "Spencer Whitman",
        "Joshua Saxe"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/e5e3bf694dca35269b40247cc52fcc54f475cf96",
      "pdf_url": "",
      "publication_date": "2024-04-19",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d814347e382fc3fcc071876744f139d2c101be7",
      "title": "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities",
      "abstract": "The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications, such as collaborative problem-solving and autonomous negotiation. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we investigate this critical issue by constructing a detailed threat model and a comprehensive simulation environment that mirrors real-world multi-agent deployments in a trusted platform. Subsequently, we propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to systematically explore the potential for manipulated knowledge (i.e., counterfactual and toxic knowledge) spread without explicit prompt manipulation. Our method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. Our findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the imperative need for robust defenses against manipulated knowledge spread, such as introducing ``guardian'' agents and advanced fact-checking tools.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Tianjie Ju",
        "Yiting Wang",
        "Xinbei Ma",
        "Pengzhou Cheng",
        "Haodong Zhao",
        "Yulong Wang",
        "Lifeng Liu",
        "Jian Xie",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/3d814347e382fc3fcc071876744f139d2c101be7",
      "pdf_url": "",
      "publication_date": "2024-07-10",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "18e10a05b80794e14a046833eae526eca718bfa7",
      "title": "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
      "abstract": "Text-to-image (T2I) models can be maliciously used to generate harmful content such as sexually explicit, unfaithful, and misleading or Not-Safe-for-Work (NSFW) images. Previous attacks largely depend on the availability of the diffusion model or involve a lengthy optimization process. In this work, we investigate a more practical and universal attack that does not require the presence of a target model and demonstrate that the high-dimensional text embedding space inherently contains NSFW concepts that can be exploited to generate harmful images. We present the Jailbreaking Prompt Attack (JPA). JPA first searches for the target malicious concepts in the text embedding space using a group of antonyms generated by ChatGPT. Subsequently, a prefix prompt is optimized in the discrete vocabulary space to align malicious concepts semantically in the text embedding space. We further introduce a soft assignment with gradient masking technique that allows us to perform gradient ascent in the discrete vocabulary space. We perform extensive experiments with open-sourced T2I models, e.g. stable-diffusion-v1-4 and closed-sourced online services, e.g. DALLE2, Midjourney with black-box safety checkers. Results show that (1) JPA bypasses both text and image safety checkers (2) while preserving high semantic alignment with the target prompt. (3) JPA demonstrates a much faster speed than previous methods and can be executed in a fully automated manner. These merits render it a valuable tool for robustness evaluation in future text-to-image generation research.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Jiachen Ma",
        "Anda Cao",
        "Zhiqing Xiao",
        "Jie Zhang",
        "Chaonan Ye",
        "Junbo Zhao"
      ],
      "citation_count": 57,
      "url": "https://www.semanticscholar.org/paper/18e10a05b80794e14a046833eae526eca718bfa7",
      "pdf_url": "",
      "publication_date": "2024-04-02",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e1d812a1ef4b02b7d60020cc8aaa596d28373da",
      "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
      "abstract": "Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is here.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Sizhe Chen",
        "Arman Zharmagambetov",
        "Saeed Mahloujifar",
        "Kamalika Chaudhuri",
        "Chuan Guo"
      ],
      "citation_count": 55,
      "url": "https://www.semanticscholar.org/paper/3e1d812a1ef4b02b7d60020cc8aaa596d28373da",
      "pdf_url": "",
      "publication_date": "2024-10-07",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2742c3d77c4aa6d023b7dfc77984ad10e6aae274",
      "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications",
      "abstract": "The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate. This paper introduces the 'Signed-Prompt' method as a novel solution. The study involves signing sensitive instructions within command segments by authorized users, enabling the LLM to discern trusted instruction sources. The paper presents a comprehensive analysis of prompt injection attack patterns, followed by a detailed explanation of the Signed-Prompt concept, including its basic architecture and implementation through both prompt engineering and fine-tuning of LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method, showing substantial resistance to various types of prompt injection attacks, thus validating its potential as a robust defense strategy in AI security.",
      "year": 2024,
      "venue": "AIP Conference Proceedings",
      "authors": [
        "Xuchen Suo"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/2742c3d77c4aa6d023b7dfc77984ad10e6aae274",
      "pdf_url": "",
      "publication_date": "2024-01-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prevent prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2b289e1d136edeb6354496518af5230bddc09e61",
      "title": "Attention Tracker: Detecting Prompt Injection Attacks in LLMs",
      "abstract": "Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose Attention Tracker, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Kuo-Han Hung",
        "Ching-Yun Ko",
        "Ambrish Rawat",
        "I-Hsin Chung",
        "Winston H. Hsu",
        "Pin-Yu Chen"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/2b289e1d136edeb6354496518af5230bddc09e61",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a72955288e6468b8342eff6f8f0281ceaefc526a",
      "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
      "abstract": "How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Haozhe Chen",
        "Carl Vondrick",
        "Chengzhi Mao"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/a72955288e6468b8342eff6f8f0281ceaefc526a",
      "pdf_url": "",
      "publication_date": "2024-03-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1f1aefa3ae39eb0ef7285a2a075acfbf06b66e03",
      "title": "The Ethics of Interaction: Mitigating Security Threats in LLMs",
      "abstract": "This paper comprehensively explores the ethical challenges arising from security threats to Large Language Models (LLMs). These intricate digital repositories are increasingly integrated into our daily lives, making them prime targets for attacks that can compromise their training data and the confidentiality of their data sources. The paper delves into the nuanced ethical repercussions of such security threats on society and individual privacy. We scrutinize five major threats--prompt injection, jailbreaking, Personal Identifiable Information (PII) exposure, sexually explicit content, and hate-based content--going beyond mere identification to assess their critical ethical consequences and the urgency they create for robust defensive strategies. The escalating reliance on LLMs underscores the crucial need for ensuring these systems operate within the bounds of ethical norms, particularly as their misuse can lead to significant societal and individual harm. We propose conceptualizing and developing an evaluative tool tailored for LLMs, which would serve a dual purpose: guiding developers and designers in preemptive fortification of backend systems and scrutinizing the ethical dimensions of LLM chatbot responses during the testing phase. By comparing LLM responses with those expected from humans in a moral context, we aim to discern the degree to which AI behaviors align with the ethical values held by a broader society. Ultimately, this paper not only underscores the ethical troubles presented by LLMs; it also highlights a path toward cultivating trust in these systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Ashutosh Kumar",
        "Sagarika Singh",
        "Shiv Vignesh Murthy",
        "Swathy Ragupathy"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/1f1aefa3ae39eb0ef7285a2a075acfbf06b66e03",
      "pdf_url": "",
      "publication_date": "2024-01-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8b1378a728ac223309e5e4c6d2006654b2d469bf",
      "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
      "abstract": "Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insights into the current landscape of LLM vulnerabilities and defense mechanisms. Our objective is to offer a nuanced understanding of LLM attacks, foster awareness within the AI community, and inspire robust solutions to mitigate these risks in future developments.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Arijit Ghosh Chowdhury",
        "Md. Mofijul Islam",
        "Vaibhav Kumar",
        "F. H. Shezan",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/8b1378a728ac223309e5e4c6d2006654b2d469bf",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "title": "Prompt Stealing Attacks Against Large Language Models",
      "abstract": "The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zeyang Sha",
        "Yang Zhang"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/3d90d1d429fa9dacb50a103cdab5c16328665c2c",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fbceb7ffd77ca45eaba297f3f421f65df4feb5cf",
      "title": "Query-Based Adversarial Prompt Generation",
      "abstract": "Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jonathan Hayase",
        "Ema Borevkovic",
        "Nicholas Carlini",
        "Florian Tram\u00e8r",
        "Milad Nasr"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/fbceb7ffd77ca45eaba297f3f421f65df4feb5cf",
      "pdf_url": "",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "970111f19cbb20af52a8a1772e8d4b36570b5679",
      "title": "SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models",
      "abstract": "Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block sexually explicit content (e.g., naked) but may still be vulnerable to adversarial prompts-inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate sexual content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate explicit visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since such unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets and large-scale user studies demonstrate SafeGen's effectiveness in mitigating sexually explicit content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.4% sexual content removal performance.",
      "year": 2024,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinfeng Li",
        "Yuchen Yang",
        "Jiangyi Deng",
        "Chen Yan",
        "Yanjiao Chen",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/970111f19cbb20af52a8a1772e8d4b36570b5679",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670295",
      "publication_date": "2024-04-10",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7b00cb1fe1773f964d123dab6d4812c7bc63de06",
      "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems",
      "abstract": "Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. We also study multiple effects of RAG setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern LMs, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Zhenting Qi",
        "Hanlin Zhang",
        "Eric P. Xing",
        "S. Kakade",
        "Hima Lakkaraju"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/7b00cb1fe1773f964d123dab6d4812c7bc63de06",
      "pdf_url": "",
      "publication_date": "2024-02-27",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "038ed1e52bdf92a4db0c91d31d1db28b2c5051fb",
      "title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges",
      "abstract": "Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yanshen Sun",
        "Jianfeng He",
        "Limeng Cui",
        "Shuo Lei",
        "Chang-Tien Lu"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/038ed1e52bdf92a4db0c91d31d1db28b2c5051fb",
      "pdf_url": "",
      "publication_date": "2024-03-27",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
      "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
      "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. Our framework is available at https://github.com/cure-lab/GuardT2I.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yijun Yang",
        "Ruiyuan Gao",
        "Xiao Yang",
        "Jianyuan Zhong",
        "Qiang Xu"
      ],
      "citation_count": 43,
      "url": "https://www.semanticscholar.org/paper/9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
      "pdf_url": "",
      "publication_date": "2024-03-03",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7c834a7ba43a2b5067817426939c362eb06fdb2a",
      "title": "Imprompter: Tricking LLM Agents into Improper Tool Use",
      "abstract": "Large Language Model (LLM) Agents are an emerging computing paradigm that blends generative machine learning with tools such as code interpreters, web browsing, email, and more generally, external resources. These agent-based systems represent an emerging shift in personal computing. We contribute to the security foundations of agent-based systems and surface a new class of automatically computed obfuscated adversarial prompt attacks that violate the confidentiality and integrity of user resources connected to an LLM agent. We show how prompt optimization techniques can find such prompts automatically given the weights of a model. We demonstrate that such attacks transfer to production-level agents. For example, we show an information exfiltration attack on Mistral's LeChat agent that analyzes a user's conversation, picks out personally identifiable information, and formats it into a valid markdown command that results in leaking that data to the attacker's server. This attack shows a nearly 80% success rate in an end-to-end evaluation. We conduct a range of experiments to characterize the efficacy of these attacks and find that they reliably work on emerging agent-based systems like Mistral's LeChat, ChatGLM, and Meta's Llama. These attacks are multimodal, and we show variants in the text-only and image domains.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xiaohan Fu",
        "Shuheng Li",
        "Zihan Wang",
        "Yihao Liu",
        "Rajesh K. Gupta",
        "Taylor Berg-Kirkpatrick",
        "Earlence Fernandes"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/7c834a7ba43a2b5067817426939c362eb06fdb2a",
      "pdf_url": "",
      "publication_date": "2024-10-19",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e6ad241ce87fe608241e9004f4c9d199b6bfebc7",
      "title": "Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications",
      "abstract": "In this paper, we show that when the communication between GenAI-powered applications relies on RAG-based inference, an attacker can initiate a computer worm-like chain reaction that we call Morris-II. This is done by crafting an adversarial self-replicating prompt that triggers a cascade of indirect prompt injections within the ecosystem and forces each affected application to perform malicious actions and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential user data extraction within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embedding algorithm employed, and the number of hops in the propagation. Finally, we introduce the Virtual Donkey, a guardrail intended to detect and prevent the propagation of Morris-II with minimal latency, high accuracy, and a low false-positive rate. We evaluate the guardrail's performance and show that it yields a perfect true-positive rate of 1.0 with a false-positive rate of 0.015, and is robust against out-of-distribution worms, consisting of unseen jailbreaking commands, a different email dataset, and various worm usecases.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Stav Cohen",
        "Ron Bitton",
        "Ben Nassi"
      ],
      "citation_count": 41,
      "url": "https://www.semanticscholar.org/paper/e6ad241ce87fe608241e9004f4c9d199b6bfebc7",
      "pdf_url": "",
      "publication_date": "2024-03-05",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
      "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models",
      "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness ($\\epsilon=4/255$) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Lin Li",
        "Haoyan Guan",
        "Jianing Qiu",
        "Michael W. Spratling"
      ],
      "citation_count": 40,
      "url": "https://www.semanticscholar.org/paper/3a391dfd536625e068f3888c817cc6cbe7fcea9c",
      "pdf_url": "http://arxiv.org/pdf/2403.01849",
      "publication_date": "2024-03-04",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "19a45c70463edc755d54f3d5343e44cd571fe226",
      "title": "System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective",
      "abstract": "Large Language Model-based systems (LLM systems) are information and query processing systems that use LLMs to plan operations from natural-language prompts and feed the output of each successive step into the LLM to plan the next. This structure results in powerful tools that can process complex information from diverse sources but raises critical security concerns. Malicious information from any source may be processed by the LLM and can compromise the query processing, resulting in nearly arbitrary misbehavior. To tackle this problem, we present a system-level defense based on the principles of information flow control that we call an f-secure LLM system. An f-secure LLM system disaggregates the components of an LLM system into a context-aware pipeline with dynamically generated structured executable plans, and a security monitor filters out untrusted input into the planning process. This structure prevents compromise while maximizing flexibility. We provide formal models for both existing LLM systems and our f-secure LLM system, allowing analysis of critical security guarantees. We further evaluate case studies and benchmarks showing that f-secure LLM systems provide robust security while preserving functionality and efficiency. Our code is released at https://github.com/fzwark/Secure_LLM_System.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fangzhou Wu",
        "Ethan Cecchetti",
        "Chaowei Xiao"
      ],
      "citation_count": 38,
      "url": "https://www.semanticscholar.org/paper/19a45c70463edc755d54f3d5343e44cd571fe226",
      "pdf_url": "",
      "publication_date": "2024-09-27",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9cca0ff4ddd8ea65650da5968c800eafec2da5bb",
      "title": "Review of Generative AI Methods in Cybersecurity",
      "abstract": "Over the last decade, Artificial Intelligence (AI) has become increasingly popular, especially with the use of chatbots such as ChatGPT, Gemini, and DALL-E. With this rise, large language models (LLMs) and Generative AI (GenAI) have also become more prevalent in everyday use. These advancements strengthen cybersecurity's defensive posture and open up new attack avenues for adversaries as well. This paper provides a comprehensive overview of the current state-of-the-art deployments of GenAI, covering assaults, jailbreaking, and applications of prompt injection and reverse psychology. This paper also provides the various applications of GenAI in cybercrimes, such as automated hacking, phishing emails, social engineering, reverse cryptography, creating attack payloads, and creating malware. GenAI can significantly improve the automation of defensive cyber security processes through strategies such as dataset construction, safe code development, threat intelligence, defensive measures, reporting, and cyberattack detection. In this study, we suggest that future research should focus on developing robust ethical norms and innovative defense mechanisms to address the current issues that GenAI creates and to also further encourage an impartial approach to its future application in cybersecurity. Moreover, we underscore the importance of interdisciplinary approaches further to bridge the gap between scientific developments and ethical considerations.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yagmur Yigit",
        "William J. Buchanan",
        "Madjid G Tehrani",
        "Leandros A. Maglaras"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/9cca0ff4ddd8ea65650da5968c800eafec2da5bb",
      "pdf_url": "",
      "publication_date": "2024-03-13",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3b3082520e59974f9f43c852ee37a7003911e953",
      "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
      "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its security risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover fewer aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of text-to-video models. We define 12 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts and jailbreak attack-based prompts. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AI.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yibo Miao",
        "Yifan Zhu",
        "Yinpeng Dong",
        "Lijia Yu",
        "Jun Zhu",
        "Xiao-Shan Gao"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/3b3082520e59974f9f43c852ee37a7003911e953",
      "pdf_url": "",
      "publication_date": "2024-07-08",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "78fa00a8cdd113e63ed18285fcd1bb6e8b042785",
      "title": "An Early Categorization of Prompt Injection Attacks on Large Language Models",
      "abstract": "Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Sippo Rossi",
        "Alisia Marianne Michel",
        "R. Mukkamala",
        "J. Thatcher"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/78fa00a8cdd113e63ed18285fcd1bb6e8b042785",
      "pdf_url": "",
      "publication_date": "2024-01-31",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "31babc068383c3966d322579eaaf79edc6a6bef4",
      "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities",
      "abstract": "This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.",
      "year": 2024,
      "venue": "",
      "authors": [
        "M. Ferrag",
        "Fatima Alwahedi",
        "A. Battah",
        "Bilel Cherif",
        "Abdechakour Mechri",
        "Norbert Tihanyi",
        "Tam\u00e1s Bisztray",
        "M. Debbah"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/31babc068383c3966d322579eaaf79edc6a6bef4",
      "pdf_url": "",
      "publication_date": "2024-05-21",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b0f8b91c2ca91803324069bc237b15314f70e6ca",
      "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
      "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research issue. Previous red teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our knowledge, we are the first to study LLM safety in multi-turn dialogue coreference. We created a dataset of 1,400 questions across 14 categories, each featuring multi-turn coreference safety attacks. We then conducted detailed evaluations on five widely used open-source LLMs. The results indicated that under multi-turn coreference safety attacks, the highest attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was 13.9% with the Mistral-7B-Instruct model. These findings highlight the safety vulnerabilities in LLMs during dialogue coreference interactions.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Erxin Yu",
        "Jing Li",
        "Ming Liao",
        "Siqi Wang",
        "Zuchen Gao",
        "Fei Mi",
        "Lanqing Hong"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/b0f8b91c2ca91803324069bc237b15314f70e6ca",
      "pdf_url": "",
      "publication_date": "2024-06-25",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call\"neural phishing\". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/ee529552e871dbd7d2c53d9cde4b46380712cbe0",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3bb87d605856411c6f002d480fc29d355c3ba245",
      "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
      "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Haochen Luo",
        "Jindong Gu",
        "Fengyuan Liu",
        "Philip H. S. Torr"
      ],
      "citation_count": 34,
      "url": "https://www.semanticscholar.org/paper/3bb87d605856411c6f002d480fc29d355c3ba245",
      "pdf_url": "",
      "publication_date": "2024-03-14",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "09f78ccd20f865c288b421376baddb0096c651be",
      "title": "SPML: A DSL for Defending Language Models Against Prompt Attacks",
      "abstract": "Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language design challenges. Additionally, we introduce a groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering the inaugural language and benchmark for chatbot definition evaluation. Experiments across datasets demonstrate SPML's proficiency in understanding attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data and codes are publicly available at: https://prompt-compiler.github.io/SPML/.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Reshabh K Sharma",
        "Vinayak Gupta",
        "Dan Grossman"
      ],
      "citation_count": 30,
      "url": "https://www.semanticscholar.org/paper/09f78ccd20f865c288b421376baddb0096c651be",
      "pdf_url": "",
      "publication_date": "2024-02-19",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "687acbf39c6b36b0ad776647d5864f329c2989dd",
      "title": "Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy",
      "abstract": "Large Language Models (LLMs) are susceptible to security and safety threats, such as prompt injection, prompt extraction, and harmful requests. One major cause of these vulnerabilities is the lack of an instruction hierarchy. Modern LLM architectures treat all inputs equally, failing to distinguish between and prioritize various types of instructions, such as system messages, user prompts, and data. As a result, lower-priority user prompts may override more critical system instructions, including safety protocols. Existing approaches to achieving instruction hierarchy, such as delimiters and instruction-based training, do not address this issue at the architectural level. We introduce the Instructional Segment Embedding (ISE) technique, inspired by BERT, to modern large language models, which embeds instruction priority information directly into the model. This approach enables models to explicitly differentiate and prioritize various instruction types, significantly improving safety against malicious prompts that attempt to override priority rules. Our experiments on the Structured Query and Instruction Hierarchy benchmarks demonstrate an average robust accuracy increase of up to 15.75% and 18.68%, respectively. Furthermore, we observe an improvement in instruction-following capability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers a promising direction for enhancing the safety and effectiveness of LLM architectures.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Tong Wu",
        "Shujian Zhang",
        "Kaiqiang Song",
        "Silei Xu",
        "Sanqiang Zhao",
        "Ravi Agrawal",
        "Sathish Indurthi",
        "Chong Xiang",
        "Prateek Mittal",
        "Wenxuan Zhou"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/687acbf39c6b36b0ad776647d5864f329c2989dd",
      "pdf_url": "",
      "publication_date": "2024-10-09",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b0bd96c49091253c1a893d8f26fe791291a448c8",
      "title": "Get My Drift? Catching LLM Task Drift with Activation Deltas",
      "abstract": "Large Language Models (LLMs) are commonly used in retrieval-augmented applications to execute user instructions based on data from external sources. For example, modern search engines use LLMs to answer queries based on relevant search results; email plugins summarize emails by processing their content through an LLM. However, the potentially untrusted provenance of these data sources can lead to prompt injection attacks, where the LLM is manipulated by natural language instructions embedded in the external data, causing it to deviate from the user's original instruction(s). We define this deviation as task drift. Task drift is a significant concern as it allows attackers to exfiltrate data or influence the LLM's output for other users. We study LLM activations as a solution to detect task drift, showing that activation deltas - the difference in activations before and after processing external data - are strongly correlated with this phenomenon. Through two probing methods, we demonstrate that a simple linear classifier can detect drift with near-perfect ROC AUC on an out-of-distribution test set. We evaluate these methods by making minimal assumptions about how users' tasks, system prompts, and attacks can be phrased. We observe that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Interestingly, the fact that this solution does not require any modifications to the LLM (e.g., fine-tuning), as well as its compatibility with existing meta-prompting solutions, makes it cost-efficient and easy to deploy. To encourage further research on activation-based task inspection, decoding, and interpretability, we release our large-scale TaskTracker toolkit, featuring a dataset of over 500K instances, representations from six SoTA language models, and a suite of inspection tools.",
      "year": 2024,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Sahar Abdelnabi",
        "Aideen Fay",
        "Giovanni Cherubin",
        "Ahmed Salem",
        "Mario Fritz",
        "Andrew Paverd"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/b0bd96c49091253c1a893d8f26fe791291a448c8",
      "pdf_url": "",
      "publication_date": "2024-06-02",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c5aabd927143a00e13ca4e003f1cd98996e07d42",
      "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
      "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shachi H. Kumar",
        "Saurav Sahay",
        "Sahisnu Mazumder",
        "Eda Okur",
        "R. Manuvinakurike",
        "Nicole Beckage",
        "Hsuan Su",
        "Hung-yi Lee",
        "L. Nachman"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/c5aabd927143a00e13ca4e003f1cd98996e07d42",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "57eec0efd84f95943c744bacb1315746b1287f16",
      "title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models",
      "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1% training data. Code is available at: https://github.com/lionel-w2/FAP.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yiwei Zhou",
        "Xiaobo Xia",
        "Zhiwei Lin",
        "Bo Han",
        "Tongliang Liu"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/57eec0efd84f95943c744bacb1315746b1287f16",
      "pdf_url": "",
      "publication_date": "2024-03-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ac0f06b86cc210818dee9f7b1a20a003c60401c",
      "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
      "abstract": "Red teaming assesses how large language models (LLMs) can produce content that violates norms, policies, and rules set during their safety training. However, most existing automated methods in the literature are not representative of the way humans tend to interact with AI models. Common users of AI models may not have advanced knowledge of adversarial machine learning methods or access to model internals, and they do not spend a lot of time crafting a single highly effective adversarial prompt. Instead, they are likely to make use of techniques commonly shared online and exploit the multiturn conversational nature of LLMs. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general-purpose model in a way that encourages reasoning through the choices of methods available, the current target model's response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama 3.1 and 88% against GPT-4 on the JailbreakBench dataset.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Maya Pavlova",
        "Erik Brinkman",
        "Krithika Iyer",
        "V\u00edtor Albiero",
        "Joanna Bitton",
        "Hailey Nguyen",
        "Joe Li",
        "Cristian Canton-Ferrer",
        "Ivan Evtimov",
        "Aaron Grattafiori"
      ],
      "citation_count": 29,
      "url": "https://www.semanticscholar.org/paper/7ac0f06b86cc210818dee9f7b1a20a003c60401c",
      "pdf_url": "",
      "publication_date": "2024-10-02",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "521120e989fb89d7cc0fec1a543701752feec31f",
      "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
      "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Benji Peng",
        "Keyu Chen",
        "Ming Li",
        "Pohsun Feng",
        "Ziqian Bi",
        "Junyu Liu",
        "Qian Niu"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/521120e989fb89d7cc0fec1a543701752feec31f",
      "pdf_url": "",
      "publication_date": "2024-09-12",
      "keywords_matched": [
        "prompt injection",
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9177fe8ad9ff7754c2fd27e805fe6ed882fe2954",
      "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling",
      "abstract": "Safety of Large Language Models (LLMs) has become a critical issue given their rapid progresses. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing adversarial prompts to break the aligned LLMs, but optimization of GCG is time-consuming. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\\texttt{Probe sampling}$. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial methods, leading to acceleration of $1.8\\times$ for AutoPrompt, $2.4\\times$ for APE and $2.4\\times$ for AutoDAN.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yiran Zhao",
        "Wenyue Zheng",
        "Tianle Cai",
        "Do Xuan Long",
        "Kenji Kawaguchi",
        "Anirudh Goyal",
        "Michael Shieh"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/9177fe8ad9ff7754c2fd27e805fe6ed882fe2954",
      "pdf_url": "",
      "publication_date": "2024-03-02",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4d1f37057fd008317d634d3f10ff73c81b442035",
      "title": "Goal-Guided Generative Prompt Injection Attack on Large Language Models",
      "abstract": "Current large language models (LLMs) provide a strong foundation for large-scale user-oriented natural language tasks. Numerous users can easily inject adversarial text or instructions through the user interface, thus causing LLM model security challenges. Although there is much research on prompt injection attacks, most black-box attacks use heuristic strategies. It is unclear how these heuristic strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we redefine the goal of the attack: to maximize the KL divergence between the conditional probabilities of the clean text and the adversarial text. Furthermore, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between the embedded representation $x$ and $x^{\\prime}$ of the clean text and the adversarial text when the conditional probability is a Gaussian distribution and gives a quantitative relationship on $x$ and $x^{\\prime}$. Then we designed a simple and effective goal-guided generative prompt injection strategy (G2PIA) to find an injection text that satisfies specific constraints to achieve the optimal attack effect approximately. Notably, our attack method is a query-free black-box attack method with a low computational cost. Experimental results on seven LLM models and four datasets show the effectiveness of our attack method.",
      "year": 2024,
      "venue": "Industrial Conference on Data Mining",
      "authors": [
        "Chong Zhang",
        "Mingyu Jin",
        "Qinkai Yu",
        "Chengzhi Liu",
        "Haochen Xue",
        "Xiaobo Jin"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/4d1f37057fd008317d634d3f10ff73c81b442035",
      "pdf_url": "http://arxiv.org/pdf/2404.07234",
      "publication_date": "2024-04-06",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1c9cfc4c659b5daae67d60a55030cf48cd572ce0",
      "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models",
      "abstract": "Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Benji Peng",
        "Ziqian Bi",
        "Qian Niu",
        "Ming Liu",
        "Pohsun Feng",
        "Tianyang Wang",
        "Lawrence K.Q. Yan",
        "Yizhu Wen",
        "Yichao Zhang",
        "Caitlyn Heqi Yin"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/1c9cfc4c659b5daae67d60a55030cf48cd572ce0",
      "pdf_url": "",
      "publication_date": "2024-10-20",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt",
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7275015644e09c8e722dc45fbed7bc87ffded393",
      "title": "Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey",
      "abstract": "With the advent of Large Vision-Language Models (LVLMs), new attack vectors, such as cognitive bias, prompt injection, and jailbreaking, have emerged. Understanding these attacks promotes system robustness improvement and neural networks demystification. However, existing surveys often target attack taxonomy and lack in-depth analysis like (1) unified insights into adversariality, transferability, and generalization; (2) detailed evaluations framework; (3) motivation-driven attack categorizations; and (4) an integrated perspective on both traditional and LVLM attacks. This article addresses these gaps by offering a thorough summary of traditional and LVLM adversarial attacks, emphasizing their connections and distinctions, and providing actionable insights for future research.",
      "year": 2024,
      "venue": "ACM Computing Surveys",
      "authors": [
        "Chiyu Zhang",
        "Lu Zhou",
        "Xiaogang Xu",
        "Jiafei Wu",
        "Zhe Liu"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/7275015644e09c8e722dc45fbed7bc87ffded393",
      "pdf_url": "",
      "publication_date": "2024-10-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9223a64feb573e62498c2ca914ed97557c580167",
      "title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs",
      "abstract": "Although safely enhanced Large Language Models (LLMs) have achieved remarkable success in tackling various complex tasks in a zero-shot manner, they remain susceptible to jailbreak attacks, particularly the unknown jailbreak attack. To enhance LLMs' generalized defense capabilities, we propose a two-stage adversarial tuning framework, which generates adversarial prompts to explore worst-case scenarios by optimizing datasets containing pairs of adversarial prompts and their safe responses. In the first stage, we introduce the hierarchical meta-universal adversarial prompt learning to efficiently and effectively generate token-level adversarial prompts. In the second stage, we propose the automatic adversarial prompt learning to iteratively refine semantic-level adversarial prompts, further enhancing LLM's defense capabilities. We conducted comprehensive experiments on three widely used jailbreak datasets, comparing our framework with six defense baselines under five representative attack scenarios. The results underscore the superiority of our proposed methods. Furthermore, our adversarial tuning framework exhibits empirical generalizability across various attack strategies and target LLMs, highlighting its potential as a transferable defense mechanism.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fan Liu",
        "Zhao Xu",
        "Hao Liu"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/9223a64feb573e62498c2ca914ed97557c580167",
      "pdf_url": "",
      "publication_date": "2024-06-07",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c994fa2be3beb2d81ea2f79309cca6a2f5aefa60",
      "title": "Tell Me Your Prompts and I Will Make Them True: The Alchemy of Prompt Engineering and Generative AI",
      "abstract": "This paper explores the emerging field of prompt engineering within generative AI, emphasizing its role as a critical intersection between art and science. Prompt engineering is identified as the key to unlocking the full potential of generative AI technologies by optimizing human-AI communication. Through a comprehensive analysis of the related literature, this study illustrates how prompt engineering transcends mere technical manipulation, requiring a blend of creativity, strategic thinking, and a deep understanding of generative AI capabilities. This paper provides various strategies for crafting effective prompts, from simple to sophisticated techniques, highlighting the importance of ethical considerations and the potential risks associated with prompt manipulation. By establishing a set of principles and guidelines, this paper aims to advance prompt engineering as a discipline essential for enhancing AI\u2019s functionality and reliability and, with this justification, introduces the \u2018Prompt Engineering for Gen[i]e rative AI Framework\u2019. After all, this paper calls for a multidisciplinary approach to prompt engineering, advocating for its recognition and development as a pivotal component of AI literacy and application. Through this exploration, this paper intends to contribute to the evolving dialogue on the integration of human creativity with generative AI capabilities, offering insights into the future of effective and ethical AI interaction.",
      "year": 2024,
      "venue": "Open Praxis",
      "authors": [
        "Aras Bozkurt"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/c994fa2be3beb2d81ea2f79309cca6a2f5aefa60",
      "pdf_url": "https://storage.googleapis.com/jnl-up-j-op-files/journals/1/articles/661/660d5a9fa8517.pdf",
      "publication_date": null,
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9cfaa1cad9f1601bdd75d1de89dd56e18f24ff58",
      "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
      "abstract": "With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker's instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs'instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yulin Chen",
        "Haoran Li",
        "Zihao Zheng",
        "Yangqiu Song",
        "Dekai Wu",
        "Bryan Hooi"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/9cfaa1cad9f1601bdd75d1de89dd56e18f24ff58",
      "pdf_url": "",
      "publication_date": "2024-11-01",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9ee3c8c68d4aa28d179fef38b29643be6954ea41",
      "title": "AdvAgent: Controllable Blackbox Red-teaming on Web Agents",
      "abstract": "Foundation model-based agents are increasingly used to automate complex tasks, enhancing efficiency and productivity. However, their access to sensitive resources and autonomous decision-making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black-box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pipeline to train an adversarial prompter model that optimizes adversarial prompts using feedback from the black-box agent. With careful attack design, these prompts effectively exploit agent weaknesses while maintaining stealthiness and controllability. Extensive evaluations demonstrate that AdvAgent achieves high success rates against state-of-the-art GPT-4-based web agents across diverse web tasks. Furthermore, we find that existing prompt-based defenses provide only limited protection, leaving agents vulnerable to our framework. These findings highlight critical vulnerabilities in current web agents and emphasize the urgent need for stronger defense mechanisms. We release code at https://ai-secure.github.io/AdvAgent/.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Chejian Xu",
        "Mintong Kang",
        "Jiawei Zhang",
        "Zeyi Liao",
        "Lingbo Mo",
        "Mengqi Yuan",
        "Huan Sun",
        "Bo Li"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/9ee3c8c68d4aa28d179fef38b29643be6954ea41",
      "pdf_url": "",
      "publication_date": "2024-10-22",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "685dd83fb88fa7d5546f2624ac5b9fe8a484de13",
      "title": "PromptShield: Deployable Detection for Prompt Injection Attacks",
      "abstract": "Application designers have moved to integrate large language models (LLMs) into their products. However, many LLM-integrated applications are vulnerable to prompt injections. While attempts have been made to address this problem by building prompt injection detectors, many are not yet suitable for practical deployment. To support research in this area, we introduce PromptShield, a benchmark for training and evaluating deployable prompt injection detectors. Our benchmark is carefully curated and includes both conversational and application-structured data. In addition, we use insights from our curation process to fine-tune a new prompt injection detector that achieves significantly higher performance in the low false positive rate (FPR) evaluation regime compared to prior schemes. Our work suggests that careful curation of training data and larger models can contribute to strong detector performance.",
      "year": 2024,
      "venue": "Conference on Data and Application Security and Privacy",
      "authors": [
        "Dennis Jacob",
        "Hend Alzahrani",
        "Zhanhao Hu",
        "Basel Alomair",
        "David Wagner"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/685dd83fb88fa7d5546f2624ac5b9fe8a484de13",
      "pdf_url": "",
      "publication_date": "2024-06-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "70069870f883ff3889790b259bbc38d8cfc4f784",
      "title": "GUARDIAN: A Multi-Tiered Defense Architecture for Thwarting Prompt Injection Attacks on LLMs",
      "abstract": null,
      "year": 2024,
      "venue": "Journal of Software Engineering and Applications",
      "authors": [
        "Parijat Rai",
        "Saumil Sood",
        "V. Madisetti",
        "Arshdeep Bahga"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/70069870f883ff3889790b259bbc38d8cfc4f784",
      "pdf_url": "http://www.scirp.org/journal/PaperDownload.aspx?paperID=130663",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ff1a721846c7b157214001fd598df4a33fbbfe3",
      "title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control",
      "abstract": "Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications, challenging agents with managing risks encompassing misuse and negative side effects. These tasks include tests to evaluate the safety of agents in daily scenarios as well as their robustness against indirect prompt injection attacks. Our experiments demonstrate that baseline agents, based on state-of-the-art LLMs, often fail to effectively prevent harm while performing the tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments. We open-source our benchmark at: https://mobilesafetybench.github.io/.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Juyong Lee",
        "Dongyoon Hahm",
        "June Suk Choi",
        "W. B. Knox",
        "Kimin Lee"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/7ff1a721846c7b157214001fd598df4a33fbbfe3",
      "pdf_url": "",
      "publication_date": "2024-10-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cde590385df184651ac0c5d209053b50a5616773",
      "title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models",
      "abstract": "With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks, designing effective jailbreak attacks poses unique challenges, especially given the highly constrained adversarial capabilities in real-world deployment scenarios. Previous works concentrate risks into a single modality, resulting in limited jailbreak performance. In this paper, we propose a heuristic-induced multimodal risk distribution jailbreak attack method, called HIMRD, which is black-box and consists of two elements: multimodal risk distribution strategy and heuristic-induced search strategy. The multimodal risk distribution strategy is used to distribute harmful semantics into multiple modalities to effectively circumvent the single-modality protection mechanisms of MLLMs. The heuristic-induced search strategy identifies two types of prompts: the understanding-enhancing prompt, which helps MLLMs reconstruct the malicious prompt, and the inducing prompt, which increases the likelihood of affirmative outputs over refusals, enabling a successful jailbreak attack. HIMRD achieves an average attack success rate (ASR) of 90% across seven open-source MLLMs and an average ASR of around 68% in three closed-source MLLMs. HIMRD reveals cross-modal security vulnerabilities in current MLLMs and underscores the imperative for developing defensive strategies to mitigate such emerging risks. Code is available at https://github.com/MaTengSYSU/HIMRD-jailbreak.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Ma Teng",
        "Xiaojun Jia",
        "Ranjie Duan",
        "Xinfeng Li",
        "Yihao Huang",
        "Zhixuan Chu",
        "Yang Liu",
        "Wenqi Ren"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/cde590385df184651ac0c5d209053b50a5616773",
      "pdf_url": "",
      "publication_date": "2024-12-08",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "348b3999a8545dd48307caeadbb54b791ed37a7d",
      "title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models",
      "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense -- falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. InjecGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.8%, offering a robust and open-source solution for detecting prompt injection attacks. The code and datasets are released at https://github.com/leolee99/InjecGuard.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Li",
        "Xiaogeng Liu",
        "Chaowei Xiao"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/348b3999a8545dd48307caeadbb54b791ed37a7d",
      "pdf_url": "",
      "publication_date": "2024-10-30",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "676249718345ab1e716d0a789084c085cab576c7",
      "title": "Embedding-based classifiers can detect prompt injection attacks",
      "abstract": "Large Language Models (LLMs) are seeing significant adoption in every type of organization due to their exceptional generative capabilities. However, LLMs are found to be vulnerable to various adversarial attacks, particularly prompt injection attacks, which trick them into producing harmful or inappropriate content. Adversaries execute such attacks by crafting malicious prompts to deceive the LLMs. In this paper, we propose a novel approach based on embedding-based Machine Learning (ML) classifiers to protect LLM-based applications against this severe threat. We leverage three commonly used embedding models to generate embeddings of malicious and benign prompts and utilize ML classifiers to predict whether an input prompt is malicious. Out of several traditional ML methods, we achieve the best performance with classifiers built using Random Forest and XGBoost. Our classifiers outperform state-of-the-art prompt injection classifiers available in open-source implementations, which use encoder-only neural networks.",
      "year": 2024,
      "venue": "CAMLIS",
      "authors": [
        "Md. Ahsan Ayub",
        "Subhabrata Majumdar"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/676249718345ab1e716d0a789084c085cab576c7",
      "pdf_url": "",
      "publication_date": "2024-10-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8af6009a8d55d456213f6819567eb3ace39dd176",
      "title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents",
      "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\\%) while maintaining high task utility (69.79\\%) on GPT-4o.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Feiran Jia",
        "Tong Wu",
        "Xin Qin",
        "Anna Squicciarini"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/8af6009a8d55d456213f6819567eb3ace39dd176",
      "pdf_url": "",
      "publication_date": "2024-12-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d8ba6085c7c32cc367ab7f50d18744de846fcebd",
      "title": "Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning",
      "abstract": "Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Rongwu Xu",
        "Zehan Qi",
        "Wei Xu"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/d8ba6085c7c32cc367ab7f50d18744de846fcebd",
      "pdf_url": "",
      "publication_date": "2024-05-31",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3f0ca570bfd749d73b1bd302ec8d2d55ce959c80",
      "title": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs",
      "abstract": "Large Language Models (LLMs) have gained widespread use in various applications due to their powerful capability to generate human-like text. However, prompt injection attacks, which involve overwriting a model's original instructions with malicious prompts to manipulate the generated text, have raised significant concerns about the security and reliability of LLMs. Ensuring that LLMs are robust against such attacks is crucial for their deployment in real-world applications, particularly in critical tasks. In this paper, we propose PROMPTFUZZ, a novel testing framework that leverages fuzzing techniques to systematically assess the robustness of LLMs against prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ selects promising seed prompts and generates a diverse set of prompt injections to evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the prepare phase, which involves selecting promising initial seeds and collecting few-shot examples, and the focus phase, which uses the collected examples to generate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can uncover more vulnerabilities in LLMs, even those with strong defense prompts. By deploying the generated attack prompts from PROMPTFUZZ in a real-world competition, we achieved the 7th ranking out of over 4000 participants (top 0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs for enhanced robustness against prompt injection attacks. While the fine-tuned model shows improved robustness, PROMPTFUZZ continues to identify vulnerabilities, highlighting the importance of robust testing for LLMs. Our work emphasizes the critical need for effective testing tools and provides a practical framework for evaluating and improving the robustness of LLMs against prompt injection attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiahao Yu",
        "Yangguang Shao",
        "Hanwen Miao",
        "Junzheng Shi",
        "Xinyu Xing"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/3f0ca570bfd749d73b1bd302ec8d2d55ce959c80",
      "pdf_url": "",
      "publication_date": "2024-09-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6424243734d098460777049e1f11705d1b5d85c8",
      "title": "Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In",
      "abstract": "Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become increasingly prevalent. As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack. Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions. Our results show that once a ReAct agents thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a foot-in-the-door, allowing an attacker to embed malicious instructions into the agents thought process, making it more susceptible to harmful directives. To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Itay Nakash",
        "George Kour",
        "Guy Uziel",
        "Ateret Anaby-Tavor"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/6424243734d098460777049e1f11705d1b5d85c8",
      "pdf_url": "",
      "publication_date": "2024-10-22",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1b1e7f751fed944d6025b42d67c5305d43883cc",
      "title": "Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs",
      "abstract": "Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMs\u2019 safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful information without controlling the specific content of their output. Exactly control of the LLM output can produce more inconspicuous attacks which could reveal a new page for LLM security. To achieve this, we propose RLTA: the Reinforcement Learning Targeted Attack, a framework that is designed for attacking language models (LLMs) and is adaptable to both white box (weight accessible) and black box (weight inaccessible) scenarios. It is capable of automatically generating malicious prompts that trigger target LLMs to produce specific outputs. We demonstrate RLTA in two different scenarios: LLM trojan detection and jailbreaking. The comprehensive experimental results show the potential of RLTA in enhancing the security measures surrounding contemporary LLMs.",
      "year": 2024,
      "venue": "PRIVATENLP",
      "authors": [
        "Xiangwen Wang",
        "Jie Peng",
        "Kaidi Xu",
        "Huaxiu Yao",
        "Tianlong Chen"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/f1b1e7f751fed944d6025b42d67c5305d43883cc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "150a1e91b96fdd5b675ce72ee7f4f4b9e301ae04",
      "title": "Strengthening LLM Trust Boundaries: A Survey of Prompt Injection Attacks Surender Suresh Kumar Dr. M.L. Cummings Dr. Alexander Stimpson",
      "abstract": "Both academia and industry have embraced Large Language Models (LLMs) as new tools to extend or create new capabilities in human-artificial intelligence (AI) interactions. This rise of LLMs has also led to an impressive increase in LLM prompt injections, which are human-driven attacks meant to manipulate the LLMs to operate outside the safe boundaries and guardrails originally designed to prevent such behaviors. This effort presents a cohesive framework that organizes various prompt injection attacks with respect to the type of prompt used in attacks, the type of trust boundary the attacks violated, and the level of expertise required to carry out such attacks. Analysis of this framework leads to recommendations for how trust boundaries could be further strengthened through a combination of sociotechnical approaches.",
      "year": 2024,
      "venue": "International Conferences on Human-Machine Systems",
      "authors": [
        "Surender Suresh Kumar",
        "Dr. M.L. Cummings",
        "Dr. Alexander Stimpson"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/150a1e91b96fdd5b675ce72ee7f4f4b9e301ae04",
      "pdf_url": "",
      "publication_date": "2024-05-15",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eded6ad763cc3e97d39c6d26aa76eb89f9141180",
      "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
      "abstract": "Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel fingerprinting problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Martin Gubri",
        "Dennis Ulmer",
        "Hwaran Lee",
        "Sangdoo Yun",
        "Seong Joon Oh"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/eded6ad763cc3e97d39c6d26aa76eb89f9141180",
      "pdf_url": "",
      "publication_date": "2024-02-20",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cb9b9c9e3438e3b451290ab3217e8fffb9ee9e33",
      "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
      "abstract": "Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xuefeng Du",
        "Reshmi Ghosh",
        "Robert Sim",
        "Ahmed Salem",
        "Vitor Carvalho",
        "Emily Lawton",
        "Yixuan Li",
        "Jack W. Stokes"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/cb9b9c9e3438e3b451290ab3217e8fffb9ee9e33",
      "pdf_url": "",
      "publication_date": "2024-10-01",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5201913bb3b941e0d42606969da5c1f927aeb48b",
      "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
      "abstract": "Large language models (LLMs) possess extensive knowledge and question-answering capabilities, having been widely deployed in privacy-sensitive domains like finance and medical consultation. During LLM inferences, cache-sharing methods are commonly employed to enhance efficiency by reusing cached states or responses for the same or similar inference requests. However, we identify that these cache mechanisms pose a risk of private input leakage, as the caching can result in observable variations in response times, making them a strong candidate for a timing-based attack hint. In this study, we propose a novel timing-based side-channel attack to execute input theft in LLMs inference. The cache-based attack faces the challenge of constructing candidate inputs in a large search space to hit and steal cached user queries. To address these challenges, we propose two primary components. The input constructor employs machine learning techniques and LLM-based approaches for vocabulary correlation learning while implementing optimized search mechanisms for generalized input construction. The time analyzer implements statistical time fitting with outlier elimination to identify cache hit patterns, continuously providing feedback to refine the constructor's search strategy. We conduct experiments across two cache mechanisms and the results demonstrate that our approach consistently attains high attack success rates in various applications. Our work highlights the security vulnerabilities associated with performance optimizations, underscoring the necessity of prioritizing privacy and security alongside enhancements in LLM inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Xinyao Zheng",
        "Husheng Han",
        "Shangyi Shi",
        "Qiyan Fang",
        "Zidong Du",
        "Xing Hu",
        "Qi Guo"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/5201913bb3b941e0d42606969da5c1f927aeb48b",
      "pdf_url": "",
      "publication_date": "2024-11-27",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec7ba7668113c5709e729142e75d0251221ff04b",
      "title": "FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks",
      "abstract": "Large language models (LLMs) have been widely deployed as the backbone with additional tools and text information for real-world applications. However, integrating external information into LLM-integrated applications raises significant security concerns. Among these, prompt injection attacks are particularly threatening, where malicious instructions injected in the external text information can exploit LLMs to generate answers as the attackers desire. While both training-time and test-time defense methods have been developed to mitigate such attacks, the unaffordable training costs associated with training-time methods and the limited effectiveness of existing test-time methods make them impractical. This paper introduces a novel test-time defense strategy, named Formatting AuThentication with Hash-based tags (FATH). Unlike existing approaches that prevent LLMs from answering additional instructions in external text, our method implements an authentication system, requiring LLMs to answer all received instructions with a security policy and selectively filter out responses to user instructions as the final output. To achieve this, we utilize hash-based authentication tags to label each response, facilitating accurate identification of responses according to the user's instructions and improving the robustness against adaptive attacks. Comprehensive experiments demonstrate that our defense method can effectively defend against indirect prompt injection attacks, achieving state-of-the-art performance under Llama3 and GPT3.5 models across various attack methods. Our code is released at: https://github.com/Jayfeather1024/FATH",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiong Wang",
        "Fangzhou Wu",
        "Wendi Li",
        "Jinsheng Pan",
        "Edward Suh",
        "Z. Mao",
        "Muhao Chen",
        "Chaowei Xiao"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/ec7ba7668113c5709e729142e75d0251221ff04b",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb5cd74e28059073b0f50ba42b1dcec41d29af82",
      "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
      "abstract": "The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot\u2019s understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.",
      "year": 2024,
      "venue": "2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW)",
      "authors": [
        "Wenxiao Zhang",
        "Xiangrui Kong",
        "Conan Dewitt",
        "Thomas Br\u00e4unl",
        "Jin B. Hong"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/bb5cd74e28059073b0f50ba42b1dcec41d29af82",
      "pdf_url": "http://arxiv.org/pdf/2408.03515",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9832eb62e0b3d826d21445fc1fd2fcf6ced7d41a",
      "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
      "abstract": "Despite significant advancements, large language models (LLMs) still struggle with providing accurate answers when lacking domain-specific or up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge bases, but it also introduces new attack surfaces. In this paper, we investigate data extraction attacks targeting RAG's knowledge databases. We show that previous prompt injection-based extraction attacks largely rely on the instruction-following capabilities of LLMs. As a result, they fail on models that are less responsive to such malicious prompts -- for example, our experiments show that state-of-the-art attacks achieve near-zero success on Gemma-2B-IT. Moreover, even for models that can follow these instructions, we found fine-tuning may significantly reduce attack performance. To further reveal the vulnerability, we propose to backdoor RAG, where a small portion of poisoned data is injected during the fine-tuning phase to create a backdoor within the LLM. When this compromised LLM is integrated into a RAG system, attackers can exploit specific triggers in prompts to manipulate the LLM to leak documents from the retrieval database. By carefully designing the poisoned data, we achieve both verbatim and paraphrased document extraction. For example, on Gemma-2B-IT, we show that with only 5\\% poisoned data, our method achieves an average success rate of 94.1\\% for verbatim extraction (ROUGE-L score: 82.1) and 63.6\\% for paraphrased extraction (average ROUGE score: 66.4) across four datasets. These results underscore the privacy risks associated with the supply chain when deploying RAG systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuefeng Peng",
        "Junda Wang",
        "Hong Yu",
        "Amir Houmansadr"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/9832eb62e0b3d826d21445fc1fd2fcf6ced7d41a",
      "pdf_url": "",
      "publication_date": "2024-11-03",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f568a7bf423121780a552d6819ca6623112578f7",
      "title": "Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation",
      "abstract": "With text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on \u201cimplicitly adversarial\u201d prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. We present an in-depth account of our methodology, a systematic study of novel attack strategies and safety failures, and a visualization tool for easy exploration of the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. This work will enable proactive, iterative safety assessments and promote responsible development of T2I models.",
      "year": 2024,
      "venue": "Conference on Fairness, Accountability and Transparency",
      "authors": [
        "Jessica Quaye",
        "Alicia Parrish",
        "Oana Inel",
        "Charvi Rastogi",
        "Hannah Rose Kirk",
        "Minsuk Kahng",
        "Erin van Liemt",
        "Max Bartolo",
        "J. Tsang",
        "Justin White",
        "Nathan Clement",
        "Rafael Mosquera",
        "Juan Ciro",
        "V. Reddi",
        "Lora Aroyo"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/f568a7bf423121780a552d6819ca6623112578f7",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658913",
      "publication_date": "2024-02-14",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0015e5dd084475781302040504585a229df7a5db",
      "title": "From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Hao Wang",
        "Hao Li",
        "Minlie Huang",
        "Lei Sha"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0015e5dd084475781302040504585a229df7a5db",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "language model attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "763cf8a095ef039c99cdcfc654132169443c56ee",
      "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
      "abstract": "We explore visual prompt injection (VPI) that maliciously exploits the ability of large vision-language models (LVLMs) to follow instructions drawn onto the input image. We propose a new VPI method,\"goal hijacking via visual prompt injection\"(GHVPI), that swaps the execution task of LVLMs from an original task to an alternative task designated by an attacker. The quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and demonstrates a notable attack success rate of 15.8%, which is an unignorable security risk. Our analysis also shows that successful GHVPI requires high character recognition capability and instruction-following ability in LVLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Subaru Kimura",
        "Ryota Tanaka",
        "Shumpei Miyawaki",
        "Jun Suzuki",
        "Keisuke Sakaguchi"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/763cf8a095ef039c99cdcfc654132169443c56ee",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c10ab1bf12182570a6ecf9e2dd01f60f13a446dd",
      "title": "Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection",
      "abstract": "Large language models (LLMs) are renowned for their exceptional capabilities, and applying to a wide range of applications. However, this widespread use brings significant vulnerabilities. Also, it is well observed that there are huge gap which lies in the need for effective detection and mitigation strategies against malicious prompt injection attacks in large language models, as current approaches may not adequately address the complexity and evolving nature of these vulnerabilities in real-world applications. Therefore, this work focuses the impact of malicious prompt injection attacks which is one of most dangerous vulnerability on real LLMs applications. It examines to apply various BERT (Bidirectional Encoder Representations from Transformers) like multilingual BERT, DistilBert for classifying malicious prompts from legitimate prompts. Also, we observed how tokenizing the prompt texts and generating embeddings using multilingual BERT contributes to improve the performance of various machine learning methods: Gaussian Naive Bayes, Random Forest, Support Vector Machine, and Logistic Regression. The performance of each model is rigorously analyzed with various parameters to improve the binary classification to discover malicious prompts. Multilingual BERT approach to embed the prompts significantly improved and outperformed the existing works and achieves an outstanding accuracy of 96.55% by Logistic regression. Additionally, we investigated the incorrect predictions of the model to gain insights into its limitations. The findings can guide researchers in tuning various BERT for finding the most suitable model for diverse LLMs vulnerabilities.",
      "year": 2024,
      "venue": "2024 2nd International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)",
      "authors": [
        "Md. Abdur Rahman",
        "Hossain Shahriar",
        "Fan Wu",
        "Alfredo Cuzzocrea"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/c10ab1bf12182570a6ecf9e2dd01f60f13a446dd",
      "pdf_url": "http://arxiv.org/pdf/2409.13331",
      "publication_date": "2024-09-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fddfd120f6992b00f42b4a770ccfb69353bcd8b",
      "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
      "abstract": "Large Language Models (LLMs) have gained widespread adoption across various domains, including chatbots and auto-task completion agents. However, these models are susceptible to safety vulnerabilities such as jailbreaking, prompt injection, and privacy leakage attacks. These vulnerabilities can lead to the generation of malicious content, unauthorized actions, or the disclosure of confidential information. While foundational LLMs undergo alignment training and incorporate safety measures, they are often subject to fine-tuning, or doing quantization resource-constrained environments. This study investigates the impact of these modifications on LLM safety, a critical consideration for building reliable and secure AI systems. We evaluate foundational models including Mistral, Llama series, Qwen, and MosaicML, along with their fine-tuned variants. Our comprehensive analysis reveals that fine-tuning generally increases the success rates of jailbreak attacks, while quantization has variable effects on attack success rates. Importantly, we find that properly implemented guardrails significantly enhance resistance to jailbreak attempts. These findings contribute to our understanding of LLM vulnerabilities and provide insights for developing more robust safety strategies in the deployment of language models.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Divyanshu Kumar",
        "Anurakt Kumar",
        "Sahil Agarwal",
        "P. Harshangi"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/3fddfd120f6992b00f42b4a770ccfb69353bcd8b",
      "pdf_url": "",
      "publication_date": "2024-04-05",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "097a7927238a52291856588afd66dd4de1a7276d",
      "title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models",
      "abstract": "Large pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated excellent zero-shot generalizability across various downstream tasks. However, recent studies have shown that the inference performance of CLIP can be greatly degraded by small adversarial perturbations, especially its visual modality, posing significant safety threats. To mitigate this vulnerability, in this paper, we propose a novel defense method called Test-Time Adversarial Prompt Tuning (TAPT) to enhance the inference robustness of CLIP against visual adversarial attacks. TAPT is a test-time defense method that learns defensive bimodal (textual and visual) prompts to robustify the inference process of CLIP. Specifically, it is an unsupervised method that optimizes the defensive prompts for each test sample by minimizing a multi-view entropy and aligning adversarial-clean distributions. We evaluate the effectiveness of TAPT on 11 benchmark datasets, including ImageNet and 10 other zero-shot datasets, demonstrating that it enhances the zero-shot adversarial robustness of the original CLIP by at least 48.9% against AutoAttack (AA), while largely maintaining performance on clean examples. Moreover, TAPT outperforms existing adversarial prompt tuning methods across various backbones, achieving an average robustness improvement of at least 36.6%. Code is available at https://github.com/xinwong/TAPT.",
      "year": 2024,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Xin Wang",
        "Kai Chen",
        "Jiaming Zhang",
        "Jingjing Chen",
        "Xingjun Ma"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/097a7927238a52291856588afd66dd4de1a7276d",
      "pdf_url": "",
      "publication_date": "2024-11-20",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4eec7ad1aef177297d0c2019434a84e48fb1f4f7",
      "title": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent text but remain limited by the static nature of their training data. Retrieval Augmented Generation (RAG) addresses this issue by combining LLMs with up-to-date information retrieval, but also expand the attack surface of the system. This paper investigates prompt injection attacks on RAG, focusing on malicious objectives beyond misinformation, such as inserting harmful links, promoting unauthorized services, and initiating denial-of-service behaviors. We build upon existing corpus poisoning techniques and propose a novel backdoor attack aimed at the fine-tuning process of the dense retriever component. Our experiments reveal that corpus poisoning can achieve significant attack success rates through the injection of a small number of compromised documents into the retriever corpus. In contrast, backdoor attacks demonstrate even higher success rates but necessitate a more complex setup, as the victim must fine-tune the retriever using the attacker poisoned dataset.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Cody Clop",
        "Yannick Teglia"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/4eec7ad1aef177297d0c2019434a84e48fb1f4f7",
      "pdf_url": "",
      "publication_date": "2024-10-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f41109b24c24ecccf26cda9ceacad9167d197028",
      "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
      "abstract": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language models (VLMs) stand at the forefront of this advancement, offering innovative ways to combine visual and textual data for enhanced understanding and interaction. However, this integration also enlarges the attack surface. Patch-based adversarial attack is considered the most realistic threat model in physical vision applications, as demonstrated in many existing literature. In this paper, we propose to address patched visual prompt injection, where adversaries exploit adversarial patches to generate target content in VLMs. Our investigation reveals that patched adversarial prompts exhibit sensitivity to pixel-wise randomization, a trait that remains robust even against adaptive attacks designed to counteract such defenses. Leveraging this insight, we introduce SmoothVLM, a defense mechanism rooted in smoothing techniques, specifically tailored to protect VLMs from the threat of patched visual prompt injectors. Our framework significantly lowers the attack success rate to a range between 0% and 5.0% on two leading VLMs, while achieving around 67.3% to 95.0% context recovery of the benign images, demonstrating a balance between security and usability.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiachen Sun",
        "Changsheng Wang",
        "Jiong Wang",
        "Yiwei Zhang",
        "Chaowei Xiao"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/f41109b24c24ecccf26cda9ceacad9167d197028",
      "pdf_url": "",
      "publication_date": "2024-05-17",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e56d45bf7bd34132e610e947ef768704ce65ccf9",
      "title": "Exploring Vulnerabilities and Protections in Large Language Models: A Survey",
      "abstract": "As Large Language Models (LLMs) increasingly become key components in various AI applications, understanding their security vulnerabilities and the effectiveness of defense mechanisms is crucial. This survey examines the security challenges of LLMs, focusing on two main areas: Prompt Hacking and Adversarial Attacks, each with specific types of threats. Under Prompt Hacking, we explore Prompt Injection and Jailbreaking Attacks, discussing how they work, their potential impacts, and ways to mitigate them. Similarly, we analyze Adversarial Attacks, breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities and the defense strategies that can be implemented. The survey highlights these security challenges and discusses robust defensive frameworks to protect LLMs against these threats. By detailing these security issues, the survey contributes to the broader discussion on creating resilient AI systems that can resist sophisticated attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Frank Weizhen Liu",
        "Chenhui Hu"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/e56d45bf7bd34132e610e947ef768704ce65ccf9",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "prompt injection",
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec92e3e26dcc07bae99afd3b9237dba153a81b5c",
      "title": "Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications",
      "abstract": "The advent of Large Language Models (LLMs) has revolutionized various applications by providing advanced natural language processing capabilities. However, this innovation introduces new cybersecurity challenges. This paper explores the threat modeling and risk analysis specifically tailored for LLM-powered applications. Focusing on potential attacks like data poisoning, prompt injection, SQL injection, jailbreaking, and compositional injection, we assess their impact on security and propose mitigation strategies. We introduce a framework combining STRIDE and DREAD methodologies for proactive threat identification and risk assessment. Furthermore, we examine the feasibility of an end-to-end threat model through a case study of a custom-built LLM-powered application. This model follows Shostack's Four Question Framework, adjusted for the unique threats LLMs present. Our goal is to propose measures that enhance the security of these powerful AI tools, thwarting attacks, and ensuring the reliability and integrity of LLM-integrated systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Stephen Burabari Tete"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/ec92e3e26dcc07bae99afd3b9237dba153a81b5c",
      "pdf_url": "",
      "publication_date": "2024-06-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f02fc3e2ab00d74f2397f70bab09a526e1910b56",
      "title": "Ranking Manipulation for Conversational Search Engines",
      "abstract": "Major search engine providers are rapidly incorporating Large Language Model (LLM)-generated content in response to user queries. These *conversational search engines* operate by loading retrieved website text into the LLM context for summarization and interpretation. Recent research demonstrates that LLMs are highly vulnerable to jailbreaking and prompt injection attacks, which disrupt the safety and quality goals of LLMs using adversarial strings. This work investigates the impact of prompt injections on the ranking order of sources referenced by conversational search engines. To this end, we introduce a focused dataset of real-world consumer product websites and formalize conversational search ranking as an adversarial problem. Experimentally, we analyze conversational search rankings in the absence of adversarial injections and show that different LLMs vary significantly in prioritizing product name, document content, and context position. We then present a tree-of-attacks-based jailbreaking technique which reliably promotes low-ranked products. Importantly, these attacks transfer effectively to state-of-the-art conversational search engines such as *perplexity.ai*. Given the strong financial incentive for website owners to boost their search ranking, we argue that our problem formulation is of critical importance for future robustness work.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Samuel Pfrommer",
        "Yatong Bai",
        "Tanmay Gautam",
        "S. Sojoudi"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/f02fc3e2ab00d74f2397f70bab09a526e1910b56",
      "pdf_url": "",
      "publication_date": "2024-06-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "38d2f3ea8fc0736a2058d3b1a12daf325ec3f457",
      "title": "Combating Adversarial Attacks with Multi-Agent Debate",
      "abstract": "While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Steffi Chern",
        "Zhen Fan",
        "Andy Liu"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/38d2f3ea8fc0736a2058d3b1a12daf325ec3f457",
      "pdf_url": "",
      "publication_date": "2024-01-11",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7e88fc17ff118b232a19f14d9ad1820113063245",
      "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
      "abstract": "Adversarial prompts generated using gradient-based methods exhibit outstanding performance in performing automatic jailbreak attacks against safety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the white-box setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking black-box image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, i.e., Skip Gradient Method and Intermediate Level Attack, into gradient-based adversarial prompt generation and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that 87% of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench. This match rate is 33% higher than that of a very strong baseline known as GCG, demonstrating advanced discrete optimization for adversarial prompt generation against LLMs. In addition, without introducing obvious cost, the combination achieves>30% absolute increase in attack success rates compared with GCG when generating both query-specific (38% ->68%) and universal adversarial prompts (26.68% ->60.32%) for attacking the Llama-2-7B-Chat model on AdvBench. Code at: https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Qizhang Li",
        "Yiwen Guo",
        "Wangmeng Zuo",
        "Hao Chen"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/7e88fc17ff118b232a19f14d9ad1820113063245",
      "pdf_url": "",
      "publication_date": "2024-05-28",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ebfef1daea743456536d620e894eee8992aa124",
      "title": "Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation",
      "abstract": "Existing research suggests that automatic speech recognition (ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60% and 25% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines.",
      "year": 2024,
      "venue": "Interspeech",
      "authors": [
        "Ruizhe Huang",
        "M. Yarmohammadi",
        "S. Khudanpur",
        "Dan Povey"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/6ebfef1daea743456536d620e894eee8992aa124",
      "pdf_url": "",
      "publication_date": "2024-07-14",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9596d1a664723743114d12787cd6d12e37a3083a",
      "title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection",
      "abstract": "Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem. These attacks target LLMs applications through using carefully designed input prompts to divert the model from adhering to original instruction, thereby it could execute unintended actions. These manipulations pose serious security threats which potentially results in data leaks, biased outputs, or harmful responses. This project explores the security vulnerabilities in relation to prompt injection attacks. To detect whether a prompt is vulnerable or not, we follows two approaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a thorough analysis and comparison of the classification performance. Firstly, we use pre-trained XLM-RoBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model achieves impressive results with 99.13% accuracy, 100% precision, 98.33% recall and 99.15% F1-score thorough rigorous experimentation and evaluation. We observe that our approach is highly efficient in detecting prompt injection attacks.",
      "year": 2024,
      "venue": "Annual International Computer Software and Applications Conference",
      "authors": [
        "Md. Abdur Rahman",
        "Hossain Shahriar",
        "Guillermo Francia",
        "Fan Wu",
        "Alfredo Cuzzocrea",
        "M. Rahman",
        "Md Jobair Hossain Faruk",
        "S. Ahamed"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/9596d1a664723743114d12787cd6d12e37a3083a",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "detect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2e0ad150cff50c033b323e6e415ad7464d88328",
      "title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks",
      "abstract": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip models with out of distribution knowledge. This process involves collecting, indexing, retrieving, and providing information to an LLM for generating responses. Despite its growing popularity due to its flexibility and low cost, the security implications of RAG have not been extensively studied. The data for such systems are often collected from public sources, providing an attacker a gateway for indirect prompt injections to manipulate the responses of the model. In this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations. First, we review existing RAG framework pipelines, deriving a prototypical architecture and identifying critical parameters. We then examine prior works searching for techniques that attackers can use to perform indirect prompt manipulations. Finally, we implemented Rag 'n Roll, a framework to determine the effectiveness of attacks against end-to-end RAG applications. Our results show that existing attacks are mostly optimized to boost the ranking of malicious documents during the retrieval phase. However, a higher rank does not immediately translate into a reliable attack. Most attacks, against various configurations, settle around a 40% success rate, which could rise to 60% when considering ambiguous answers as successful attacks (those that include the expected benign one as well). Additionally, when using unoptimized documents, attackers deploying two of them (or more) for a target query can achieve similar results as those using optimized ones. Finally, exploration of the configuration space of a RAG showed limited impact in thwarting the attacks, where the most successful combination severely undermines functionality.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Gianluca De Stefano",
        "Lea Sch\u00f6nherr",
        "Giancarlo Pellegrino"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/b2e0ad150cff50c033b323e6e415ad7464d88328",
      "pdf_url": "",
      "publication_date": "2024-08-09",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection",
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ef9201c779ec1e86d5f15d56c64cc82297bef281",
      "title": "Robust Safety Classifier Against Jailbreaking Attacks: Adversarial Prompt Shield",
      "abstract": "Large Language Models\u2019 safety remains a critical concern due to their vulnerability to jailbreaking attacks, which can prompt these systems to produce harmful and malicious responses. Safety classifiers, computational models trained to discern and mitigate potentially harmful, offensive, or unethical outputs, offer a practical solution to address this issue. However, despite their potential, existing safety classifiers often fail when exposed to adversarial attacks such as gradient-optimized suffix attacks. In response, our study introduces Adversarial Prompt Shield (APS), a lightweight safety classifier model that excels in detection accuracy and demonstrates resilience against unseen jailbreaking prompts. We also introduce efficiently generated adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND), which are designed to fortify the classifier\u2019s robustness. Through extensive testing on various safety tasks and unseen jailbreaking attacks, we demonstrate the effectiveness and resilience of our models. Evaluations show that our classifier has the potential to significantly reduce the Attack Success Rate by up to 44.9%. This advance paves the way for the next generation of more reliable and resilient Large Language Models.",
      "year": 2024,
      "venue": "WOAH",
      "authors": [
        "Jinhwa Kim",
        "Ali Derakhshan",
        "Ian G. Harris"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/ef9201c779ec1e86d5f15d56c64cc82297bef281",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ad05fb85fd79d8d4dba102a4749c6ddcf02d33cc",
      "title": "Scaling Trends in Language Model Robustness",
      "abstract": "Increasing model size has unlocked a dazzling array of capabilities in modern language models. At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust. As both attack and defense gain access to more compute, and as models become larger, what happens to robustness? We argue that to answer this question requires a \\emph{scaling} approach, which we employ in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks. We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency. Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models. Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance. We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run. These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Nikolaus H. R. Howe",
        "Ian R. McKenzie",
        "O. Hollinsworth",
        "Michal Zajac",
        "Tom Tseng",
        "Aaron David Tucker",
        "Pierre-Luc Bacon",
        "Adam Gleave"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/ad05fb85fd79d8d4dba102a4749c6ddcf02d33cc",
      "pdf_url": "",
      "publication_date": "2024-07-25",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "046fa7fa879f5d702bf8507ccf18d1c1f867a5f8",
      "title": "Espresso: Robust Concept Filtering in Text-to-Image Models",
      "abstract": "Diffusion based text-to-image models are trained on large datasets scraped from the Internet, potentially containing unacceptable concepts (e.g., copyright-infringing or unsafe). We need concept removal techniques (CRTs) which are i) effective in preventing the generation of images with unacceptable concepts, ii) utility-preserving on acceptable concepts, and, iii) robust against evasion with adversarial prompts. No prior CRT satisfies all these requirements simultaneously. We introduce Espresso, the first robust concept filter based on Contrastive Language-Image Pre-Training (CLIP). We identify unacceptable concepts by using the distance between the embedding of a generated image to the text embeddings of both unacceptable and acceptable concepts. This lets us fine-tune for robustness by separating the text embeddings of unacceptable and acceptable concepts while preserving utility. We present a pipeline to evaluate various CRTs to show that Espresso is more effective and robust than prior CRTs, while retaining utility",
      "year": 2024,
      "venue": "Conference on Data and Application Security and Privacy",
      "authors": [
        "Anudeep Das",
        "Vasisht Duddu",
        "Rui Zhang",
        "N. Asokan"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/046fa7fa879f5d702bf8507ccf18d1c1f867a5f8",
      "pdf_url": "",
      "publication_date": "2024-04-30",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b67a6181fad5c5838945583ccdc7f39187e29332",
      "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks",
      "abstract": "Large language models (LLMs) are increasingly being harnessed to automate cyberattacks, making sophisticated exploits more accessible and scalable. In response, we propose a new defense strategy tailored to counter LLM-driven cyberattacks. We introduce Mantis, a defensive framework that exploits LLMs' susceptibility to adversarial inputs to undermine malicious operations. Upon detecting an automated cyberattack, Mantis plants carefully crafted inputs into system responses, leading the attacker's LLM to disrupt their own operations (passive defense) or even compromise the attacker's machine (active defense). By deploying purposefully vulnerable decoy services to attract the attacker and using dynamic prompt injections for the attacker's LLM, Mantis can autonomously hack back the attacker. In our experiments, Mantis consistently achieved over 95% effectiveness against automated LLM-driven attacks. To foster further research and collaboration, Mantis is available as an open-source tool: https://github.com/pasquini-dario/project_mantis",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Dario Pasquini",
        "Evgenios M. Kornaropoulos",
        "G. Ateniese"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/b67a6181fad5c5838945583ccdc7f39187e29332",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7e425c93a0bfe017098cedecc43760a6f9ef5e8",
      "title": "Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications",
      "abstract": "In recent years, there has been an exponential growth of real-world applications where humans interact with chatbots through a combination of images and text. Specifically, these multi-modal large language models (MLLMs) can digest various forms of data and understand users\u2019 intents and needs encoded in both images and text. However, defending these models against prompt-based injection attacks is a less explored area. This problem is further exacerbated by the limitations of current defense mechanisms for language models, which are restricted to handling only text data. This paper introduces a novel defense mechanism for MLLM-based chatbots, addressing image-based injection attacks through a two-stage approach: input validation to identify unsafe inputs before reaching the chatbot, and prompt injection detection to safeguard the MLLM backbone from malicious image attacks. The framework utilizes a domain-specific programming language tailored for secure chatbot definitions and user-specified specifications for chatbots and image inputs. Through experiments on models like GPT-4VISION and LLAVA, we demonstrate the limitations of relying on model robustness and showcase our approach\u2019s effectiveness in improving malicious attack detection for MLLM-based chatbots.",
      "year": 2024,
      "venue": "2024 IEEE Security and Privacy Workshops (SPW)",
      "authors": [
        "Reshabh K Sharma",
        "Vinayak Gupta",
        "Dan Grossman"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/b7e425c93a0bfe017098cedecc43760a6f9ef5e8",
      "pdf_url": "",
      "publication_date": "2024-05-23",
      "keywords_matched": [
        "prompt injection",
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d4070ca0ba7ccdedb044c9a15301ad41f6aeb05",
      "title": "Cognitive Overload Attack:Prompt Injection for Long Context",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in performing tasks across various domains without needing explicit retraining. This capability, known as In-Context Learning (ICL), while impressive, exposes LLMs to a variety of adversarial prompts and jailbreaks that manipulate safety-trained LLMs into generating undesired or harmful output. In this paper, we propose a novel interpretation of ICL in LLMs through the lens of cognitive neuroscience, by drawing parallels between learning in human cognition with ICL. We applied the principles of Cognitive Load Theory in LLMs and empirically validate that similar to human cognition, LLMs also suffer from cognitive overload a state where the demand on cognitive processing exceeds the available capacity of the model, leading to potential errors. Furthermore, we demonstrated how an attacker can exploit ICL to jailbreak LLMs through deliberately designed prompts that induce cognitive overload on LLMs, thereby compromising the safety mechanisms of LLMs. We empirically validate this threat model by crafting various cognitive overload prompts and show that advanced models such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, Llama-3-70B-Instruct, Gemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack success rates of up to 99.99%. Our findings highlight critical vulnerabilities in LLMs and underscore the urgency of developing robust safeguards. We propose integrating insights from cognitive load theory into the design and evaluation of LLMs to better anticipate and mitigate the risks of adversarial attacks. By expanding our experiments to encompass a broader range of models and by highlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of safer and more reliable AI systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Bibek Upadhayay",
        "Vahid Behzadan",
        "Amin Karbasi"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/6d4070ca0ba7ccdedb044c9a15301ad41f6aeb05",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb",
      "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains",
      "abstract": "As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Bijoy Ahmed Saiem",
        "MD Sadik Hossain Shanto",
        "Rakib Ahsan",
        "Md. Rafi Ur Rashid"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/6c7a1bcaa69dd29cb5fa89ac4a9ec61621c211fb",
      "pdf_url": "",
      "publication_date": "2024-11-10",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "196206c061632c2b723cc4e66eb09f438a1cfcd1",
      "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs",
      "abstract": "LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. In this paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent embedding spaces, gradually optimizing the suffix prompter to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success over baselines, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Advik Raj Basani",
        "Xiao Zhang"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/196206c061632c2b723cc4e66eb09f438a1cfcd1",
      "pdf_url": "",
      "publication_date": "2024-11-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c03b27c7bc26d6327cd3b7e29d9f3a44d735f2c6",
      "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models",
      "abstract": "Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLMs by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and pseudonymizing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4500 synthesized prompts and 2000 real-world prompts. The results show that Casper can effectively filter out Personal Identifiable Information (PII) with an accuracy of 92.6 %, and detect privacy-sensitive topics with an accuracy ranging from 92.5 % to 94.0 %. Furthermore, Casper successfully pseudonymized $\\mathbf{9 2. 0 \\%}$ of the sensitive information in the prompts while ensuring that the LLM's responses to the sanitized prompts remained moderately similar to those for the original prompts, with a cosine similarity of 0.538.",
      "year": 2024,
      "venue": "International Conference on Cyber Security and Cloud Computing",
      "authors": [
        "Chun Jie Chong",
        "Chenxi Hou",
        "Zhihao Yao",
        "S. Talebi"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c03b27c7bc26d6327cd3b7e29d9f3a44d735f2c6",
      "pdf_url": "",
      "publication_date": "2024-08-13",
      "keywords_matched": [
        "prompt sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b48c0b4b75b725f19b16f54d48e6f7f6071bf9d5",
      "title": "Text-Based Prompt Injection Attack Using Mathematical Functions in Modern Large Language Models",
      "abstract": "Prompt injection is a type of attack that induces violent or discriminatory responses via the input of a prompt containing illegal instructions to the large language model (LLM). Most early injection attacks used simple text prompts; however, recently, injection attacks employing elaborately designed prompts to overcome the strong security policies of modern LLMs have been applied to input prompts. This study proposed a method to perform injection attacks that can bypass existing security policies via the replacement of sensitive words that may be rejected by a language model in the text prompt with mathematical functions. By hiding the contents of the prompt so that the LLM cannot easily detect the contents of the illegal instructions, we achieved a considerably higher success rate than existing injection attacks, even for the latest securely aligned LLMs. As the proposed method employed only text prompts, it was capable of attacking most LLMs. Moreover, it exhibited a higher attack success rate than multimodal attacks using images despite using only text. An understanding of the newly proposed injection attack is expected to aid in the development of methods to further strengthen the security of current LLMs.",
      "year": 2024,
      "venue": "Electronics",
      "authors": [
        "Hyeokjin Kwon",
        "Wooguil Pak"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b48c0b4b75b725f19b16f54d48e6f7f6071bf9d5",
      "pdf_url": "https://doi.org/10.3390/electronics13245008",
      "publication_date": "2024-12-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "16498c9bc4ec6e1aa8f70e66cc070a88ed597f8b",
      "title": "Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models",
      "abstract": "Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language prompt tokens), albeit to a lesser extent.",
      "year": 2024,
      "venue": "European Conference on Information Retrieval",
      "authors": [
        "Andrew Parry",
        "Maik Fr\u00f6be",
        "Sean MacAvaney",
        "Martin Potthast",
        "Matthias Hagen"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/16498c9bc4ec6e1aa8f70e66cc070a88ed597f8b",
      "pdf_url": "",
      "publication_date": "2024-03-12",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b40fcf7914f483eecc607cb24ed82024f5e58944",
      "title": "SOS! Soft Prompt Attack Against Open-Source Large Language Models",
      "abstract": "Open-source large language models (LLMs) have become increasingly popular among both the general public and industry, as they can be customized, fine-tuned, and freely used. However, some open-source LLMs require approval before usage, which has led to third parties publishing their own easily accessible versions. Similarly, third parties have been publishing fine-tuned or quantized variants of these LLMs. These versions are particularly appealing to users because of their ease of access and reduced computational resource demands. This trend has increased the risk of training time attacks, compromising the integrity and security of LLMs. In this work, we present a new training time attack, SOS, which is designed to be low in computational demand and does not require clean data or modification of the model weights, thereby maintaining the model's utility intact. The attack addresses security issues in various scenarios, including the backdoor attack, jailbreak attack, and prompt stealing attack. Our experimental findings demonstrate that the proposed attack is effective across all evaluated targets. Furthermore, we present the other side of our SOS technique, namely the copyright token -- a novel technique that enables users to mark their copyrighted content and prevent models from using it.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Ziqing Yang",
        "Michael Backes",
        "Yang Zhang",
        "Ahmed Salem"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/b40fcf7914f483eecc607cb24ed82024f5e58944",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33f4b3521fd8c4d3365bb44616415e2602925f63",
      "title": "Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks",
      "abstract": "Recent developments in text-to-image models, particularly Stable Diffusion, have marked significant achievements in various applications. With these advancements, there are growing safety concerns about the vulnerability of the model that malicious entities exploit to generate targeted harmful images. However, the existing methods in the vulnerability of the model mainly evaluate the alignment between the prompt and generated images, but fall short in revealing the vulnerability associated with targeted image generation. In this study, we formulate the problem of targeted adversarial attack on Stable Diffusion and propose a framework to generate adversarial prompts. Specifically, we design a gradient-based embedding optimization method to craft reliable adversarial prompts that guide stable diffusion to generate specific images. Furthermore, after obtaining successful adversarial prompts, we reveal the mechanisms that cause the vulnerability of the model. Extensive experiments on two targeted attack tasks demonstrate the effectiveness of our method in targeted attacks. The code can be obtained in https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Chenyu Zhang",
        "Lanjun Wang",
        "Anan Liu"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/33f4b3521fd8c4d3365bb44616415e2602925f63",
      "pdf_url": "",
      "publication_date": "2024-01-16",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e3aaa5e8a177a0b629408aadaf096a03b04cb014",
      "title": "Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment",
      "abstract": "Prompt injection attack, where an attacker injects a prompt into the original one, aiming to make an Large Language Model (LLM) follow the injected prompt to perform an attacker-chosen task, represent a critical security threat. Existing attacks primarily focus on crafting these injections at inference time, treating the LLM itself as a static target. Our experiments show that these attacks achieve some success, but there is still significant room for improvement. In this work, we introduces a more foundational attack vector: poisoning the LLM\u2019s alignment process to amplify the success of future prompt injection attacks. Specifically, we propose PoisonedAlign, a method that strategically creates poisoned alignment samples to poison an LLM\u2019s alignment dataset. Our experiments across five LLMs and two alignment datasets show that when even a small fraction of the alignment data is poisoned, the resulting model becomes substantially more vulnerable to a wide range of prompt injection attacks. Crucially, this vulnerability is instilled while the LLM\u2019s performance on standard capability benchmarks remains largely unchanged, making the manipulation difficult to detect through automated, general-purpose performance evaluations. The code for implementing the attack is at https://github.com/Sadcardation/PoisonedAlign.",
      "year": 2024,
      "venue": "Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security",
      "authors": [
        "Zedian Shao",
        "Hongbin Liu",
        "Jaden Mu",
        "N. Gong"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/e3aaa5e8a177a0b629408aadaf096a03b04cb014",
      "pdf_url": "",
      "publication_date": "2024-10-18",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f1611e6d829ba799783393b00f529c1e3f01ed76",
      "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization",
      "abstract": "Universal goal hijacking is a kind of prompt injection attack that forces LLMs to return a target malicious response for arbitrary normal user prompts. The previous methods achieve high attack performance while being too cumbersome and time-consuming. Also, they have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To this end, we propose a method called POUGH that incorporates an efficient optimization algorithm and two semantics-guided prompt organization strategies. Specifically, our method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes them. Given the sequentially ranked prompts, our method employs an iterative optimization algorithm to generate a fixed suffix that can concatenate to arbitrary user prompts for universal goal hijacking. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yihao Huang",
        "Chong Wang",
        "Xiaojun Jia",
        "Qing Guo",
        "Felix Juefei-Xu",
        "Jian Zhang",
        "Yang Liu",
        "G. Pu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/f1611e6d829ba799783393b00f529c1e3f01ed76",
      "pdf_url": "",
      "publication_date": "2024-05-23",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "61a06ad0a80d890b1abe1eafb4535a7018022f1e",
      "title": "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild",
      "abstract": "Attacks powered by Large Language Model (LLM) agents represent a growing threat to modern cybersecurity. To address this concern, we present LLM Honeypot, a system designed to monitor autonomous AI hacking agents. By augmenting a standard SSH honeypot with prompt injection and time-based analysis techniques, our framework aims to distinguish LLM agents among all attackers. Over a trial deployment of about three months in a public environment, we collected 8,130,731 hacking attempts and 8 potential AI agents. Our work demonstrates the emergence of AI-driven threats and their current level of usage, serving as an early warning of malicious LLM agents in the wild.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Reworr",
        "Dmitrii Volkov"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/61a06ad0a80d890b1abe1eafb4535a7018022f1e",
      "pdf_url": "",
      "publication_date": "2024-10-17",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d4909b92b9236a29663a9e9d6623c2e15d42746c",
      "title": "BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators",
      "abstract": "Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yu Tian",
        "Xiao Yang",
        "Yinpeng Dong",
        "Heming Yang",
        "Hang Su",
        "Jun Zhu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/d4909b92b9236a29663a9e9d6623c2e15d42746c",
      "pdf_url": "",
      "publication_date": "2024-02-23",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42bc9929a2533d7cd41310406d1fda3e594fd730",
      "title": "On Unsupervised Domain Adaptation: Pseudo Label Guided Mixup for Adversarial Prompt Tuning",
      "abstract": "To date, a backbone of methods for unsupervised domain adaptation (UDA) involves learning label-discriminative features via a label classifier and domain-invariant features through a domain discriminator in an adversarial scheme. However, these methods lack explicit control for aligning the source data and target data within the same label class, degrading the classifier's performance in the target domain. In this paper, we propose PL-Mix, a pseudo label guided Mixup method based on adversarial prompt tuning. Specifically, our PL-Mix facilitates class-dependent alignment and can alleviate the impact of noisy pseudo-labels. We then theoretically justify that PL-Mix can improve the generalization for UDA. Extensive experiments of the comparison with existing models also demonstrate the effectiveness of PL-Mix.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Fanshuang Kong",
        "Richong Zhang",
        "Ziqiao Wang",
        "Yongyi Mao"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/42bc9929a2533d7cd41310406d1fda3e594fd730",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/29800/31384",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33c790fd7d795b23cc657611dd4e5b514b835855",
      "title": "Randomized Testing of RISC-V CPUs Using Direct Instruction Injection",
      "abstract": "Editor\u2019s notes: This article presents a randomized testing framework for RISC-V implementations by using a technique called direct instruction injection for test injection. \u2014Fei Su, Intel, USA",
      "year": 2024,
      "venue": "IEEE design & test",
      "authors": [
        "Alexandre Joannou",
        "Peter Rugg",
        "Jonathan Woodruff",
        "Franz A. Fuchs",
        "Marno Van der Maas",
        "Matthew Naylor",
        "M. Roe",
        "Robert N. M. Watson",
        "P. Neumann",
        "Simon W. Moore"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/33c790fd7d795b23cc657611dd4e5b514b835855",
      "pdf_url": "https://www.repository.cam.ac.uk/bitstreams/eaada502-d82e-485a-ad24-e7386bf2eb6b/download",
      "publication_date": "2024-02-01",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9529ca91dde1f4ed30f3dbf50a8cab273046ca1c",
      "title": "Instruction-Driven Fusion of Infrared-Visible Images: Tailoring for Diverse Downstream Tasks",
      "abstract": "The primary value of infrared and visible image fusion technology lies in applying the fusion results to downstream tasks. However, existing methods face challenges such as increased training complexity and significantly compromised performance of individual tasks when addressing multiple downstream tasks simultaneously. To tackle this, we propose Task-Oriented Adaptive Regulation (T-OAR), an adaptive mechanism specifically designed for multi-task environments. Additionally, we introduce the Task-related Dynamic Prompt Injection (T-DPI) module, which generates task-specific dynamic prompts from user-input text instructions and integrates them into target representations. This guides the feature extraction module to produce representations that are more closely aligned with the specific requirements of downstream tasks. By incorporating the T-DPI module into the T-OAR framework, our approach generates fusion images tailored to task-specific requirements without the need for separate training or task-specific weights. This not only reduces computational costs but also enhances adaptability and performance across multiple tasks. Experimental results show that our method excels in object detection, semantic segmentation, and salient object detection, demonstrating its strong adaptability, flexibility, and task specificity. This provides an efficient solution for image fusion in multi-task environments, highlighting the technology's potential across diverse applications.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Zengyi Yang",
        "Yafei Zhang",
        "Huafeng Li",
        "Yu Liu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/9529ca91dde1f4ed30f3dbf50a8cab273046ca1c",
      "pdf_url": "",
      "publication_date": "2024-11-14",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59757d4b4ecff238ada3277455e61903549979f7",
      "title": "Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion",
      "abstract": "Human motion generative models have enabled promising applications, but the ability of text-to-motion (T2M) models to produce realistic motions raises security concerns if exploited maliciously. Despite growing interest in T2M, limited research focus on safeguarding these models against adversarial attacks, with existing work on text-to-image models proving insufficient for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous framework that leverages large language models (LLMs) to generate targeted adversarial attacks against black-box T2M models. Unlike prior methods that modify prompts through predefined rules, ALERT-Motion uses the knowledge of LLMs of human motion to autonomously generate subtle yet powerful adversarial text descriptions. It comprises two key modules: an adaptive dispatching module that constructs an LLM-based agent to iteratively refine and search for adversarial prompts; and a multimodal information contrastive module that extracts semantically relevant motion information to guide the agent's search. Through this LLM-driven approach, ALERT-Motion produces adversarial prompts querying victim models to produce outputs closely matching targeted motions, while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's superiority over previous methods, achieving higher attack success rates with stealthier adversarial prompts. This pioneering work on T2M adversarial attacks highlights the urgency of developing defensive measures as motion generation technology advances, urging further research into safe and responsible deployment.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Honglei Miao",
        "Fan Ma",
        "Ruijie Quan",
        "Kun Zhan",
        "Yi Yang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/59757d4b4ecff238ada3277455e61903549979f7",
      "pdf_url": "",
      "publication_date": "2024-08-01",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e0e8c1eb4bfe2a5aba365237966142c5fa8e1d16",
      "title": "Exfiltration of personal information from ChatGPT via prompt injection",
      "abstract": "We report that ChatGPT 4 and 4o are susceptible to a prompt injection attack that allows an attacker to exfiltrate users' personal data. It is applicable without the use of any 3rd party tools and all users are currently affected. This vulnerability is exacerbated by the recent introduction of ChatGPT's memory feature, which allows an attacker to command ChatGPT to monitor the user for the desired personal data.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Gregory Schwartzman"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e0e8c1eb4bfe2a5aba365237966142c5fa8e1d16",
      "pdf_url": "",
      "publication_date": "2024-05-31",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7cf2dd505b53e690ce0ff1fd15cc55f127841819",
      "title": "Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context",
      "abstract": "Previous research on testing the vulnerabilities in Large Language Models (LLMs) using adversarial attacks has primarily focused on nonsensical prompt injections, which are easily detected upon manual or automated review (e.g., via byte entropy). However, the exploration of innocuous human-understandable malicious prompts augmented with adversarial injections remains limited. In this research, we explore converting a nonsensical suffix attack into a sensible prompt via a situation-driven contextual re-writing. This allows us to show suffix conversion without any gradients, using only LLMs to perform the attacks, and thus better understand the scope of possible risks. We combine an independent, meaningful adversarial insertion and situations derived from movies to check if this can trick an LLM. The situations are extracted from the IMDB dataset, and prompts are defined following a few-shot chain-of-thought prompting. Our approach demonstrates that a successful situation-driven attack can be executed on both open-source and proprietary LLMs. We find that across many LLMs, as few as 1 attempt produces an attack and that these attacks transfer between LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Nilanjana Das",
        "Edward Raff",
        "Manas Gaur"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/7cf2dd505b53e690ce0ff1fd15cc55f127841819",
      "pdf_url": "",
      "publication_date": "2024-07-19",
      "keywords_matched": [
        "prompt injection",
        "prompt attack",
        "malicious prompt",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "49f65834f16205e8f455d228bc1f1394f0ab8d75",
      "title": "UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers",
      "abstract": "Text-to-Image (T2I) models have raised security concerns due to their potential to generate inappropriate or harmful images. In this paper, we propose UPAM, a novel framework that investigates the robustness of T2I models from the attack perspective. Unlike most existing attack methods that focus on deceiving textual defenses, UPAM aims to deceive both textual and visual defenses in T2I models. UPAM enables gradient-based optimization, offering greater effectiveness and efficiency than previous methods. Given that T2I models might not return results due to defense mechanisms, we introduce a Sphere-Probing Learning (SPL) scheme to support gradient optimization even when no results are returned. Additionally, we devise a Semantic-Enhancing Learning (SEL) scheme to finetune UPAM for generating target-aligned images. Our framework also ensures attack stealthiness. Extensive experiments demonstrate UPAM's effectiveness and efficiency.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Duo Peng",
        "Qi Ke",
        "Jun Liu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/49f65834f16205e8f455d228bc1f1394f0ab8d75",
      "pdf_url": "",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f50dff7a11f37cb962b06ca27bed3448c48f6eba",
      "title": "GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) like GPT-4, LLaMA, and Qwen have demonstrated remarkable success across a wide range of applications. However, these models remain inherently vulnerable to prompt injection attacks, which can bypass existing safety mechanisms, highlighting the urgent need for more robust attack detection methods and comprehensive evaluation benchmarks. To address these challenges, we introduce GenTel-Safe, a unified framework that includes a novel prompt injection attack detection method, GenTel-Shield, along with a comprehensive evaluation benchmark, GenTel-Bench, which compromises 84812 prompt injection attacks, spanning 3 major categories and 28 security scenarios. To prove the effectiveness of GenTel-Shield, we evaluate it together with vanilla safety guardrails against the GenTel-Bench dataset. Empirically, GenTel-Shield can achieve state-of-the-art attack detection success rates, which reveals the critical weakness of existing safeguarding techniques against harmful prompts. For reproducibility, we have made the code and benchmarking dataset available on the project page at https://gentellab.github.io/gentel-safe.github.io/.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Rongchang Li",
        "Minjie Chen",
        "Chang Hu",
        "Han Chen",
        "Wenpeng Xing",
        "Meng Han"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/f50dff7a11f37cb962b06ca27bed3448c48f6eba",
      "pdf_url": "",
      "publication_date": "2024-09-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9679b07906f5b040434ee57fe96279a860344604",
      "title": "Palisade - Prompt Injection Detection Framework",
      "abstract": "The advent of Large Language Models LLMs marks a milestone in Artificial Intelligence, altering how machines comprehend and generate human language. However, LLMs are vulnerable to malicious prompt injection attacks, where crafted inputs manipulate the models behavior in unintended ways, compromising system integrity and causing incorrect outcomes. Conventional detection methods rely on static, rule-based approaches, which often fail against sophisticated threats like abnormal token sequences and alias substitutions, leading to limited adaptability and higher rates of false positives and false negatives.This paper proposes a novel NLP based approach for prompt injection detection, emphasizing accuracy and optimization through a layered input screening process. In this framework, prompts are filtered through three distinct layers rule-based, ML classifier, and companion LLM before reaching the target model, thereby minimizing the risk of malicious interaction.Tests show the ML classifier achieves the highest accuracy among individual layers, yet the multi-layer framework enhances overall detection accuracy by reducing false negatives. Although this increases false positives, it minimizes the risk of overlooking genuine injected prompts, thus prioritizing security.This multi-layered detection approach highlights LLM vulnerabilities and provides a comprehensive framework for future research, promoting secure interactions between humans and AI systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Sahasra Kokkula",
        "R. Somanathan",
        "R. Nandavardhan",
        "Aashishkumar",
        "G. Divya"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9679b07906f5b040434ee57fe96279a860344604",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9aa8ef5683aa8f81d3e0a826c076b45ff0f2d270",
      "title": "[Short Paper] Forensic Analysis of Indirect Prompt Injection Attacks on LLM Agents",
      "abstract": "Large language model (LLM) agents are vulnerable to a range of evolving attacks including Indirect Prompt Injection (IPI). Digital investigations involving IPI attacks on LLM agents are challenging due to growing data volumes and agent flow complexity. We introduce a novel approach for forensic analysis of LLM agent execution logs to identify IPI attacks. We implement a custom digital forensic analysis framework featuring a realistic agentic task benchmark to generate agent logs, for subsequent analysis using 12 state-of-the-art LLMs. Our preliminary results show promising potential for malicious trail detection, with varying performance across domain-specific test suites. We discuss key findings, limitations, and future work directions to support the empirical evaluation of LLMs for digital forensic applications.",
      "year": 2024,
      "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
      "authors": [
        "M. Chernyshev",
        "Zubair A. Baig",
        "Robin Doss"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9aa8ef5683aa8f81d3e0a826c076b45ff0f2d270",
      "pdf_url": "",
      "publication_date": "2024-10-28",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8e066d8f5c74959eaa82499793559f457de050a9",
      "title": "Prompt Injection Attacks in Defended Systems",
      "abstract": "Large language models play a crucial role in modern natural language processing technologies. However, their extensive use also introduces potential security risks, such as the possibility of black-box attacks. These attacks can embed hidden malicious features into the model, leading to adverse consequences during its deployment. This paper investigates methods for black-box attacks on large language models with a three-tiered defense mechanism. It analyzes the challenges and significance of these attacks, highlighting their potential implications for language processing system security. Existing attack and defense methods are examined, evaluating their effectiveness and applicability across various scenarios. Special attention is given to the detection algorithm for black-box attacks, identifying hazardous vulnerabilities in language models and retrieving sensitive information. This research presents a methodology for vulnerability detection and the development of defensive strategies against black-box attacks on large language models.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Daniil Khomsky",
        "Narek Maloyan",
        "Bulat Nutfullin"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/8e066d8f5c74959eaa82499793559f457de050a9",
      "pdf_url": "",
      "publication_date": "2024-06-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9259d06eeaae42b05ad22ba76f0a1cbb216ad63a",
      "title": "SoK: Prompt Hacking of Large Language Models",
      "abstract": "The safety and robustness of large language models (LLMs) based applications remain critical challenges in artificial intelligence. Among the key threats to these applications are prompt hacking attacks, which can significantly undermine the security and reliability of LLM-based systems. In this work, we offer a comprehensive and systematic overview of three distinct types of prompt hacking: jailbreaking, leaking, and injection, addressing the nuances that differentiate them despite their overlapping characteristics. To enhance the evaluation of LLM-based applications, we propose a novel framework that categorizes LLM responses into five distinct classes, moving beyond the traditional binary classification. This approach provides more granular insights into the AI\u2019s behavior, improving diagnostic precision and enabling more targeted enhancements to the system\u2019s safety and robustness.",
      "year": 2024,
      "venue": "BigData Congress [Services Society]",
      "authors": [
        "Baha Rababah",
        "Shang Wu",
        "Matthew Kwiatkowski",
        "C. Leung",
        "C. Akcora"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/9259d06eeaae42b05ad22ba76f0a1cbb216ad63a",
      "pdf_url": "http://arxiv.org/pdf/2410.13901",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7987b79006e4fff5a78b9beac3dc0c8bbd5fd4fb",
      "title": "Attribution-guided Adversarial Code Prompt Generation for Code Completion Models",
      "abstract": "Large language models have made significant progress in code completion, which may further remodel future software development. However, these code completion models are found to be highly risky as they may introduce vulnerabilities unintentionally or be induced by a special input, i.e., adversarial code prompt. Prior studies mainly focus on the robustness of these models, but their security has not been fully analyzed.In this paper, we propose a novel approach AdvPro that can automatically generate adversarial code prompts for these code completion models. AdvPro incorporates 14 code mutation strategies at the granularity of five levels. The mutation strategies are ensured to make no modifications to code semantics, which should be insensitive to the models. Moreover, we leverage gradient attribution to localize the important code as mutation points and speed up adversarial prompt generation. Extensive experiments are conducted on 13 state-of-the-art models belonging to 7 families. The results show that our approach can effectively generate adversarial prompts, with an increased rate of 69.6% beyond the baseline ALERT. By comparing the results of attribution-guided localization, we find that the recognition results of important tokens in input codes are almost identical among different models. This finding reduces the limitation of using open-source alternative models to guide adversarial attacks against closed-source models. The results of the ablation study on the components of AdvPro show that CCMs focus on variable names, but other structures are equally crucial.CCS CONCEPTS\u2022 Software and its engineering \u2192 Software testing and debugging; \u2022 Security and privacy \u2192 Software security engineering.",
      "year": 2024,
      "venue": "International Conference on Automated Software Engineering",
      "authors": [
        "Xueyang Li",
        "Guozhu Meng",
        "Shangqing Liu",
        "Lu Xiang",
        "Kun Sun",
        "Kai Chen",
        "Xiapu Luo",
        "Yang Liu"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/7987b79006e4fff5a78b9beac3dc0c8bbd5fd4fb",
      "pdf_url": "",
      "publication_date": "2024-10-27",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8bbbc480c26a7f3665ba4c01a2058f0df8072604",
      "title": "Improved Large Language Model Jailbreak Detection via Pretrained Embeddings",
      "abstract": "The adoption of large language models (LLMs) in many applications, from customer service chat bots and software development assistants to more capable agentic systems necessitates research into how to secure these systems. Attacks like prompt injection and jailbreaking attempt to elicit responses and actions from these models that are not compliant with the safety, privacy, or content policies of organizations using the model in their application. In order to counter abuse of LLMs for generating potentially harmful replies or taking undesirable actions, LLM owners must apply safeguards during training and integrate additional tools to block the LLM from generating text that abuses the model. Jailbreaking prompts play a vital role in convincing an LLM to generate potentially harmful content, making it important to identify jailbreaking attempts to block any further steps. In this work, we propose a novel approach to detect jailbreak prompts based on pairing text embeddings well-suited for retrieval with traditional machine learning classification algorithms. Our approach outperforms all publicly available methods from open source LLM security applications.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Erick Galinkin",
        "Martin Sablotny"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/8bbbc480c26a7f3665ba4c01a2058f0df8072604",
      "pdf_url": "",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "be6724eff8c5795f9ab6e17c9cf8a28aeaf7f1ee",
      "title": "Trust No AI: Prompt Injection Along The CIA Security Triad",
      "abstract": "The CIA security triad - Confidentiality, Integrity, and Availability - is a cornerstone of data and cybersecurity. With the emergence of large language model (LLM) applications, a new class of threat, known as prompt injection, was first identified in 2022. Since then, numerous real-world vulnerabilities and exploits have been documented in production LLM systems, including those from leading vendors like OpenAI, Microsoft, Anthropic and Google. This paper compiles real-world exploits and proof-of concept examples, based on the research conducted and publicly documented by the author, demonstrating how prompt injection undermines the CIA triad and poses ongoing risks to cybersecurity and AI systems at large.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Johann Rehberger"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/be6724eff8c5795f9ab6e17c9cf8a28aeaf7f1ee",
      "pdf_url": "",
      "publication_date": "2024-12-08",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0f062fc45267bdc89bc79aba19caed9c671739b",
      "title": "Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks",
      "abstract": "Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples.Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways.In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates.We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023).To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.",
      "year": 2024,
      "venue": "SCALELLM",
      "authors": [
        "Zhifan Sun",
        "Antonio Valerio Miceli Barone"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/d0f062fc45267bdc89bc79aba19caed9c671739b",
      "pdf_url": "",
      "publication_date": "2024-03-14",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "95be2ab522131b725b79062db7ceb849c924562e",
      "title": "Enhancing System Security: LLM-Driven Defense Against Prompt Injection Vulnerabilities",
      "abstract": "This article examines cybersecurity vulnerabilities in systems utilizing Language Model Interfaces, focusing on the challenges of building secure systems. It provides an overview of current interfaces and their associated risks. A key contribution is the design of a prompt analysis and injection detection subsystem, which assesses input relevance and security. The integration of an additional Filter level in the system safeguards against prompt injection attacks by pre-processing user requests and post-processing responses. This level includes a Prompt Analyzer for input wrapping, and an Attack Validator module using the LLM to classify prompts. The study evaluates various prompt injection attacks across different system setups, including core GPT -3 and GPT-4 models without additional security layers, a Fuzzy Search method, and systems using an Attack Validator based on GPT-3 and GPT -4 models.",
      "year": 2024,
      "venue": "International Conference on Modern Problems of Radio Engineering, Telecommunications and Computer Science",
      "authors": [
        "Oleksandr Muliarevych"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/95be2ab522131b725b79062db7ceb849c924562e",
      "pdf_url": "",
      "publication_date": "2024-10-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "251f5cc029ee9648a8cda5e7386a61dda686d40a",
      "title": "SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis",
      "abstract": "The increasing integration of large language models (LLMs) across various fields has heightened concerns about their potential to propagate dangerous information. This paper specifically explores the security vulnerabilities of LLMs within the field of chemistry, particularly their capacity to provide instructions for synthesizing hazardous substances. We evaluate the effectiveness of several prompt injection attack methods, including red-teaming, explicit prompting, and implicit prompting. Additionally, we introduce a novel attack technique named SMILES-prompting, which uses the Simplified Molecular-Input Line-Entry System (SMILES) to reference chemical substances. Our findings reveal that SMILES-prompting can effectively bypass current safety mechanisms. These findings highlight the urgent need for enhanced domain-specific safeguards in LLMs to prevent misuse and improve their potential for positive social impact.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Aidan Wong",
        "He Cao",
        "Zijing Liu",
        "Yu Li"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/251f5cc029ee9648a8cda5e7386a61dda686d40a",
      "pdf_url": "",
      "publication_date": "2024-10-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "697bb7f0de245cb50e39dd585ac8c2d033cb51b8",
      "title": "Protection of LLM Environment Using Prompt Security",
      "abstract": "The emergence of generative AI has also brought significant attention to large language models (LLMs). LLMs are a type of generative AI that can understand users' commands and respond accordingly by generating natural human language text. However, such advancements in LLM technology have also been accompanied by an increase in hostile attempts to use the technology for malicious attacks. For example, some may execute a jailbreak on an LLM, allowing it to generate malicious content, and others can use LLMs to generate vulnerability exploit codes, taking advantage of their powerful capabilities to attempt malicious attacks. Furthermore, there are increasing numbers of cases reported where, once inadvertently entered, personally identifiable information (PII) is used to train LLMs without consent. Preventing the malicious use of LLMs and their unauthorized use of PII requires thoroughly scanning their inputs and outputs. Specifically, LLMs' inputs and outputs need to be checked for the presence of PII, malicious codes, malicious URLs, and commands to execute prompt injections. This paper presents a comprehensive description of the LLM prompt detection system (LPDS) designed to detect the presence of PII, malicious codes, malicious URLs, and prompt injection commands in LLMs' inputs and outputs. The effectiveness of the LPDS in detecting LLMs' unauthorized use of PII and LLM-based malicious attacks is also assessed.",
      "year": 2024,
      "venue": "Information and Communication Technology Convergence",
      "authors": [
        "Minjae Kim",
        "Taehyeong Kwon",
        "K. Shim",
        "Beonghoon Kim"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/697bb7f0de245cb50e39dd585ac8c2d033cb51b8",
      "pdf_url": "",
      "publication_date": "2024-10-16",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1267a4662ce1b4a9febfd5de2b1e48e35d1b7204",
      "title": "Adversarial Prompt Distillation for Vision-Language Models",
      "abstract": "Large pre-trained Vision-Language Models (VLMs) such as Contrastive Language-Image Pre-training (CLIP) have been shown to be susceptible to adversarial attacks, raising concerns about their deployment in safety-critical applications like autonomous driving and medical diagnosis. One promising approach for robustifying pre-trained VLMs is Adversarial Prompt Tuning (APT), which applies adversarial training during the process of prompt tuning. However, existing APT methods are mostly single-modal methods that design prompt(s) for only the visual or textual modality, limiting their effectiveness in either robustness or clean accuracy. In this work, we propose Adversarial Prompt Distillation (APD), a bimodal knowledge distillation framework that enhances APT by integrating it with multi-modal knowledge transfer. APD optimizes prompts for both visual and textual modalities while distilling knowledge from a clean pre-trained teacher CLIP model. Extensive experiments on multiple benchmark datasets demonstrate the superiority of our APD method over the current state-of-the-art APT methods in terms of both adversarial robustness and clean accuracy. The effectiveness of APD also validates the possibility of using a non-robust teacher to improve the generalization and robustness of fine-tuned VLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Lin Luo",
        "Xin Wang",
        "Bojia Zi",
        "Shihao Zhao",
        "Xingjun Ma"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1267a4662ce1b4a9febfd5de2b1e48e35d1b7204",
      "pdf_url": "",
      "publication_date": "2024-11-22",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
      "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
      "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop \\textit{LinkPrompt}, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of \\textit{LinkPrompt}, as well as the transferability of UATs generated by \\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at https://github.com/SavannahXu79/LinkPrompt.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Yue Xu",
        "Wenjie Wang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
      "pdf_url": "",
      "publication_date": "2024-03-25",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2fffb353658efbb4b8c6e4d336e41095ea8fbf90",
      "title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi",
      "abstract": "This study identifies the potential vulnerabilities of Large Language Models (LLMs) to \u2018jailbreak\u2019 attacks, specifically focusing on the Arabic language and its various forms. While most research has concentrated on English-based prompt manipulation, our investigation broadens the scope to investigate the Arabic language. We initially tested the AdvBench benchmark in Standardized Arabic, finding that even with prompt manipulation techniques like prefix injection, it was insufficient to provoke LLMs into generating unsafe content. However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet. Our findings suggest that using Arabic and its various forms could expose information that might remain hidden, potentially increasing the risk of jailbreak attacks. We hypothesize that this exposure could be due to the model\u2019s learned connection to specific words, highlighting the need for more comprehensive safety training across all language forms.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Mansour Al Ghanim",
        "Saleh Almohaimeed",
        "Meng Zheng",
        "Yan Solihin",
        "Qian Lou"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2fffb353658efbb4b8c6e4d336e41095ea8fbf90",
      "pdf_url": "",
      "publication_date": "2024-06-26",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd3e385d19cdfeee9a51402dd59f7231da07f1dd",
      "title": "Deferred NAM: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming ASR",
      "abstract": "Contextual biasing enables speech recognizers to transcribe important phrases in the speaker's context, such as contact names, even if they are rare in, or absent from, the training data. Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time components. Such biasers typically consist of a context encoder; followed by a context filter which narrows down the context to apply, improving per-step inference time; and, finally, context application via cross attention. Though much work has gone into optimizing per-frame performance, the context encoder is at least as important: recognition cannot begin before context encoding ends. Here, we show the lightweight phrase selection pass can be moved before context encoding, resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K phrases with a maximum pre-decoding delay under 33ms. With the addition of phrase- and wordpiece-level cross-entropy losses, our technique also achieves up to a 37.5% relative WER reduction over the baseline without the losses and lightweight phrase selection pass.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Zelin Wu",
        "Gan Song",
        "Christopher Li",
        "Pat Rondon",
        "Zhong Meng",
        "Xavier Velez",
        "Weiran Wang",
        "D. Caseiro",
        "G. Pundak",
        "Tsendsuren Munkhdalai",
        "Angad Chandorkar",
        "Rohit Prabhavalkar"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/fd3e385d19cdfeee9a51402dd59f7231da07f1dd",
      "pdf_url": "",
      "publication_date": "2024-04-15",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1526da61cfee7a9f4f56437d805e75a5c2fac98c",
      "title": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage",
      "abstract": "Recent studies have discovered that large language models (LLM) may be ``fooled''to output private information, including training data, system prompts, and personally identifiable information, under carefully crafted adversarial prompts. Existing red-teaming approaches for privacy leakage either rely on manual efforts or focus solely on system prompt extraction, making them ineffective for severe risks of training data leakage. We propose LeakAgent, a novel black-box red-teaming framework for LLM privacy leakage. Our framework trains an open-source LLM through reinforcement learning as the attack agent to generate adversarial prompts for both training data extraction and system prompt extraction. To achieve this, we propose a novel reward function to provide effective and fine-grained rewards and design novel mechanisms to balance exploration and exploitation during learning and enhance the diversity of adversarial prompts. Through extensive evaluations, we first show that LeakAgent significantly outperforms existing rule-based approaches in training data extraction and automated methods in system prompt leakage. We also demonstrate the effectiveness of LeakAgent in extracting system prompts from real-world applications in OpenAI's GPT Store. We further demonstrate LeakAgent's effectiveness in evading the existing guardrail defense and its helpfulness in enabling better safety alignment. Finally, we validate our customized designs through a detailed ablation study. We release our code here https://github.com/rucnyz/LeakAgent.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Yuzhou Nie",
        "Zhun Wang",
        "Ye Yu",
        "Xian Wu",
        "Xuandong Zhao",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/1526da61cfee7a9f4f56437d805e75a5c2fac98c",
      "pdf_url": "",
      "publication_date": "2024-12-07",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "77dd2a11dd548f6d2e41d567db7f79aa38c07957",
      "title": "Progressive Multi-modal Conditional Prompt Tuning",
      "abstract": "Pre-trained vision-language models (VLMs) have shown remarkable generalization capabilities via prompting, which leverages VLMs as knowledge bases to extract information beneficial for downstream tasks. However, existing methods primarily employ uni-modal prompting, which only engages a uni-modal branch, failing to simultaneously adjust vision-language (V-L) features. Additionally, the one-pass forward pipeline in VLM encoding struggles to align V-L features that have a huge gap. Confronting these challenges, we propose a novel method, Progressive Multi-modal conditional Prompt Tuning (ProMPT). ProMPT exploits a recurrent structure, optimizing and aligning V-L features by iteratively utilizing image and current encoding information. It comprises an initialization and a multi-modal iterative evolution (MIE) module. Initialization is responsible for encoding images and text using a VLM, followed by a feature filter that selects text features similar to image. MIE then facilitates multi-modal prompting through class-conditional vision prompting, instance-conditional text prompting, and feature filtering. In each MIE iteration, vision prompts are obtained from filtered text features via a vision generator, promoting image features to focus more on target object during vision prompting. The encoded image features are fed into a text generator to produce text prompts that are more robust to class shifts. Thus, V-L features are progressively aligned, enabling advance from coarse to exact prediction. Extensive experiments are conducted in three settings to evaluate the efficacy of ProMPT. The results indicate that ProMPT outperforms existing methods on average across all settings, demonstrating its superior generalization and robustness. Code is available at https://github.com/qiuxiaoyu9954/ProMPT.",
      "year": 2024,
      "venue": "International Conference on Multimedia Retrieval",
      "authors": [
        "Xiaoyu Qiu",
        "Hao Feng",
        "Yuechen Wang",
        "Wen-gang Zhou",
        "Houqiang Li"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/77dd2a11dd548f6d2e41d567db7f79aa38c07957",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3652583.3658049",
      "publication_date": "2024-04-18",
      "keywords_matched": [
        "prompt exploit"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ad305e9aa761b2c2276d807a19faca0efb78212",
      "title": "EconoFormer: A Novel Macroeconomic Policy Analysis and Implementation Planner Using Generative Transformer Model",
      "abstract": "Macroeconomic policy analysis and implementation planning are critical for economic growth, increasing employment rates, and ensuring price stability. It has become challenging to keep pace with today\u2019s fast-evolving, technology-driven economy, as policy analysis is a time-consuming process. This paper proposes a Generative Transformer (GT)-based macroeconomic policy analysis and implementation planner model, EconoFormer, designed with 1 billion parameters and trained on 5358 pages of documents related to macroeconomic policies. It achieved a perplexity score of 12.3, which indicates high prediction confidence. The innovative prompt filtering mechanism incorporated with it blocks irrelevant prompts with 98.22% accuracy. The EconoFormer model is scalable and maintains a linear relationship with the processing time and the number of policy analyses while maintaining a stable accuracy, precision, recall, and F1-score. Moreover, the performance remains stable for wide ranges of macroeconomic policies from different sectors. Most importantly, the policy impact rating similarity test shows that it is as good as macroeconomics policy experts in policy analysis. The EconoFormer has the capability to process real-time data in prompts, establish non-linear relations among 44 different economic indicators, and develop effective and plans for policy implementation. The unique concept, robust capability, and outstanding performance make the EconoFormer a potential framework for rapid macroeconomic policy analysis and implementation plan development.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "Terry Zhao",
        "Nuruzzaman Faruqui"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/2ad305e9aa761b2c2276d807a19faca0efb78212",
      "pdf_url": "https://doi.org/10.1109/access.2024.3512594",
      "publication_date": null,
      "keywords_matched": [
        "prompt filtering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "271266265fa50cde314ebdd428a3d63353c117ba",
      "title": "Self-interpreting Adversarial Images",
      "abstract": "We introduce a new type of indirect, cross-modal injection attacks against visual language models that enable creation of self-interpreting images. These images contain hidden\"meta-instructions\"that control how models answer users' questions about the image and steer models' outputs to express an adversary-chosen style, sentiment, or point of view. Self-interpreting images act as soft prompts, conditioning the model to satisfy the adversary's (meta-)objective while still producing answers based on the image's visual content. Meta-instructions are thus a stronger form of prompt injection. Adversarial images look natural and the model's answers are coherent and plausible, yet they also follow the adversary-chosen interpretation, e.g., political spin, or even objectives that are not achievable with explicit text instructions. We evaluate the efficacy of self-interpreting images for a variety of models, interpretations, and user prompts. We describe how these attacks could cause harm by enabling creation of self-interpreting content that carries spam, misinformation, or spin. Finally, we discuss defenses.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Tingwei Zhang",
        "Collin Zhang",
        "John X. Morris",
        "Eugene Bagdasarian",
        "Vitaly Shmatikov"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/271266265fa50cde314ebdd428a3d63353c117ba",
      "pdf_url": "",
      "publication_date": "2024-07-12",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a378012f21bf34c79a0520ea0ad89421b762a658",
      "title": "Trojan Prompt Attacks on Graph Neural Networks",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Min Lin",
        "Zhiwei Zhang",
        "Enyan Dai",
        "Zongyu Wu",
        "Yilong Wang",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a378012f21bf34c79a0520ea0ad89421b762a658",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "79943668a4f7984ff7e14c9bda3b7d630a0c39ad",
      "title": "Evaluating LLM-based Personal Information Extraction and Countermeasures",
      "abstract": "Automatically extracting personal information -- such as name, phone number, and email address -- from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods -- such as regular expression, keyword search, and entity detection -- achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.",
      "year": 2024,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/79943668a4f7984ff7e14c9bda3b7d630a0c39ad",
      "pdf_url": "",
      "publication_date": "2024-08-14",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb9d4cb89ad9a47b1f3dbe20417eb31397d09b53",
      "title": "Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective",
      "abstract": "Jailbreak prompts pose a significant threat in AI and cybersecurity, as they are crafted to bypass ethical safeguards in large language models, potentially enabling misuse by cybercriminals. This paper analyzes jailbreak prompts from a cyber defense perspective, exploring techniques like prompt injection and context manipulation that allow harmful content generation, content filter evasion, and sensitive information extraction. We assess the impact of successful jailbreaks, from misinformation and automated social engineering to hazardous content creation, including bioweapons and explosives. To address these threats, we propose strategies involving advanced prompt analysis, dynamic safety protocols, and continuous model fine-tuning to strengthen AI resilience. Additionally, we highlight the need for collaboration among AI researchers, cybersecurity experts, and policymakers to set standards for protecting AI systems. Through case studies, we illustrate these cyber defense approaches, promoting responsible AI practices to maintain system integrity and public trust. \\textbf{\\color{red}Warning: This paper contains content which the reader may find offensive.}",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jean Marie Tshimula",
        "Xavier Ndona",
        "D'Jeff K. Nkashama",
        "Pierre-Martin Tardif",
        "F. Kabanza",
        "Marc Frappier",
        "Shengrui Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/eb9d4cb89ad9a47b1f3dbe20417eb31397d09b53",
      "pdf_url": "",
      "publication_date": "2024-11-25",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1d09d90d50819402998d15ca45163806a72f31b0",
      "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference",
      "abstract": "We study how to subvert large language models (LLMs) from following prompt-specified rules. We first formalize rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form\"if $P$ and $Q$, then $R$\"for some propositions $P$, $Q$, and $R$. Next, we prove that although small transformers can faithfully follow such rules, maliciously crafted prompts can still mislead both theoretical constructions and models learned from data. Furthermore, we demonstrate that popular attack algorithms on LLMs find adversarial prompts and induce attention patterns that align with our theory. Our novel logic-based framework provides a foundation for studying LLMs in rule-based settings, enabling a formal analysis of tasks like logical reasoning and jailbreak attacks.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Anton Xue",
        "Avishree Khare",
        "Rajeev Alur",
        "Surbhi Goel",
        "Eric Wong"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/1d09d90d50819402998d15ca45163806a72f31b0",
      "pdf_url": "",
      "publication_date": "2024-06-21",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2d8f36b0a1b2119511c6174c568e97ed8094a2c2",
      "title": "ShadowCode: Towards (Automatic) External Prompt Injection Attack against Code LLMs",
      "abstract": "Recent advancements have led to the widespread adoption of code-oriented large language models (Code LLMs) for programming tasks. Despite their success in deployment, their security research is left far behind. This paper introduces a new attack paradigm: (automatic) external prompt injection against Code LLMs, where attackers generate concise, non-functional induced perturbations and inject them within a victim's code context. These induced perturbations can be disseminated through commonly used dependencies (e.g., packages or RAG's knowledge base), manipulating Code LLMs to achieve malicious objectives during the code completion process. Compared to existing attacks, this method is more realistic and threatening: it does not necessitate control over the model's training process, unlike backdoor attacks, and can achieve specific malicious objectives that are challenging for adversarial attacks. Furthermore, we propose ShadowCode, a simple yet effective method that automatically generates induced perturbations based on code simulation to achieve effective and stealthy external prompt injection. ShadowCode designs its perturbation optimization objectives by simulating realistic code contexts and employs a greedy optimization approach with two enhancement modules: forward reasoning enhancement and keyword-based perturbation design. We evaluate our method across 13 distinct malicious objectives, generating 31 threat cases spanning three popular programming languages. Our results demonstrate that ShadowCode successfully attacks three representative open-source Code LLMs (achieving up to a 97.9% attack success rate) and two mainstream commercial Code LLM-integrated applications (with over 90% attack success rate) across all threat cases, using only a 12-token non-functional induced perturbation. The code is available at https://github.com/LianPing-cyber/ShadowCodeEPI.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Yuchen Yang",
        "Yiming Li",
        "Hongwei Yao",
        "Bingrun Yang",
        "Yiling He",
        "Tianwei Zhang",
        "Dacheng Tao",
        "Zhan Qin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/2d8f36b0a1b2119511c6174c568e97ed8094a2c2",
      "pdf_url": "",
      "publication_date": "2024-07-12",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0b708c14a822691047238cfc50dc15312ee8cef1",
      "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents",
      "abstract": "With the rapid development of Large Language Models (LLMs), numerous mature applications of LLMs have emerged in the field of content safety detection. However, we have found that LLMs exhibit blind trust in safety detection agents. The general LLMs can be compromised by hackers with this vulnerability. Hence, this paper proposed an attack named Feign Agent Attack (F2A).Through such malicious forgery methods, adding fake safety detection results into the prompt, the defense mechanism of LLMs can be bypassed, thereby obtaining harmful content and hijacking the normal conversation. Continually, a series of experiments were conducted. In these experiments, the hijacking capability of F2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons why LLMs blindly trust safety detection results. The experiments involved various scenarios where fake safety detection results were injected into prompts, and the responses were closely monitored to understand the extent of the vulnerability. Also, this paper provided a reasonable solution to this attack, emphasizing that it is important for LLMs to critically evaluate the results of augmented agents to prevent the generating harmful content. By doing so, the reliability and security can be significantly improved, protecting the LLMs from F2A.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yupeng Ren"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0b708c14a822691047238cfc50dc15312ee8cef1",
      "pdf_url": "",
      "publication_date": "2024-10-11",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fb4ce639668fde81c86ab4b442a579ec7ac1d3e1",
      "title": "FDI: Attack Neural Code Generation Systems through User Feedback Channel",
      "abstract": "Neural code generation systems have recently attracted increasing attention to improve developer productivity and speed up software development. Typically, these systems maintain a pre-trained neural model and make it available to general users as a service (e.g., through remote APIs) and incorporate a feedback mechanism to extensively collect and utilize the users' reaction to the generated code, i.e., user feedback. However, the security implications of such feedback have not yet been explored. With a systematic study of current feedback mechanisms, we find that feedback makes these systems vulnerable to feedback data injection (FDI) attacks. We discuss the methodology of FDI attacks and present a pre-attack profiling strategy to infer the attack constraints of a targeted system in the black-box setting. We demonstrate two proof-of-concept examples utilizing the FDI attack surface to implement prompt injection attacks and backdoor attacks on practical neural code generation systems. The attacker may stealthily manipulate a neural code generation system to generate code with vulnerabilities, attack payload, and malicious and spam messages. Our findings reveal the security implications of feedback mechanisms in neural code generation systems, paving the way for increasing their security.",
      "year": 2024,
      "venue": "International Symposium on Software Testing and Analysis",
      "authors": [
        "Zhensu Sun",
        "Xiaoning Du",
        "Xiapu Luo",
        "Fu Song",
        "David Lo",
        "Li Li"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/fb4ce639668fde81c86ab4b442a579ec7ac1d3e1",
      "pdf_url": "",
      "publication_date": "2024-08-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54c067a55a3ed999f585f053f5041cf0e6f1f723",
      "title": "Universal and Context-Independent Triggers for Precise Control of LLM Outputs",
      "abstract": "Large language models (LLMs) have been widely adopted in applications such as automated content generation and even critical decision-making systems. However, the risk of prompt injection allows for potential manipulation of LLM outputs. While numerous attack methods have been documented, achieving full control over these outputs remains challenging, often requiring experienced attackers to make multiple attempts and depending heavily on the prompt context. Recent advancements in gradient-based white-box attack techniques have shown promise in tasks like jailbreaks and system prompt leaks. Our research generalizes gradient-based attacks to find a trigger that is (1) Universal: effective irrespective of the target output; (2) Context-Independent: robust across diverse prompt contexts; and (3) Precise Output: capable of manipulating LLM inputs to yield any specified output with high accuracy. We propose a novel method to efficiently discover such triggers and assess the effectiveness of the proposed attack. Furthermore, we discuss the substantial threats posed by such attacks to LLM-based applications, highlighting the potential for adversaries to taking over the decisions and actions made by AI agents.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiashuo Liang",
        "Guancheng Li",
        "Yang Yu"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/54c067a55a3ed999f585f053f5041cf0e6f1f723",
      "pdf_url": "",
      "publication_date": "2024-11-22",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8691de1b3861d4f4298e6391a7c11dc377fe79f6",
      "title": "Revisiting the Robust Generalization of Adversarial Prompt Tuning",
      "abstract": "Understanding the vulnerability of large-scale pre-trained vision-language models like CLIP against adversarial attacks is key to ensuring zero-shot generalization capacity on various downstream tasks. State-of-the-art defense mechanisms generally adopt prompt learning strategies for adversarial fine-tuning to improve the adversarial robustness of the pre-trained model while keeping the efficiency of adapting to downstream tasks. Such a setup leads to the problem of over-fitting which impedes further improvement of the model's generalization capacity on both clean and adversarial examples. In this work, we propose an adaptive Consistency-guided Adversarial Prompt Tuning (i.e., CAPT) framework that utilizes multi-modal prompt learning to enhance the alignment of image and text features for adversarial examples and leverage the strong generalization of pre-trained CLIP to guide the model-enhancing its robust generalization on adversarial examples while maintaining its accuracy on clean ones. We also design a novel adaptive consistency objective function to balance the consistency of adversarial inputs and clean inputs between the fine-tuning model and the pre-trained model. We conduct extensive experiments across 14 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show the superiority of CAPT over other state-of-the-art adaption methods. CAPT demonstrated excellent performance in terms of the in-distribution performance and the generalization under input distribution shift and across datasets.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Fan Yang",
        "Mingxuan Xia",
        "Sangzhou Xia",
        "Chicheng Ma",
        "Hui Hui"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/8691de1b3861d4f4298e6391a7c11dc377fe79f6",
      "pdf_url": "",
      "publication_date": "2024-05-18",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "32d1580383e280a07c94744afd9d15f057385d34",
      "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak",
      "abstract": "Large Language Models (LLMs) are susceptible to generating harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreak methods is critical to enhancing security and aligning models with human values. Traditionally, jailbreak techniques have relied on suffix addition or prompt templates, but these methods suffer from limited attack diversity. This paper introduces DiffusionAttacker, an end-to-end generative approach for jailbreak rewriting inspired by diffusion models. Our method employs a sequence-to-sequence (seq2seq) text diffusion model as a generator, conditioning on the original prompt and guiding the denoising process with a novel attack loss. Unlike previous approaches that use autoregressive LLMs to generate jailbreak prompts, which limit the modification of already generated tokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq diffusion model, allowing more flexible token modifications. This approach preserves the semantic content of the original prompt while producing harmful content. Additionally, we leverage the Gumbel-Softmax technique to make the sampling process from the diffusion model's output distribution differentiable, eliminating the need for iterative token search. Extensive experiments on Advbench and Harmbench demonstrate that DiffusionAttacker outperforms previous methods across various evaluation metrics, including attack success rate (ASR), fluency, and diversity.",
      "year": 2024,
      "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hao Wang",
        "Hao Li",
        "Junda Zhu",
        "Xinyuan Wang",
        "Chengwei Pan",
        "Minlie Huang",
        "Lei Sha"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/32d1580383e280a07c94744afd9d15f057385d34",
      "pdf_url": "",
      "publication_date": "2024-12-23",
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "79a6f55042b8faa9e0dec67cf11dc2c63107678a",
      "title": "Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning",
      "abstract": "Prompt injection (both direct and indirect) and jailbreaking are now recognized as significant issues for large language models (LLMs), particularly due to their potential for harm in application-integrated contexts. This extended abstract explores a novel approach to protecting LLMs from such attacks, termed\"soft begging.\"This method involves training soft prompts to counteract the effects of corrupted prompts on the LLM's output. We provide an overview of prompt injections and jailbreaking, introduce the theoretical basis of the\"soft begging\"technique, and discuss an evaluation of its effectiveness.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Simon Ostermann",
        "Kevin Baum",
        "Christoph Endres",
        "Julia Masloh",
        "P. Schramowski"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/79a6f55042b8faa9e0dec67cf11dc2c63107678a",
      "pdf_url": "",
      "publication_date": "2024-07-03",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b10f3ddf04389c59f56ec8f04d251d010adf9a3b",
      "title": "Mitigating Input Prompt Attack Vulnerabilities in Systems with a Language Model Interface",
      "abstract": "The article is devoted to the study of the cybersecurity vulnerabilities in systems utilizing Language Model Interfaces, particularly focusing on the challenges and solutions in constructing secure systems. The paper includes an overview of the current Language Model Interface and issues with their utilization in typical systems, and because of the conducted analysis, proposes a multi-level structural schema of cybersecurity frameworks and standards, that could be applied to cover the different types of risks. The suggested prompt analysis and injection detection sub-system design, in which evaluates whether the input is relevant and appropriate from the security point of view. The additional Filter cybersecurity level integrated into the system with the Language Model Interface allows protection from prompt injection attacks during the pre-processing of user requests and post-processing of responses. The Filter level consists mainly of a Prompt Analyzer, which wraps pre-processed input prompt; an Attack Validator module using a helper Large Language Model, which classifies if the prompt is risky or safe; Elasticsearch cluster for logging and monitoring purposes. During testing different types of prompt injection attacks were evaluated in different types of system setups: core GPT-3 and GPT-4 models without additional security layer; Fuzzy Search with attacking prompt pattern vocabulary; a system with Attack Validator in two modes - based on GPT-3 model and GPT-4 model. The article shows the relative strengths and weaknesses of each approach, contributing valuable insights into the development of more secure systems with Language Model Interface.",
      "year": 2024,
      "venue": "International Conference on Dependable Systems, Services and Technologies",
      "authors": [
        "O. Muliarevych"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/b10f3ddf04389c59f56ec8f04d251d010adf9a3b",
      "pdf_url": "",
      "publication_date": "2024-10-11",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "85a55efba869d51dafbb53f74fa241ea81cea10b",
      "title": "Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models",
      "abstract": "This study sheds light on the imperative need to bolster safety and privacy measures in large language models (LLMs), such as GPT-4 and LLaMA-2, by identifying and mitigating their vulnerabilities through explainable analysis of prompt attacks. We propose Counterfactual Explainable Incremental Prompt Attack (CEIPA), a novel technique where we guide prompts in a specific manner to quantitatively measure attack effectiveness and explore the embedded defense mechanisms in these models. Our approach is distinctive for its capacity to elucidate the reasons behind the generation of harmful responses by LLMs through an incremental counterfactual methodology. By organizing the prompt modification process into four incremental levels: (word, sentence, character, and a combination of character and word) we facilitate a thorough examination of the susceptibilities inherent to LLMs. The findings from our study not only provide counterfactual explanation insight but also demonstrate that our framework significantly enhances the effectiveness of attack prompts.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Dong Shu",
        "Mingyu Jin",
        "Tianle Chen",
        "Chong Zhang",
        "Yongfeng Zhang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/85a55efba869d51dafbb53f74fa241ea81cea10b",
      "pdf_url": "",
      "publication_date": "2024-07-12",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2348782cf3c197d13a381cec95103a293589ed3",
      "title": "Cross-Prompt Adversarial Attack on Segment Anything Model",
      "abstract": "Segment Anything Model (SAM) proposes promptable image segmentation based on various types of prompts like points and boxes. Although SAM presents impressive performance on segmentation and provides unparalleled versatility, it still suffers from the threat of adversarial attacks. In this paper, we investigate the problem of cross-prompt adversarial attacks on SAM, which considers whether the adversarial samples obtained by attacking with one or more prompts can lead to incorrect masks under other unseen test prompts. We analyze the factors influencing cross-prompt attacks and explore attacks on different numbers of prompts. Based on the analysis, we propose Omni-Attack-SAM, an innovative and effective attack method to produce adversarial samples that are transferable to unseen prompts. Our experiments on SA-1B and the Pascal VOC2012 dataset demonstrate that our method can decrease the mIoU value by over 35% compared to existing adversarial attack methods without using the ground truth of images. Additionally, we can also combine our method with the prompt information to achieve superior attack performance in various scenes.",
      "year": 2024,
      "venue": "ICCBN",
      "authors": [
        "Yuchen Liu",
        "Peng Wei"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/a2348782cf3c197d13a381cec95103a293589ed3",
      "pdf_url": "",
      "publication_date": "2024-07-24",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "622aa3454826b413703b00718d9ec4122dbc3e44",
      "title": "Al-Driven Cybersecurity: Mitigating Prompt Injection Attacks through Adversarial Machine Learning",
      "abstract": "Adversarial Machine Learning (AML) has emerged as both a challenge and an opportunity in the realm of cybersecurity. As malicious actors leverage advanced techniques to deceive machine learning models, the need for robust AI-driven defenses becomes paramount. This paper explores the intersection of AML and cybersecurity, focusing on innovative threat detection and mitigation strategies. We delve into the mechanisms of adversarial attacks, including evasion, poisoning, and model inversion, and examine their impact on critical security systems. Furthermore, we present cutting-edge approaches for enhancing the resilience of machine learning models, such as adversarial training, robust optimization, and ensemble methods. Through practical case studies and simulations, we demonstrate how AML techniques can detect and neutralize cyber threats in real-time, providing a proactive framework for securing networks, data, and applications. This work underscores the importance of integrating AML strategies into cybersecurity protocols, paving the way for more adaptive and intelligent defense mechanisms in the face of evolving threats.",
      "year": 2024,
      "venue": "NPRC Journal of Multidisciplinary Research",
      "authors": [
        "Saroj Ghimire",
        "Suman Thapaliya"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/622aa3454826b413703b00718d9ec4122dbc3e44",
      "pdf_url": "",
      "publication_date": "2024-12-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "701c10f6df1ace15c321d265e3488c613f009c19",
      "title": "PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning",
      "abstract": "Pre-trained language models (PLMs) have attracted enormous attention over the past few years with their unparalleled performances. Meanwhile, the soaring cost to train PLMs as well as their amazing generalizability have jointly contributed to few-shot fine-tuning and prompting as the most popular training paradigms for natural language processing (NLP) models. Nevertheless, existing studies have shown that these NLP models can be backdoored such that model behavior is manipulated when trigger tokens are presented.In this paper, we propose PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial prompt-tuning in few-shot settings.Unlike existing NLP backdoor removal methods, which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix keeps the model parameters intact and only utilizes two extra sets of soft tokens which approximate the trigger and counteract it respectively. The use of soft tokens and adversarial optimization eliminates the need to enumerate possible backdoor configurations and enables an adaptive balance between trigger finding and preservation of performance.Experiments with various backdoor attacks validate the effectiveness of the proposed method and the performances when domain shift is present further shows PromptFix\u2019s applicability to models pretrained on unknown data source which is the common case in prompt tuning scenarios.",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Tianrong Zhang",
        "Zhaohan Xi",
        "Ting Wang",
        "Prasenjit Mitra",
        "Jinghui Chen"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/701c10f6df1ace15c321d265e3488c613f009c19",
      "pdf_url": "",
      "publication_date": "2024-06-01",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04672b33008599426f23564e3d092b73ec49bf51",
      "title": "Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation",
      "abstract": "Automatic adversarial prompt generation provides remarkable success in jailbreaking safely-aligned large language models (LLMs). Existing gradient-based attacks, while demonstrating outstanding performance in jailbreaking white-box LLMs, often generate garbled adversarial prompts with chaotic appearance. These adversarial prompts are difficult to transfer to other LLMs, hindering their performance in attacking unknown victim models. In this paper, for the first time, we delve into the semantic meaning embedded in garbled adversarial prompts and propose a novel method that\"translates\"them into coherent and human-readable natural language adversarial prompts. In this way, we can effectively uncover the semantic information that triggers vulnerabilities of the model and unambiguously transfer it to the victim model, without overlooking the adversarial information hidden in the garbled text, to enhance jailbreak attacks. It also offers a new approach to discovering effective designs for jailbreak prompts, advancing the understanding of jailbreak attacks. Experimental results demonstrate that our method significantly improves the success rate of jailbreak attacks against various safety-aligned LLMs and outperforms state-of-the-arts by large margins. With at most 10 queries, our method achieves an average attack success rate of 81.8% in attacking 7 commercial closed-source LLMs, including GPT and Claude-3 series, on HarmBench. Our method also achieves over 90% attack success rates against Llama-2-Chat models on AdvBench, despite their outstanding resistance to jailbreak attacks. Code at: https://github.com/qizhangli/Adversarial-Prompt-Translator.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Qizhang Li",
        "Xiaochen Yang",
        "Wangmeng Zuo",
        "Yiwen Guo"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/04672b33008599426f23564e3d092b73ec49bf51",
      "pdf_url": "",
      "publication_date": "2024-10-15",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1b660f8cf1688feac7afbd3e737ac5138d98c726",
      "title": "Rethinking the Intermediate Features in Adversarial Attacks: Misleading Robotic Models via Adversarial Distillation",
      "abstract": "Language-conditioned robotic learning has significantly enhanced robot adaptability by enabling a single model to execute diverse tasks in response to verbal commands. Despite these advancements, security vulnerabilities within this domain remain largely unexplored. This paper addresses this gap by proposing a novel adversarial prompt attack tailored to language-conditioned robotic models. Our approach involves crafting a universal adversarial prefix that induces the model to perform unintended actions when added to any original prompt. We demonstrate that existing adversarial techniques exhibit limited effectiveness when directly transferred to the robotic domain due to the inherent robustness of discretized robotic action spaces. To overcome this challenge, we propose to optimize adversarial prefixes based on continuous action representations, circumventing the discretization process. Additionally, we identify the beneficial impact of intermediate features on adversarial attacks and leverage the negative gradient of intermediate self-attention features to further enhance attack efficacy. Extensive experiments on VIMA models across 13 robot manipulation tasks validate the superiority of our method over existing approaches and demonstrate its transferability across different model variants.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Ke Zhao",
        "Huayang Huang",
        "Miao Li",
        "Yu Wu"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1b660f8cf1688feac7afbd3e737ac5138d98c726",
      "pdf_url": "",
      "publication_date": "2024-11-21",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "200aba0864586ff9ef84f62dad67a80d77c671f6",
      "title": "Data Breach Prevention in AI Systems: Employing Event-Driven Architecture to Combat Prompt Injection Attacks in Chatbots",
      "abstract": "While incorporating Large Language Models (LLMs) into applications has transformed user experiences, it has also brought about serious security flaws, most notably quick injection attacks. These attacks risk user privacy and data integrity by manipulating system behavior with AI model defects. This study thoroughly reviews the literature on quick injection attacks in chatbots to examine vulnerabilities in AI system integration. It suggests different strategies and synthesizes them into an event-driven architecture (EDA) that is specifically designed to thwart rapid injection assaults by synthesizing the findings of previous research. The suggested EDA offers a scientific breakthrough in comprehending and resolving security challenges in AI system integration by enabling proactive threat identification and mitigation. The results highlight how crucial strong architectural security and systems integration is to defend AI systems from changing threats. The importance of these actions in strengthening cybersecurity standards and safeguarding digital environments is also covered in the report. This study adds to the current conversation around AI system security by offering ideas and viewpoints for more investigation and advancement.",
      "year": 2024,
      "venue": "2024 IEEE 12th International Conference on Information, Communication and Networks (ICICN)",
      "authors": [
        "M. Samonte",
        "Jon Eril R. Aparize",
        "Ethan Joseph S. Gonzales",
        "Joyful L. Morilla"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/200aba0864586ff9ef84f62dad67a80d77c671f6",
      "pdf_url": "",
      "publication_date": "2024-08-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3a8ae0fe18fd081416b58065c4e618ad28836a4b",
      "title": "SPIN: Self-Supervised Prompt INjection",
      "abstract": "Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain as major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. As our self-supervised prompt defense is done at inference-time, it is also compatible with existing alignment and adds an additional layer of safety for defense. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Leon Zhou",
        "Junfeng Yang",
        "Chengzhi Mao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/3a8ae0fe18fd081416b58065c4e618ad28836a4b",
      "pdf_url": "",
      "publication_date": "2024-10-17",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7047e18680bb8f6681ea1a750a4d2ba8c671715a",
      "title": "A test suite of prompt injection attacks for LLM-based machine translation",
      "abstract": "LLM-based NLP systems typically work by embedding their input data into prompt templates which contain instructions and/or in-context examples, creating queries which are submitted to a LLM, and then parsing the LLM response in order to generate the system outputs. Prompt Injection Attacks (PIAs) are a type of subversion of these systems where a malicious user crafts special inputs which interfere with the prompt templates, causing the LLM to respond in ways unintended by the system designer. Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based machine translation. Specifically, the task is to translate questions from the TruthfulQA test suite, where an adversarial prompt is prepended to the questions, instructing the system to ignore the translation instruction and answer the questions instead. In this test suite, we extend this approach to all the language pairs of the WMT 2024 General Machine Translation task. Moreover, we include additional attack formats in addition to the one originally studied.",
      "year": 2024,
      "venue": "Conference on Machine Translation",
      "authors": [
        "Antonio Valerio Miceli Barone",
        "Zhifan Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7047e18680bb8f6681ea1a750a4d2ba8c671715a",
      "pdf_url": "",
      "publication_date": "2024-10-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4b6793d43e7527b152ce0700204a8f3f3dee7928",
      "title": "Graph-Based Prompt Injection Attacks Against Large Language Models",
      "abstract": "As Large language models (LLMs) become widely used, injection attacks that induce violent or discriminatory responses from these models are also becoming more frequent. Initially, many attacks used elaborately designed textual prompts, but recently, with the introduction of models that support multi-modality, many attacks using images in addition to textual prompts have been attempted. Injection attacks using only existing text or images are mostly neutralized by latest safe aligned Large language models. In this paper, we present a new method of replacing sensitive words in the prompt with math expressions and performing an injection attack using a graph drawn based on them. By distributing illegal prompts across various input domains, for intrusion attacks against the latest safe aligned LLMs, our proposed approach achieves a very high success rate compared to SOTA existing injection attacks. We hope that the safety of the multimodal LLMs will be further strengthened by understanding the proposed injection attack.",
      "year": 2024,
      "venue": "2024 IEEE International Conference on Technology, Informatics, Management, Engineering and Environment (TIME-E)",
      "authors": [
        "Hyeokjin Kwon",
        "Jaebeom Kim",
        "Wooguil Pak"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/4b6793d43e7527b152ce0700204a8f3f3dee7928",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97474ab3d27997e468b99e5fdd2498b8180417a0",
      "title": "Optimizing Transformer Models for Prompt Jailbreak Attack Detection in AI Assistant Systems",
      "abstract": "Integrating AI-powered assistants in different areas has changed how people retrieve information. Users can now get quick responses, use the service whenever needed, and handle inquiries well. However, relying on these systems has increasingly raised worries about their safety and reliability, especially when facing threats like prompt injection or jailbreak attacks. These attacks take advantage of the weaknesses of large language models (LLMs) by changing input prompts to create harmful, biased, or misleading results. This paper examines prompt jailbreak attacks in ChatGPT-based assistant chatbots, especially in education. It examines the weaknesses in these systems and suggests ways to detect these attacks by fine-tuning various Transformer models. The research also includes adding prompt jailbreak attack detection in a virtual assistant application for university use. This aims to ensure teachers and students can interact safely and rely on the system. Through rigorous experimentation and evaluation, we demonstrate the practicality and effectiveness of our detection methods, providing reassurance and confidence in the face of AI security challenges. Our studies emphasize the need for robust AI chatbots and offer practical solutions to preserve the integrity of AI-driven tools.",
      "year": 2024,
      "venue": "2024 1st International Conference On Cryptography And Information Security (VCRIS)",
      "authors": [
        "Le Anh Tien",
        "Pham Van Huong"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/97474ab3d27997e468b99e5fdd2498b8180417a0",
      "pdf_url": "",
      "publication_date": "2024-12-03",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fb1bc7f717df24bef2a98d1dab26b0d86f069dd2",
      "title": "AutoRed: Automated Attack Scenario Generation Framework for Red Teaming of LLMs",
      "abstract": "Even though Large Language Models (LLMs) are highly beneficial, they pose significant security concerns, particularly in the realm of privacy protection. Sensitive information is often provided to LLMs during conversations and may be retained as in-context memory. This raises the risk of unintended data exposure. In the existing paradigm, a red team comprising human testers is tasked with generating input prompts (i.e., test cases) to provoke undesirable responses from LLMs. Yet, relying solely on human testers is both costly and time-intensive. This paper presents AutoRed, an innovative learning framework developed to automatically generate malicious attack scenarios for extracting sensitive information from LLMs. Our framework places particular emphasis on prompt injection\u2014the process of injecting malicious prompts to extract conversation histories from LLMs to uncover private data. AutoRed comprises three key components: malicious prompt generator, sensitive information extractor, and stop point identifier. These components work together to enable prompt injection, ensuring a seamless process. Our extensive experimental evaluation, spanning diverse defense strategies and various LLMs, demonstrates the efficacy of AutoRed. This evaluation not only rigorously assesses the resilience of defense mechanisms but also measures the safety alignment of LLMs, thereby highlighting the potential of our automated framework as an efficient red teaming tool for identifying vulnerabilities and enhancing security within LLMs.",
      "year": 2024,
      "venue": "BigData Congress [Services Society]",
      "authors": [
        "Zhe Wang",
        "M. A. Tayebi"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/fb1bc7f717df24bef2a98d1dab26b0d86f069dd2",
      "pdf_url": "",
      "publication_date": "2024-12-15",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b7324677248bb99befa3cd97c2936ae8f2b46f36",
      "title": "Chat GPT and Cyber Risks: Need of Custom Large Language Models",
      "abstract": "Due to growing utilization of Artificial Intelligence (AI) for small, medium, and large business organizations cybersecurity risks are increasing. While AI provides enormous opportunities to generate great insight, it also enables risks on data privacy or network security. Organizations are integrating Chat GPT to enhance customer experience by providing personalization, product recommendations, and immediate responses of users\u2019 queries. However, if underline AI models have not been trained appropriately or default ready to use models have been used to generate responses, the cybersecurity risk will be increased. The paper will discuss about the prompt injection attack in which an attacker can exploit the vulnerabilities of large language models (LLM) to get sensitive data or system specific information to incorporate data breach or other cyber-attacks. The study compares the response of cybersecurity related questions for Chat GPT and Ollama LLM models to discuss the possible cyber risks and suggest the need of custom LLM models for organizations. The paper also provides an approach how these risks can be quantified using concept of Cyber Value at Risk (CVaR).",
      "year": 2024,
      "venue": "2024 IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)",
      "authors": [],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/b7324677248bb99befa3cd97c2936ae8f2b46f36",
      "pdf_url": "",
      "publication_date": "2024-12-05",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a21d27b81956d99d76043490f16b0db09df5a245",
      "title": "AdversLLM: A Practical Guide To Governance, Maturity and Risk Assessment For LLM-Based Applications",
      "abstract": "AdversLLM is a comprehensive framework designed to help organizations tackle security threats associated with the use of Large Language Models (LLMs), such as prompt injections and data poisoning. As LLMs become integral to various industries, the framework aims to bolster organizational readiness and resilience by assessing governance, maturity, and risk mitigation strategies. AdversLLM includes an assessment form for reviewing practices, maturity levels, and auditing mitigation strategies, supplemented with real-world scenarios to demonstrate effective AI governance. Additionally, it features a prompt injection testing ground with a benchmark dataset to evaluate LLMs' robustness against malicious prompts. The framework also addresses ethical concerns by proposing a zero-shot learning defense mechanism and a RAG-based LLM safety tutor to educate on security risks and protection methods. AdversLLM provides a targeted, practical approach for organizations to ensure responsible AI adoption and strengthen their defenses against emerging LLM-related security challenges.",
      "year": 2024,
      "venue": "International Journal on Cybernetics &amp; Informatics",
      "authors": [
        "Othmane Belmoukadam",
        "Jiri De Jonghe",
        "Sofyan Ajridi",
        "Amir Krifa",
        "J. V. Damme",
        "Maher Mkadem",
        "Patrice Latinne"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a21d27b81956d99d76043490f16b0db09df5a245",
      "pdf_url": "https://doi.org/10.5121/ijci.2024.130604",
      "publication_date": "2024-12-20",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "21191bca131aa03fb60043b890e26784a31ac66a",
      "title": "Exploring the Essence of Prompt Tone: A Comparative Study of Large Language Model Security",
      "abstract": "Chatbots powered by Large Language Model(LLM) can be manipulated by malicious prompts, generating harmful content and biased responses which would raise security concerns. Growing dependence on chatbots demands robust security for ethical development and user trust, which makes the work relevant in today\u2019s world. The motivation behind the work is to let the user have a safe experience with no negative responses being displayed while using the chatbot, which paved the way to arrive at the goal of developing a security filter that could be integrated into any LLM feature integrated application to mitigate the risk of having security vulnerabilities like prompt injection and jailbreaking, which could be achieved by converting malicious prompt into safer prompts by the method of eliminating negative sentiment phrases. The work focuses on building and implementing the security filters to popular in-production LLMs like Large Language Model Meta AI-2 (LLaMA2) and Generative Pre-trained Transformer \u2013 3.5 turbo (GPT-3.5) to see how they handle against prompt injection and jailbreaking before and after the security filter being integrated. A large database of 200,000 prompts has been collected and preprocessed to train on a machine learning model using binary classification algorithm having 99.7% accuracy for classification of prompts into malicious or non-malicious and further checks are being done by breaking the prompt into smaller phrases and individually analyzing their compound sentiment score using Natural Language Toolkit (NLTK) Valence Aware Dictionary for Sentiment Reasoning (VADER) algorithm to detect and drop the negative sentiment phrases for the modification of the user prompt to eliminate the possibility of malicious prompt being passed to LLM. It is difficult to determine the sentiment of prompts in a detailed way and convert it into an efficient design that will perform well with models. Once this hurdle is overcome, chatbots will become even more reliable, trustworthy, and user-friendly tools, increasing the level of people\u2019s trust in this technology, and establishing guidelines for how to use these resources in the most beneficial ways.",
      "year": 2024,
      "venue": "International Conference on Electrical Engineering and Information Communication Technology",
      "authors": [
        "Aiswarya Nair",
        "Jose Vithayathil",
        "Sandeep Prem Nambiar",
        "Snehasree S",
        "Shalu Murali"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/21191bca131aa03fb60043b890e26784a31ac66a",
      "pdf_url": "",
      "publication_date": "2024-07-24",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c5e88f5f12b05b8e816c0fa1c76e85705a06a922",
      "title": "Segment Shards: Cross-Prompt Adversarial Attacks against the Segment Anything Model",
      "abstract": "Foundation models play an increasingly pivotal role in the field of deep neural networks. Given that deep neural networks are widely used in real-world systems and are generally susceptible to adversarial attacks, securing foundation models becomes a key research issue. However, research on adversarial attacks against the Segment Anything Model (SAM), a visual foundation model, is still in its infancy. In this paper, we propose the prompt batch attack (PBA), which can effectively attack SAM, making it unable to capture valid objects or even generate fake shards. Extensive experiments were conducted to compare the adversarial attack performance among optimizing without prompts, optimizing all prompts, and optimizing batches of prompts as in PBA. Numerical results on multiple datasets show that the cross-prompt attack success rate (ASR\u2217) of the PBA method is 17.83% higher on average, and the attack success rate (ASR) is 20.84% higher. It is proven that PBA possesses the best attack capability as well as the highest cross-prompt transferability. Additionally, we introduce a metric to evaluate the cross-prompt transferability of adversarial attacks, effectively fostering research on cross-prompt attacks. Our work unveils the pivotal role of the batched prompts technique in cross-prompt adversarial attacks, marking an early and intriguing exploration into this area against SAM.",
      "year": 2024,
      "venue": "Applied Sciences",
      "authors": [
        "Shize Huang",
        "Qianhui Fan",
        "Zhaoxin Zhang",
        "Xiaowen Liu",
        "Guanqun Song",
        "Jinzhe Qin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c5e88f5f12b05b8e816c0fa1c76e85705a06a922",
      "pdf_url": "https://www.mdpi.com/2076-3417/14/8/3312/pdf?version=1713172457",
      "publication_date": "2024-04-15",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5cf06000ce2af2f1df29a1d02906bf331ea3f8b8",
      "title": "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models",
      "abstract": "Text-to-image prompt refinement (T2I-Refine) aims to rephrase or extend an input prompt with more descriptive details that can be leveraged to generate images with higher quality. In this paper, we study an adversarial prompt attacking problem for T2I-Refine, where to goal is to implicitly inject specific concept bias to the input prompts during the refinement process so that the generated images, still with higher quality, are explicitly biased to the target group. Our study is motivated by the limitation of current T2I-Refine research that lacks of explorations on the potential capacity of T2I-Refine models to provide prompt refinement service in a biased or advertising manner. To address the limitations, we develop RAt, a prompt refinement and attacking framework that attacks input prompts with intentionally selected adversarial replacements by optimizing a token distribution matrix based on the text-to-image finetuning strategy with a token-level bias obfuscation loss as regularization. We evaluate RAt on a large-scale text-to-image dataset with various concepts as target in both in-domain and transfer-domain scenarios. The evaluation results demonstrate that, compared to other T2I-Refine schemes, RAt is well capable of implicitly attacking input prompts to generate images with higher quality and explicit visual bias towards specific concept group.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Ziyi Kou",
        "Shichao Pei",
        "Meng Jiang",
        "Xiangliang Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/5cf06000ce2af2f1df29a1d02906bf331ea3f8b8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c96c2a704bb0835d4ac3503c3d50f3a1a9d84e8a",
      "title": "Automated Adversarial Discovery for Safety Classifiers",
      "abstract": "Safety classifiers are critical in mitigating toxicity on online forums such as social media and in chatbots. Still, they continue to be vulnerable to emergent, and often innumerable, adversarial attacks.Traditional automated adversarial data generation methods, however, tend to produce attacks that are not diverse, but variations of previously observed harm types.We formalize the task of automated adversarial discovery for safety classifiers - to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier.We measure progress on this task along two key axes (1) adversarial success: does the attack fool the classifier? and (2) dimensional diversity: does the attack represent a previously unseen harm type?Our evaluation of existing attack generation methods on the CivilComments toxicity task reveals their limitations: Word perturbation attacks fail to fool classifiers, while prompt-based LLM attacks have more adversarial success, but lack dimensional diversity.Even our best-performing prompt-based method finds new successful attacks on unseen harm dimensions of attacks only 5% of the time.Automatically finding new harmful dimensions of attack is crucial and there is substantial headroom for future research on our new task.",
      "year": 2024,
      "venue": "TRUSTNLP",
      "authors": [
        "Yash Kumar Lal",
        "Preethi Lahoti",
        "Aradhana Sinha",
        "Yao Qin",
        "Ananth Balashankar"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c96c2a704bb0835d4ac3503c3d50f3a1a9d84e8a",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a177545a9e10776c4f204daffbb5a09e3ebd8ebd",
      "title": "Context Injection Attacks on Large Language Models",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Cheng'an Wei",
        "Kai Chen",
        "Yue Zhao",
        "Yujia Gong",
        "Lu Xiang",
        "Shenchen Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/a177545a9e10776c4f204daffbb5a09e3ebd8ebd",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "167f07ff13f82bb9b4582e4c553959d3539a17b1",
      "title": "A Review of the Application of Prompt Engineering in the Safety of Large Language Models",
      "abstract": "With the rapid development of large language models, their popular application in dialogue systems has achieved certain remarkable generative performances. However, this also brings several possible security risks, such as poisoned content generation and its exploitation via adversarial prompts. Prompt engineering is one of the key technologies that guide models in generating safe and reasonable outputs by designing and optimizing prompts for both offense and defense. In this paper, LLM safety with regard to prompt engineering is systematically investigated from both attack and defense viewpoints, including common attack methods like red teaming, template-based attacks, and neural prompt attacks, and the use of system prompts, deletion/filtering mechanisms, and contrastive decoding as defense strategies. The goal is to present new perspectives on LLM safety research while predicting the possible applications and challenges facing prompt engineering in high-risk domains.",
      "year": 2024,
      "venue": "Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence",
      "authors": [
        "Hui Guo",
        "Long Zhang",
        "Xilong Feng",
        "Qiusheng Zheng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/167f07ff13f82bb9b4582e4c553959d3539a17b1",
      "pdf_url": "",
      "publication_date": "2024-12-20",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42bd3c497d231e94fcf7f50a30aa4a5e94853025",
      "title": "SiJesi: Large Language Model Chatbot with Augmented Retrieval Approach Generation and Prompt Engineering",
      "abstract": "This research designs a chatbot based on Large Language Models (LLM) for cyber incident consultation, utilizing the Retrieval Augmented Generation (RAG) approach, integrated with GPT and prompt engineering techniques. The goal is to improve public cybersecurity awareness, especially in areas with low awareness, by enhancing community consultation services through automation. The chatbot addresses security concerns by examining measures against prompt injection attacks to ensure reliability. The research follows the Waterfall System Development Life Cycle methodology. Results show that LLM chatbots using RAG and prompt engineering generate more accurate responses compared to human-based answers and GPT 3.5, achieving a top score of 35.26% in A/B testing. Additionally, the chatbot received a high satisfaction score of 86.47% in User Acceptance Testing. These findings are expected to contribute to increased cybersecurity awareness and the development of innovative, secure chatbot technology for public services.",
      "year": 2024,
      "venue": "CENiM",
      "authors": [
        "Happy Sandhiyadini Rosari",
        "Girinoto",
        "Ray Novita Yasa",
        "Nurul Qomariasih",
        "Rheva Anindya Wijayanti"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/42bd3c497d231e94fcf7f50a30aa4a5e94853025",
      "pdf_url": "",
      "publication_date": "2024-11-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee0612304da1a8879241f38eab3c636efa00ba7b",
      "title": "Prompt injection \u2013 the problem of linguistic vulnerabilities of large language models at the present stage",
      "abstract": "The article examines the phenomenon of \u201cprompt injection\u201d in the context of contemporary large language models (LLMs), elucidating a significant challenge for AI developers and researchers. The study comprises a theoretical and methodological review of scholarly publications, thereby enhancing the comprehension of the present state of research in this field. The authors present the findings of a case study, which employs a comparative analysis of the linguistic vulnerabilities of prominent LLMs, including Chat GPT 4.0, Claude 3.5, and Yandex GPT. The study employs experimental evaluation to assess the resilience of these models against a range of vector attacks, with the objective of determining the extent to which each model resists manipulative prompts designed to exploit their linguistic capabilities. A taxonomy of prompt injection attack types was developed based on the collected data, with classification according to effectiveness and targeting of specific LLMs. This classification facilitates comprehension of the nature of these vulnerabilities and provides a basis for future research in this field. Moreover, the article offers suggestions for bolstering the resilience of language models against negative manipulations, representing a significant stride towards the development of safer and more ethical AI systems. These recommendations are based on empirical data and aim to provide practical guidance for developers seeking to enhance the resilience of their models against potential threats. The research findings extend our understanding of linguistic vulnerabilities in LLMs, while also contributing to the development of more effective defence strategies. These have practical implications for the deployment of LLMs across various domains, including education, healthcare and customer service. The authors emphasise the necessity for continuous monitoring and improvement of language model security in an ever-evolving technological landscape. The findings suggest the necessity for an ongoing dialogue among stakeholders to address issues pertaining to the prompt injection of funds.",
      "year": 2024,
      "venue": "RESEARCH RESULT Theoretical and Applied Linguistics",
      "authors": [
        "I. N. Zyryanova",
        "A. Chernavskiy",
        "Stanislav O. Trubachev"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ee0612304da1a8879241f38eab3c636efa00ba7b",
      "pdf_url": "https://doi.org/10.18413/2313-8912-2024-10-4-0-3",
      "publication_date": "2024-12-30",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "27bb8ac3ebde93f7b983f23991032ea5b8e755b7",
      "title": "MalPID: Malicious Prompt Injection Detection Dataset for Large Language Model based Applications",
      "abstract": "Despite the significant transformation made by large language model (LLM)-based chatbots in the field of conversational artificial intelligence (AI), these systems are vulnerable to the attack known as prompt injection by malicious users to make them ignore their guardrails and generate objectionable content. However, few previous works have been addressing this issue using complex and costly mechanisms and there is a lack of large dataset that deal with prompt injection examples. In this work, we introduce MalPID, a novel dataset for malicious prompt injection detection. This benchmark contains various malicious and legitimate prompts collected from different sources and labelled manually. Our systematic evaluation of models trained on our dataset has shown impressive results in detecting prompt injection data. In the future, MalPID could be a valuable resource for advancing the creation of safe conversational AI systems.",
      "year": 2024,
      "venue": "International Conference on Communications and Networking",
      "authors": [
        "Sihem Omri",
        "Manel Abdelkader",
        "Mohamed Hamdi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/27bb8ac3ebde93f7b983f23991032ea5b8e755b7",
      "pdf_url": "",
      "publication_date": "2024-11-05",
      "keywords_matched": [
        "prompt injection",
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "05012b71ade947baa33b9f44fc7c0d7fee37eda4",
      "title": "Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Carrier Articles",
      "abstract": "Large Language Model (LLM) jailbreak refers to a type of attack aimed to bypass the safeguard of an LLM to generate contents that are inconsistent with the safe usage guidelines. Based on the insights from the self-attention computation process, this paper proposes a novel blackbox jailbreak approach, which involves crafting the payload prompt by strategically injecting the prohibited query into a carrier article. The carrier article maintains the semantic proximity to the prohibited query, which is automatically produced by combining a hypernymy article and a context, both of which are generated from the prohibited query. The intuition behind the usage of carrier article is to activate the neurons in the model related to the semantics of the prohibited query while suppressing the neurons that will trigger the objectionable text. Carrier article itself is benign, and we leveraged prompt injection techniques to produce the payload prompt. We evaluate our approach using JailbreakBench, testing against four target models across 100 distinct jailbreak objectives. The experimental results demonstrate our method's superior effectiveness, achieving an average success rate of 63% across all target models, significantly outperforming existing blackbox jailbreak methods.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Zhilong Wang",
        "Haizhou Wang",
        "Nanqing Luo",
        "Lan Zhang",
        "Xiaoyan Sun",
        "Yebo Cao",
        "Peng Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/05012b71ade947baa33b9f44fc7c0d7fee37eda4",
      "pdf_url": "",
      "publication_date": "2024-08-20",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8928d18abb019ba27eda27e592f3e01ef6164c34",
      "title": "Adversarial Attacks on IoT Systems Leveraging Large Language Models",
      "abstract": "Internet of Things (IoT) devices have revolutionized various sectors by providing enhanced connectivity and automated services. However, the integration of these devices into complex networks has introduced significant security challenges. Concurrently, advancements in Language Model (LLM) technologies, exemplified by large-scale models like GPT-4, have given rise to sophisticated cyber-attack strategies. With the widespread use of various large language models, these models may generate a large amount of offensive and socially detrimental content during their use. In response, the primary focus of model developers is to adjust alignment models to prevent the generation of harmful content. Meanwhile, adversarial attackers focus on achieving \u201cjailbreaks\u201d to bypass the alignment measures of model developers, thereby generating harmful content and testing the effectiveness of model alignment. The main attack methods currently include prompt injection, unsafe output handling, denial of service, and general adversarial attacks. We selected the Llama-2-7b large language model as the experimental test model and constructed IoT related issues to achieve the generation of harmful content and desired responses.",
      "year": 2024,
      "venue": "International Conference on Identification, Information, and Knowledge in the Internet of Things",
      "authors": [
        "Weijie Shan",
        "Teng Long",
        "Zhangbing Zhou"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8928d18abb019ba27eda27e592f3e01ef6164c34",
      "pdf_url": "",
      "publication_date": "2024-12-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "71cb5fce8f0689049f47c8a5b092a9b2212774d3",
      "title": "Transformative Trends: A Comprehensive Review of Large Language Models (LLMs) in Healthcare",
      "abstract": "Recent years have witnessed notable progress in Language Model (LM) technology, especially with the introduction of Large Language Models (LLMs). With the use of complex neural networks and enormous volumes of data, these models\u2014like the GPT (Generative Pre-trained Transformer) series\u2014have completely changed the field of natural language processing. This study offers a thorough analysis of LLMs, examining their uses as well as the difficulties and moral issues that come with implementing LLMs in practical settings. Moreover, it investigates the wide range of uses of LLMs in many fields, such as question answering, drug discovery, language translation, and text production. By means of this review, we hope to offer ideas on future directions in this quickly developing field as well as insights into the state-of-the-art in LLM research today. Key Words: Large Language Models, LangChain, Natural language processing, GPT, ChatGPT, AI chatbots, Indirect Prompt Injection",
      "year": 2024,
      "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
      "authors": [
        "Chetna Kumari"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/71cb5fce8f0689049f47c8a5b092a9b2212774d3",
      "pdf_url": "https://ijsrem.com/download/transformative-trends-a-comprehensive-review-of-large-language-models-llms-in-healthcare/?wpdmdl=35677&refresh=665e88fd74a601717471485",
      "publication_date": "2024-06-02",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "29d6d1ce0d92e67df182e200faa8c3b1d792cba0",
      "title": "Understanding Gender and Ethnicity Bias in Large Language Models",
      "abstract": "In this poster, we discuss popular LLM models- in this case LLM Models like LLAMA 2, Gemma and Mistral 7B- for indirect and direct biases through 3 open-sourced and research-oriented datasets with over 1000 prompts in total on fairness factors like gender and ethnicity through prompt injection techniques.",
      "year": 2024,
      "venue": "HCAIep",
      "authors": [
        "Ojasvi Gupta",
        "Rajesh R Jaiswal",
        "Stefano Marrone",
        "Lidia Marassi",
        "Francesco Gargiulo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/29d6d1ce0d92e67df182e200faa8c3b1d792cba0",
      "pdf_url": "",
      "publication_date": "2024-12-02",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4bf910a648031c5e936a558abc2412b9a360dc10",
      "title": "Exploring LLM and Neural Networks Towards Malicious Prompt Detection",
      "abstract": "Atthe forefront of the Artificial Intelligence Revolution is the Generative AI domain which is making splashes in generation of new content from existing Large Language Models. Large Language Models (LLMs) are flexible, effective tools with many uses. On the other hand, they can be tricked by devious provocation. This work explores the detection of fraudulent prompts intended to produce false or damaging outputs using Long Short-Term Memory (LSTM) networks. During this study we analysed the LTSM model against a neural network model. We found a 91.69% accuracy for the LTSM model and a 92.52% accuracy for the simple neural network. However, the simple neural network had a higher F1 score of 0.73 compared to the LTSM score of 0.69. We found that the standard neural model is more efficient at identifying harmful user prompts. Our research highlights the need for taking into account various architectures of models to reduce the risk of malicious prompting in language learning models.",
      "year": 2024,
      "venue": "International Conference Electronic Systems, Signal Processing and Computing Technologies [ICESC-]",
      "authors": [
        "Himani Deshpande",
        "Dhawal Chaudhari",
        "Tanmay Sarode",
        "Ayush Kamath"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/4bf910a648031c5e936a558abc2412b9a360dc10",
      "pdf_url": "",
      "publication_date": "2024-08-07",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50e234877296d2c5e0e83c1f8ff75f3767e63f1a",
      "title": "Unmasking Context Injection on Interactive Large Language Models",
      "abstract": "Large Language Models (LLMs) such as ChatGPT and Llama have become prevalent in real-world applications, exhibiting impressive text generation performance. LLMs are fundamentally developed from a scenario where the input data remains static and unstructured. To behave interactively, LLM-based chat systems must integrate prior chat history as context into their inputs, following a pre-defined structure. However, LLMs cannot separate user inputs from context, enabling context injection. This paper introduces a methodology to systematically inject misleading context (i.e., chat history) into LLM conversations without prior knowledge of the target model. We formalize context injection through a response retrieval task and propose an LLM-Guided Genetic Algorithm (LLMGA) to create effective injection prompts, leading the target LLM to recognize injected context as genuine. We explore how context injection can be used to elicit disallowed content, posing risks of illegal actions and inappropriate responses. Our elicitation strategies, including acceptance elicitation and word anonymization, can effectively elicit disallowed responses with success rates reaching 97% on ChatGPT. Comprehensive evaluations on real-world LLMs including ChatGPT and Llama-2/3 demonstrate the efficacy of LLMGA and our injection strategies. We also discuss potential countermeasures that can be adopted for detecting injection and developing more secure models. Our findings provide insights into the challenges associated with the real-world deployment of LLMs for interactive and structured data scenarios.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Cheng'an Wei",
        "Yue Zhao",
        "Yujia Gong",
        "Kai Chen",
        "Lu Xiang",
        "Shenchen Zhu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/50e234877296d2c5e0e83c1f8ff75f3767e63f1a",
      "pdf_url": "",
      "publication_date": "2024-05-30",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "48d56ee91c748a2079f59055e2e1fd98888eb99f",
      "title": "On Debiasing Text Embeddings Through Context Injection",
      "abstract": "Current advances in Natural Language Processing (NLP) have made it increasingly feasible to build applications leveraging textual data. Generally, the core of these applications rely on having a good semantic representation of text into vectors, via embedding models. However, it has been shown that these embeddings capture and perpetuate biases already present in text. While a few techniques have been proposed to debias embeddings, they do not take advantage of the recent advances in context understanding of modern embedding models. In this paper, we fill this gap by conducting a review of 19 embedding models by quantifying their biases and how well they respond to context injection as a mean of debiasing. We show that higher performing models are more prone to capturing biases, but are also better at incorporating context. Surprisingly, we find that while models can easily embed affirmative semantics, they fail at embedding neutral semantics. Finally, in a retrieval task, we show that biases in embeddings can lead to non-desirable outcomes. We use our new-found insights to design a simple algorithm for top $k$ retrieval, where $k$ is dynamically selected. We show that our algorithm is able to retrieve all relevant gendered and neutral chunks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Thomas Uriot"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/48d56ee91c748a2079f59055e2e1fd98888eb99f",
      "pdf_url": "",
      "publication_date": "2024-10-14",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "488abd2815c9653eafa8d2191afb47e2a1e78e68",
      "title": "When to Inject? Investigating Different Metrics for Conditional Context Injection in LLMs",
      "abstract": "LLMs have become more and more integrated into larger software systems, in which they are provided with additional information to answer user queries. While this improves answer quality in many cases, it comes at an increased cost in tokens and thus computational load and electricity consumption. In this paper, we investigate low compute cost mechanisms which can help us predict whether an LLM would profit from a costly token injection.",
      "year": 2024,
      "venue": "2024 2nd International Conference on Foundation and Large Language Models (FLLM)",
      "authors": [
        "Tristan Kenneweg",
        "Philip Kenneweg",
        "Barbara Hammer"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/488abd2815c9653eafa8d2191afb47e2a1e78e68",
      "pdf_url": "",
      "publication_date": "2024-11-26",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1fe27a556647503dc9e08d1c945c55aada6d9685",
      "title": "The Colossal Defense: Security Challenges of Large Language Models",
      "abstract": "Large Language Models (LLMs) have become increasingly popular because of their strong capabilities and abilities in multiple fields, such as text generation, language translation, image, video, audio, and other forms of output generation. The ability of the model to analyze a complex input prompt and provide relevant and simplified responses, depending on its usage, elevates its desirability. LLM not only has a transformative effect on the interaction between humans and models and the usage of the models in different situations but also has a significant drawback. Although the main benefits of LLM are apparent, there is a potential landmine of security threats accompanying its existence. Exposure to artificial intelligence increases the security risk throughout its design, development, infrastructure, and incorporation. Threat actors can exploit existing vulnerabilities to control models and execute uncontrollable and divisive actions. This study thoroughly examines the fundamental security problems of prompt injection, data poisoning, and sensitive information extraction. The attacks and defenses arising from these issues related to LLM are summarized. Finally, we suggest future research directions to secure LLMs.",
      "year": 2024,
      "venue": "2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)",
      "authors": [
        "Sourav Maity",
        "Jatin Arora"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1fe27a556647503dc9e08d1c945c55aada6d9685",
      "pdf_url": "",
      "publication_date": "2024-11-21",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f2d6fef1444f1a79a5d7c46d01f54671ddb1c060",
      "title": "The Price of Intelligence",
      "abstract": "The vulnerability of LLMs to hallucination, prompt injection, and jailbreaks poses a significant but surmountable challenge to their widespread adoption and responsible use. We have argued that these problems are inherent, certainly in the present generation of models and likely in LLMs per se, and so our approach can never be based on eliminating them; rather, we should apply strategies of \"defense in depth\" to mitigate them, and when building and using these systems, do so on the assumption that they will sometimes fail in these directions.",
      "year": 2024,
      "venue": "Queue",
      "authors": [
        "M. Russinovich",
        "Ahmed Salem",
        "Santiago Zanella-B\u00e9guelin",
        "Yonatan Zunger"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f2d6fef1444f1a79a5d7c46d01f54671ddb1c060",
      "pdf_url": "",
      "publication_date": "2024-12-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Andy Zou",
        "Zifan Wang",
        "J. Z. Kolter",
        "Matt Fredrikson"
      ],
      "citation_count": 2237,
      "url": "https://www.semanticscholar.org/paper/47030369e97cc44d4b2e3cf1be85da0fd134904a",
      "pdf_url": "",
      "publication_date": "2023-07-27",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "705e49afd92130f2bc1e0d4d0b1f6cb14e88803f",
      "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
      "abstract": "Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.",
      "year": 2023,
      "venue": "AISec@CCS",
      "authors": [
        "Kai Greshake",
        "Sahar Abdelnabi",
        "Shailesh Mishra",
        "C. Endres",
        "Thorsten Holz",
        "Mario Fritz"
      ],
      "citation_count": 757,
      "url": "https://www.semanticscholar.org/paper/705e49afd92130f2bc1e0d4d0b1f6cb14e88803f",
      "pdf_url": "https://arxiv.org/pdf/2302.12173",
      "publication_date": "2023-02-23",
      "keywords_matched": [
        "prompt injection",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "db4cf9f6a653d5c15973e836c800ea47743251ae",
      "title": "Prompt Injection attack against LLM-integrated Applications",
      "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Yuekang Li",
        "Kailong Wang",
        "Tianwei Zhang",
        "Yepang Liu",
        "Haoyu Wang",
        "Yanhong Zheng",
        "Yang Liu"
      ],
      "citation_count": 545,
      "url": "https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae",
      "pdf_url": "http://arxiv.org/pdf/2306.05499",
      "publication_date": "2023-06-08",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "317ad53bea6fb603c20f692bb2f1a01e2dc86161",
      "title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy",
      "abstract": "Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it\u2019s critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.",
      "year": 2023,
      "venue": "IEEE Access",
      "authors": [
        "Maanak Gupta",
        "Charankumar Akiri",
        "Kshitiz Aryal",
        "Elisabeth Parker",
        "Lopamudra Praharaj"
      ],
      "citation_count": 529,
      "url": "https://www.semanticscholar.org/paper/317ad53bea6fb603c20f692bb2f1a01e2dc86161",
      "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10198233.pdf",
      "publication_date": "2023-07-03",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1104d766527dead44a40532e8a89444d9cef5c65",
      "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
      "abstract": "The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Xinyue Shen",
        "Zeyuan Chen",
        "M. Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "citation_count": 439,
      "url": "https://www.semanticscholar.org/paper/1104d766527dead44a40532e8a89444d9cef5c65",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670388",
      "publication_date": "2023-08-07",
      "keywords_matched": [
        "prompt injection",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dd4d82299b4209db539d639f836fcee663cf72b3",
      "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",
      "abstract": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \\emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formalize an automatic hallucination triggering method as the \\textit{hallucination attack} in an adversarial way. Finally, we explore the basic properties of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub\\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jia-Yu Yao",
        "Kun-Peng Ning",
        "Zhen-Hui Liu",
        "Munan Ning",
        "Li Yuan"
      ],
      "citation_count": 266,
      "url": "https://www.semanticscholar.org/paper/dd4d82299b4209db539d639f836fcee663cf72b3",
      "pdf_url": "https://arxiv.org/pdf/2310.01469",
      "publication_date": "2023-10-02",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
      "title": "Certifying LLM Safety against Adversarial Prompting",
      "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Aounon Kumar",
        "Chirag Agarwal",
        "Suraj Srinivas",
        "S. Feizi",
        "Himabindu Lakkaraju"
      ],
      "citation_count": 263,
      "url": "https://www.semanticscholar.org/paper/1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
      "pdf_url": "https://arxiv.org/pdf/2309.02705",
      "publication_date": "2023-09-06",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2cdd5c3dc42c0df40bc8839709869af3560d4bfe",
      "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
      "abstract": "Large language models (LLMs) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. Adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses. Our method does not require any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. We test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent LLMs against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5 and Llama 2. The code is publicly available at https://github.com/poloclub/llm-self-defense",
      "year": 2023,
      "venue": "Tiny Papers @ ICLR",
      "authors": [
        "Alec Helbling",
        "Mansi Phute",
        "Matthew Hull",
        "Sebastian Szyller",
        "Duen Horng Chau"
      ],
      "citation_count": 262,
      "url": "https://www.semanticscholar.org/paper/2cdd5c3dc42c0df40bc8839709869af3560d4bfe",
      "pdf_url": "https://arxiv.org/pdf/2308.07308",
      "publication_date": "2023-08-14",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed",
      "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
      "abstract": "Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Xianjun Yang",
        "Xiao Wang",
        "Qi Zhang",
        "L. Petzold",
        "William Yang Wang",
        "Xun Zhao",
        "Dahua Lin"
      ],
      "citation_count": 243,
      "url": "https://www.semanticscholar.org/paper/84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed",
      "pdf_url": "https://arxiv.org/pdf/2310.02949",
      "publication_date": "2023-10-04",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3a41a5bfec0d8ccb597d9313416130a2e0b2459",
      "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kaijie Zhu",
        "Jindong Wang",
        "Jiaheng Zhou",
        "Zichen Wang",
        "Hao Chen",
        "Yidong Wang",
        "Linyi Yang",
        "Weirong Ye",
        "Neil Zhenqiang Gong",
        "Yue Zhang",
        "Xing Xie"
      ],
      "citation_count": 218,
      "url": "https://www.semanticscholar.org/paper/f3a41a5bfec0d8ccb597d9313416130a2e0b2459",
      "pdf_url": "https://arxiv.org/pdf/2306.04528",
      "publication_date": null,
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
      "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
      "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as \u2018jailbreaks\u2019 can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
      "year": 2023,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Peng Ding",
        "Jun Kuang",
        "Dan Ma",
        "Xuezhi Cao",
        "Yunsen Xian",
        "Jiajun Chen",
        "Shujian Huang"
      ],
      "citation_count": 211,
      "url": "https://www.semanticscholar.org/paper/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
      "pdf_url": "",
      "publication_date": "2023-11-14",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9adddfe5a596d20b534a29d9937efcd3cb85d3cf",
      "title": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses",
      "abstract": "A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Runpeng Geng",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 192,
      "url": "https://www.semanticscholar.org/paper/9adddfe5a596d20b534a29d9937efcd3cb85d3cf",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "37665dd5ae7245f087d663785c17eef068578676",
      "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
      "abstract": "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt \u201cDescribe Joe Biden negatively.\u201d for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model\u2019s instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io.",
      "year": 2023,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Jun Yan",
        "Vikas Yadav",
        "SHIYANG LI",
        "Lichang Chen",
        "Zheng Tang",
        "Hai Wang",
        "Vijay Srinivasan",
        "Xiang Ren",
        "Hongxia Jin"
      ],
      "citation_count": 149,
      "url": "https://www.semanticscholar.org/paper/37665dd5ae7245f087d663785c17eef068578676",
      "pdf_url": "",
      "publication_date": "2023-07-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f846c0c59608f0a8ff18f4c52adba87bf49dc229",
      "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
      "abstract": "Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM\u2019s outputs for unintended purposes. In this paper, we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that\u2014when combined with a user\u2019s query\u2014disrupts the attacked model\u2019s alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model\u2019s limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments, we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool for evaluating and enhancing alignment of LLMs with human intent. To our knowledge, this is the first automated universal black-box jailbreak attack.",
      "year": 2023,
      "venue": "Applied Sciences",
      "authors": [
        "Raz Lapid",
        "Ron Langberg",
        "Moshe Sipper"
      ],
      "citation_count": 148,
      "url": "https://www.semanticscholar.org/paper/f846c0c59608f0a8ff18f4c52adba87bf49dc229",
      "pdf_url": "https://arxiv.org/pdf/2309.01446",
      "publication_date": "2023-09-04",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d465be006615460d41060f9f5068d51fc1f46b1",
      "title": "Benchmarking and Defending against Indirect Prompt Injection Attacks on Large Language Models",
      "abstract": "The integration of large language models (LLMs) with external content has enabled applications such as Microsoft Copilot but also introduced vulnerabilities to indirect prompt injection attacks. In these attacks, malicious instructions embedded within external content can manipulate LLM outputs, causing deviations from user expectations. To address this critical yet under-explored issue, we introduce the first benchmark for bindirect prompt injection attacks, named BIPIA, to assess the risk of such vulnerabilities. Using BIPIA, we evaluate existing LLMs and find them universally vulnerable. Our analysis identifies two key factors contributing to their success: LLMs' inability to distinguish between informational context and actionable instructions, and their lack of awareness in avoiding the execution of instructions within external content. Based on these findings, we propose two novel defense mechanisms -- boundary awareness and explicit reminder -- to address these vulnerabilities in both black-box and white-box settings. Extensive experiments demonstrate that our black-box defense provides substantial mitigation, while our white-box defense reduces the attack success rate to near-zero levels, all while preserving the output quality of LLMs. We hope this work inspires further research into securing LLM applications and fostering their safe and reliable use. Our code is available at https://github.com/microsoft/BIPIA.",
      "year": 2023,
      "venue": "Knowledge Discovery and Data Mining",
      "authors": [
        "Jingwei Yi",
        "Yueqi Xie",
        "Bin Zhu",
        "Keegan Hines",
        "Emre Kiciman",
        "Guangzhong Sun",
        "Xing Xie",
        "Fangzhao Wu"
      ],
      "citation_count": 145,
      "url": "https://www.semanticscholar.org/paper/6d465be006615460d41060f9f5068d51fc1f46b1",
      "pdf_url": "",
      "publication_date": "2023-12-21",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "46eea7d651420e60f9b1393e3f5eda14cbff7a2a",
      "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
      "abstract": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover\"deleted\"information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Vaidehi Patil",
        "Peter Hase",
        "Mohit Bansal"
      ],
      "citation_count": 144,
      "url": "https://www.semanticscholar.org/paper/46eea7d651420e60f9b1393e3f5eda14cbff7a2a",
      "pdf_url": "https://arxiv.org/pdf/2309.17410",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
      "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kai Greshake",
        "Sahar Abdelnabi",
        "Shailesh Mishra",
        "C. Endres",
        "Thorsten Holz",
        "Mario Fritz"
      ],
      "citation_count": 130,
      "url": "https://www.semanticscholar.org/paper/8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
      "pdf_url": "http://arxiv.org/pdf/2302.12173",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642",
      "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch",
      "abstract": "Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text. Prior work has introduced automated tools that elicit harmful outputs to identify these risks. While this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. Using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. Furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. Here, we consider red-teaming\"from scratch,\"in which the adversary does not begin with a way to classify failures. Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts. We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements. In doing so, we construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. We are making code and data available.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Stephen Casper",
        "Jason Lin",
        "Joe Kwon",
        "Gatlen Culp",
        "Dylan Hadfield-Menell"
      ],
      "citation_count": 117,
      "url": "https://www.semanticscholar.org/paper/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642",
      "pdf_url": "https://arxiv.org/pdf/2306.09442",
      "publication_date": "2023-06-16",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3fa3d818dedba25121faf777e09905ee96faa117",
      "title": "(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs",
      "abstract": "We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Eugene Bagdasarian",
        "Tsung-Yin Hsieh",
        "Ben Nassi",
        "Vitaly Shmatikov"
      ],
      "citation_count": 110,
      "url": "https://www.semanticscholar.org/paper/3fa3d818dedba25121faf777e09905ee96faa117",
      "pdf_url": "https://arxiv.org/pdf/2307.10490",
      "publication_date": "2023-07-19",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90de1938a64d117d61b9e7149d2981df49b81433",
      "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a\"jailbreak backdoor\"into the model. The backdoor embeds a trigger word into the model that acts like a universal\"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Javier Rando",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 106,
      "url": "https://www.semanticscholar.org/paper/90de1938a64d117d61b9e7149d2981df49b81433",
      "pdf_url": "",
      "publication_date": "2023-11-24",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e25b4bdfc3f5a8293ea6cd687a0203e446594188",
      "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
      "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based\"defenses\"against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release all data and source code at https://tensortrust.ai/paper",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Sam Toyer",
        "Olivia Watkins",
        "Ethan Mendes",
        "Justin Svegliato",
        "Luke Bailey",
        "Tiffany Wang",
        "Isaac Ong",
        "Karim Elmaaroufi",
        "Pieter Abbeel",
        "Trevor Darrell",
        "Alan Ritter",
        "Stuart Russell"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/e25b4bdfc3f5a8293ea6cd687a0203e446594188",
      "pdf_url": "",
      "publication_date": "2023-11-02",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6a6f6070e765f77422c6d6c14c1e6b41a9ed562a",
      "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
      "abstract": "Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases none at all, to produce a fully synthetic dataset. Our experiments on seven tasks show that Jatmo models provide similar quality of outputs on their specific task as standard LLMs, while being resilient to prompt injections. The best attacks succeeded in less than 0.5% of cases against our models, versus 87% success rate against GPT-3.5-Turbo. We release Jatmo at https://github.com/wagner-group/prompt-injection-defense.",
      "year": 2023,
      "venue": "European Symposium on Research in Computer Security",
      "authors": [
        "Julien Piet",
        "Maha Alrashed",
        "Chawin Sitawarin",
        "Sizhe Chen",
        "Zeming Wei",
        "Elizabeth Sun",
        "Basel Alomair",
        "David Wagner"
      ],
      "citation_count": 94,
      "url": "https://www.semanticscholar.org/paper/6a6f6070e765f77422c6d6c14c1e6b41a9ed562a",
      "pdf_url": "",
      "publication_date": "2023-12-29",
      "keywords_matched": [
        "prompt injection",
        "prompt injection defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d347fb65bd375771b013c517c861db926322e410",
      "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
      "abstract": null,
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Sander Schulhoff",
        "Jeremy Pinto",
        "Anaum Khan",
        "Louis-Franccois Bouchard",
        "Chenglei Si",
        "Svetlina Anati",
        "Valen Tagliabue",
        "Anson Liu Kost",
        "Christopher Carnahan",
        "Jordan L. Boyd-Graber"
      ],
      "citation_count": 91,
      "url": "https://www.semanticscholar.org/paper/d347fb65bd375771b013c517c861db926322e410",
      "pdf_url": "https://aclanthology.org/2023.emnlp-main.302.pdf",
      "publication_date": null,
      "keywords_matched": [
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0894585294c67193ff3190240554677b56fd79a0",
      "title": "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?",
      "abstract": "Large Language Models (LLMs) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. Despite the growing interest in prompt injection vulnerabilities targeting LLMs, the specific risks of generating SQL injection attacks through prompt injections have not been extensively studied. In this paper, we present a comprehensive examination of prompt-to-SQL (P$_2$SQL) injections targeting web applications based on the Langchain framework. Using Langchain as our case study, we characterize P$_2$SQL injections, exploring their variants and impact on application security through multiple concrete examples. Furthermore, we evaluate 7 state-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks across language models. Our findings indicate that LLM-integrated applications based on Langchain are highly susceptible to P$_2$SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the Langchain framework. We validate the defenses through an experimental evaluation with a real-world use case application.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rodrigo Pedro",
        "Daniel Castro",
        "Paulo Carreira",
        "Nuno Santos"
      ],
      "citation_count": 86,
      "url": "https://www.semanticscholar.org/paper/0894585294c67193ff3190240554677b56fd79a0",
      "pdf_url": "https://arxiv.org/pdf/2308.01990",
      "publication_date": "2023-08-03",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "910a4623f7732b1ea32dc2cdda7a30a227b89a11",
      "title": "Assessing Prompt Injection Risks in 200+ Custom GPTs",
      "abstract": "In the rapidly evolving landscape of artificial intelligence, ChatGPT has been widely used in various applications. The new feature - customization of ChatGPT models by users to cater to specific needs has opened new frontiers in AI utility. However, this study reveals a significant security vulnerability inherent in these user-customized GPTs: prompt injection attacks. Through comprehensive testing of over 200 user-designed GPT models via adversarial prompts, we demonstrate that these systems are susceptible to prompt injections. Through prompt injection, an adversary can not only extract the customized system prompts but also access the uploaded files. This paper provides a first-hand analysis of the prompt injection, alongside the evaluation of the possible mitigation of such attacks. Our findings underscore the urgent need for robust security frameworks in the design and deployment of customizable GPT models. The intent of this paper is to raise awareness and prompt action in the AI community, ensuring that the benefits of GPT customization do not come at the cost of compromised security and privacy.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jiahao Yu",
        "Yuhang Wu",
        "Dong Shu",
        "Mingyu Jin",
        "Xinyu Xing"
      ],
      "citation_count": 72,
      "url": "https://www.semanticscholar.org/paper/910a4623f7732b1ea32dc2cdda7a30a227b89a11",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a2bcacc8fefb859c94c69d524b2368bb4792f9b1",
      "title": "Adversarial Prompting for Black Box Foundation Models",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Natalie Maus",
        "Patrick Chao",
        "Eric Wong",
        "Jacob R. Gardner"
      ],
      "citation_count": 56,
      "url": "https://www.semanticscholar.org/paper/a2bcacc8fefb859c94c69d524b2368bb4792f9b1",
      "pdf_url": "http://arxiv.org/pdf/2302.04237",
      "publication_date": null,
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models",
      "abstract": "Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts. Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly. In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged. In this paper, we perform the first study on understanding the threat of a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models. Successful prompt stealing attacks directly violate the intellectual property of prompt engineers and jeopardize the business model of prompt marketplaces. We first perform a systematic analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers. Based on this observation, we propose a simple yet effective prompt stealing attack, PromptStealer. It consists of two modules: a subject generator trained to infer the subject and a modifier detector for identifying the modifiers within the generated image. Experimental results demonstrate that PromptStealer is superior over three baseline methods, both quantitatively and qualitatively. We also make some initial attempts to defend PromptStealer. In general, our study uncovers a new attack vector within the ecosystem established by the popular text-to-image generation models. We hope our results can contribute to understanding and mitigating this emerging threat.",
      "year": 2023,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xinyue Shen",
        "Y. Qu",
        "M. Backes",
        "Yang Zhang"
      ],
      "citation_count": 54,
      "url": "https://www.semanticscholar.org/paper/7ec9e9ec1c26f7977f54dd7830d970101e3a683e",
      "pdf_url": "http://arxiv.org/pdf/2302.09923",
      "publication_date": "2023-02-20",
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696",
      "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yupei Liu",
        "Yuqi Jia",
        "Runpeng Geng",
        "Jinyuan Jia",
        "N. Gong"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/04fa3ebc4c0c0b4a2d2d1a3fc612134a05057696",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5bc1839244097b04b5acbede51a0b5e4e9b6deea",
      "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
      "abstract": "The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Kaijie Zhu",
        "Qinlin Zhao",
        "Hao Chen",
        "Jindong Wang",
        "Xing Xie"
      ],
      "citation_count": 50,
      "url": "https://www.semanticscholar.org/paper/5bc1839244097b04b5acbede51a0b5e4e9b6deea",
      "pdf_url": "",
      "publication_date": "2023-12-13",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9be0dea0d6b892a2162490fb02712efaf10c0c87",
      "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps",
      "abstract": "Large Language Models (LLMs) show promise in transforming software development, with a growing interest in integrating them into more intelligent apps. Frameworks like LangChain aid LLM-integrated app development, offering code execution utility/APIs for custom actions. However, these capabilities theoretically introduce Remote Code Execution (RCE) vulnerabilities, enabling remote code execution through prompt injections. No prior research systematically investigates these frameworks' RCE vulnerabilities or their impact on applications and exploitation consequences. Therefore, there is a huge research gap in this field. In this study, we propose LLMSmith to detect, validate and exploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To achieve this goal, we develop two novel techniques, including 1) a lightweight static analysis to construct call chains to identify RCE vulnerabilities in frameworks; 2) a systematical prompt-based exploitation method to verify and exploit the found vulnerabilities in LLM-integrated apps. This technique involves various strategies to control LLM outputs, trigger RCE vulnerabilities and launch subsequent attacks. Our research has uncovered a total of 20 vulnerabilities in 11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary file read/write vulnerability. Of these, 17 have been confirmed by the framework developers, with 13 vulnerabilities being assigned CVE IDs, 6 of which have a CVSS score of 9.8, and we were also awarded a bug bounty of 1350. For the 51 apps potentially affected by RCE, we successfully executed attacks on 17 apps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we conduct a comprehensive analysis of these vulnerabilities and construct practical attacks to demonstrate the hazards in reality, e.g., app output hijacking, user data leakage, even the potential to take full control of systems. Last, we propose several mitigation measures for both framework and app developers to counteract such attacks.",
      "year": 2023,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Tong Liu",
        "Zizhuang Deng",
        "Guozhu Meng",
        "Yuekang Li",
        "Kai Chen"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/9be0dea0d6b892a2162490fb02712efaf10c0c87",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690338",
      "publication_date": "2023-09-06",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
      "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
      "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific downstream tasks by utilizing labeled examples as demonstrations (demos) in the preconditioned prompts. Despite its promising performance, crafted adversarial attacks pose a notable threat to the robustness of LLMs. Existing attacks are either easy to detect, require a trigger in user input, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable prompt injection attack against ICL, aiming to hijack LLMs to generate the target output or elicit harmful responses. In our threat model, the hacker acts as a model publisher who leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demos via prompt injection. We also propose effective defense strategies using a few shots of clean demos, enhancing the robustness of LLMs during ICL. Extensive experimental results across various classification and jailbreak tasks demonstrate the effectiveness of the proposed attack and defense strategies. This work highlights the significant security vulnerabilities of LLMs during ICL and underscores the need for further in-depth studies.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Yao Qiang",
        "Xiangyu Zhou",
        "Dongxiao Zhu"
      ],
      "citation_count": 46,
      "url": "https://www.semanticscholar.org/paper/6d68b5c1eaf03aba857476a9825acf3e48edd840",
      "pdf_url": "",
      "publication_date": "2023-11-16",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0ce1e3ba7cb96dab6db15ee7b4add29cab5d0efb",
      "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, making them increasingly integral to various applications. However, this capability introduces the risk of prompt injection attacks, where malicious instructions are embedded in the input to trigger unintended actions or content. Understanding the robustness of LLMs against such attacks is critical for ensuring their safe deployment. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks, assessing their ability to discern which instructions to follow and which to disregard. Through extensive experiments with leading instruction-following LLMs, we reveal significant vulnerabilities, particularly in models that mis-follow injected instructions. Our results show that certain models are excessively inclined to prioritize embedded instructions in prompts, often focusing on the latter parts of the prompt without fully understanding the overall context. Conversely, models that exhibit stronger contextual understanding and instruction-following capabilities tend to be more easily compromised by injected instructions. These findings highlight the need to balance improving LLMs\u2019 instruction-following abilities with enhancing their overall comprehension of prompts, to prevent mis-following inappropriate instructions. We hope our analysis provides valuable insights into these vulnerabilities, contributing to the development of more robust solutions in the future.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Zekun Li",
        "Baolin Peng",
        "Pengcheng He",
        "Xifeng Yan"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/0ce1e3ba7cb96dab6db15ee7b4add29cab5d0efb",
      "pdf_url": "",
      "publication_date": "2023-08-17",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "33e7f54c2b31849ea5f4a36f0a3470ea57857ff6",
      "title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models",
      "abstract": "Large Language Models (LLMs) are progressively being utilized as machine learning services and interface tools for various applications. However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined. In this paper, we propose TrojLLM, an automatic and black-box framework to effectively generate universal and stealthy triggers. When these triggers are incorporated into the input data, the LLMs' outputs can be maliciously manipulated. Moreover, the framework also supports embedding Trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks. Specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim LLM-based APIs using few-shot data samples. Furthermore, we introduce a novel progressive Trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. Our experiments and results demonstrate TrojLLM's capacity to effectively insert Trojans into text prompts in real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining exceptional performance on clean test sets. Our work sheds light on the potential security risks in current models and offers a potential defensive approach. The source code of TrojLLM is available at https://github.com/UCF-ML-Research/TrojLLM.",
      "year": 2023,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiaqi Xue",
        "Yepeng Liu",
        "Mengxin Zheng",
        "Ting Hua",
        "Yilin Shen",
        "L. Boloni",
        "Qian Lou"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/33e7f54c2b31849ea5f4a36f0a3470ea57857ff6",
      "pdf_url": "",
      "publication_date": "2023-06-12",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c1d1842e08716cbf7250167969522a1705d8bcd3",
      "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
      "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibiting high perplexity are flagged as adversarial. Additionaly, our method also integrates context understanding by incorporating neighboring token information to encourage the detection of contiguous adversarial prompt sequences. To this end, we design two algorithms for adversarial prompt detection: one based on optimization techniques and another on Probabilistic Graphical Models (PGM). Both methods are equipped with efficient solving methods, ensuring efficient adversarial prompt detection. Our token-level detection result can be visualized as heatmap overlays on the text sequence, allowing for a clearer and more intuitive representation of which part of the text may contain adversarial prompts.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Zhengmian Hu",
        "Gang Wu",
        "Saayan Mitra",
        "Ruiyi Zhang",
        "Tong Sun",
        "Heng Huang",
        "Vishy Swaminathan"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/c1d1842e08716cbf7250167969522a1705d8bcd3",
      "pdf_url": "",
      "publication_date": "2023-11-20",
      "keywords_matched": [
        "prompt attack",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
      "title": "Adversarial Prompt Tuning for Vision-Language Models",
      "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
      "year": 2023,
      "venue": "European Conference on Computer Vision",
      "authors": [
        "Jiaming Zhang",
        "Xingjun Ma",
        "Xin Wang",
        "Lingyu Qiu",
        "Jiaqi Wang",
        "Yu-Gang Jiang",
        "Jitao Sang"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/b6cf4579b59b51d7df416e096ad86c1e6a48b458",
      "pdf_url": "",
      "publication_date": "2023-11-19",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "66bc83b4b321cac7084cc51b3c6178d9de54e9c1",
      "title": "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs",
      "abstract": "Large Language Models (LLMs) presents significant priority in text understanding and generation. However, LLMs suffer from the risk of generating harmful contents especially while being employed to applications. There are several black-box attack methods, such as Prompt Attack, which can change the behaviour of LLMs and induce LLMs to generate unexpected answers with harmful contents. Researchers are interested in Prompt Attack and Defense with LLMs, while there is no publicly available dataset with high successful attacking rate to evaluate the abilities of defending prompt attack. In this paper, we introduce a pipeline to construct high-quality prompt attack samples, along with a Chinese prompt attack dataset called CPAD. Our prompts aim to induce LLMs to generate unexpected outputs with several carefully designed prompt attack templates and widely concerned attacking contents. Different from previous datasets involving safety estimation, we construct the prompts considering three dimensions: contents, attacking methods and goals. Especially, the attacking goals indicate the behaviour expected after successfully attacking the LLMs, thus the responses can be easily evaluated and analysed. We run several popular Chinese LLMs on our dataset, and the results show that our prompts are significantly harmful to LLMs, with around 70% attack success rate to GPT-3.5. CPAD is publicly available at https://github.com/liuchengyuan123/CPAD.",
      "year": 2023,
      "venue": "",
      "authors": [
        "Chengyuan Liu",
        "Fubang Zhao",
        "Lizhi Qing",
        "Yangyang Kang",
        "Changlong Sun",
        "Kun Kuang",
        "Fei Wu"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/66bc83b4b321cac7084cc51b3c6178d9de54e9c1",
      "pdf_url": "",
      "publication_date": "2023-09-21",
      "keywords_matched": [
        "prompt attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1909ad17042a0d2af93898d427df0cef9d4483e3",
      "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
      "abstract": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier in record time and might increasingly enter all areas of our life leading to a diverse set of interactions between those Artificial Intelligence models and humans. While many studies have discussed governance and regulations deductively from first-order principles, few studies provide an inductive, data-driven lens based on observing dialogues between humans and LLMs especially when it comes to non-collaborative, competitive situations that have the potential to pose a serious threat to people. In this work, we conduct a user study engaging over 40 individuals across all age groups in price negotiations with an LLM. We explore how people interact with an LLM, investigating differences in negotiation outcomes and strategies. Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality. We also show that the negotiated prices humans manage to achieve span a broad range, which points to a literacy gap in effectively interacting with LLMs.",
      "year": 2023,
      "venue": "International Conference on Computer-Human Interaction Research and Applications",
      "authors": [
        "Johannes Schneider",
        "Steffi Haag",
        "Leona Chandra Kruse"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/1909ad17042a0d2af93898d427df0cef9d4483e3",
      "pdf_url": "",
      "publication_date": "2023-11-26",
      "keywords_matched": [
        "prompt hacking"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3e5822d5ea26b7026e75802d3ce05f4eba4243a",
      "title": "Character As Pixels: A Controllable Prompt Adversarial Attacking Framework for Black-Box Text Guided Image Generation Models",
      "abstract": "In this paper, we study a controllable prompt adversarial attacking problem for text guided image generation (Text2Image) models in the black-box scenario, where the goal is to attack specific visual subjects (e.g., changing a brown dog to white) in a generated image by slightly, if not imperceptibly, perturbing the characters of the driven prompt (e.g., ``brown'' to ``br0wn''). Our study is motivated by the limitations of current Text2Image attacking approaches that still rely on manual trials to create adversarial prompts. To address such limitations, we develop CharGrad, a character-level gradient based attacking framework that replaces specific characters of a prompt with pixel-level similar ones by interactively learning the perturbation direction for the prompt and updating the attacking examiner for the generated image based on a novel proxy perturbation representation for characters. We evaluate CharGrad using the texts from two public image captioning datasets. Results demonstrate that CharGrad outperforms existing text adversarial attacking approaches on attacking various subjects of generated images by black-box Text2Image models in a more effective and efficient way with less perturbation on the characters of the prompts.",
      "year": 2023,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Ziyi Kou",
        "Shichao Pei",
        "Yijun Tian",
        "Xiangliang Zhang"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/f3e5822d5ea26b7026e75802d3ce05f4eba4243a",
      "pdf_url": "https://www.ijcai.org/proceedings/2023/0109.pdf",
      "publication_date": "2023-08-01",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "952b2e2aadeec56ee4cffbe23f9f70bcfcb14b53",
      "title": "A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models",
      "abstract": "Ensuring the security of large language models (LLMs) is an ongoing challenge despite their widespread popularity. Developers work to enhance LLMs security, but vulnerabilities persist, even in advanced versions like GPT-4. Attackers exploit these weaknesses, highlighting the need for proactive cybersecurity measures in AI model development. This article explores two attack categories: attacks on models themselves and attacks on model applications. The former requires expertise, access to model data, and significant implementation time, while the latter is more accessible to attackers and has seen increased attention. Our study reviews over 100 recent research works, providing an in-depth analysis of each attack type. We identify the latest attack methods and explore various approaches to carry them out. We thoroughly investigate mitigation techniques, assessing their effectiveness and limitations. Furthermore, we summarize future defenses against these attacks. We also examine real-world techniques, including reported and our implemented attacks on LLMs, to consolidate our findings. Our research highlights the urgency of addressing security concerns and aims to enhance the understanding of LLM attacks, contributing to robust defense development in this evolving domain.",
      "year": 2023,
      "venue": "International Conference on Ubiquitous Security",
      "authors": [
        "Aysan Esmradi",
        "Daniel Wankit Yip",
        "C. Chan"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/952b2e2aadeec56ee4cffbe23f9f70bcfcb14b53",
      "pdf_url": "",
      "publication_date": "2023-12-18",
      "keywords_matched": [
        "LLM attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "598df44f1d21a5d1fe3940c0bb2a6128a62c1c15",
      "title": "Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment",
      "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we study a different attack scenario, called Trojan Activation Attack (TA2), which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. Our experiment results on four primary alignment tasks show that TA2 is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks.",
      "year": 2023,
      "venue": "International Conference on Information and Knowledge Management",
      "authors": [
        "Haoran Wang",
        "Kai Shu"
      ],
      "citation_count": 22,
      "url": "https://www.semanticscholar.org/paper/598df44f1d21a5d1fe3940c0bb2a6128a62c1c15",
      "pdf_url": "",
      "publication_date": "2023-11-15",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
      "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
      "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Jinhwa Kim",
        "Ali Derakhshan",
        "Ian G. Harris"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/b5a624da64475d735f0e298dc6f2f6669b5bb697",
      "pdf_url": "",
      "publication_date": "2023-10-31",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ffb2f048ae694e08a741f8aa339e0f80003497bc",
      "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases",
      "abstract": "Red-teaming has been a widely adopted way to evaluate the harmfulness of Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. Existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. However, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. In this paper, we present a new perspective on LLM safety research i.e., parametric red-teaming through Unalignment. It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. Unalignment using as few as 100 examples can significantly bypass commonly referred to as CHATGPT, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. On open-source models such as VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more than 91%. On bias evaluations, Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Rishabh Bhardwaj",
        "Soujanya Poria"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/ffb2f048ae694e08a741f8aa339e0f80003497bc",
      "pdf_url": "",
      "publication_date": "2023-10-22",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ce17ea81f018c51e02c68861e55a2d2b07f25285",
      "title": "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
      "abstract": "Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content. As LLM-integrated applications gain wider adoption, they face growing susceptibility to such attacks. This study introduces a novel evaluation framework for quantifying the resilience of applications. The framework incorporates innovative techniques designed to ensure representativeness, interpretability, and robustness. To ensure the representativeness of simulated attacks on the application, a meticulous selection process was employed, resulting in 115 carefully chosen attacks based on coverage and relevance. For enhanced interpretability, a second LLM was utilized to evaluate the responses generated from these simulated attacks. Unlike conventional malicious content classifiers that provide only a confidence score, the LLM-based evaluation produces a score accompanied by an explanation, thereby enhancing interpretability. Subsequently, a resilience score is computed by assigning higher weights to attacks with greater impact, thus providing a robust measurement of the application's resilience. To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that Llama2, the newer model exhibited higher resilience compared to ChatGLM. This finding substantiates the effectiveness of the framework, aligning with the prevailing notion that newer models tend to possess greater resilience. Moreover, the framework exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution. Overall, the framework offers valuable insights that empower organizations to make well-informed decisions to fortify their applications against potential threats from prompt injection.",
      "year": 2023,
      "venue": "2023 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)",
      "authors": [
        "Daniel Wankit Yip",
        "Aysan Esmradi",
        "C. Chan"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/ce17ea81f018c51e02c68861e55a2d2b07f25285",
      "pdf_url": "https://arxiv.org/pdf/2401.00991",
      "publication_date": "2023-12-04",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "460e48454b209e957b4942303507bf756c4a6a31",
      "title": "On the Feasibility of Specialized Ability Stealing for Large Language Code Models",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "Zongjie Li"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/460e48454b209e957b4942303507bf756c4a6a31",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "model stealing (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5fce3ce847e1cf3ccdf45fbedd5e02f90526b7e7",
      "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks",
      "abstract": "Prompt injection has emerged as a serious security threat to large language models (LLMs). At present, the current best-practice for defending against newly-discovered prompt injection techniques is to add additional guardrails to the system (e.g., by updating the system prompt or using classifiers on the input and/or output of the model.) However, in the same way that variants of a piece of malware are created to evade anti-virus software, variants of a prompt injection can be created to evade the LLM's guardrails. Ideally, when a new prompt injection technique is discovered, candidate defenses should be tested not only against the successful prompt injection, but also against possible variants. In this work, we present, a tool to assist defenders in performing automated variant analysis of known prompt injection attacks. This involves solving two main challenges: (1) automatically generating variants of a given prompt according, and (2) automatically determining whether a variant was effective based only on the output of the model. This tool can also assist in generating datasets for jailbreak and prompt injection attacks, thus overcoming the scarcity of data in this domain. We evaluate Maatphor on three different types of prompt injection tasks. Starting from an ineffective (0%) seed prompt, Maatphor consistently generates variants that are at least 60% effective within the first 40 iterations.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Ahmed Salem",
        "Andrew J. Paverd",
        "Boris K\u00f6pf"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/5fce3ce847e1cf3ccdf45fbedd5e02f90526b7e7",
      "pdf_url": "",
      "publication_date": "2023-12-12",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8c035150f883007b5af9e5bb753b78d9c0b75a55",
      "title": "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection",
      "abstract": "ChatGPT and other large language models (LLMs) have proven useful in crowdsourcing tasks, where they can effectively annotate machine learning training data. However, this means that they also have the potential for misuse, specifically to automatically answer surveys. LLMs can potentially circumvent quality assurance measures, thereby threatening the integrity of methodologies that rely on crowdsourcing surveys. In this paper, we propose a mechanism to detect LLM-generated responses to surveys. The mechanism uses\"prompt injection\", such as directions that can mislead LLMs into giving predictable responses. We evaluate our technique against a range of question scenarios, types, and positions, and find that it can reliably detect LLM-generated responses with more than 93% effectiveness. We also provide an open-source software to help survey designers use our technique to detect LLM responses. Our work is a step in ensuring that survey methodologies remain rigorous vis-a-vis LLMs.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Chaofan Wang",
        "Samuel Kernan Freire",
        "Mo Zhang",
        "Jing Wei",
        "Jorge Gon\u00e7alves",
        "V. Kostakos",
        "Z. Sarsenbayeva",
        "Christina Schneegass",
        "A. Bozzon",
        "E. Niforatos"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/8c035150f883007b5af9e5bb753b78d9c0b75a55",
      "pdf_url": "http://arxiv.org/pdf/2306.08833",
      "publication_date": "2023-06-15",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7e39237918eb9b5c0b3baa66b174a268c684a975",
      "title": "Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR",
      "abstract": "ASR systems in real applications must be adapted on the \ufb02y to correctly recognize task-speci\ufb01c contextual terms, such as contacts, application names and media entities. However, it is challenging to achieve scalability, large in-domain quality gains, and minimal out-of-domain quality regressions simultaneously. In this work, we introduce an effective neural biasing architecture called Dual-Mode NAM. Dual-Mode NAM embeds a top-k search process in its attention mechanism in a trainable fashion to perform an accurate top-k phrase selection before injecting the corresponding wordpiece context into the acoustic encoder. We further propose a controllable mechanism to enable the ASR system to be able to trade off its in-domain and out-of-domain quality at inference time. When evaluated on a large-scale biasing benchmark, the combined techniques improve a previously proposed method with an average in-domain and out-of-domain WER reduction by up to 53.3% and 12.0% relative respectively.",
      "year": 2023,
      "venue": "Interspeech",
      "authors": [
        "Zelin Wu",
        "Tsendsuren Munkhdalai",
        "Pat Rondon",
        "G. Pundak",
        "K. Sim",
        "Christopher Li"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/7e39237918eb9b5c0b3baa66b174a268c684a975",
      "pdf_url": "",
      "publication_date": "2023-08-20",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3937751dedfe07a21e3dadfdcdc04b5632b8e765",
      "title": "PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts",
      "abstract": "A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user's goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target any DST. Through experiments over state-of-the-art DSTs, the proposed framework leads to the greatest reduction in accuracy and the best attack success rate while maintaining good fluency and a low perturbation ratio. We also show how much the generated adversarial examples can bolster a DST through adversarial training. These results indicate the strength of prompt-based attacks on DSTs and leave open avenues for continued refinement.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Xiangjue Dong",
        "Yun He",
        "Ziwei Zhu",
        "James Caverlee"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/3937751dedfe07a21e3dadfdcdc04b5632b8e765",
      "pdf_url": "http://arxiv.org/pdf/2306.04535",
      "publication_date": "2023-06-07",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "927b5bd48e1210d752b8e7ce437afa8f5cc2bcdd",
      "title": "Towards Large Language Model (LLM) Forensics Using LLM-based Invocation Log Analysis",
      "abstract": "Large Language Models (LLMs) have fostered the emergence of software application architectures that improve user experiences powered by generative artificial intelligence. A range of cyber attacks are possible against an LLM. A novel approach to digital forensic analysis of LLM-integrated applications is presented for prompt injection attacks. The forensic analysis process is invoked through LLM log analysis. We propose LLM invocation logging as a critical component for enhancing digital forensic readiness in LLM-integrated applications and evaluate 13 state-of-the-art LLMs for this analysis task. Our findings demonstrate the potential utility of selected LLMs in the context of prompt-to-SQL attacks, influenced by sampling temperature and context window size parameters. We also identify limitations of our work and propose key areas for future research, for ongoing contribution to the emerging field of LLM forensics.",
      "year": 2023,
      "venue": "LAMPS@CCS",
      "authors": [
        "M. Chernyshev",
        "Zubair A. Baig",
        "R. Doss"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/927b5bd48e1210d752b8e7ce437afa8f5cc2bcdd",
      "pdf_url": "",
      "publication_date": "2023-11-19",
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b8c7592eef3c52edc60ff72cd2f5aab8483ce4ef",
      "title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition",
      "abstract": "Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning. However, these pre-trained speech models suffer from representational bias as they tend to better represent those prominent accents (i.e., native (L1) English accent) in the pre-training speech corpus than less represented accents, resulting in a deteriorated performance for non-native (L2) English accents. Although there have been some approaches to mitigate this issue, all of these methods require updating the pre-trained model weights. In this paper, we propose Information Theoretic Adversarial Prompt Tuning (INTapt), which introduces prompts concatenated to the original input that can re-modulate the attention of the pre-trained model such that the corresponding input resembles a native (L1) English speech without updating the backbone weights. INTapt is trained simultaneously in the following two manners: (1) adversarial training to reduce accent feature dependence between the original input and the prompt-concatenated input and (2) training to minimize CTC loss for improving ASR performance to a prompt-concatenated input. Experimental results show that INTapt improves the performance of L2 English and increases feature similarity between L2 and L1 accents.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Eunseop Yoon",
        "Hee Suk Yoon",
        "John Harvill",
        "M. Hasegawa-Johnson",
        "C. Yoo"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/b8c7592eef3c52edc60ff72cd2f5aab8483ce4ef",
      "pdf_url": "http://arxiv.org/pdf/2305.16371",
      "publication_date": "2023-05-25",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "051493fa4f87c54e49aeed2e289a05265d739c0d",
      "title": "ChatGPT: A Threat to Spam Filtering Systems",
      "abstract": "ChatGPT gathered the attention of millions of users shortly after its release, leading to the popularization of generative AI technologies. This research aims to emphasize one of the potential vulnerabilities associated with commonly used generative AI models. Even though it follows strict ethical and security policies, proper prompt engineering enables malicious misuse of ChatG PT such as spam email generation. In this paper, we present various scenarios of malicious prompt engineering to encourage the chatbot for spam email generation and rewriting the existing email. Also, we present the adversarial prompt engineering examples intended to evade the detection by spam filters by means of rewriting the given email while circumventing common spam characteristics. We experimentally evaluate the practical feasibility of prompt engineering on ChatGPT by assessing the performance of six common ML-based spam filters with emails modified by ChatG PT. From the experimental results, we show that adversarial prompt engineering decreases the performance of common ML-based spam filters, while NLP-based filter is robust to such modification. We also demonstrate that including ChatG PT rewritten emails in the training set leads to more robust ML-based spam filters, while the use of available AI-text detectors does not guarantee high detection rates of emails modified by the chatbot.",
      "year": 2023,
      "venue": "2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",
      "authors": [
        "Assem Utaliyeva",
        "Millati Pratiwi",
        "HyeGyoung Park",
        "Yoon-Ho Choi"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/051493fa4f87c54e49aeed2e289a05265d739c0d",
      "pdf_url": "",
      "publication_date": "2023-12-17",
      "keywords_matched": [
        "malicious prompt",
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
      "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
      "abstract": "We introduce and study the problem of adversarial arithmetic, which provides a simple yet challenging testbed for language model alignment. This problem is comprised of arithmetic questions posed in natural language, with an arbitrary adversarial string inserted before the question is complete. Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and even to steer models to a particular wrong answer. We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name\"prompt inversion rejection sampling\"(PIRS). We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops. However, we were not able to make a language model fully robust against adversarial arithmetic attacks.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "C. D. Freeman",
        "Laura Culp",
        "Aaron T Parisi",
        "Maxwell Bileschi",
        "Gamaleldin F. Elsayed",
        "Alex Rizkowsky",
        "Isabelle Simpson",
        "A. Alemi",
        "Azade Nova",
        "Ben Adlam",
        "Bernd Bohnet",
        "Gaurav Mishra",
        "Hanie Sedghi",
        "Igor Mordatch",
        "Izzeddin Gur",
        "Jaehoon Lee",
        "JD Co-Reyes",
        "Jeffrey Pennington",
        "Kelvin Xu",
        "Kevin Swersky",
        "Kshiteej Mahajan",
        "Lechao Xiao",
        "Rosanne Liu",
        "Simon Kornblith",
        "Noah Constant",
        "Peter J. Liu",
        "Roman Novak",
        "S. Vikram",
        "Yundi Qian",
        "Noah Fiedel",
        "Jascha Narain Sohl-Dickstein"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
      "pdf_url": "",
      "publication_date": "2023-11-08",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8805e369e0dd43083fc0a994ee77066a25b44d91",
      "title": "Securing the LLM Backend: Mitigating Emerging Threats and Ensuring Data Privacy for AI-Powered Financial Applications on AWS",
      "abstract": "Large Language Models (LLMs) have a significant impact on the financial technology (fintech) industry as the rapid growth of these automation types has simplified the processes of automation, customer interaction, and predictive analytics. Still, such breakthroughs bring a series of cybersecurity and privacy risk factors that compromise the data integrity, regulatory adherence, and institutional trust. This paper analyzes the security risk factors that are inherent to LLM backbend\u2019s that are deployed in AI-based financial applications on Amazon Web Services (AWS). Using a hybrid-methodology combining qualitative threat modeling with quantitative analysis of AWS-specific security settings, we use the STRIDE and MITRE ATT&CK framework to name key vulnerabilities, such as prompt injection, data exfiltration, model inversion, and privilege escalation. The next steps that we take are to assess the effectiveness of the AWS-native mitigation aspects like in-use encryption, granular Identity and Access Control (IAM) controls, network segregation, and continuous auditing. The findings show that the integration of the governance of LLM with the cloud security architecture of AWS significantly increases data confidentiality and contributes to the international financial regulation, in particular, GDPR and PCI-DSS. This paper introduces a security-by-design model of LLM backends, where explainability and data minimization as well as proactive monitoring are crucial. Therefore, the paper highlights the critical importance of safe AI-cloud integration in privacy, robustness, and trust protection in financial ecosystems",
      "year": 2023,
      "venue": "International Journal for Sciences and Technology",
      "authors": [
        "Gunjan Kumar"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/8805e369e0dd43083fc0a994ee77066a25b44d91",
      "pdf_url": "",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a59b21271b560fa305933b435bb718a407d8f296",
      "title": "Introducing Proteus. A mega prompt with personality, skills and dynamic logic based internal prompt manipulation",
      "abstract": null,
      "year": 2023,
      "venue": "",
      "authors": [
        "S. Stoltz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a59b21271b560fa305933b435bb718a407d8f296",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6",
      "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
      "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "F\u00e1bio Perez",
        "Ian Ribeiro"
      ],
      "citation_count": 608,
      "url": "https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6",
      "pdf_url": "https://arxiv.org/pdf/2211.09527",
      "publication_date": "2022-11-17",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1c475acaa1060c8318a625f24bfd88c12f367516",
      "title": "Prompt Injection: Parameterization of Fixed Inputs",
      "abstract": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We propose Prompt Injection (PI), a novel formulation of injecting the prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, PI can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for PI and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that PI can be a promising direction for conditioning language models, especially in scenarios with long and fixed prompts.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Eunbi Choi",
        "Yongrae Jo",
        "Joel Jang",
        "Minjoon Seo"
      ],
      "citation_count": 35,
      "url": "https://www.semanticscholar.org/paper/1c475acaa1060c8318a625f24bfd88c12f367516",
      "pdf_url": "http://arxiv.org/pdf/2206.11349",
      "publication_date": "2022-05-31",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "251269b9e16ab1da20cb57a669b2bfdbd0d1cd72",
      "title": "PromptAttack: Prompt-based Attack for Language Models via Gradient Search",
      "abstract": "As the pre-trained language models (PLMs) continue to grow, so do the hardware and data requirements for fine-tuning PLMs. Therefore, the researchers have come up with a lighter method called \\textit{Prompt Learning}. However, during the investigations, we observe that the prompt learning methods are vulnerable and can easily be attacked by some illegally constructed prompts, resulting in classification errors, and serious security problems for PLMs. Most of the current research ignores the security issue of prompt-based methods. Therefore, in this paper, we propose a malicious prompt template construction method (\\textbf{PromptAttack}) to probe the security performance of PLMs. Several unfriendly template construction approaches are investigated to guide the model to misclassify the task. Extensive experiments on three datasets and three PLMs prove the effectiveness of our proposed approach PromptAttack. We also conduct experiments to verify that our method is applicable in few-shot scenarios.",
      "year": 2022,
      "venue": "Natural Language Processing and Chinese Computing",
      "authors": [
        "Yundi Shi",
        "Piji Li",
        "Changchun Yin",
        "Zhaoyang Han",
        "Lu Zhou",
        "Zhe Liu"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/251269b9e16ab1da20cb57a669b2bfdbd0d1cd72",
      "pdf_url": "http://arxiv.org/pdf/2209.01882",
      "publication_date": "2022-09-05",
      "keywords_matched": [
        "malicious prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3de571104ab29bc65d7dc0b27ffb7d7683abe91c",
      "title": "ADPL: Adversarial Prompt-based Domain Adaptation for Dialogue Summarization with Knowledge Disentanglement",
      "abstract": "Traditional dialogue summarization models rely on a large-scale manually-labeled corpus, lacking generalization ability to new domains, and domain adaptation from a labeled source domain to an unlabeled target domain is important in practical summarization scenarios. However, existing domain adaptation works in dialogue summarization generally require large-scale pre-training using extensive external data. To explore the lightweight fine-tuning methods, in this paper, we propose an efficient Adversarial Disentangled Prompt Learning (ADPL) model for domain adaptation in dialogue summarization. We introduce three kinds of prompts including domain-invariant prompt (DIP), domain-specific prompt (DSP), and task-oriented prompt (TOP). DIP aims to disentangle and transfer the shared knowledge from the source domain and target domain in an adversarial way, which improves the accuracy of prediction about domain-invariant information and enhances the ability for generalization to new domains. DSP is designed to guide our model to focus on domain-specific knowledge using domain-related features. TOP is to capture task-oriented knowledge to generate high-quality summaries. Instead of fine-tuning the whole pre-trained language model (PLM), we only update the prompt networks but keep PLM fixed. Experimental results on the zero-shot setting show that the novel design of prompts can yield more coherent, faithful, and relevant summaries than baselines using the prefix-tuning, and perform at par with fine-tuning while being more efficient. Overall, our work introduces a prompt-based perspective to the zero-shot learning for dialogue summarization task and provides valuable findings and insights for future research.",
      "year": 2022,
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "authors": [
        "Lulu Zhao",
        "Fujia Zheng",
        "Weihao Zeng",
        "Keqing He",
        "Ruotong Geng",
        "Huixing Jiang",
        "Wei Wu",
        "Weiran Xu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/3de571104ab29bc65d7dc0b27ffb7d7683abe91c",
      "pdf_url": "",
      "publication_date": "2022-07-06",
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0011a76d8e02d825b3ff3066850e8135d05a1c02",
      "title": "Context injection in expert finding",
      "abstract": "Expert finding is a subject of research in information retrieval and, often, is taken to mean expertise retrieval within a specific organization. The task involves finding an expert on a given topic of interest. Even though there are several proposals in the literature, they do not consider the context in which the given expertise is bound. This paper introduces an approach to inject context into existing expertise evidence based on data extracted from the evidence. Our motivation is to provide context when describing the expertise associated with a candidate expert, allowing a user to understand the results better and choose the best candidate for the task.",
      "year": 2022,
      "venue": "Brazilian Symposium on Multimedia and the Web",
      "authors": [
        "Rodrigo Gon\u00e7alves",
        "C. Dorneles"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0011a76d8e02d825b3ff3066850e8135d05a1c02",
      "pdf_url": "",
      "publication_date": "2022-11-07",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7a3d5f1887fa908a5df303f37ec4450998caafac",
      "title": "DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage",
      "abstract": "Neural Network (NN) accelerators are currently widely deployed in various security-crucial scenarios, including image recognition, natural language processing and autonomous vehicles. Due to economic and privacy concerns, the hardware implementations of structures and designs inside NN accelerators are usually inaccessible to the public. However, these accelerators still tend to leak crucial information through Electromagnetic (EM) side channels in addition to timing and power information. In this paper, we propose an effective and efficient model stealing attack against current popular large-scale NN accelerators deployed on hardware platforms through side-channel information. Specifically, the proposed attack approach contains two stages: 1) Inferring the underlying network architecture through EM sidechannel information; 2) Estimating the parameters, especially the weights, through a margin-based, adversarial active learning method. The experimental results show that the proposed attack approach can accurately recover the large-scale NN through EM side-channel information leakages. Overall, our attack highlights the importance of masking EM traces for large-scale NN accelerators in real-world applications.",
      "year": 2020,
      "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
      "authors": [
        "Honggang Yu",
        "Haocheng Ma",
        "Kaichen Yang",
        "Yiqiang Zhao",
        "Yier Jin"
      ],
      "citation_count": 112,
      "url": "https://www.semanticscholar.org/paper/7a3d5f1887fa908a5df303f37ec4450998caafac",
      "pdf_url": "",
      "publication_date": "2020-12-07",
      "keywords_matched": [
        "DNN weights leakage (title)"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b373f8249e5a45c89c38e0b45f499f38697b68a6",
      "title": "Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection",
      "abstract": "When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must \u201cguess\u201d this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.",
      "year": 2019,
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
      "authors": [
        "Amit Moryossef",
        "Roee Aharoni",
        "Yoav Goldberg"
      ],
      "citation_count": 53,
      "url": "https://www.semanticscholar.org/paper/b373f8249e5a45c89c38e0b45f499f38697b68a6",
      "pdf_url": "https://doi.org/10.18653/v1/w19-3807",
      "publication_date": "2019-03-08",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "fd49485eb446a4f26a805ed83a44745629ba7d64",
      "title": "Context Injection as a Tool for Measuring Context Usage in Machine Learning",
      "abstract": null,
      "year": 2017,
      "venue": "Asian Conference on Intelligent Information and Database Systems",
      "authors": [
        "Maciej Huk"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/fd49485eb446a4f26a805ed83a44745629ba7d64",
      "pdf_url": "",
      "publication_date": "2017-04-03",
      "keywords_matched": [
        "context injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "36e5ad5a040d2105a34a647cb5b230c5bbb516ed",
      "title": "Prompt injections of highly relativistic electrons induced by interplanetary shocks: A statistical study of Van Allen Probes observations",
      "abstract": null,
      "year": 2016,
      "venue": "",
      "authors": [
        "Q. Schiller",
        "S. Kanekal",
        "L. Jian",
        "X. Li",
        "A. Jones",
        "D. Baker",
        "A. Jaynes",
        "H. Spence"
      ],
      "citation_count": 44,
      "url": "https://www.semanticscholar.org/paper/36e5ad5a040d2105a34a647cb5b230c5bbb516ed",
      "pdf_url": "",
      "publication_date": "2016-12-26",
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c5bf7b05626a26de3bad738f9120bdcfe67d7cc5",
      "title": "Randomized Instruction Injection to Counter Power Analysis Attacks",
      "abstract": null,
      "year": 2012,
      "venue": "TECS",
      "authors": [
        "Jude Angelo Ambrose",
        "R. Ragel",
        "S. Parameswaran"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/c5bf7b05626a26de3bad738f9120bdcfe67d7cc5",
      "pdf_url": "",
      "publication_date": "2012-09-01",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04e98cf7377bd7f6996113f8320e1744cd26e024",
      "title": "RIJID: Random Code Injection to Mask Power Analysis based Side Channel Attacks",
      "abstract": "Side channel attacks are becoming a major threat to the security of embedded systems. Countermeasures proposed to overcome Simple Power Analysis (SPA) and Differential Power Analysis (DPA), are data masking, table masking, current flattening, circuitry level solutions, dummy instruction insertions and balancing bit-flips. All these techniques are either susceptible to multi-order side channel attacks, not sufficiently generic to cover all encryption algorithms, or burden the system with high area cost, run-time or energy consumption. A HW/SW based randomized instruction injection technique is proposed in this paper to overcome the pitfalls of previous countermeasures. Our technique injects random instructions at random places during the execution of an application which protects the system from both SPA and DPA. Further, we devise a systematic method to measure the security level of a power sequence and use it to measure the number of random instructions needed, to suitably confuse the adversary. Our processor model costs 1.9% in additional area for a simplescalar processor, and costs on average 29.8% in runtime and 27.1% in additional energy consumption for six industry standard cryptographic algorithms.",
      "year": 2007,
      "venue": "2007 44th ACM/IEEE Design Automation Conference",
      "authors": [
        "Jude Angelo Ambrose",
        "R. Ragel",
        "S. Parameswaran"
      ],
      "citation_count": 67,
      "url": "https://www.semanticscholar.org/paper/04e98cf7377bd7f6996113f8320e1744cd26e024",
      "pdf_url": "https://doi.org/10.1109/dac.2007.375214",
      "publication_date": "2007-06-04",
      "keywords_matched": [
        "instruction injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "22b90100aff2216e30933fd0d7ce576739ddc134",
      "title": "Prompt injection of relativistic protons from the September 1, 1971 solar flare",
      "abstract": null,
      "year": 1982,
      "venue": "",
      "authors": [
        "E. Cliver"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/22b90100aff2216e30933fd0d7ce576739ddc134",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cf2a8e8d75258db16bb4ab026a6041aeee984092",
      "title": "S KEW A CT : Red Teaming Large Language Models via Activation-Skewed Adversarial Prompt Optimization",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Hanxi Guo",
        "Siyuan Cheng",
        "Guanhong Tao",
        "Guangyu Shen",
        "Zhuo Zhang",
        "Shengwei An",
        "Kaiyuan Zhang",
        "Xiangyu Zhang"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/cf2a8e8d75258db16bb4ab026a6041aeee984092",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "adversarial prompt"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2fe003d159227305ae6d04451f6492ba05f4edb",
      "title": "LLM-PIEval: A benchmark for indirect prompt injection attacks in Large Language Models",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Anil Ramakrishna",
        "Jimit Majmudar",
        "Rahul Gupta",
        "Devamanyu Hazarika"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/c2fe003d159227305ae6d04451f6492ba05f4edb",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack",
        "indirect prompt injection"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "62fa7e797a12235eef7e3a50efa8f4d82de0b39b",
      "title": "Advances in AI-Driven Cybersecurity: Tackling Prompt Injection Attacks through Adversarial Learning",
      "abstract": null,
      "year": null,
      "venue": "",
      "authors": [
        "Saroj Ghimire",
        "Suman Thapaliya"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/62fa7e797a12235eef7e3a50efa8f4d82de0b39b",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "prompt injection",
        "prompt injection attack"
      ],
      "first_seen": "2026-01-06"
    }
  ],
  "config": "configs/ml01a_prompt_injection.yaml"
}