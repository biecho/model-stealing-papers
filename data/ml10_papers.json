{
  "updated": "2026-01-06",
  "total": 143,
  "owasp_id": "ML10",
  "owasp_name": "Model Poisoning",
  "description": "Attacks that embed backdoors or trojans in machine learning models. This\n        includes neural trojans, backdoor attacks, trigger-based attacks, and\n        techniques to insert hidden malicious behavior that activates on specific\n        inputs while maintaining normal performance on clean data.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "8862c9a1b92c8f14f86734d1238c5126310686a2",
      "title": "CIP-ES: Causal Input Perturbation for Explanation Surrogates",
      "abstract": "With current advances in Machine Learning and its growing use in high-impact scenarios, the demand for interpretable and explainable models becomes crucial. Causality research tries to go beyond statistical correlations by focusing on causal relationships, which is fundamental for Interpretable and Explainable Artificial Intelligence. In this paper, we perturb the input for explanation surrogates based on causal graphs. We present an approach to combine surrogate-based explanations with causal knowledge. We apply the perturbed data to the Local Interpretable Model-agnostic Explanations (LIME) approach to showcase how causal graphs improve explanations of surrogate models. We thus integrate features from both domains by adding a causal component to local explanations. The proposed approach enables explanations that suit the expectations of the user by having the user define an appropriate causal graph. Accordingly, these expectations are true to the user. We demonstrate the suitability of our method using real world data.",
      "year": 2023,
      "venue": "CACML",
      "authors": [
        "Sebastian Steindl",
        "Martin Surner"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8862c9a1b92c8f14f86734d1238c5126310686a2",
      "pdf_url": "",
      "publication_date": "2023-03-17",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "42d3207b464f535f5b115a409c45935afcc9bef5",
      "title": "Explaining Deep Graph Networks via Input Perturbation",
      "abstract": "Deep graph networks (DGNs) are a family of machine learning models for structured data which are finding heavy application in life sciences (drug repurposing, molecular property predictions) and on social network data (recommendation systems). The privacy and safety-critical nature of such domains motivates the need for developing effective explainability methods for this family of models. So far, progress in this field has been challenged by the combinatorial nature and complexity of graph structures. In this respect, we present a novel local explanation framework specifically tailored to graph data and DGNs. Our approach leverages reinforcement learning to generate meaningful local perturbations of the input graph, whose prediction we seek an interpretation for. These perturbed data points are obtained by optimizing a multiobjective score taking into account similarities both at a structural level as well as at the level of the deep model outputs. By this means, we are able to populate a set of informative neighboring samples for the query graph, which is then used to fit an interpretable model for the predictive behavior of the deep network locally to the query graph prediction. We show the effectiveness of the proposed explainer by a qualitative analysis on two chemistry datasets, TOX21 and Estimated SOLubility (ESOL) and by quantitative results on a benchmark dataset for explanations, CYCLIQ.",
      "year": 2022,
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "authors": [
        "D. Bacciu",
        "Danilo Numeroso"
      ],
      "citation_count": 17,
      "url": "https://www.semanticscholar.org/paper/42d3207b464f535f5b115a409c45935afcc9bef5",
      "pdf_url": "",
      "publication_date": "2022-04-21",
      "keywords_matched": [
        "input perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d2238af352969a6a2cc9c4ae410cd6319d13616f",
      "title": "DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints",
      "abstract": "Backdoor attack is a type of serious security threat to deep learning models. An adversary can provide users with a model trained on poisoned data to manipulate prediction behavior in test stage using a backdoor. The backdoored models behave normally on clean images, yet can be activated and output incorrect prediction if the input is stamped with a specific trigger pattern. Most existing backdoor attacks focus on manually defining imperceptible triggers in input space without considering the abnormality of triggers' latent representations in the poisoned model. These attacks are susceptible to backdoor detection algorithms and even visual inspection. In this paper, We propose a novel and stealthy backdoor attack - DEFEAT. It poisons the clean data using adaptive imperceptible perturbation and restricts latent representation during training process to strengthen our attack's stealthiness and resistance to defense algorithms. We conduct extensive experiments on multiple image classifiers using real-world datasets to demonstrate that our attack can 1) hold against the state-of-the-art defenses, 2) deceive the victim model with high attack success without jeopardizing model utility, and 3) provide practical stealthiness on image data.",
      "year": 2022,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Zhendong Zhao",
        "Xiaojun Chen",
        "Yu Xuan",
        "Ye Dong",
        "Dakui Wang",
        "K. Liang"
      ],
      "citation_count": 86,
      "url": "https://www.semanticscholar.org/paper/d2238af352969a6a2cc9c4ae410cd6319d13616f",
      "pdf_url": "https://repository.tudelft.nl/file/File_f3360d07-f829-4859-8e1c-200f8cd6b7a9",
      "publication_date": "2022-06-01",
      "keywords_matched": [
        "input perturbation",
        "perturbation attack",
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "223ebc1abe8e8b0367ed0480f59d9721dc3c13a8",
      "title": "KerbNet: A QoE-Aware Kernel-Based Backdoor Attack Framework",
      "abstract": "Deep neural networks are vulnerable to backdoor attacks, where a specially-designed trigger will lead to misclassification of any benign samples. However, existing backdoor attacks usually impose conspicuous patch triggers on images, which are easily detected by humans and defense algorithms. Existing works on invisible triggers, however, either have reduced attack success rate or yield detectable patterns to visual inspections. In this article, we propose KerbNet, a kernel-based backdoor attack framework, which applies kernel operations to clean samples as the trigger to incur misclassification. The kernel-processed samples achieve a high attack success rate while appearing natural with high Quality-of-Experience (QoE). We carefully design the kernel trigger generation algorithm by exploiting the neural network structure to propagate the influence of the trigger to the target misclassification label under the QoE constraint. We conduct extensive experiments on five datasets, i.e., MNIST, GTSRB, CIFAR-10, CelebA, and ImageNette to evaluate the effectiveness and practicality of KerbNet under the impact of various factors, including neuron-residing layer, kernel size, base image, loss function, model structure, and so on. We also show that our proposed attacks can evade state-of-the-art defense strategies and visual inspections. Code will be available after publication.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Huayang Huang",
        "Weihan Kong",
        "Ziyao Wang",
        "Chao Shen",
        "Qianqian Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/223ebc1abe8e8b0367ed0480f59d9721dc3c13a8",
      "pdf_url": "",
      "publication_date": "2024-07-01",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cd844915e1dd1a6ea348a9c0a645271cd33a1024",
      "title": "A Practical Trigger-Free Backdoor Attack on Neural Networks",
      "abstract": "Backdoor attacks on deep neural networks have emerged as significant security threats, especially as DNNs are increasingly deployed in security-critical applications. However, most existing works assume that the attacker has access to the original training data. This limitation restricts the practicality of launching such attacks in real-world scenarios. Additionally, using a specified trigger to activate the injected backdoor compromises the stealthiness of the attacks. To address these concerns, we propose a trigger-free backdoor attack that does not require access to any training data. Specifically, we design a novel fine-tuning approach that incorporates the concept of malicious data into the concept of the attacker-specified class, resulting the misclassification of trigger-free malicious data into the attacker-specified class. Furthermore, instead of relying on training data to preserve the model's knowledge, we employ knowledge distillation methods to maintain the performance of the infected model on benign samples, and introduce a parameter importance evaluation mechanism based on elastic weight constraints to facilitate the fine-tuning of the infected model. The effectiveness, practicality, and stealthiness of the proposed attack are comprehensively evaluated on three real-world datasets. Furthermore, we explore the potential for enhancing the attack through the use of auxiliary datasets and model inversion.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiahao Wang",
        "Xianglong Zhang",
        "Xiuzhen Cheng",
        "Pengfei Hu",
        "Guoming Zhang"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/cd844915e1dd1a6ea348a9c0a645271cd33a1024",
      "pdf_url": "",
      "publication_date": "2024-08-21",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3e9ee43d8914fbc4dd7e130ae70eab34b4b7bd74",
      "title": "Backdoor Attack on Deep Neural Networks in Perception Domain",
      "abstract": "As deep neural networks (DNNs) are widely deployed in various applications, the security of pretrained DNNs is crucial since backdoors can be introduced through poisoned training. A backdoored DNN model works properly when benign inputs are provided, but it produces targeted misclassification on the inputs with an intended pattern known as a trojan trigger. Current technologies for trigger generation mainly focus on the physical and model domains. In this work, we investigate trojan triggers from the perception domain, especially the physical process of collecting light rays when they pass through the lens and hit the optical sensors. A new type of backdoor attack, Lens Flare attack, is introduced. It concentrates on the perception domain and is more physically plausible and stealthy. Experiments show that the DNNs with Lens Flare backdoor can achieve accuracy comparable to their original counterpart on benign input while misclassifying the input with high certainty if the Lens Flare trigger is present. It is also demonstrated that the Lens Flare backdoor is resistant to state-of-the-art backdoor defenses.",
      "year": 2023,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Xiaoxing Mo",
        "L. Zhang",
        "Nan Sun",
        "Wei Luo",
        "Shang Gao"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3e9ee43d8914fbc4dd7e130ae70eab34b4b7bd74",
      "pdf_url": "",
      "publication_date": "2023-06-18",
      "keywords_matched": [
        "misclassification attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "abdb6e912fe86a60600b438b0e36b502f6412b24",
      "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
      "abstract": "The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \\textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \\url{https://github.com/git-disl/Vaccine}.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Ling Liu"
      ],
      "citation_count": 78,
      "url": "https://www.semanticscholar.org/paper/abdb6e912fe86a60600b438b0e36b502f6412b24",
      "pdf_url": "",
      "publication_date": "2024-02-02",
      "keywords_matched": [
        "perturbation attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0223d77aa79957097b09fb8d83fc44a61b00d076",
      "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks",
      "abstract": "In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of non-watermarked images detected as watermarked ones) upon an application of diffusion purification attack. To validate our theoretical findings, we also provide empirical evidence demonstrating that diffusion purification effectively removes low perturbation budget watermarks by applying minimal changes to images. The diffusion purification attack is ineffective for high perturbation watermarking methods where notable changes are applied to images. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images identified as watermarked ones, damaging the reputation of the developers. In particular, with black-box access to the watermarking method, a watermarked noise image can be generated and added to real images, causing them to be incorrectly classified as watermarked. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Mehrdad Saberi",
        "Vinu Sankar Sadasivan",
        "Keivan Rezaei",
        "Aounon Kumar",
        "Atoosa Malemir Chegini",
        "Wenxiao Wang",
        "S. Feizi"
      ],
      "citation_count": 70,
      "url": "https://www.semanticscholar.org/paper/0223d77aa79957097b09fb8d83fc44a61b00d076",
      "pdf_url": "https://arxiv.org/pdf/2310.00076",
      "publication_date": "2023-09-29",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9e3125c041e96be417ae53c7f9d02508234e4751",
      "title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression",
      "abstract": "Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Zhuoran Liu",
        "Zhengyu Zhao",
        "M. Larson"
      ],
      "citation_count": 47,
      "url": "https://www.semanticscholar.org/paper/9e3125c041e96be417ae53c7f9d02508234e4751",
      "pdf_url": "http://arxiv.org/pdf/2301.13838",
      "publication_date": "2023-01-31",
      "keywords_matched": [
        "image perturbation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "687730f512b34b75c873f2e4ccd669c1b9907eb9",
      "title": "MaSS: Model-agnostic, Semantic and Stealthy Data Poisoning Attack on Knowledge Graph Embedding",
      "abstract": "Open-source knowledge graphs are attracting increasing attention. Nevertheless, the openness also raises the concern of data poisoning attacks, that is, the attacker could submit malicious facts to bias the prediction of knowledge graph embedding (KGE) models. Existing studies on such attacks adopt a clear-box setting and neglect the semantic information of the generated facts, making them fail to attack in real-world scenarios. In this work, we consider a more rigorous setting and propose a model-agnostic, semantic, and stealthy data poisoning attack on KGE models from a practical perspective. The main design of our work is to inject indicative paths to make the infected model predict certain malicious facts. With the aid of the proposed opaque-box path injection theory, we theoretically reveal that the attack success rate under the opaque-box setting is determined by the plausibility of triplets on the indicative path. Based on this, we develop a novel and efficient algorithm to search paths that maximize the attack goal, satisfy certain semantic constraints, and preserve certain stealthiness, i.e., the normal functionality of the target KGE will not be influenced although it predicts wrong facts given certain queries. Through extensive evaluation of benchmark datasets and 6 typical knowledge graph embedding models as the victims, we validate the effectiveness in terms of attack success rate (ASR) under opaque-box setting and stealthiness. For example, on FB15k-237, our attack achieves a ASR on DeepPath, with an average ASR over when attacking various KGE models under the opaque-box setting.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "X. You",
        "Beina Sheng",
        "Daizong Ding",
        "Mi Zhang",
        "Xudong Pan",
        "Min Yang",
        "Fuli Feng"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/687730f512b34b75c873f2e4ccd669c1b9907eb9",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ae7d4ab81a3a577e914fbc9876286f3bee7e865",
      "title": "Stealthy Targeted Data Poisoning Attack on Knowledge Graphs",
      "abstract": "A host of different KG embedding techniques have emerged recently and have been empirically shown to be very effective in accurately predicting missing facts in a KG, thus improving its coverage and quality. Unfortunately, embedding techniques can fall prey to adversarial data poisoning attack. In this form of attack, facts may be added to or deleted from a KG, called performing perturbations, that results in the manipulation of the plausibility of target facts in a KG. While recent works confirm this intuition, the attacks considered there ignore the risk of exposure. Intuitively, an attack is of limited value if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the exposure risk and propose a novel problem of attacking a KG by means of perturbations where the goal is to maximize the manipulation of the target fact\u2019s plausibility while keeping the risk of exposure under a given budget. We design a deep reinforcement learning-based framework, called RATA, that learns to use low-risk perturbations without compromising on the performance, i.e., manipulation of target fact plausibility. We test the performance of RATA against recently proposed strategies for KG attacks, on two different benchmark datasets and on different kinds of target facts. Our experiments show that RATA achieves state-of-the-art performance even while using a fraction of the risk.",
      "year": 2021,
      "venue": "IEEE International Conference on Data Engineering",
      "authors": [
        "Prithu Banerjee",
        "Lingyang Chu",
        "Yong Zhang",
        "L. Lakshmanan",
        "Lanjun Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/2ae7d4ab81a3a577e914fbc9876286f3bee7e865",
      "pdf_url": "",
      "publication_date": "2021-04-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41c5c6b6074d9672e3bd639f5a6fc5a3f2422c2f",
      "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
      "abstract": "We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of settings, they fail to remove the effects of data poisoning across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, are required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned data without having to retrain, our work suggests that these methods are not yet ``ready for prime time,'' and currently provide limited benefit over retraining.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Martin Pawelczyk",
        "Jimmy Z. Di",
        "Yiwei Lu",
        "Gautam Kamath",
        "Ayush Sekhari",
        "Seth Neel"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/41c5c6b6074d9672e3bd639f5a6fc5a3f2422c2f",
      "pdf_url": "",
      "publication_date": "2024-06-25",
      "keywords_matched": [
        "data poisoning attack",
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "77702dc45e9af19b287e9347cecc932e33cfd724",
      "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning",
      "abstract": "Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Tingchen Fu",
        "Mrinank Sharma",
        "Philip Torr",
        "Shay B. Cohen",
        "David Krueger",
        "Fazl Barez"
      ],
      "citation_count": 25,
      "url": "https://www.semanticscholar.org/paper/77702dc45e9af19b287e9347cecc932e33cfd724",
      "pdf_url": "",
      "publication_date": "2024-10-11",
      "keywords_matched": [
        "data poisoning attack",
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "df4aa9f6d933299f841832feee3708a02c3008a8",
      "title": "Model Poisoning Attack on Neural Network Without Reference Data",
      "abstract": "Due to the substantial computational cost of neural network training, adopting third-party models has become increasingly popular. However, recent works demonstrate that third-party models can be poisoned. Nonetheless, most model poisoning attacks require reference data, e.g., training dataset or data belonging to the target label, making them difficult to launch in practice. In this paper, we propose a reference data independent model poisoning attack that can (1) directly search for sensitive features with respect to the target label, (2) quantify the positive and negative effects of the model parameters on sensitive features, and (3) accomplish the training of poisoned model by our parameter selective update strategy. The extensive evaluation on datasets with a few classes and numerous classes show that the attack is (I) effective: the trigger input can be labeled as a deliberate class by the poisoned model with high probability; (II) covert: the performance of the poisoned model is almost indistinguishable from the intact model on non-trigger inputs; and (III) straightforward: an adversary only needs a little background knowledge to launch the attack. Overall, the evaluation results show that our attack achieves 95%, 100%, 81%, 96%, and 96% success rates on Cifar10, Cifar100, ISIC2018, FaceScrub, and ImageNet datasets, respectively.",
      "year": 2023,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Xiang Zhang",
        "Huanle Zhang",
        "Guoming Zhang",
        "Hong Li",
        "Dongxiao Yu",
        "Xiuzhen Cheng",
        "Pengfei Hu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/df4aa9f6d933299f841832feee3708a02c3008a8",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d13046e66581cc01f3538f601c5535f210b38476",
      "title": "Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving",
      "abstract": "In autonomous driving, behavior prediction is funda-mental for safe motion planning, hence the security and ro-bustness of prediction models against adversarial attacks are of paramount importance. We propose a novel adver-sarial backdoor attack against trajectory prediction models as a means of studying their potential vulnerabilities. Our attack affects the victim at training time via naturalistic, hence stealthy, poisoned samples crafted using a novel two-step approach. First, the triggers are crafted by perturbing the trajectory of attacking vehicle and then disguised by transforming the scene using a bi-level optimization technique. The proposed attack does not depend on a particu-lar model architecture and operates in a black-box manner, thus can be effective without any knowledge of the victim model. We conduct extensive empirical studies using state-of-the-art prediction models on two benchmark datasets using metrics customized for trajectory prediction. We show that the proposed attack is highly effective, as it can sig-nificantly hinder the performance of prediction models, un-noticeable by the victims, and efficient as it forces the vic-tim to generate malicious behavior even under constrained conditions. Via ablative studies, we analyze the impact of different attack design choices followed by an evaluation of existing defence mechanisms against the proposed attack.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Mozhgan Pourkeshavarz",
        "M. Sabokrou",
        "Amir Rasouli"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/d13046e66581cc01f3538f601c5535f210b38476",
      "pdf_url": "http://arxiv.org/pdf/2306.15755",
      "publication_date": "2023-06-27",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d6abad29855389dd6055ae20f2d329a53deeedc9",
      "title": "Turning Generative Models Degenerate: The Power of Data Poisoning Attacks",
      "abstract": "The increasing use of large language models (LLMs) trained by third parties raises significant security concerns. In particular, malicious actors can introduce backdoors through poisoning attacks to generate undesirable outputs. While such attacks have been extensively studied in image domains and classification tasks, they remain underexplored for natural language generation (NLG) tasks. To address this gap, we conduct an investigation of various poisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a Parameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness across two generative tasks: text summarization and text completion; and we also introduce new metrics to quantify the success and stealthiness of such NLG poisoning attacks. Through our experiments, we find that the prefix-tuning hyperparameters and trigger designs are the most crucial factors to influence attack success and stealthiness. Moreover, we demonstrate that existing popular defenses are ineffective against our poisoning attacks. Our study presents the first systematic approach to understanding poisoning attacks targeting NLG tasks during fine-tuning via PEFT across a wide range of triggers and attack settings. We hope our findings will aid the AI security community in developing effective defenses against such threats.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shuli Jiang",
        "S. Kadhe",
        "Yi Zhou",
        "Farhan Ahmed",
        "Ling Cai",
        "Nathalie Baracaldo"
      ],
      "citation_count": 12,
      "url": "https://www.semanticscholar.org/paper/d6abad29855389dd6055ae20f2d329a53deeedc9",
      "pdf_url": "",
      "publication_date": "2024-07-17",
      "keywords_matched": [
        "data poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a33c83bd6d5119950415ea67b8643c964e2621d4",
      "title": "FLAIR: Defense against Model Poisoning Attack in Federated Learning",
      "abstract": "Federated learning\u2014multi-party, distributed learning in a decentralized environment\u2014is vulnerable to model poisoning attacks, more so than centralized learning. This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Bulyan, FABA, and FoolsGold. However, a recently developed untargeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. In this work, we develop FLAIR\u2014a defense against this directed deviation attack (DDA), a state-of-the-art model poisoning attack. FLAIR is based on our intuition that in federated learning, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. FLAIR assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that where the existing defense baselines of FABA [IJCAI \u201919], FoolsGold [Usenix \u201920], and FLTrust [NDSS \u201921] fail when 20-30% of the clients are malicious, FLAIR provides byzantine-robustness upto a malicious client percentage of 45%. We also show that FLAIR provides robustness against even a white-box version of DDA.",
      "year": 2023,
      "venue": "ACM Asia Conference on Computer and Communications Security",
      "authors": [
        "Atul Sharma",
        "Wei Chen",
        "Joshua C. Zhao",
        "Qiang Qiu",
        "S. Bagchi",
        "S. Chaterji"
      ],
      "citation_count": 18,
      "url": "https://www.semanticscholar.org/paper/a33c83bd6d5119950415ea67b8643c964e2621d4",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3579856.3582836",
      "publication_date": "2023-07-10",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a913d77fe5b479f5e8d008bb342c735d2bf77322",
      "title": "FedRecAttack: Model Poisoning Attack to Federated Recommendation",
      "abstract": "Federated Recommendation (FR) has received con-siderable popularity and attention in the past few years. In FR, for each user, its feature vector and interaction data are kept locally on its own client thus are private to others. Without the access to above information, most existing poisoning attacks against recommender systems or federated learning lose validity. Benifiting from this characteristic, FR is commonly considered fairly secured. However, we argue that there is still possible and necessary security improvement could be made in FR. To prove our opinion, in this paper we present FedRecAttack, a model poisoning attack to FR aiming to raise the exposure ratio of target items. In most recommendation scenarios, apart from pri-vate user-item interactions (e.g., clicks, watches and purchases), some interactions are public (e.g., likes, follows and comments). Motivated by this point, in FedRecAttack we make use of the public interactions to approximate users' feature vectors, thereby attacker can generate poisoned gradients accordingly and control malicious users to upload the poisoned gradients in a well-designed way. To evaluate the effectiveness and side effects of FedRecAttack, we conduct extensive experiments on three real-world datasets of different sizes from two completely different scenarios. Experimental results demonstrate that our proposed FedRecAttack achieves the state-of-the-art effectiveness while its side effects are negligible. Moreover, even with small proportion (3%) of malicious users and small proportion (1%) of public interactions, FedRecAttack remains highly effective, which reveals that FR is more vulnerable to attack than people commonly considered.",
      "year": 2022,
      "venue": "IEEE International Conference on Data Engineering",
      "authors": [
        "Dazhong Rong",
        "Shuai Ye",
        "Ruoyan Zhao",
        "Hon Ning Yuen",
        "Jianhai Chen",
        "Qinming He"
      ],
      "citation_count": 77,
      "url": "https://www.semanticscholar.org/paper/a913d77fe5b479f5e8d008bb342c735d2bf77322",
      "pdf_url": "https://arxiv.org/pdf/2204.01499",
      "publication_date": "2022-04-01",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "946203983a2df10caea3bd26da7a95e2e9322b66",
      "title": "Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis",
      "abstract": "Model poisoning attacks on federated learning intrude in the entire system via compromising an edge model, resulting in malfunctioning of machine learning models. Such compromised models are tampered with to perform adversary-desired behaviors. In particular, we considered a semi-targeted situation where the source class is predetermined however the target class is not. The goal is to cause the global classifier to misclassify data of the source class. Though approaches such as label flipping have been adopted to inject poisoned parameters into federated learning, it has been shown that their performances are usually class-sensitive varying with different target classes applied. Typically, an attack can become less effective when shifting to a different target class. To overcome this challenge, we propose the Attacking Distance-aware Attack (ADA) to enhance a poisoning attack by finding the optimized target class in the feature space. Moreover, we studied a more challenging situation where an adversary had limited prior knowledge about a client's data. To tackle this problem, ADA deduces pair-wise distances between different classes in the latent feature space from shared model parameters based on the backward error analysis. We performed extensive empirical evaluations on ADA by varying the factor of attacking frequency in three different image classification tasks. As a result, ADA succeeded in increasing the attack performance by 1.8 times in the most challenging case with an attacking frequency of 0.01.",
      "year": 2022,
      "venue": "IEEE International Joint Conference on Neural Network",
      "authors": [
        "Yuwei Sun",
        "H. Ochiai",
        "Jun Sakuma"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/946203983a2df10caea3bd26da7a95e2e9322b66",
      "pdf_url": "https://arxiv.org/pdf/2203.11633",
      "publication_date": "2022-03-22",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2824ad1c27459d43e3482a6ec56579fb3e751233",
      "title": "Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning",
      "abstract": "Are Federated Learning (FL) systems free from backdoor poisoning with the arsenal of various defense strategies deployed? This is an intriguing problem with significant practical implications regarding the utility of FL services. Despite the recent flourish of poisoning-resilient FL methods, our study shows that carefully tuning the collusion between malicious participants can minimize the trigger-induced bias of the poisoned local model from the poison-free one, which plays the key role in delivering stealthy backdoor attacks and circumventing a wide spectrum of state-of-the-art defense methods in FL. In our work, we instantiate the attack strategy by proposing a distributed backdoor attack method, namely Cerberus Poisoning (CerP). It jointly tunes the backdoor trigger and controls the poisoned model changes on each malicious participant to achieve a stealthy yet successful backdoor attack against a wide spectrum of defensive mechanisms of federated learning techniques. Our extensive study on 3 large-scale benchmark datasets and 13 mainstream defensive mechanisms confirms that Cerberus Poisoning raises a significantly severe threat to the integrity and security of federated learning practices, regardless of the flourish of robust Federated Learning methods.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Xiaoting Lyu",
        "Yufei Han",
        "Wen Wang",
        "Jingkai Liu",
        "Bin Wang",
        "Jiqiang Liu",
        "Xiangliang Zhang"
      ],
      "citation_count": 92,
      "url": "https://www.semanticscholar.org/paper/2824ad1c27459d43e3482a6ec56579fb3e751233",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26083/25855",
      "publication_date": "2023-06-26",
      "keywords_matched": [
        "poisoning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d18362341e4d7ffdbc94fef984fb686950f928b3",
      "title": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data",
      "abstract": "Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \\emph{anti-backdoor learning}, aiming to train \\emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \\emph{clean} and the \\emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data; 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \\emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \\url{https://github.com/bboylyg/ABL}.",
      "year": 2021,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Yige Li",
        "X. Lyu",
        "Nodens F. Koren",
        "L. Lyu",
        "Bo Li",
        "Xingjun Ma"
      ],
      "citation_count": 399,
      "url": "https://www.semanticscholar.org/paper/d18362341e4d7ffdbc94fef984fb686950f928b3",
      "pdf_url": "",
      "publication_date": "2021-10-22",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "430a024aaa7b6f6fc759b9b1005a66717fb4d117",
      "title": "Progressive Poisoned Data Isolation for Training-time Backdoor Defense",
      "abstract": "Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning. It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset. Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes. In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones. Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model. Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data. Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense. For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods. The code is available at https://github.com/RorschachChen/PIPD.git.",
      "year": 2023,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yiming Chen",
        "Haiwei Wu",
        "Jiantao Zhou"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/430a024aaa7b6f6fc759b9b1005a66717fb4d117",
      "pdf_url": "",
      "publication_date": "2023-12-20",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "02bf602272266def52bef6060cead5f70a348504",
      "title": "The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data",
      "abstract": "Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Zixuan Zhu",
        "Rui Wang",
        "Cong Zou",
        "Lihua Jing"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/02bf602272266def52bef6060cead5f70a348504",
      "pdf_url": "http://arxiv.org/pdf/2404.11265",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a6fc0530686cade3899bdb1fce6292dcddf7ce06",
      "title": "Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data",
      "abstract": "Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhibiting robust resistance to backdoor triggers.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Lukas Struppek",
        "Martin Hentschel",
        "Clifton A. Poth",
        "Dominik Hintersdorf",
        "K. Kersting"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/a6fc0530686cade3899bdb1fce6292dcddf7ce06",
      "pdf_url": "https://arxiv.org/pdf/2310.06372",
      "publication_date": "2023-10-10",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90de1938a64d117d61b9e7149d2981df49b81433",
      "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a\"jailbreak backdoor\"into the model. The backdoor embeds a trigger word into the model that acts like a universal\"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Javier Rando",
        "Florian Tram\u00e8r"
      ],
      "citation_count": 106,
      "url": "https://www.semanticscholar.org/paper/90de1938a64d117d61b9e7149d2981df49b81433",
      "pdf_url": "",
      "publication_date": "2023-11-24",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6dc724ddae9f5e1913036dd532af6a89b3267c1f",
      "title": "Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples",
      "abstract": null,
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Weixin Chen",
        "Baoyuan Wu",
        "Haoqian Wang"
      ],
      "citation_count": 100,
      "url": "https://www.semanticscholar.org/paper/6dc724ddae9f5e1913036dd532af6a89b3267c1f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoned data",
        "backdoor removal",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5e31fa4e69d1a3587230f5d134c0b7e2ed84a742",
      "title": "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models",
      "abstract": "Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.",
      "year": 2021,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Wenkai Yang",
        "Lei Li",
        "Zhiyuan Zhang",
        "Xuancheng Ren",
        "Xu Sun",
        "Bin He"
      ],
      "citation_count": 169,
      "url": "https://www.semanticscholar.org/paper/5e31fa4e69d1a3587230f5d134c0b7e2ed84a742",
      "pdf_url": "https://aclanthology.org/2021.naacl-main.165.pdf",
      "publication_date": "2021-03-29",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e01f18b468b8e4c3634b1ef3ca5faca019c3ee2c",
      "title": "FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-Based Node Classification",
      "abstract": "Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which will generate a poisoned graph as the input to the GNN models. We present FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison injected by attackers. Specifically, FocusedCleaner provides a sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reverse the attack process to steadily sanitize the graph while the detection module provides the \u201cfocus\u201d \u2013 a narrowed and more accurate search region \u2013 to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. As an important application, we show that the adversarial robustness of GNNs trained over the sanitized graph for the node classification task is significantly improved. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art baselines both on poisoned graph sanitation and improving robustness.",
      "year": 2022,
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "authors": [
        "Yulin Zhu",
        "Liang Tong",
        "Gaolei Li",
        "Xiapu Luo",
        "Kai Zhou"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/e01f18b468b8e4c3634b1ef3ca5faca019c3ee2c",
      "pdf_url": "http://arxiv.org/pdf/2210.13815",
      "publication_date": "2022-10-25",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f494970aadc58616fa8cf40936cb0e6dbff6e750",
      "title": "SBPA: Sybil-Based Backdoor Poisoning Attacks for Distributed Big Data in AIoT-Based Federated Learning System",
      "abstract": "Federated learning (FL) enables a great deal of distributed independent participants to collaborate in training without sharing data. Malicious adversary can poison the local model by backdoor poisoning attacks and utilize the characteristic that server cannot trace original data to make the poisoned model directly aggregated. Especially in the AIoT-FL network that generates large amounts of data in real-time, such an attack is more powerful. In this article, we design a sybil-based backdoor poisoning attacks (SBPA) against the above vulnerability. Malicious participants inject backdoor triggers into distributed Big Data to covertly complete data poisoning. During subsequent iterative aggregation, the joint model activates the backdoor during testing to achieve misclassification for the backdoor images. Besides, malicious participants create some sybil nodes to join the aggregation by taking advantage of the vulnerability of system devices to be easily disconnected. They are committed to making the poisoned local models aggregated with higher probability. Their goal works on making the final global model misclassify backdoor images while keeping high classification accuracy on the other non-backdoor samples. We conduct extensive experiments on multiple datasets and exhibit more robust performance than the state-of-the-art in various metrics under both data distribution including i.i.d. and non-i.i.d. scenarios.",
      "year": 2024,
      "venue": "IEEE Transactions on Big Data",
      "authors": [
        "Xiong Xiao",
        "Zhuo Tang",
        "Chuanying Li",
        "Bingting Jiang",
        "Kenli Li"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/f494970aadc58616fa8cf40936cb0e6dbff6e750",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "de8da73cafd5973aaa1a793b4a0e6e3f118cdb86",
      "title": "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency",
      "abstract": "Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimization problem, leveraging a novel SPC-based loss function as the primary optimization objective. Our innovation unfolds in several key aspects. First, we revisit the vanilla SPC method, unveiling its limitations in addressing the proposed backdoor identification problem. Subsequently, we develop a bi-level optimization-based approach to precisely identify backdoor data by minimizing the advanced SPC loss. Finally, we demonstrate the efficacy of our proposal against a spectrum of backdoor attacks, encompassing basic label-corrupted attacks as well as more sophisticated clean-label attacks, evaluated across various benchmark datasets. Experiment results show that our approach often surpasses the performance of current baselines in identifying backdoor data points, resulting in about 4%-36% improvement in average AUROC. Codes are available at https://github.com/OPTML-Group/BackdoorMSPC.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Soumyadeep Pal",
        "Yuguang Yao",
        "Ren Wang",
        "Bingquan Shen",
        "Sijia Liu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/de8da73cafd5973aaa1a793b4a0e6e3f118cdb86",
      "pdf_url": "",
      "publication_date": "2024-03-15",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "122a1b8f36a6b7d7c4aef30ac19401e839e0adf8",
      "title": "Poisoned classifiers are not only backdoored, they are fundamentally broken",
      "abstract": "Under a commonly-studied \"backdoor\" poisoning attack against classification models, an attacker adds a small \"trigger\" to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is fundamentally incorrect. We demonstrate that anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a recent process called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images. We demonstrate the effectiveness of our attack through extensive experiments on ImageNet and TrojAI datasets, including a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers can in fact look entirely different from the original trigger, highlighting that the backdoor actually learned by the classifier differs substantially from the trigger image itself. Thus, we argue that there is no such thing as a \"secret\" backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier. Code is available at this https URL.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Mingjie Sun",
        "Siddhant Agarwal",
        "J. Z. Kolter"
      ],
      "citation_count": 26,
      "url": "https://www.semanticscholar.org/paper/122a1b8f36a6b7d7c4aef30ac19401e839e0adf8",
      "pdf_url": "",
      "publication_date": "2020-10-18",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d6069969df311f9c4a4d8bbec1cd6f27a9ca1b94",
      "title": "Mitigating poisoned content with forwarding strategy",
      "abstract": null,
      "year": 2016,
      "venue": "Conference on Computer Communications Workshops",
      "authors": [
        "Stephanie DiBenedetto",
        "C. Papadopoulos"
      ],
      "citation_count": 60,
      "url": "https://www.semanticscholar.org/paper/d6069969df311f9c4a4d8bbec1cd6f27a9ca1b94",
      "pdf_url": "",
      "publication_date": "2016-04-10",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9adcf96784c51c8fb30375d930be03e05de0838c",
      "title": "Data-Efficient Backdoor Attacks",
      "abstract": "Recent studies have proven that deep neural networks are vulnerable to backdoor attacks. Specifically, by mixing a small number of poisoned samples into the training set, the behavior of the trained model can be maliciously controlled. Existing attack methods construct such adversaries by randomly selecting some clean data from the benign set and then embedding a trigger into them. However, this selection strategy ignores the fact that each poisoned sample contributes inequally to the backdoor injection, which reduces the efficiency of poisoning. In this paper, we formulate improving the poisoned data efficiency by the selection as an optimization problem and propose a Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the same attack success rate can be achieved with only 47% to 75% of the poisoned sample volume compared to the random selection strategy. More importantly, the adversaries selected according to one setting can generalize well to other settings, exhibiting strong transferability. The prototype code of our method is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",
      "year": 2022,
      "venue": "International Joint Conference on Artificial Intelligence",
      "authors": [
        "Pengfei Xia",
        "Ziqiang Li",
        "W. Zhang",
        "Bin Li"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/9adcf96784c51c8fb30375d930be03e05de0838c",
      "pdf_url": "https://arxiv.org/pdf/2204.12281",
      "publication_date": "2022-04-22",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "59a41d9178dd7e6187a1db7d1a5bf89372f4189c",
      "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
      "abstract": "In adversarial machine learning, new defenses against attacks on deep learning systems are routinely broken soon after their release by more powerful attacks. In this context, forensic tools can offer a valuable complement to existing defenses, by tracing back a successful attack to its root cause, and offering a path forward for mitigation to prevent similar attacks in the future. In this paper, we describe our efforts in developing a forensic traceback tool for poison attacks on deep neural networks. We propose a novel iterative clustering and pruning solution that trims\"innocent\"training samples, until all that remains is the set of poisoned data responsible for the attack. Our method clusters training samples based on their impact on model parameters, then uses an efficient data unlearning method to prune innocent clusters. We empirically demonstrate the efficacy of our system on three types of dirty-label (backdoor) poison attacks and three types of clean-label poison attacks, across domains of computer vision and malware classification. Our system achieves over 98.4% precision and 96.8% recall across all attacks. We also show that our system is robust against four anti-forensics measures specifically designed to attack it.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Shawn Shan",
        "A. Bhagoji",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 61,
      "url": "https://www.semanticscholar.org/paper/59a41d9178dd7e6187a1db7d1a5bf89372f4189c",
      "pdf_url": "",
      "publication_date": "2021-10-13",
      "keywords_matched": [
        "poisoned data"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "69b8f3abcdf994736164f3b45de4e60bb6f290ab",
      "title": "The Poisoned Pediatric Patient",
      "abstract": null,
      "year": 2017,
      "venue": "Pediatrics in review",
      "authors": [
        "Michael S. Toce",
        "M. Burns"
      ],
      "citation_count": 20,
      "url": "https://www.semanticscholar.org/paper/69b8f3abcdf994736164f3b45de4e60bb6f290ab",
      "pdf_url": "",
      "publication_date": "2017-05-01",
      "keywords_matched": [
        "poisoned data",
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d189162410d79137d7a8589b73270537cca578ed",
      "title": "Machine learning in physics: the pitfalls of poisoned training sets",
      "abstract": "Known for their ability to identify hidden patterns in data, artificial neural networks are among the most powerful machine learning tools. Most notably, neural networks have played a central role in identifying states of matter and phase transitions across condensed matter physics. To date, most studies have focused on systems where different phases of matter and their phase transitions are known, and thus the performance of neural networks is well controlled. While neural networks present an exciting new tool to detect new phases of matter, here we demonstrate that when the training sets are poisoned (i.e. poor training data or mislabeled data) it is easy for neural networks to make misleading predictions.",
      "year": 2020,
      "venue": "Machine Learning: Science and Technology",
      "authors": [
        "Chao Fang",
        "Amin Barzeger",
        "H. Katzgraber"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d189162410d79137d7a8589b73270537cca578ed",
      "pdf_url": "https://iopscience.iop.org/article/10.1088/2632-2153/aba821/pdf",
      "publication_date": "2020-03-11",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9add9e7f6fe00a14672364f4080714025cdfadf7",
      "title": "Revisiting Data-Free Knowledge Distillation with Poisoned Teachers",
      "abstract": "Data-free knowledge distillation (KD) helps transfer knowledge from a pre-trained model (known as the teacher model) to a smaller model (known as the student model) without access to the original training data used for training the teacher model. However, the security of the synthetic or out-of-distribution (OOD) data required in data-free KD is largely unknown and under-explored. In this work, we make the first effort to uncover the security risk of data-free KD w.r.t. untrusted pre-trained models. We then propose Anti-Backdoor Data-Free KD (ABD), the first plug-in defensive method for data-free KD methods to mitigate the chance of potential backdoors being transferred. We empirically evaluate the effectiveness of our proposed ABD in diminishing transferred backdoor knowledge while maintaining compatible downstream performances as the vanilla KD. We envision this work as a milestone for alarming and mitigating the potential backdoors in data-free KD. Codes are released at https://github.com/illidanlab/ABD.",
      "year": 2023,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Junyuan Hong",
        "Yi Zeng",
        "Shuyang Yu",
        "L. Lyu",
        "R. Jia",
        "Jiayu Zhou"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/9add9e7f6fe00a14672364f4080714025cdfadf7",
      "pdf_url": "http://arxiv.org/pdf/2306.02368",
      "publication_date": "2023-06-04",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6c20a12376619a3119e53202692b091635ff03c5",
      "title": "Backdoor Defense via Decoupling the Training Process",
      "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \\emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \\url{https://github.com/SCLBD/DBD}.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Kunzhe Huang",
        "Yiming Li",
        "Baoyuan Wu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "citation_count": 232,
      "url": "https://www.semanticscholar.org/paper/6c20a12376619a3119e53202692b091635ff03c5",
      "pdf_url": "",
      "publication_date": "2022-02-05",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "255516bee4d83caea22235fc586e73d399801ecf",
      "title": "SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks",
      "abstract": "Abstract Modern NLP models are often trained on public datasets drawn from diverse sources, rendering them vulnerable to data poisoning attacks. These attacks can manipulate the model\u2019s behavior in ways engineered by the attacker. One such tactic involves the implantation of backdoors, achieved by poisoning specific training instances with a textual trigger and a target class label. Several strategies have been proposed to mitigate the risks associated with backdoor attacks by identifying and removing suspected poisoned examples. However, we observe that these strategies fail to offer effective protection against several advanced backdoor attacks. To remedy this deficiency, we propose a novel defensive mechanism that first exploits training dynamics to identify poisoned samples with high precision, followed by a label propagation step to improve recall and thus remove the majority of poisoned instances. Compared with recent advanced defense methods, our method considerably reduces the success rates of several backdoor attacks while maintaining high classification accuracy on clean test sets.",
      "year": 2024,
      "venue": "Transactions of the Association for Computational Linguistics",
      "authors": [
        "Xuanli He",
        "Qiongkai Xu",
        "Jun Wang",
        "Benjamin I. P. Rubinstein",
        "Trevor Cohn"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/255516bee4d83caea22235fc586e73d399801ecf",
      "pdf_url": "https://doi.org/10.1162/tacl_a_00684",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0b6e3878f57b01c673081eed9ef53cae4aca1c1f",
      "title": "Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks",
      "abstract": "Backdoor (Trojan) attacks are emerging threats against deep neural networks (DNN). A DNN being attacked will predict to an attacker-desired target class whenever a test sample from any source class is embedded with a backdoor pattern, while correctly classifying clean (attack-free) test samples. Existing backdoor defenses have shown success in detecting whether a DNN is attacked and in reverse-engineering the backdoor pattern in a \"post-training\" scenario: the defender has access to the DNN to be inspected and a small, clean dataset collected independently, but has no access to the (possibly poisoned) training set of the DNN. However, these defenses neither catch culprits in the act of triggering the backdoor mapping, nor mitigate the backdoor attack at test-time. In this paper, we propose an \"in-flight\" unsupervised defense against backdoor attacks on image classification that 1) detects use of a backdoor trigger at test-time; and 2) infers the class of origin (source class) for a detected trigger example. The effectiveness of our defense is demonstrated experimentally for a wide variety of DNN architectures, datasets, and backdoor attack configurations.",
      "year": 2021,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Xi Li",
        "Zhen Xiang",
        "David J. Miller",
        "G. Kesidis"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/0b6e3878f57b01c673081eed9ef53cae4aca1c1f",
      "pdf_url": "http://arxiv.org/pdf/2112.03350",
      "publication_date": "2021-12-06",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6d40467d852788361f83f8fdba635aa3f2b41da2",
      "title": "Textual Backdoor Defense via Poisoned Sample Recognition",
      "abstract": "Deep learning models are vulnerable to backdoor attacks. The success rate of textual backdoor attacks based on data poisoning in existing research is as high as 100%. In order to enhance the natural language processing model\u2019s defense against backdoor attacks, we propose a textual backdoor defense method via poisoned sample recognition. Our method consists of two parts: the first step is to add a controlled noise layer after the model embedding layer, and to train a preliminary model with incomplete or no backdoor embedding, which reduces the effectiveness of poisoned samples. Then, we use the model to initially identify the poisoned samples in the training set so as to narrow the search range of the poisoned samples. The second step uses all the training data to train an infection model embedded in the backdoor, which is used to reclassify the samples selected in the first step, and finally identify the poisoned samples. Through detailed experiments, we have proved that our defense method can effectively defend against a variety of backdoor attacks (character-level, word-level and sentence-level backdoor attacks), and the experimental effect is better than the baseline method. For the BERT model trained by the IMDB dataset, this method can even reduce the success rate of word-level backdoor attacks to 0%.",
      "year": 2021,
      "venue": "Applied Sciences",
      "authors": [
        "Kun Shao",
        "Yu Zhang",
        "Jun-an Yang",
        "Hui Liu"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/6d40467d852788361f83f8fdba635aa3f2b41da2",
      "pdf_url": "https://www.mdpi.com/2076-3417/11/21/9938/pdf?version=1635147283",
      "publication_date": "2021-10-25",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "eb16eae728f54962992e6115c5dcd0df3be28c89",
      "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
      "abstract": "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model\u2019s generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Shuai Zhao",
        "Meihuizi Jia",
        "Anh Tuan Luu",
        "Fengjun Pan",
        "Jinming Wen"
      ],
      "citation_count": 69,
      "url": "https://www.semanticscholar.org/paper/eb16eae728f54962992e6115c5dcd0df3be28c89",
      "pdf_url": "http://arxiv.org/pdf/2401.05949",
      "publication_date": "2024-01-11",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c973cd07dc3cd3d768dabb122dce346fb8b44199",
      "title": "Revisiting the Assumption of Latent Separability for Backdoor Defenses",
      "abstract": null,
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Xiangyu Qi",
        "Tinghao Xie",
        "Yiming Li",
        "Saeed Mahloujifar",
        "Prateek Mittal"
      ],
      "citation_count": 128,
      "url": "https://www.semanticscholar.org/paper/c973cd07dc3cd3d768dabb122dce346fb8b44199",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4696ec65c7f9067d638ab2209214ba732307a8aa",
      "title": "IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency",
      "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training. This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a `firewall' to filter out malicious testing images. Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters. In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon. We also design an adaptive method to select BN layers to scale up for effective detection. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks. Codes are available at \\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox}.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "authors": [
        "Linshan Hou",
        "Ruili Feng",
        "Zhongyun Hua",
        "Wei Luo",
        "Leo Yu Zhang",
        "Yiming Li"
      ],
      "citation_count": 37,
      "url": "https://www.semanticscholar.org/paper/4696ec65c7f9067d638ab2209214ba732307a8aa",
      "pdf_url": "",
      "publication_date": "2024-05-16",
      "keywords_matched": [
        "poisoned training"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "68b582a0a34595e823e21306ef142c2cd613a0e7",
      "title": "Data Sanitization Approach to Mitigate Clean-Label Attacks Against Malware Detection Systems",
      "abstract": "Machine learning (ML) models are increasingly being used in the development of Malware Detection Systems. Existing research in this area primarily focuses on developing new architectures and feature representation techniques to improve the accuracy of the model. However, recent studies have shown that existing state-of-the art techniques are vulnerable to adversarial machine learning (AML) attacks. Among those, data poisoning attacks have been identified as a top concern for ML practitioners. A recent study on clean-label poisoning attacks in which an adversary intentionally crafts training samples in order for the model to learn a backdoor watermark was shown to degrade the performance of state-of-the-art classifiers. Defenses against such poisoning attacks have been largely under-explored. We investigate a recently proposed clean-label poisoning attack and leverage an ensemble-based Nested Training technique to remove most of the poisoned samples from a poisoned training dataset. Our technique leverages the relatively large sensitivity of poisoned samples to feature noise that disproportionately affects the accuracy of a backdoored model. In particular, we show that for two state-of-the art architectures trained on the EMBER dataset affected by the clean-label attack, the Nested Training approach improves the accuracy of backdoor malware samples from 3.42% to 93.2%. We also show that samples produced by the clean-label attack often successfully evade malware classification even when the classifier is not poisoned during training. However, even in such scenarios, our Nested Training technique can mitigate the effect of such clean-label-based evasion attacks by recovering the model's accuracy of malware detection from 3.57% to 93.2%.",
      "year": 2022,
      "venue": "IEEE Military Communications Conference",
      "authors": [
        "Samson Ho",
        "A. Reddy",
        "S. Venkatesan",
        "R. Izmailov",
        "R. Chadha",
        "Alina Oprea"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/68b582a0a34595e823e21306ef142c2cd613a0e7",
      "pdf_url": "",
      "publication_date": "2022-11-28",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5f6d40b490a9ad93953eda2d7514e495f6eb2696",
      "title": "Ultra-fast data sanitization of SRAM by back-biasing to resist a cold boot attack",
      "abstract": "Although SRAM is a well-established type of volatile memory, data remanence has been observed at low temperature even for a power-off state, and thus it is vulnerable to a physical cold boot attack. To address this, an ultra-fast data sanitization method within 5 ns is demonstrated with physics-based simulations for avoidance of the cold boot attack to SRAM. Back-bias, which can control device parameters of CMOS, such as threshold voltage and leakage current, was utilized for the ultra-fast data sanitization. It is applicable to temporary erasing with data recoverability against a low-level attack as well as permanent erasing with data irrecoverability against a high-level attack.",
      "year": 2021,
      "venue": "Scientific Reports",
      "authors": [
        "Seong-Joo Han",
        "Joon-Kyu Han",
        "Gyeong-Jun Yun",
        "Mun-Woo Lee",
        "Ji\u2010Man Yu",
        "Yang\u2010Kyu Choi"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/5f6d40b490a9ad93953eda2d7514e495f6eb2696",
      "pdf_url": "https://www.nature.com/articles/s41598-021-03994-2.pdf",
      "publication_date": "2021-05-10",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d7cfa3f4ab6ea107af6dc2c405e6b70563dc2ebe",
      "title": "A Machine Learning Model for Data Sanitization",
      "abstract": null,
      "year": 2021,
      "venue": "Comput. Networks",
      "authors": [
        "Usman Ahmed",
        "Gautam Srivastava",
        "Chun-Wei Lin"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/d7cfa3f4ab6ea107af6dc2c405e6b70563dc2ebe",
      "pdf_url": "https://doi.org/10.1016/j.comnet.2021.107914",
      "publication_date": "2021-04-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "02bee56eb7a6ea9a4140152601651fed017264f9",
      "title": "CloudDLP: Transparent and Scalable Data Sanitization for Browser-Based Cloud Storage",
      "abstract": "Browser-based cloud storage services are still broadly used in enterprises for online sharing and collaboration. However, sensitive information in images or documents may be easily leaked outside trusted enterprise on-premises due to such cloud services. Existing solutions to prevent data leakage in cloud storage services either limit many functionalities of cloud applications or are difficult to be scaled to various cloud applications. In this paper, we propose CloudDLP, a transparent and scalable approach for enterprises to automatically sanitize sensitive data in images and documents with various browser-based cloud applications. CloudDLP is deployed as an internet gateway within the premises of an enterprise using JavaScript injecting techniques and deep learning methods to sanitize sensitive premise data. It neither compromises the user experience nor significantly affects application functionalities in browser-based cloud storage services. We have evaluated CloudDLP with a number of real-world cloud applications. Our experimental results show that it can achieve automatic data sanitization with cloud storage services while preserving most functionalities of cloud applications.",
      "year": 2020,
      "venue": "IEEE Access",
      "authors": [
        "Peiyi Han",
        "Chuanyi Liu",
        "Jiahao Cao",
        "Shaoming Duan",
        "Hezhong Pan",
        "Zekun Cao",
        "Binxing Fang"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/02bee56eb7a6ea9a4140152601651fed017264f9",
      "pdf_url": "https://doi.org/10.1109/access.2020.2985870",
      "publication_date": null,
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "7dc97c1e34807f51d94a8a260eb3c379f1db6896",
      "title": "CloudDLP: Transparent and Automatic Data Sanitization for Browser-Based Cloud Storage",
      "abstract": "Because cloud storage services have been broadly used in enterprises for online sharing and collaboration, sensitive information in images or documents may be easily leaked outside the trust enterprise on-premises due to such cloud services. Existing solutions to this problem have not fully explored the tradeoffs among application performance, service scalability, and user data privacy. Therefore, we propose CloudDLP, a generic approach for enterprises to automatically sanitize sensitive data in images and documents in browser-based cloud storage. To the best of our knowledge, CloudDLP is the first system that automatically and transparently detects and sanitizes both sensitive images and textual documents without compromising user experience or application functionality on browser-based cloud storage. To prevent sensitive information escaping from on-premises, CloudDLP utilizes deep learning methods to detect sensitive information in both images and textual documents. We have evaluated the proposed method on a number of typical cloud applications. Our experimental results show that it can achieve transparent and automatic data sanitization on the cloud storage services with relatively low overheads, while preserving most application functionalities.",
      "year": 2019,
      "venue": "International Conference on Computer Communications and Networks",
      "authors": [
        "Chuanyi Liu",
        "Peiyi Han",
        "Yingfei Dong",
        "Hezhong Pan",
        "Shaoming Duan",
        "Binxing Fang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/7dc97c1e34807f51d94a8a260eb3c379f1db6896",
      "pdf_url": "",
      "publication_date": "2019-07-01",
      "keywords_matched": [
        "data sanitization"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "414219b33eae1dae03f23c94e88624fe6508668f",
      "title": "Investigating LLM Memorization: Bridging Trojan Detection and Training Data Extraction",
      "abstract": null,
      "year": 0,
      "venue": "",
      "authors": [
        "Manoj Acharya",
        "Xiao Lin"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/414219b33eae1dae03f23c94e88624fe6508668f",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "training data extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2cf5625cf3ad259eba8486f26fe5992ef47a6154",
      "title": "AgrEvader: Poisoning Membership Inference against Byzantine-robust Federated Learning",
      "abstract": "The Poisoning Membership Inference Attack (PMIA) is a newly emerging privacy attack that poses a significant threat to federated learning (FL). An adversary conducts data poisoning (i.e., performing adversarial manipulations on training examples) to extract membership information by exploiting the changes in loss resulting from data poisoning. The PMIA significantly exacerbates the traditional poisoning attack that is primarily focused on model corruption. However, there has been a lack of a comprehensive systematic study that thoroughly investigates this topic. In this work, we conduct a benchmark evaluation to assess the performance of PMIA against the Byzantine-robust FL setting that is specifically designed to mitigate poisoning attacks. We find that all existing coordinate-wise averaging mechanisms fail to defend against the PMIA, while the detect-then-drop strategy was proven to be effective in most cases, implying that the poison injection is memorized and the poisonous effect rarely dissipates. Inspired by this observation, we propose AgrEvader, a PMIA that maximizes the adversarial impact on the victim samples while circumventing the detection by Byzantine-robust mechanisms. AgrEvader significantly outperforms existing PMIAs. For instance, AgrEvader achieved a high attack accuracy of between 72.78% (on CIFAR-10) to 97.80% (on Texas100), which is an average accuracy increase of 13.89% compared to the strongest PMIA reported in the literature. We evaluated AgrEvader on five datasets across different domains, against a comprehensive list of threat models, which included black-box, gray-box and white-box models for targeted and non-targeted scenarios. AgrEvader demonstrated consistent high accuracy across all settings tested. The code is available at: https://github.com/PrivSecML/AgrEvader.",
      "year": 2023,
      "venue": "The Web Conference",
      "authors": [
        "Yanjun Zhang",
        "Guangdong Bai",
        "Pathum Chamikara Mahawaga Arachchige",
        "Mengyao Ma",
        "Liyue Shen",
        "Jingwei Wang",
        "Surya Nepal",
        "Minhui Xue",
        "Long Wang",
        "Joseph K. Liu"
      ],
      "citation_count": 27,
      "url": "https://www.semanticscholar.org/paper/2cf5625cf3ad259eba8486f26fe5992ef47a6154",
      "pdf_url": "",
      "publication_date": "2023-04-30",
      "keywords_matched": [
        "membership information"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1a8dd8e74e657332de14c87287d9d1ba7bc7aed6",
      "title": "High-Fidelity Extraction of Neural Network Models",
      "abstract": null,
      "year": 2019,
      "venue": "arXiv.org",
      "authors": [
        "Matthew Jagielski",
        "Nicholas Carlini",
        "David Berthelot",
        "Alexey Kurakin",
        "Nicolas Papernot"
      ],
      "citation_count": 36,
      "url": "https://www.semanticscholar.org/paper/1a8dd8e74e657332de14c87287d9d1ba7bc7aed6",
      "pdf_url": "",
      "publication_date": "2019-09-03",
      "keywords_matched": [
        "prevent model extraction",
        "prevent membership inference"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d686733493565e67be82956364fc229899206017",
      "title": "What Does a Model Really Look at?: Extracting Model-Oriented Concepts for Explaining Deep Neural Networks",
      "abstract": "Model explainability is one of the crucial ingredients for building trustable AI systems, especially in the applications requiring reliability such as automated driving and diagnosis. Many explainability methods have been studied in the literature. Among many others, this article focuses on a research line that tries to visually explain a pre-trained image classification model such as Convolutional Neural Network by discovering concepts learned by the model, which is so-called the concept-based explanation. Previous concept-based explanation methods rely on the human definition of concepts (e.g., the Broden dataset) or semantic segmentation techniques like Slic (Simple Linear Iterative Clustering). However, we argue that the concepts identified by those methods may show image parts which are more in line with a human perspective or cropped by a segmentation method, rather than purely reflect a model's own perspective. We propose Model-Oriented Concept Extraction (MOCE), a novel approach to extracting key concepts based solely on a model itself, thereby being able to capture its unique perspectives which are not affected by any external factors. Experimental results on various pre-trained models confirmed the advantages of extracting concepts by truly representing the model's point of view.",
      "year": 2024,
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "Seonggyeom Kim",
        "Dong-Kyu Chae"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/d686733493565e67be82956364fc229899206017",
      "pdf_url": "",
      "publication_date": "2024-01-23",
      "keywords_matched": [
        "extracting model"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0d3384cb78be25ed28a3544f175cab59236093dd",
      "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
      "abstract": "Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yixu Wang",
        "Tianle Gu",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d3384cb78be25ed28a3544f175cab59236093dd",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "model extraction defense"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ee5775e16b00c2a150119c0cb6a4de8c29b30cd3",
      "title": "Deterministic Local Interpretable Model-Agnostic Explanations for Stable Explainability",
      "abstract": "Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically creates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g., linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature selection. While LIME and similar local algorithms have gained popularity due to their simplicity, the random perturbation methods result in shifts in data and instability in the generated explanations, where for the same prediction, different explanations can be generated. These are critical issues that can prevent deployment of LIME in sensitive domains. We propose a deterministic version of LIME. Instead of random perturbation, we utilize Agglomerative Hierarchical Clustering (AHC) to group the training data together and K-Nearest Neighbour (KNN) to select the relevant cluster of the new instance that is being explained. After finding the relevant cluster, a simple model (i.e., linear model or decision tree) is trained over the selected cluster to generate the explanations. Experimental results on six public (three binary and three multi-class) and six synthetic datasets show the superiority for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME), where we quantitatively determine the stability and faithfulness of DLIME compared to LIME.",
      "year": 2021,
      "venue": "Machine Learning and Knowledge Extraction",
      "authors": [
        "Muhammad Rehman Zafar",
        "N. Khan"
      ],
      "citation_count": 278,
      "url": "https://www.semanticscholar.org/paper/ee5775e16b00c2a150119c0cb6a4de8c29b30cd3",
      "pdf_url": "https://www.mdpi.com/2504-4990/3/3/27/pdf?version=1625044194",
      "publication_date": "2021-06-30",
      "keywords_matched": [
        "prevent model extraction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d5c52fd68d5b56a6f440364082bc74975806c6ad",
      "title": "Boosting Query Efficiency of Meta Attack With Dynamic Fine-Tuning",
      "abstract": "In black-box attack, excessive queries to target model may cause suspicion and expose attacker's identity. Equipped with advanced meta learning technique, Meta Attack simulates the target model with a surrogate model, significantly reducing the queries. However, it queries for ZOO-gradients to correct the estimated meta-gradients with a fixed frequency, thereby still leading to massive unnecessary queries. To overcome this limitation, this letter takes the dynamic changes of the accuracy of the estimated gradients as a starting point, and develops a Dynamic Meta Attack (DMA). At the beginning of each fine-tuning round, DMA computes the distance between the above two types of gradients. Such distance metric can reflect the accuracy of the meta-gradients, and guide the dynamic adjustment of query frequency for the ZOO-gradients. Moreover, the working flow of the dynamic fine-tuning process can be controlled by a set of parameters, which are of physical significance and easy to be tuned. By this means, DMA merely launches queries at critical moments, greatly saving query resource. Experiments conducted on MNIST and CIFAR10 show that the proposed DMA requires far fewer queries than existing methods while maintaining a satisfying attack success rate and distortion.",
      "year": 2022,
      "venue": "IEEE Signal Processing Letters",
      "authors": [
        "Da Lin",
        "Yuan-Gen Wang",
        "Weixuan Tang",
        "Xiangui Kang"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/d5c52fd68d5b56a6f440364082bc74975806c6ad",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack",
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ff3d64942ee32c12816cd0d60d338d858edbc99f",
      "title": "Model Zoo: A Growing Brain That Learns Continually",
      "abstract": "This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. Code is available at https://github.com/grasp-lyrl/modelzoo_continual.",
      "year": 2021,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Rahul Ramesh",
        "P. Chaudhari"
      ],
      "citation_count": 76,
      "url": "https://www.semanticscholar.org/paper/ff3d64942ee32c12816cd0d60d338d858edbc99f",
      "pdf_url": "",
      "publication_date": "2021-06-06",
      "keywords_matched": [
        "model zoo attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a83acadacea7531dd8d627888fb7eeb377d8c151",
      "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
      "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: \\textbf{it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack}, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Tiansheng Huang",
        "Sihao Hu",
        "Fatih Ilhan",
        "S. Tekin",
        "Ling Liu"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/a83acadacea7531dd8d627888fb7eeb377d8c151",
      "pdf_url": "",
      "publication_date": "2025-01-29",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2ae0b48d987a93ac646309047710cca869eb3945",
      "title": "BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment",
      "abstract": "Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model\u2019s safety has been significantly compromised by fine-tuning on users\u2019 uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a \u201cbackdoor trigger\u201d. By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the \u201cbackdoor attack,\u201d establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiong Wang",
        "Jiazhao Li",
        "Yiquan Li",
        "Xiangyu Qi",
        "Junjie Hu",
        "Sharon Li",
        "P. McDaniel",
        "Muhao Chen",
        "Bo Li",
        "Chaowei Xiao"
      ],
      "citation_count": 24,
      "url": "https://www.semanticscholar.org/paper/2ae0b48d987a93ac646309047710cca869eb3945",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2cfa4d147d3f1e1fcf9584a7d4947321b4160c6",
      "title": "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Jiong Wang",
        "Jiazhao Li",
        "Yiquan Li",
        "Xiangyu Qi",
        "Junjie Hu",
        "Yixuan Li",
        "P. McDaniel",
        "Muhao Chen",
        "Bo Li",
        "Chaowei Xiao"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c2cfa4d147d3f1e1fcf9584a7d4947321b4160c6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b24fd97988e71585e943a6d30725926bab5ded97",
      "title": "Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment",
      "abstract": "Despite the general capabilities of Large Language Models (LLM), these models still request fine-tuning or adaptation with customized data when meeting specific business demands. However, this process inevitably introduces new threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning users' uploaded examples contain just a few harmful examples. Though potential defenses have been proposed that the service providers can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a\"backdoor trigger\". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the\"backdoor attack\", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Jiong Wang",
        "Jiazhao Li",
        "Yiquan Li",
        "Xiangyu Qi",
        "Junjie Hu",
        "Yixuan Li",
        "P. McDaniel",
        "Muhao Chen",
        "Bo Li",
        "Chaowei Xiao"
      ],
      "citation_count": 28,
      "url": "https://www.semanticscholar.org/paper/b24fd97988e71585e943a6d30725926bab5ded97",
      "pdf_url": "",
      "publication_date": "2024-02-22",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation",
      "abstract": "Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Mainstream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile -- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution -- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behavior, though it leads to a degradation in the model's fine-tuning performance. To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model's safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning performance. As a by-product, we analyze the optimized perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at https://github.com/w-yibo/Panacea",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Yibo Wang",
        "Tiansheng Huang",
        "Li Shen",
        "Huanjin Yao",
        "Haotian Luo",
        "Rui Liu",
        "Naiqiang Tan",
        "Jiaxing Huang",
        "Dacheng Tao"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "pdf_url": "",
      "publication_date": "2025-01-30",
      "keywords_matched": [
        "fine-tuning attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "44a423ac14d4daac76813fac37c07917d3f7c824",
      "title": "Interpreting Pretrained Source-code Models using Neuron Redundancy Analyses",
      "abstract": null,
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Arushi Sharma",
        "Zefu Hu",
        "Christopher J. Quinn",
        "Ali Jannesari"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/44a423ac14d4daac76813fac37c07917d3f7c824",
      "pdf_url": "http://arxiv.org/pdf/2305.00875",
      "publication_date": null,
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50e4cc2e61f2b1329c3273f637735600517e7767",
      "title": "A Fuzzing Method for Security Testing of Sensors",
      "abstract": "The security issues in sensors impede the experience of using sensors to improve the quality of our life. By analyzing security issues in open-source programs for sensors, we can understand the security problems in sensors. Fuzzing is one of the most effective techniques to identify potential software vulnerabilities. Most fuzzers aim to improve code coverage. However, a tester may want to focus on examining some specific code regions. In this article, we proposed a deep learning (DL)-guided fuzzing for software vulnerability detection, named DeFuzz. DeFuzz includes two main schemes: 1) we employ a pretrained DL prediction model to identify the potentially vulnerable functions and the locations (i.e., vulnerable addresses). Precisely, we employ bidirectional-LSTM (BiLSTM) to identify attention words and the vulnerabilities associated with these attention words in functions; and 2) then, we employ directed fuzzing to examine the potential vulnerabilities by generating inputs that tend to arrive at the predicted locations. To evaluate the effectiveness of the proposed DeFuzz technique, we have conducted experiments on real-world data sets. Experimental results show that our DeFuzz can discover more coverage and run faster than AFL. Moreover, DeFuzz exposes 43 more bugs than AFL on real-world applications.",
      "year": 2024,
      "venue": "IEEE Sensors Journal",
      "authors": [
        "Xiaogang Zhu",
        "Shigang Liu",
        "A. Jolfaei"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/50e4cc2e61f2b1329c3273f637735600517e7767",
      "pdf_url": "",
      "publication_date": "2024-03-01",
      "keywords_matched": [
        "pretrained model vulnerability"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "84a33d6966cbb2cf8f5192087b286122e806a242",
      "title": "Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation",
      "abstract": null,
      "year": 2022,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Xudong Pan",
        "Mi Zhang",
        "Beina Sheng",
        "Jiaming Zhu",
        "Min Yang"
      ],
      "citation_count": 123,
      "url": "https://www.semanticscholar.org/paper/84a33d6966cbb2cf8f5192087b286122e806a242",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "trigger pattern",
        "transfer backdoor",
        "transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6ad93900b1c956020242653e33ac447824f75fc6",
      "title": "PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models",
      "abstract": "Prompts have significantly improved the performance of pre-trained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model\u2019s normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present PoisonPrompt, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of PoisonPrompt through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.",
      "year": 2023,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Hongwei Yao",
        "Jian Lou",
        "Zhan Qin"
      ],
      "citation_count": 59,
      "url": "https://www.semanticscholar.org/paper/6ad93900b1c956020242653e33ac447824f75fc6",
      "pdf_url": "https://arxiv.org/pdf/2310.12439",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "downstream attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a0173909e4515d6bd8de58752f17a457ba003667",
      "title": "Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models",
      "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have achieved great success in diverse fields. The typical paradigm is to pre-train a big deep learning model on large-scale data sets, and then fine-tune the model on small task-specific data sets for downstream tasks. Although PTMs have rapidly progressed with wide real-world applications, they also pose significant risks of potential attacks. Existing backdoor attacks or data poisoning methods often build up the assumption that the attacker invades the computers of victims or accesses the target data, which is challenging in real-world scenarios. In this paper, we propose a novel framework for an invisible attack on PTMs with enhanced MD5 collision. The key idea is to generate two equal-size models with the same MD5 checksum by leveraging the MD5 chosen-prefix collision. Afterwards, the two ``same\"models will be deployed on public websites to induce victims to download the poisoned model. Unlike conventional attacks on deep learning models, this new attack is flexible, covert, and model-independent. Additionally, we propose a simple defensive strategy for recognizing the MD5 chosen-prefix collision and provide a theoretical justification for its feasibility. We extensively validate the effectiveness and stealthiness of our proposed attack and defensive method on different models and data sets.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Minghan Deng",
        "Zhong Zhang",
        "Junming Shao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/a0173909e4515d6bd8de58752f17a457ba003667",
      "pdf_url": "https://arxiv.org/pdf/2309.13579",
      "publication_date": "2023-09-24",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "36ccb93d809b2d3166bf975a7f7c044a063a610c",
      "title": "LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem",
      "abstract": "Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Hongyi Liu",
        "Zirui Liu",
        "Ruixiang Tang",
        "Jiayi Yuan",
        "Mohsen Hariri",
        "Shaochen Zhong",
        "Yu-Neng Chuang",
        "Li Li",
        "Rui Chen",
        "Xia Hu"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/36ccb93d809b2d3166bf975a7f7c044a063a610c",
      "pdf_url": "",
      "publication_date": "2024-02-29",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "44cec683656c64450a007f92d6c6a520e20747f6",
      "title": "TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models",
      "abstract": "One key challenge in backdoor attacks against large foundation models is the resource limits. Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models. Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT). None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources. In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models. Our primary technical contribution is the development of a novel backdoor injection method. This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics. Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters. This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources. Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only~\\textit{one A100 GPU}. Furthermore, we design a new trigger injection method to ensure our attack stealthiness. Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPT-style models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models). Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters. Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yuzhou Nie",
        "Yanting Wang",
        "Jinyuan Jia",
        "Michael J. De Lucia",
        "Nathaniel D. Bastian",
        "Wenbo Guo",
        "D. Song"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/44cec683656c64450a007f92d6c6a520e20747f6",
      "pdf_url": "",
      "publication_date": "2024-05-27",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4758ddfa8c4a831970473a5327bb0497dd870b4",
      "title": "Multi-target Backdoor Attacks for Code Pre-trained Models",
      "abstract": "Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experimental results demonstrate that our approach effectively and stealthily attacks code-related downstream tasks.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Yanzhou Li",
        "Shangqing Liu",
        "Kangjie Chen",
        "Xiaofei Xie",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c4758ddfa8c4a831970473a5327bb0497dd870b4",
      "pdf_url": "http://arxiv.org/pdf/2306.08350",
      "publication_date": "2023-06-14",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c48451c901211ed8e383a940d4e235452d5815e1",
      "title": "A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning",
      "abstract": "Parameter-Efficient Tuning (PET) has shown remarkable performance by fine-tuning only a small number of parameters of the pre-trained language models (PLMs) for the downstream tasks, while it is also possible to construct backdoor attacks due to the vulnerability of pre-trained weights. However, a large reduction in the number of attackable parameters in PET will cause the user\u2019s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting. We find that the backdoor injection process can be regarded as multi-task learning, which has a convergence imbalance problem between the training of clean and poisoned data. And this problem might result in forgetting the backdoor. Based on this finding, we propose a gradient control method to consolidate the attack effect, comprising two strategies. One controls the gradient magnitude distribution cross layers within one task and the other prevents the conflict of gradient directions between tasks. Compared with previous backdoor attack methods in the scenario of PET, our method improve the effect of the attack on sentiment classification and spam detection respectively, which shows that our method is widely applicable to different tasks.",
      "year": 2023,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "authors": [
        "Naibin Gu",
        "Peng Fu",
        "Xiyu Liu",
        "Zhengxiao Liu",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "citation_count": 32,
      "url": "https://www.semanticscholar.org/paper/c48451c901211ed8e383a940d4e235452d5815e1",
      "pdf_url": "https://aclanthology.org/2023.acl-long.194.pdf",
      "publication_date": null,
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "a378012f21bf34c79a0520ea0ad89421b762a658",
      "title": "Trojan Prompt Attacks on Graph Neural Networks",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Min Lin",
        "Zhiwei Zhang",
        "Enyan Dai",
        "Zongyu Wu",
        "Yilong Wang",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/a378012f21bf34c79a0520ea0ad89421b762a658",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "97ae76460fa593a554336adab39eb06ea01219b1",
      "title": "BadPrompt: Backdoor Attacks on Continuous Prompts",
      "abstract": "The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available at https://github.com/papersPapers/BadPrompt.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Xiangrui Cai",
        "Haidong Xu",
        "Sihan Xu",
        "Ying Zhang",
        "Xiaojie Yuan"
      ],
      "citation_count": 82,
      "url": "https://www.semanticscholar.org/paper/97ae76460fa593a554336adab39eb06ea01219b1",
      "pdf_url": "https://arxiv.org/pdf/2211.14719",
      "publication_date": "2022-11-27",
      "keywords_matched": [
        "downstream task attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "54a73c1037dc7807b225895e674b661d18800484",
      "title": "Breaking PEFT Limitations: Leveraging Weak-to-Strong Knowledge Transfer for Backdoor Attacks in LLMs",
      "abstract": "Despite being widely applied due to their exceptional capabilities, Large Language Models (LLMs) have been proven to be vulnerable to backdoor attacks. These attacks introduce targeted vulnerabilities into LLMs by poisoning training samples and full-parameter fine-tuning (FPFT). However, this kind of backdoor attack is limited since they require significant computational resources, especially as the size of LLMs increases. Besides, parameter-efficient fine-tuning (PEFT) offers an alternative but the restricted parameter updating may impede the alignment of triggers with target labels. In this study, we first verify that backdoor attacks with PEFT may encounter challenges in achieving feasible performance. To address these issues and improve the effectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack algorithm from the weak-to-strong based on Feature Alignment-enhanced Knowledge Distillation (FAKD). Specifically, we poison small-scale language models through FPFT to serve as the teacher model. The teacher model then covertly transfers the backdoor to the large-scale student model through FAKD, which employs PEFT. Theoretical analysis reveals that FAKD has the potential to augment the effectiveness of backdoor attacks. We demonstrate the superior performance of FAKD on classification tasks across four language models, four backdoor attack algorithms, and two different architectures of teacher models. Experimental results indicate success rates close to 100% for backdoor attacks targeting PEFT.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Shuai Zhao",
        "Leilei Gan",
        "Zhongliang Guo",
        "Xiaobao Wu",
        "Luwei Xiao",
        "Xiaoyu Xu",
        "Cong-Duy Nguyen",
        "Anh Tuan Luu"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/54a73c1037dc7807b225895e674b661d18800484",
      "pdf_url": "",
      "publication_date": "2024-09-26",
      "keywords_matched": [
        "transfer backdoor",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "c4482d14239b18043560fcc2b9403dc27d692210",
      "title": "Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces",
      "abstract": "Transfer learning (TL) has been widely used in electroencephalogram (EEG)-based brain-computer interfaces (BCIs) for reducing calibration efforts. However, backdoor attacks could be introduced through TL. In such attacks, an attacker embeds a backdoor with a specific pattern into the machine learning model. As a result, the model will misclassify a test sample with the backdoor trigger into a prespecified class while still maintaining good performance on benign samples. Accordingly, this study explores backdoor attacks in the TL of EEG-based BCIs, where source-domain data are poisoned by a backdoor trigger and then used in TL. We propose several active poisoning approaches to select source-domain samples, which are most effective in embedding the backdoor pattern, to improve the attack success rate and efficiency. Experiments on four EEG datasets and three deep learning models demonstrate the effectiveness of the approaches. To our knowledge, this is the first study about backdoor attacks on TL models in EEG-based BCIs. It exposes a serious security risk in BCIs, which should be immediately addressed.",
      "year": 2023,
      "venue": "Science China Information Sciences",
      "authors": [
        "Xue Jiang",
        "Lubin Meng",
        "Siyang Li",
        "Dongrui Wu"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/c4482d14239b18043560fcc2b9403dc27d692210",
      "pdf_url": "",
      "publication_date": "2023-07-26",
      "keywords_matched": [
        "transfer backdoor",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "dea3b07c92017525f443b527824dff502e5ed590",
      "title": "Backdoor Pre-trained Models Can Transfer to All",
      "abstract": "Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.",
      "year": 2021,
      "venue": "Conference on Computer and Communications Security",
      "authors": [
        "Lujia Shen",
        "S. Ji",
        "Xuhong Zhang",
        "Jinfeng Li",
        "Jing Chen",
        "Jie Shi",
        "Chengfang Fang",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "citation_count": 144,
      "url": "https://www.semanticscholar.org/paper/dea3b07c92017525f443b527824dff502e5ed590",
      "pdf_url": "https://arxiv.org/pdf/2111.00197",
      "publication_date": "2021-10-30",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "875761089fcbef69705a402fd6088e000e12c2dc",
      "title": "Clean Teacher model Future target and associated clean data Transfer Learning Student data including Infected Student Model Latent Backdoor trigger Teacher Training Student Training Latent Backdoor Injected Live Backdoor Activated Backdoor Injection Progress Infected Teacher Model",
      "abstract": null,
      "year": 2019,
      "venue": "",
      "authors": [
        "Yuanshun Yao",
        "Huiying Li",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/875761089fcbef69705a402fd6088e000e12c2dc",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5a2ebea1ab8b203fbfc29b79a64122b4609c2459",
      "title": "Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace",
      "abstract": "Model merging has gained significant attention as a cost-effective approach to integrate multiple single-task fine-tuned models into a unified one that can perform well on multiple tasks. However, existing model merging techniques primarily focus on resolving conflicts between task-specific models, they often overlook potential security threats, particularly the risk of backdoor attacks in the open-source model ecosystem. In this paper, we first investigate the vulnerabilities of existing model merging methods to backdoor attacks, identifying two critical challenges: backdoor succession and backdoor transfer. To address these issues, we propose a novel Defense-Aware Merging (DAM) approach that simultaneously mitigates task interference and backdoor vulnerabilities. Specifically, DAM employs a meta-learning-based optimization method with dual masks to identify a shared and safety-aware subspace for model merging. These masks are alternately optimized: the Task-Shared mask identifies common beneficial parameters across tasks, aiming to preserve task-specific knowledge while reducing interference, while the Backdoor-Detection mask isolates potentially harmful parameters to neutralize security threats. This dual-mask design allows us to carefully balance the preservation of useful knowledge and the removal of potential vulnerabilities. Compared to existing merging methods, DAM achieves a more favorable balance between performance and security, reducing the attack success rate by 2-10 percentage points while sacrificing only about 1% in accuracy. Furthermore, DAM exhibits robust performance and broad applicability across various types of backdoor attacks and the number of compromised models involved in the merging process. Our codes and models are available at https://github.com/Yangjinluan/DAM.",
      "year": 2024,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Jinluan Yang",
        "A. Tang",
        "Didi Zhu",
        "Zhengyu Chen",
        "Li Shen",
        "Fei Wu"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/5a2ebea1ab8b203fbfc29b79a64122b4609c2459",
      "pdf_url": "",
      "publication_date": "2024-10-17",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "e9b9200ef71a1296ed178e0ac607505ec9bc407c",
      "title": "Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks",
      "abstract": "At present, deep neural networks are at risk from backdoor attacks, but natural language processing (NLP) lacks sufficient research on backdoor attacks. To improve the invisibility of backdoor attacks, some innovative textual backdoor attack methods utilize modern language models to generate poisoned text with backdoor triggers, which are called feature space backdoor attacks. However, this article find that texts generated by the same language model without backdoor triggers also have a high probability of activating the backdoors they injected. Therefore, this article proposes a multistyle transfer-based backdoor attack that uses multiple text styles as the backdoor trigger. Furthermore, inspired by the ability of modern language models to distinguish between texts generated by different language models, this article proposes a paraphrase-based backdoor attack, which leverages the shared characteristics of sentences generated by the same paraphrase model as the backdoor trigger. Experiments have been conducted to demonstrate that both backdoor attack methods can be effective against NLP models. More importantly, compared with other feature space backdoor attacks, the poisoned samples generated by paraphrase-based backdoor attacks have improved semantic similarity.",
      "year": 2024,
      "venue": "IEEE Transactions on Reliability",
      "authors": [
        "Xiangjun Li",
        "Xin Lu",
        "Peixuan Li"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/e9b9200ef71a1296ed178e0ac607505ec9bc407c",
      "pdf_url": "",
      "publication_date": "2024-09-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "90962f72c1ffa03aa4a0eb9652c5b111663bcee9",
      "title": "$\\tt{PoisonedGNN}$: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems",
      "abstract": "Graph neural networks (GNNs) have shown great success in detecting intellectual property (IP) piracy and hardware Trojans (HTs). However, the machine learning community has demonstrated that GNNs are susceptible to data poisoning attacks, which result in GNNs performing abnormally on graphs with pre-defined backdoor triggers (realized using crafted subgraphs). Thus, it is imperative to ensure that the adoption of GNNs should not introduce security vulnerabilities in critical security frameworks. Existing backdoor attacks on GNNs generate random subgraphs with specific sizes/densities to act as backdoor triggers. However, for Boolean circuits, backdoor triggers cannot be randomized since the added structures should not affect the functionality of a design. We explore this threat and develop PoisonedGNN as the first backdoor attack on GNNs in the context of hardware design. We design and inject backdoor triggers into the register-transfer- or the gate-level representation of a given design without affecting the functionality to evade some GNN-based detection procedures. To demonstrate the effectiveness of PoisonedGNN, we consider two case studies: (i) Hiding HTs and (ii) IP piracy. Our experiments on TrustHub datasets demonstrate that PoisonedGNN can hide HTs and IP piracy from advanced GNN-based detection platforms with an attack success rate of up to 100%.",
      "year": 2023,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Lilas Alrahis",
        "Satwik Patnaik",
        "M. Hanif",
        "Muhammad Shafique",
        "O. Sinanoglu"
      ],
      "citation_count": 23,
      "url": "https://www.semanticscholar.org/paper/90962f72c1ffa03aa4a0eb9652c5b111663bcee9",
      "pdf_url": "",
      "publication_date": "2023-03-24",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1fee4bb2c1da09fc7ce2e8cdc351d82caf5a6098",
      "title": "Krait: A Backdoor Attack Against Graph Prompt Tuning",
      "abstract": "Graph prompt tuning has emerged as a promising paradigm to effectively transfer general graph knowledge from pre-trained models to various downstream tasks, particularly in few-shot contexts. However, its susceptibility to backdoor attacks, where adversaries insert triggers to manipulate out-comes, raises a critical concern. We conduct the first study to investigate such vulnerability, revealing that backdoors can disguise benign graph prompts, thus evading detection. We introduce Krait, a novel graph prompt backdoor. Specifically, we propose a simple yet effective model-agnostic metric called label non-uniformity homophily to select poisoned candidates, significantly reducing computational complexity. To accommodate diverse attack scenarios and advanced attack types, we design three customizable trigger generation methods to craft prompts as triggers. We propose a novel centroid similarity-based loss function to optimize prompt tuning for attack effectiveness and stealthiness. Experiments on four real-world graphs demonstrate that Krait can efficiently embed triggers to merely 0.15% to 2% of training nodes, achieving high attack success rates without sacrificing clean accuracy. Notably, in one-to-one and all-to-one attacks, Krait can achieve 100% attack success rates by poisoning as few as 2 and 22 nodes, respectively. Our experiments further show that Krait remains potent across different transfer cases, attack types, and graph neural network backbones. Additionally, Krait can be successfully extended to the black-box setting, posing more severe threats. Finally, we analyze why Krait can evade both classical and state-of-the-art defenses, and provide practical insights for detecting and mitigating this class of attacks.",
      "year": 2024,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Ying Song",
        "Rita Singh",
        "Balaji Palanisamy"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/1fee4bb2c1da09fc7ce2e8cdc351d82caf5a6098",
      "pdf_url": "",
      "publication_date": "2024-07-18",
      "keywords_matched": [
        "transfer backdoor",
        "knowledge transfer attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9f2b87ecd53ff065f5a1413e605731ea6bb813ad",
      "title": "Computation and Data Efficient Backdoor Attacks",
      "abstract": "Backdoor attacks against deep neural network (DNN) models have been widely studied. Various attack techniques have been proposed for different domains and paradigms, e.g., image, point cloud, natural language processing, transfer learning, etc. The most widely-used way to embed a backdoor into a DNN model is to poison the training data. They usually randomly select samples from the benign training set for poisoning, without considering the distinct contribution of each sample to the backdoor effectiveness, making the attack less optimal.A recent work [40] proposed to use the forgetting score to measure the importance of each poisoned sample and then filter out redundant data for effective backdoor training. However, this method is empirically designed without theoretical proofing. It is also very time-consuming as it needs to go through several training stages for data selection. To address such limitations, we propose a novel confidence-based scoring methodology, which can efficiently measure the contribution of each poisoning sample based on the distance posteriors. We further introduce a greedy search algorithm to find the most informative samples for backdoor injection more promptly. Experimental evaluations on both 2D image and 3D point cloud classification tasks show that our approach can achieve comparable performance or even surpass the forgetting score-based searching method while requiring only several extra epochs\u2019 computation of a standard training process. Our code can be found at https://github.com/WU-YU-TONG/computational_efficient_backdoor",
      "year": 2023,
      "venue": "IEEE International Conference on Computer Vision",
      "authors": [
        "Yutong Wu",
        "Xingshuo Han",
        "Han Qiu",
        "Tianwei Zhang"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/9f2b87ecd53ff065f5a1413e605731ea6bb813ad",
      "pdf_url": "",
      "publication_date": "2023-10-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "cb115cbd7bb2e6d20f4ffa897ba2bd81cd61ebc6",
      "title": "NBA: defensive distillation for backdoor removal via neural behavior alignment",
      "abstract": "Recently, deep neural networks have been shown to be vulnerable to backdoor attacks. A backdoor is inserted into neural networks via this attack paradigm, thus compromising the integrity of the network. As soon as an attacker presents a trigger during the testing phase, the backdoor in the model is activated, allowing the network to make specific wrong predictions. It is extremely important to defend against backdoor attacks since they are very stealthy and dangerous. In this paper, we propose a novel defense mechanism, Neural Behavioral Alignment (NBA), for backdoor removal. NBA optimizes the distillation process in terms of knowledge form and distillation samples to improve defense performance according to the characteristics of backdoor defense. NBA builds high-level representations of neural behavior within networks in order to facilitate the transfer of knowledge. Additionally, NBA crafts pseudo samples to induce student models exhibit backdoor neural behavior. By aligning the backdoor neural behavior from the student network with the benign neural behavior from the teacher network, NBA enables the proactive removal of backdoors. Extensive experiments show that NBA can effectively defend against six different backdoor attacks and outperform five state-of-the-art defenses.",
      "year": 2023,
      "venue": "Cybersecurity",
      "authors": [
        "Zonghao Ying",
        "Bin Wu"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/cb115cbd7bb2e6d20f4ffa897ba2bd81cd61ebc6",
      "pdf_url": "https://cybersecurity.springeropen.com/counter/pdf/10.1186/s42400-023-00154-z",
      "publication_date": "2023-07-03",
      "keywords_matched": [
        "backdoor removal",
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "51c9bcadb7cd702056437e68efe3d54a3bb45229",
      "title": "BadDGA: Backdoor Attack on LSTM-Based Domain Generation Algorithm Detector",
      "abstract": "Due to the outstanding performance of deep neural networks (DNNs), many researchers have begun to transfer deep learning techniques to their fields. To detect algorithmically generated domains (AGDs) generated by domain generation algorithm (DGA) in botnets, a long short-term memory (LSTM)-based DGA detector has achieved excellent performance. However, the previous DNNs have found various inherent vulnerabilities, so cyberattackers can use these drawbacks to deceive DNNs, misleading DNNs into making wrong decisions. Backdoor attack as one of the popular attack strategies strike against DNNs has attracted widespread attention in recent years. In this paper, to cheat the LSTM-based DGA detector, we propose BadDGA, a backdoor attack against the LSTM-based DGA detector. Specifically, we offer four backdoor attack trigger construction methods: TLD-triggers, Ngram-triggers, Word-triggers, and IDN-triggers. Finally, we evaluate BadDGA on ten popular DGA datasets. The experimental results show that under the premise of 1\u2030 poisoning rate, our proposed backdoor attack can achieve a 100% attack success rate to verify the effectiveness of our method. Meanwhile, the model\u2019s utility on clean data is influenced slightly.",
      "year": 2023,
      "venue": "Electronics",
      "authors": [
        "You Zhai",
        "Liqun Yang",
        "Jian Yang",
        "Longtao He",
        "Zhoujun Li"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/51c9bcadb7cd702056437e68efe3d54a3bb45229",
      "pdf_url": "https://www.mdpi.com/2079-9292/12/3/736/pdf?version=1675259896",
      "publication_date": "2023-02-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f20bf8136c84534a6e3eeba2d21701d8b5a00f62",
      "title": "Robust Knowledge Distillation in Federated Learning: Counteracting Backdoor Attacks",
      "abstract": "Federated Learning (FL) enables collaborative model training across multiple devices while preserving data privacy. However, it remains susceptible to backdoor attacks, where malicious participants can compromise the global model. Existing defence methods are limited by strict assumptions on data heterogeneity (Non-Independent and Identically Distributed data) and the proportion of malicious clients, reducing their practicality and effectiveness. To overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defence mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models. It then employs knowledge distillation to transfer the collective insights from this ensemble to a global model. Extensive evaluations demonstrate that RKD effectively mitigates backdoor threats while maintaining high model performance, outperforming current state-of-the-art defence methods across various scenarios.",
      "year": 2025,
      "venue": "2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)",
      "authors": [
        "Ebtisaam Alharbi",
        "L. Marcolino",
        "Qiang Ni",
        "Antonios Gouglidis"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/f20bf8136c84534a6e3eeba2d21701d8b5a00f62",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f98d2bbc086fa3d2f4daefe909f60d8a81a71079",
      "title": "REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models",
      "abstract": "The rapid advancement of generative AI highlights the importance of text-to-image (T2I) security, particularly with the threat of backdoor poisoning. Timely disclosure and mitigation of security vulnerabilities in T2I models are crucial for ensuring the safe deployment of generative models. We explore a novel training-free backdoor poisoning paradigm through model editing, which is recently employed for knowledge updating in large language models. Nevertheless, we reveal the potential security risks posed by model editing techniques to image generation models. In this work, we establish the principles for backdoor attacks based on model editing, and propose a relationship-driven precise backdoor poisoning method, REDEditing. Drawing on the principles of equivalent-attribute alignment and stealthy poisoning, we develop an equivalent relationship retrieval and joint-attribute transfer approach that ensures consistent backdoor image generation through concept rebinding. A knowledge isolation constraint is proposed to preserve benign generation integrity. Our method achieves an 11\\% higher attack success rate compared to state-of-the-art approaches. Remarkably, adding just one line of code enhances output naturalness while improving backdoor stealthiness by 24\\%. This work aims to heighten awareness regarding this security vulnerability in editable image generation models.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chongye Guo",
        "Jinhu Fu",
        "Junfeng Fang",
        "Kun Wang",
        "Guorui Feng"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f98d2bbc086fa3d2f4daefe909f60d8a81a71079",
      "pdf_url": "",
      "publication_date": "2025-04-20",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d19c3457d0dc0cd36968db11553b80dd19870e6f",
      "title": "How to Backdoor the Knowledge Distillation",
      "abstract": "Knowledge distillation has become a cornerstone in modern machine learning systems, celebrated for its ability to transfer knowledge from a large, complex teacher model to a more efficient student model. Traditionally, this process is regarded as secure, assuming the teacher model is clean. This belief stems from conventional backdoor attacks relying on poisoned training data with backdoor triggers and attacker-chosen labels, which are not involved in the distillation process. Instead, knowledge distillation uses the outputs of a clean teacher model to guide the student model, inherently preventing recognition or response to backdoor triggers as intended by an attacker. In this paper, we challenge this assumption by introducing a novel attack methodology that strategically poisons the distillation dataset with adversarial examples embedded with backdoor triggers. This technique allows for the stealthy compromise of the student model while maintaining the integrity of the teacher model. Our innovative approach represents the first successful exploitation of vulnerabilities within the knowledge distillation process using clean teacher models. Through extensive experiments conducted across various datasets and attack settings, we demonstrate the robustness, stealthiness, and effectiveness of our method. Our findings reveal previously unrecognized vulnerabilities and pave the way for future research aimed at securing knowledge distillation processes against backdoor attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chen Wu",
        "Qian Ma",
        "P. Mitra",
        "Sencun Zhu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/d19c3457d0dc0cd36968db11553b80dd19870e6f",
      "pdf_url": "",
      "publication_date": "2025-04-30",
      "keywords_matched": [
        "transfer backdoor"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2",
      "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
      "abstract": "Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. [2024] reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing'' (PTST) strategy -- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Kaifeng Lyu",
        "Haoyu Zhao",
        "Xinran Gu",
        "Dingli Yu",
        "Anirudh Goyal",
        "Sanjeev Arora"
      ],
      "citation_count": 84,
      "url": "https://www.semanticscholar.org/paper/9aa53e8b6b01fcb5ae8b8828cad2bace83e941a2",
      "pdf_url": "",
      "publication_date": "2024-02-28",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8805398cab95a5c384231b9c71a2ccfc00af1998",
      "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
      "abstract": "Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g.,\"design\") versus the specific concepts the task is asked to be performed upon (e.g., a\"cycle\"vs. a\"bomb\"). Using this, we investigate three well-known safety fine-tuning methods -- supervised safety fine-tuning, direct preference optimization, and unlearning -- and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. We validate our findings, wherever possible, on real-world models -- specifically, Llama-2 7B and Llama-3 8B.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Samyak Jain",
        "E. Lubana",
        "Kemal Oksuz",
        "Tom Joy",
        "Philip H. S. Torr",
        "Amartya Sanyal",
        "P. Dokania"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/8805398cab95a5c384231b9c71a2ccfc00af1998",
      "pdf_url": "",
      "publication_date": "2024-07-14",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3f3ce105f4ade024e9b25d75b16fabbf39034b8",
      "title": "Gradient Surgery for Safe LLM Fine-Tuning",
      "abstract": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few malicious examples mixed into the user's fine-tuning dataset can compromise the safety alignment of Large Language Models (LLMs). While a recognized paradigm frames safe fine-tuning as a multi-objective optimization problem balancing user task performance with safety alignment, we find existing solutions are critically sensitive to the harmful ratio, with defenses degrading sharply as harmful ratio increases. We diagnose that this failure stems from conflicting gradients, where the user-task update directly undermines the safety objective. To resolve this, we propose SafeGrad, a novel method that employs gradient surgery. When a conflict is detected, SafeGrad nullifies the harmful component of the user-task gradient by projecting it onto the orthogonal plane of the alignment gradient, allowing the model to learn the user's task without sacrificing safety. To further enhance robustness and data efficiency, we employ a KL-divergence alignment loss that learns the rich, distributional safety profile of the well-aligned foundation model. Extensive experiments show that SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Biao Yi",
        "Jiahao Li",
        "Baolei Zhang",
        "Lihai Nie",
        "Tong Li",
        "Tiansheng Huang",
        "Zheli Liu"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/f3f3ce105f4ade024e9b25d75b16fabbf39034b8",
      "pdf_url": "",
      "publication_date": "2025-08-10",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "27a2ee8b77dbfe02b908d3e929eaba42121ecf83",
      "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin",
      "abstract": "Large language models (LLMs) are vulnerable to safety risks during fine-tuning, where small amounts of malicious or harmless data can compromise safeguards. In this paper, building on the concept of alignment direction -- defined by the weight difference between aligned and unaligned models -- we observe that perturbations along this direction preserve model safety. In contrast, perturbations along directions orthogonal to this alignment are strongly linked to harmful direction perturbations, rapidly degrading safety and framing the parameter space as a narrow safety basin. Based on this insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring Safety in Fine-Tuning), which integrates a regularization term into the training objective. This term uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring that fine-tuning is constrained within the narrow safety basin. Extensive experiments on multiple datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings. Code is available at https://github.com/PKU-YuanGroup/AsFT",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Shuo Yang",
        "Qihui Zhang",
        "Yu-Yang Liu",
        "Yue Huang",
        "Xiaojun Jia",
        "Kun-Peng Ning",
        "Jiayu Yao",
        "Jigang Wang",
        "Hailiang Dai",
        "Yibing Song",
        "Li Yuan"
      ],
      "citation_count": 8,
      "url": "https://www.semanticscholar.org/paper/27a2ee8b77dbfe02b908d3e929eaba42121ecf83",
      "pdf_url": "",
      "publication_date": "2025-06-10",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20406a42f018ddab9efc69461b6573e7364cd9bd",
      "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation",
      "abstract": "Previous chain-of-thought (CoT) distillation methods primarily focused on enhancing the reasoning capabilities of Small Language Models (SLMs) by utilizing high-quality rationales generated by powerful Large Language Models (LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM safety brought by the training, which are revealed in this study. Although there are works on safety alignment that fine-tune language models or manipulate model weights to defend against harmful inputs, they require extra computation or annotated data, and probably impact the reasoning ability of SLMs. In this paper, we investigate how to maintain the safety of SLMs during the CoT distillation process. Specifically, we propose a safe distillation method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the magnitude of model weight changes to optimize the model weights in the neighboring space near the initial weight distribution. Low-Entropy Masking masks low-entropy tokens, which are regarded as unnecessary learning targets, to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B, Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC, AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods. Furthermore, our ablation study presents the effectiveness of Slow Tuning and Low-Entropy Masking, with the former maintaining the model's safety in the early stage and the latter prolonging the safe training epochs.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ziyang Ma",
        "Qingyue Yuan",
        "Linhai Zhang",
        "Deyu Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/20406a42f018ddab9efc69461b6573e7364cd9bd",
      "pdf_url": "",
      "publication_date": "2025-08-13",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0f7308fbcae43d22813f70c334c2425df0b1cce1",
      "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
      "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "authors": [
        "Josef Dai",
        "Xuehai Pan",
        "Ruiyang Sun",
        "Jiaming Ji",
        "Xinbo Xu",
        "Mickel Liu",
        "Yizhou Wang",
        "Yaodong Yang"
      ],
      "citation_count": 520,
      "url": "https://www.semanticscholar.org/paper/0f7308fbcae43d22813f70c334c2425df0b1cce1",
      "pdf_url": "",
      "publication_date": "2023-10-19",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0391aee67b008aee6aa12aeea67a7f90c954c113",
      "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
      "abstract": "Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Additionally, we propose a bi-directional anchoring method that, during the selection process, prioritizes data points that are close to harmful examples and far from benign ones. Our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints surprisingly leads to the fine-tuned model affirmatively responding to>70% of tested harmful requests, compared to<20% after fine-tuning on randomly selected data. We also observe that the selected data frequently appear as lists, bullet points, or math questions, indicating a systematic pattern in fine-tuning data that contributes to jailbreaking.",
      "year": 2024,
      "venue": "",
      "authors": [
        "Luxi He",
        "Mengzhou Xia",
        "Peter Henderson"
      ],
      "citation_count": 80,
      "url": "https://www.semanticscholar.org/paper/0391aee67b008aee6aa12aeea67a7f90c954c113",
      "pdf_url": "",
      "publication_date": "2024-04-01",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "76ef69130c4d3e020ed94b17d2f7557574eb9c24",
      "title": "Universal Prompt Optimizer for Safe Text-to-Image Generation",
      "abstract": "Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, we propose the first universal **p**rompt **o**ptimizer for **s**afe T2**I** (**POSI**) generation in black-box scenario. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance. Our code is available at [https://github.com/wzongyu/POSI](https://github.com/wzongyu/POSI).",
      "year": 2024,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "authors": [
        "Zongyu Wu",
        "Hongcheng Gao",
        "Yueze Wang",
        "Xiang Zhang",
        "Suhang Wang"
      ],
      "citation_count": 21,
      "url": "https://www.semanticscholar.org/paper/76ef69130c4d3e020ed94b17d2f7557574eb9c24",
      "pdf_url": "",
      "publication_date": "2024-02-16",
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d95bf07bf21d8c17810da9c1f87190257d757b05",
      "title": "Developing Safe and Responsible Large Language Models - A Comprehensive Framework",
      "abstract": null,
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Shaina Raza",
        "Oluwanifemi Bamgbose",
        "Shardul Ghuge",
        "Fatemeh Tavakoli",
        "Deepak John Reji"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/d95bf07bf21d8c17810da9c1f87190257d757b05",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "4f49343543dccadc7a0b64fe6d881eee4e19a9a8",
      "title": "Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning",
      "abstract": "Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts. Numerous methods have been proposed to ensure these models generate safe images. Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such exter-nal filters do not inherently detoxify the model and can be easily bypassed. Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters. However, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods. Inspired by the phenomenon of catastrophic forgetting, we propose a training policy using contrastive learning to increase the latent space distance between clean and harmful data distribution, thereby protecting models from being fine-tuned to generate harmful images due to forgetting. The experimental results demonstrate that our methods not only maintain clean image generation capabilities before malicious fine-tuning but also effectively prevent DMs from producing harmful images after malicious fine-tuning. Our method can also be combined with other safety methods to maintain their safety against malicious fine-tuning further.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "authors": [
        "Jiadong Pan",
        "Hongcheng Gao",
        "Zongyu Wu",
        "Taihang Hu",
        "Li Su",
        "Qin Huang",
        "Liang Li"
      ],
      "citation_count": 6,
      "url": "https://www.semanticscholar.org/paper/4f49343543dccadc7a0b64fe6d881eee4e19a9a8",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "safe fine-tuning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b2162a73d79defdacf1df46eeb91661af78b663e",
      "title": "Toward Free-Riding Attack on Cross-Silo Federated Learning Through Evolutionary Game",
      "abstract": "In cross-silo federated learning (FL), due to the heterogeneous participants, free-riders can utilize information asymmetry to make profits without performing any local model training. Free-riding attack poses possibilities and opportunities for unfairness and can seriously impair the operation of the FL ecosystem. It motivates our work to explore and characterize the unique features of free-riding attack, which differ from other attacks such as poisoning attacks. In this paper, we propose an evolutionary public goods game-based incentive model (Fed-EPG), which makes the first attempt to construct the interaction model among the participants through the evolutionary public goods game. Specifically, we consider both the public good characteristics of cross-silo FL models as well as the bounded rationality and incomplete information of competitors. We first introduce asymmetric environmental feedback to represent reward and punishment strategies in evolutionary game, and then adopt a multi-segment nonlinear control method to dynamically adjust the rewards and punishments among the participants, which achieves the incentive for the participants to cooperate stably during the training process. Experimental results validate that our incentive model is effective in the mitigation of free-riding. attacks.",
      "year": 2024,
      "venue": "IEEE International Conference on Distributed Computing Systems",
      "authors": [
        "Tianxiang Chen",
        "Feng Wang",
        "Wangjie Qiu",
        "Qinnan Zhang",
        "Zehui Xiong",
        "Zhiming Zheng"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/b2162a73d79defdacf1df46eeb91661af78b663e",
      "pdf_url": "",
      "publication_date": "2024-07-23",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "2496f6586619582ee93206f7b8c0c00e6698d395",
      "title": "TrojFair: Trojan Fairness Attacks",
      "abstract": "Deep learning models play a crucial role in important sectors like healthcare, finance, and recruitment. However, these models can harbor biases and unfairness that may negatively impact those who rely on them. While there are algorithms designed to improve fairness, the resilience of these models to adversarial attacks, particularly the emerging threat of Trojan (or backdoor) attacks, has not been thoroughly examined. To address this gap, we introduce TrojFair, a Trojan fairness attack methodology. TrojFair creates a model that performs accurately and fairly under normal conditions but, when triggered, discriminates and produces incorrect results for specific groups. This type of attack is especially stealthy and dangerous as it can bypass existing fairness and trojan detection methods, appearing fair during regular use. Our results show that TrojFair achieves a success rate of over 88.77% in targeting specific groups, with a minimal average accuracy loss of less than 0.44%. Additionally, it consistently demonstrates a significant bias score, distinguishing between targeted and non-targeted groups across various datasets and model architectures.",
      "year": 2023,
      "venue": "LAMPS@CCS",
      "authors": [
        "Meng Zheng",
        "Jiaqi Xue",
        "Yi Sheng",
        "Lei Yang",
        "Qian Lou",
        "Lei Jiang"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/2496f6586619582ee93206f7b8c0c00e6698d395",
      "pdf_url": "https://arxiv.org/pdf/2312.10508",
      "publication_date": "2023-11-19",
      "keywords_matched": [
        "unfairness attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "11686792dc3f8008a87b813bbefe59124145924d",
      "title": "Predictably unequal: understanding and addressing concerns that algorithmic clinical prediction may increase health disparities",
      "abstract": "The machine learning community has become alert to the ways that predictive algorithms can inadvertently introduce unfairness in decision-making. Herein, we discuss how concepts of algorithmic fairness might apply in healthcare, where predictive algorithms are being increasingly used to support decision-making. Central to our discussion is the distinction between algorithmic fairness and algorithmic bias. Fairness concerns apply specifically when algorithms are used to support polar decisions (i.e., where one pole of prediction leads to decisions that are generally more desired than the other), such as when predictions are used to allocate scarce health care resources to a group of patients that could all benefit. We review different fairness criteria and demonstrate their mutual incompatibility. Even when models are used to balance benefits-harms to make optimal decisions for individuals (i.e., for non-polar decisions)\u2013and fairness concerns are not germane\u2013model, data or sampling issues can lead to biased predictions that support decisions that are differentially harmful/beneficial across groups. We review these potential sources of bias, and also discuss ways to diagnose and remedy algorithmic bias. We note that remedies for algorithmic fairness may be more problematic, since we lack agreed upon definitions of fairness. Finally, we propose a provisional framework for the evaluation of clinical prediction models offered for further elaboration and refinement. Given the proliferation of prediction models used to guide clinical decisions, developing consensus for how these concerns can be addressed should be prioritized.",
      "year": 2020,
      "venue": "npj Digital Medicine",
      "authors": [
        "J. Paulus",
        "D. Kent"
      ],
      "citation_count": 220,
      "url": "https://www.semanticscholar.org/paper/11686792dc3f8008a87b813bbefe59124145924d",
      "pdf_url": "https://www.nature.com/articles/s41746-020-0304-9.pdf",
      "publication_date": "2020-07-30",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0e15417371a0b863fc80d2af42a0c54ef65e374",
      "title": "Revisiting the Impact of Classification Techniques on the Performance of Defect Prediction Models",
      "abstract": null,
      "year": 2015,
      "venue": "2015 IEEE/ACM 37th IEEE International Conference on Software Engineering",
      "authors": [
        "Baljinder Ghotra",
        "Shane McIntosh",
        "A. Hassan"
      ],
      "citation_count": 410,
      "url": "https://www.semanticscholar.org/paper/d0e15417371a0b863fc80d2af42a0c54ef65e374",
      "pdf_url": "",
      "publication_date": "2015-05-16",
      "keywords_matched": [
        "biased prediction"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f0f60b1a381db0a54b9877d5bb410ea98b2bc540",
      "title": "Seldonian Toolkit: Building Software with Safe and Fair Machine Learning",
      "abstract": "We present the Seldonian Toolkit, which enables software engineers to integrate provably safe and fair machine learning algorithms into their systems. Software systems that use data and machine learning are routinely deployed in a wide range of settings from medical applications, autonomous vehicles, the criminal justice system, and hiring processes. These systems, however, can produce unsafe and unfair behavior, such as suggesting potentially fatal medical treatments, making racist or sexist predictions, or facilitating radicalization and polarization. To reduce these undesirable behaviors, software engineers need the ability to easily integrate their machine-learning-based systems with domain-specific safety and fairness requirements defined by domain experts, such as doctors and hiring managers. The Seldonian Toolkit provides special machine learning algorithms that enable software engineers to incorporate such expert-defined requirements of safety and fairness into their systems, while provably guaranteeing those requirements will be satisfied. A video demonstrating the Seldonian Toolkit is available at https://youtu.be/wHR-hDm9jX4/",
      "year": 2023,
      "venue": "2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",
      "authors": [
        "Austin Hoag",
        "James E. Kostas",
        "B. C. Silva",
        "P. Thomas",
        "Yuriy Brun"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/f0f60b1a381db0a54b9877d5bb410ea98b2bc540",
      "pdf_url": "",
      "publication_date": "2023-05-01",
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "6102a582e5b767e292fa8b423e39a575e103fab4",
      "title": "Explainable and Fair AI: Balancing Performance in Financial and Real Estate Machine Learning Models",
      "abstract": "This paper introduces a framework that integrates fairness and transparency into advanced machine learning models, specifically LightGBM and XGBoost, applied to loan approval and house price prediction datasets. The key contribution is using fairness-focused techniques, such as Calibrated Equalized Odds and Intersectional Fairness, which are not widely studied in financial and real estate contexts. To improve model transparency, SHAP (SHapley Additive exPlanations) is utilized along with a novel fairness-based interpretability method to measure both model fairness and the importance of individual features. Through comprehensive experiments, we show that LightGBM delivers high accuracy while balancing fairness and performance effectively. The broader relevance of this work is discussed in the context of governance and regulatory requirements, highlighting the importance of responsible practices in high-stakes financial decision-making processes. This research highlights the importance of fairness and transparency in real-world applications, promoting equity, trust, and adherence to evolving legal standards, and provides practical insights for data scientists, machine learning researchers, and professionals in the real estate and financial sectors.",
      "year": 2024,
      "venue": "IEEE Access",
      "authors": [
        "D. Acharya",
        "Divya Bhaskaracharya",
        "Karthigeyan Kuppan"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/6102a582e5b767e292fa8b423e39a575e103fab4",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "fair machine learning"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "64a35fc2b5e15c51a6abf9911e9cb321a2f0ef1d",
      "title": "Using traditional machine learning and deep learning methods for on- and off-target prediction in CRISPR/Cas9: a review",
      "abstract": "Abstract CRISPR/Cas9 (Clustered Regularly Interspaced Short Palindromic Repeats and CRISPR-associated protein 9) is a popular and effective two-component technology used for targeted genetic manipulation. It is currently the most versatile and accurate method of gene and genome editing, which benefits from a large variety of practical applications. For example, in biomedicine, it has been used in research related to cancer, virus infections, pathogen detection, and genetic diseases. Current CRISPR/Cas9 research is based on data-driven models for on- and off-target prediction as a cleavage may occur at non-target sequence locations. Nowadays, conventional machine learning and deep learning methods are applied on a regular basis to accurately predict on-target knockout efficacy and off-target profile of given single-guide RNAs (sgRNAs). In this paper, we present an overview and a comparative analysis of traditional machine learning and deep learning models used in CRISPR/Cas9. We highlight the key research challenges and directions associated with target activity prediction. We discuss recent advances in the sgRNA\u2013DNA sequence encoding used in state-of-the-art on- and off-target prediction models. Furthermore, we present the most popular deep learning neural network architectures used in CRISPR/Cas9 prediction models. Finally, we summarize the existing challenges and discuss possible future investigations in the field of on- and off-target prediction. Our paper provides valuable support for academic and industrial researchers interested in the application of machine learning methods in the field of CRISPR/Cas9 genome editing.",
      "year": 2023,
      "venue": "Briefings Bioinform.",
      "authors": [
        "Zeinab Sherkatghanad",
        "Moloud Abdar",
        "J\u00e9r\u00e9my Charlier",
        "V. Makarenkov"
      ],
      "citation_count": 63,
      "url": "https://www.semanticscholar.org/paper/64a35fc2b5e15c51a6abf9911e9cb321a2f0ef1d",
      "pdf_url": "https://academic.oup.com/bib/advance-article-pdf/doi/10.1093/bib/bbad131/50048495/bbad131.pdf",
      "publication_date": "2023-04-20",
      "keywords_matched": [
        "prediction manipulation"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ec193937ff0df3103efc6dfcc6a711ee4e476fd2",
      "title": "Laser Fault Injection in Memristor-Based Accelerators for AI/ML and Neuromorphic Computing",
      "abstract": "Memristive crossbar arrays (MCA) are emerging as efficient building blocks for in-memory computing and neuromorphic hardware due to their high density and parallel analog matrix-vector multiplication capabilities. However, the physical properties of their nonvolatile memory elements introduce new attack surfaces, particularly under fault injection scenarios. This work explores Laser Fault Injection as a means of inducing analog perturbations in MCA-based architectures. We present a detailed threat model in which adversaries target memristive cells to subtly alter their physical properties or outputs using laser beams. Through HSPICE simulations of a large MCA on 45 nm CMOS tech. node, we show how laser-induced photocurrent manifests in output current distributions, enabling differential fault analysis to infer internal weights with up to 99.7% accuracy, replicate the model, and compromise computational integrity through targeted weight alterations by approximately 143%.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Muhammad Faheemur Rahman",
        "Wayne P. Burleson"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ec193937ff0df3103efc6dfcc6a711ee4e476fd2",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "output integrity attack"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50668ecefa6126ec20116f21b1d8e7bf571c34bd",
      "title": "Deep-Fake Finder: Uncovering Forgery Image Through Neural Network Analysis",
      "abstract": "Digital images are a common form of media shared on social media, and their circulation can undermine news credibility and public trust. This research proposes a method for extracting image content, classifying it, verifying its authenticity, and identifying manipulations. With the exponential increase in social networking services, there has been a huge growth in the generation of image data, leading to the creation of fabricated images. These images are a major source of fake news, negatively impacting society. To verify the authenticity of these images, a method using a CNN deep learning model can be used. Pixel-based fake image detection can find tempering of an image up to a certain level. The raw image can be mangled in sections or as a whole, and it is necessary to recognize the type of image tampering and localize the tampered region. The image is converted into pixel format, and the RGB values are fed into the network's input layer. The output layer has two neurons for phony and genuine images, allowing us to infer if the image is phony and how likely it is to be tampered.",
      "year": 2024,
      "venue": "2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)",
      "authors": [
        "Deepshikha Bhargava",
        "Sudha Rani",
        "Manjeet Singh",
        "Nandita Tripathi",
        "Amitabh Bhargava",
        "Garima Panwar"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/50668ecefa6126ec20116f21b1d8e7bf571c34bd",
      "pdf_url": "",
      "publication_date": "2024-05-09",
      "keywords_matched": [
        "output tampering"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "25b0bd2c06fa48e03e9e3c32b22b27f95d3eb44c",
      "title": "Sparse Trigger Pattern Guided Deep Learning Model Watermarking",
      "abstract": "Watermarking neural networks (NNs) for ownership protection has received considerable attention recently. Resisting both model pruning and fine-tuning is commonly considered to evaluate the robustness of a watermarked NN. However, the rationale behind such a robustness is still relatively unexplored in the literature. In this paper, we study this problem to propose a so-called sparse trigger pattern (STP) guided deep learning model watermarking method. We provide empirical evidence to show that trigger patterns are able to make the distribution of model parameters compact, and thus exhibit interpretable resilience to model pruning and fine-tuning. We find the effect of STP can also be technically interpreted as the first layer dropout. Extensive experiments demonstrate the robustness of our method.",
      "year": 2022,
      "venue": "Information Hiding and Multimedia Security Workshop",
      "authors": [
        "Chun-Shien Lu"
      ],
      "citation_count": 4,
      "url": "https://www.semanticscholar.org/paper/25b0bd2c06fa48e03e9e3c32b22b27f95d3eb44c",
      "pdf_url": "",
      "publication_date": "2022-06-23",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "31675c0dea68199af03a8c58b638b89df4b9db3f",
      "title": "Hidden Trigger Backdoor Attacks",
      "abstract": "With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.",
      "year": 2019,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Aniruddha Saha",
        "Akshayvarun Subramanya",
        "H. Pirsiavash"
      ],
      "citation_count": 687,
      "url": "https://www.semanticscholar.org/paper/31675c0dea68199af03a8c58b638b89df4b9db3f",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6871/6725",
      "publication_date": "2019-09-30",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "155139e0ac1ca0ef4845d94a810480a484db862d",
      "title": "TAD: Trigger Approximation based Black-box Trojan Detection for AI",
      "abstract": "An emerging amount of intelligent applications have been developed with the surge of Machine Learning (ML). Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by the stealthy trigger. We call this vulnerable model adversarial artificial intelligence (AI). In this paper, we target to design a robust Trojan detection scheme that inspects whether a pre-trained AI model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose TAD, the first trigger approximation based Trojan detection framework that enables fast and scalable search of the trigger in the input space. Furthermore, TAD can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of the TAD across various datasets and ML models. Empirical results show that TAD achieves a ROC-AUC score of 0:91 on the public TrojAI dataset 1 and the average detection time per model is 7:1 minutes.",
      "year": 2021,
      "venue": "arXiv.org",
      "authors": [
        "Xinqiao Zhang",
        "Huili Chen",
        "F. Koushanfar"
      ],
      "citation_count": 16,
      "url": "https://www.semanticscholar.org/paper/155139e0ac1ca0ef4845d94a810480a484db862d",
      "pdf_url": "",
      "publication_date": "2021-02-03",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bb88b73c6ccea67be56cd1ae76474eeddac73a07",
      "title": "Improved Test Pattern Generation for Hardware Trojan Detection Using Genetic Algorithm and Boolean Satisfiability",
      "abstract": null,
      "year": 2015,
      "venue": "Workshop on Cryptographic Hardware and Embedded Systems",
      "authors": [
        "Sayandeep Saha",
        "R. Chakraborty",
        "Srinivasa Shashank Nuthakki",
        "Anshul",
        "Debdeep Mukhopadhyay"
      ],
      "citation_count": 136,
      "url": "https://www.semanticscholar.org/paper/bb88b73c6ccea67be56cd1ae76474eeddac73a07",
      "pdf_url": "",
      "publication_date": "2015-09-13",
      "keywords_matched": [
        "trigger pattern"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "0d8f01d28699df22d9714601100770398fb7e2f2",
      "title": "Interpretation-Empowered Neural Cleanse for Backdoor Attacks",
      "abstract": "Backdoor attacks have posed a significant threat to deep neural networks, highlighting the need for robust defense strategies. Previous research has demonstrated that attribution maps change substantially when exposed to attacks, suggesting the potential of interpreters in detecting adversarial examples. However, most existing defense methods against backdoor attacks overlook the untapped capabilities of interpreters, failing to fully leverage their potential. In this paper, we propose a novel approach called interpretation-empowered neural cleanse (IENC ) for defending backdoor attacks. Specifically, integrated gradient (IG) is adopted to bridge the interpreters and classifiers to reverse and reconstruct the high-quality backdoor trigger. Then, an interpretation-empowered adaptative pruning strategy (IEAPS) is proposed to cleanse the backdoor-related neurons without the pre-defined threshold. Additionally, a hybrid model patching approach is employed to integrate the IEAPS and preprocessing techniques to enhance the defense performance. Comprehensive experiments are constructed on various datasets, demonstrating the potential of interpretations in defending backdoor attacks and the superiority of the proposed method.",
      "year": 2024,
      "venue": "The Web Conference",
      "authors": [
        "Liang-bo Ning",
        "Zeyu Dai",
        "Jingran Su",
        "Chao Pan",
        "Luning Wang",
        "Wenqi Fan",
        "Qing Li"
      ],
      "citation_count": 7,
      "url": "https://www.semanticscholar.org/paper/0d8f01d28699df22d9714601100770398fb7e2f2",
      "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651525",
      "publication_date": "2024-05-13",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "43f7fba1f59575ebff17cd3eff5b38cc364950e6",
      "title": "Table of Content for Appendix Towards E\ufb00ective and Robust Neural Trojan Defenses via Input Filtering",
      "abstract": null,
      "year": 2022,
      "venue": "",
      "authors": [
        "Kien Do",
        "H. Harikumar",
        "Hung Le",
        "D. Nguyen",
        "T. Tran",
        "Santu Rana",
        "Dang Nguyen",
        "W. Susilo",
        "S. Venkatesh",
        "Encoder Encoder",
        "Encoder Encoder ConvBlockA"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/43f7fba1f59575ebff17cd3eff5b38cc364950e6",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "20f220878afc95fefd236e8a136acc6598bb0ad9",
      "title": "UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation",
      "abstract": "Recent advances in backdoor attacks, like leveraging complex triggers or stealthy implanting techniques, have introduced new challenges in backdoor scanning, limiting the usability of Deep Neural Networks (DNNs) in various scenarios. In this paper, we propose Unlearning-based Model Ablation (UMA), a novel approach to facilitate backdoor scanning and defend against advanced backdoor attacks. UMA filters out backdoor-irrelevant features by ablating the inherent features of the target class within the model and subsequently reveals the backdoor through dynamic trigger optimization. We evaluate our method on 1700 models (700 benign and 1000 trojaned) with 6 model structures, 7 different backdoor attacks and 4 datasets. Our results demonstrate that the proposed methodology effectively detect these advanced backdoors. Specifically, our method can achieve 91% AUC-ROC and 86.6% detection accuracy on average, which outperforms the baselines, including Neural Cleanse, ABS, K-Arm and MNTD.",
      "year": 2024,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yue Zhao",
        "Congyi Li",
        "Kai Chen"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/20f220878afc95fefd236e8a136acc6598bb0ad9",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/30183/32099",
      "publication_date": "2024-03-24",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8398b9de81d122e90d76c5e2dbf84733f877cb17",
      "title": "Reliable backdoor attack detection for various size of backdoor triggers",
      "abstract": "Backdoor attack techniques have evolved toward compromising the integrity of deep learning (DL) models. To defend against backdoor attacks, neural cleanse (NC) has been proposed as a promising backdoor attack detection method. NC detects the existence of a backdoor trigger by inserting perturbation into a benign image and then capturing the abnormality of inserted perturbation. However, NC has a significant limitation such that it fails to detect a backdoor trigger when its size exceeds a certain threshold that can be measured in anomaly index (AI). To overcome such limitation, in this paper, we propose a reliable backdoor attack detection method that successfully detects backdoor attacks regardless of the backdoor trigger size. Specifically, our proposed method inserts perturbation to backdoor images to induce them to be classified into different labels and measures the abnormality of perturbation. Thus, we assume that the amount of perturbation required to reclassify the label of backdoor images to the ground-truth label will be abnormally small compared to them for other labels. By implementing and conducting comparative experiments, we confirmed that our idea is valid, and our proposed method outperforms an existing backdoor detection method (NC) by 30%p on average in terms of backdoor detection accuracy (BDA).",
      "year": 2025,
      "venue": "IAES International Journal of Artificial Intelligence (IJ-AI)",
      "authors": [
        "Yeongrok Rah",
        "Youngho Cho"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/8398b9de81d122e90d76c5e2dbf84733f877cb17",
      "pdf_url": "",
      "publication_date": "2025-02-01",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "72e5e4e85cbeefc2389dfffd0b2da29bbefe7bfe",
      "title": "Quantization Blindspots: How Model Compression Breaks Backdoor Defenses",
      "abstract": "Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Rohan Pandey",
        "Eric Ye"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/72e5e4e85cbeefc2389dfffd0b2da29bbefe7bfe",
      "pdf_url": "",
      "publication_date": "2025-12-06",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5e44027a17ba79fdfb79138bcb294dc5589e6302",
      "title": "Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures",
      "abstract": "Since Deep Learning (DL) backdoor attacks have been revealed as one of the most insidious adversarial attacks, a number of countermeasures have been developed with certain assumptions defined in their respective threat models. However, their robustness is currently inadvertently ignored, which can introduce severe consequences, e.g., a countermeasure can be misused and result in a false implication of backdoor detection. For the first time, we critically examine the robustness of existing backdoor countermeasures. As an initial study, we first identify five potential non-robust failure factors including binary classification, poison rate, model complexity, single-model justification, and hyperparameter sensitivity. As exhaustively examining defenses is infeasible, we instead focus on influential backdoor detection-based countermeasures consisting of model-inspection ones including Neural Cleanse (S&P\u201919), ABS (CCS\u201919), and MNTD (S&P\u201921), and data-inspection ones including SCAn (USENIX SECURITY\u201921) to examine their failure cases under one or more of these factors. Although these investigated countermeasures claim that they work well under their respective threat models, they have inherent unexplored non-robust cases, which are not even rooted from delicate adaptive attacks. We demonstrate how to trivially bypass them aligned with their respective threat models by simply varying the aforementioned factors. Particularly, for each defense, formal proofs or empirical studies are used to reveal its non-robust cases where it is not as robust as it claims or expects. This work highlights the necessity of thoroughly evaluating the robustness of backdoor countermeasures to avoid their misleading security implications in unknown non-robust cases.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Huming Qiu",
        "Hua Ma",
        "Zhi Zhang",
        "A. Abuadbba",
        "Wei Kang",
        "Anmin Fu",
        "Yansong Gao"
      ],
      "citation_count": 19,
      "url": "https://www.semanticscholar.org/paper/5e44027a17ba79fdfb79138bcb294dc5589e6302",
      "pdf_url": "https://arxiv.org/pdf/2204.06273",
      "publication_date": "2022-04-13",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "04fa46382ee60b2271b1698038f17c9f16680ed3",
      "title": "Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain",
      "abstract": "With the rapid development and broad application of deep neural networks (DNNs), backdoor attacks have gradually attracted attention due to their great harmfulness. Backdoor attacks are in-sidious, and poisoned models perform well on benign samples and are only triggered when given specific inputs, which then cause the neural network to produce incorrect outputs. The state-of-the-art backdoor attack work is implemented by data poisoning, i.e., the attacker injects poisoned samples (some samples patched with a trigger) into the dataset, and the models trained with that dataset are infected with the backdoor. However, most of the triggers used in the current study are fixed patterns patched on a small fraction of an image and are often clearly mislabeled, which is easily detected by humans or some defense methods such as Neural Cleanse and SentiNet. Also, it is difficult to be learned by DNNs without mislabeling, as they may ignore small patterns. In this paper, we propose a generalized backdoor attack method based on the frequency domain, which can implement backdoor implantation without mislabeling the poisoned samples and accessing the training process. It is invisible to human beings and able to evade the commonly used defense methods. We evaluate our approach in the no-label and clean-label cases on three benchmark datasets (CIFAR-10, STL-10, and GTSRB) with two popular scenarios, including self-supervised learning and supervised learning. The results show that our approach can achieve a high attack success rate (above 90%) on all the tasks without significant performance degradation on main tasks. Also, we evaluate the bypass performance of our approach for different kinds of defenses, including the detection of training data (i.e., Activation Clustering), the preprocessing of inputs (i.e., Filtering), the detection of inputs (i.e., SentiNet), and the detection of models (i.e., Neural Cleanse). The experimental results demonstrate that our approach shows excellent robustness to such defenses. CIFAR-10 [20] dataset (i.e., some images are patched with our frequency trigger) as the feature extractor using the popular methods SimCLR [6] and MOCO V2 [7], and then transfer to the downstream tasks, including CIFAR-10, STL-10 [10] and GTSRB [41]. We train ResNet-18 on the poisoned CIFAR-10 and STL-10 datasets for supervised learning. The experimental result demonstrates that our attack achieves over 90% success rate on the poisoned samples of all the datasets, and only about 2% performance degradation on the main tasks is incurred. Furthermore, we use PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity) to evaluate the changes to the original images due to our frequency trigger. The average PSNR on CIFAR-10 and STL-10 datasets is 24.11 and 26.94, respectively, and the average SSIM is 0.9024 and 0.9044, proving that our trigger has good stealthiness. In addition, We evaluate the bypass effectiveness of our attack against common backdoor detection methods, and the experimental result shows that our approach can bypass them all with high robustness.",
      "year": 2022,
      "venue": "arXiv.org",
      "authors": [
        "Chang Yue",
        "Peizhuo Lv",
        "Ruigang Liang",
        "Kai Chen"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/04fa46382ee60b2271b1698038f17c9f16680ed3",
      "pdf_url": "http://arxiv.org/pdf/2207.04209",
      "publication_date": "2022-07-09",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f3bb835cf3d45553f0a62c390d468e06a288b748",
      "title": "Dynamic Backdoors with Global Average Pooling",
      "abstract": "Outsourced training and machine learning as a service have resulted in novel attack vectors like backdoor attacks. Such attacks embed a secret functionality in a neural network activated when the trigger is added to its input. In most works in the literature, the trigger is static, both in terms of location and pattern. The effectiveness of various detection mechanisms depends on this property. It was recently shown that countermeasures in image classification, like Neural Cleanse and ABS, could be bypassed with dynamic triggers that are effective regardless of their pattern and location. Still, such backdoors are demanding as they require a large percentage of poisoned training data. In this work, we are the first to show that dynamic backdoor attacks could happen due to a global average pooling layer without increasing the percentage of the poisoned training data. Nevertheless, our experiments in sound classification, text sentiment analysis, and image classification show this to be very difficult in practice.",
      "year": 2022,
      "venue": "International Conference on Artificial Intelligence Circuits and Systems",
      "authors": [
        "Stefanos Koffas",
        "S. Picek",
        "M. Conti"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/f3bb835cf3d45553f0a62c390d468e06a288b748",
      "pdf_url": "https://repository.ubn.ru.nl//bitstream/handle/2066/288623/288623.pdf",
      "publication_date": "2022-03-04",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "41c883754ae1983e94bff6c3ec3d40306cffe06e",
      "title": "The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models",
      "abstract": "In this paper, we introduce the TrojAI software framework, an open source set of Python tools capable of generating triggered (poisoned) datasets and associated deep learning (DL) models with trojans at scale. We utilize the developed framework to generate a large set of trojaned MNIST classifiers, as well as demonstrate the capability to produce a trojaned reinforcement-learning model using vector observations. Results on MNIST show that the nature of the trigger, training batch size, and dataset poisoning percentage all affect successful embedding of trojans. We test Neural Cleanse against the trojaned MNIST models and successfully detect anomalies in the trained models approximately $18\\%$ of the time. Our experiments and workflow indicate that the TrojAI software framework will enable researchers to easily understand the effects of various configurations of the dataset and training hyperparameters on the generated trojaned deep learning model, and can be used to rapidly and comprehensively test new trojan detection methods.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Kiran Karra",
        "C. Ashcraft",
        "Neil Fendley"
      ],
      "citation_count": 39,
      "url": "https://www.semanticscholar.org/paper/41c883754ae1983e94bff6c3ec3d40306cffe06e",
      "pdf_url": "",
      "publication_date": "2020-03-13",
      "keywords_matched": [
        "neural cleanse"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "9625e9e273614c31601d32387a88bdad704f62c1",
      "title": "Unified Neural Backdoor Removal With Only Few Clean Samples Through Unlearning and Relearning",
      "abstract": "Deep neural networks have achieved remarkable success across various applications; however, their vulnerability to backdoor attacks poses severe security risks\u2014especially in situations where only a limited set of clean samples is available for defense. In this work, we address this critical challenge by proposing ULRL (UnLearn and ReLearn for backdoor removal), a novel two-phase approach for comprehensive backdoor removal. Our method first employs an unlearning phase, in which the network\u2019s loss is intentionally maximized on a small clean dataset to expose neurons that are excessively sensitive to backdoor triggers. Subsequently, in the relearning phase, these suspicious neurons are recalibrated using targeted reinitialization and cosine similarity regularization, effectively neutralizing backdoor influences while preserving the model\u2019s performance on benign data. Extensive experiments with 12 backdoor types on multiple datasets (CIFAR-10, CIFAR-100, GTSRB, and Tiny-ImageNet) and architectures (PreAct-ResNet18, VGG19-BN, and ViT-B-16) demonstrate that ULRL significantly reduces the attack success rate without compromising clean accuracy\u2014even when only 1% of clean data is used for defense.",
      "year": 2024,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Nay Myat Min",
        "Long H. Pham",
        "Jun Sun"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/9625e9e273614c31601d32387a88bdad704f62c1",
      "pdf_url": "",
      "publication_date": "2024-05-23",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "86fe69c151370f3532a28c83726b0bcdc21c31c4",
      "title": "Perturb and Recover: Fine-tuning for Effective Backdoor Removal from CLIP",
      "abstract": "Vision-Language models like CLIP have been shown to be highly effective at linking visual perception and natural language understanding, enabling sophisticated image-text capabilities, including strong retrieval and zero-shot classification performance. Their widespread use, as well as the fact that CLIP models are trained on image-text pairs from the web, make them both a worthwhile and relatively easy target for backdoor attacks. As training foundational models, such as CLIP, from scratch is very expensive, this paper focuses on cleaning potentially poisoned models via fine-tuning. We first show that existing cleaning techniques are not effective against simple structured triggers used in Blended or BadNet backdoor attacks, exposing a critical vulnerability for potential real-world deployment of these models. Then, we introduce PAR, Perturb and Recover, a surprisingly simple yet effective mechanism to remove backdoors from CLIP models. Through extensive experiments across different encoders and types of backdoor attacks, we show that PAR achieves high backdoor removal rate while preserving good standard performance. Finally, we illustrate that our approach is effective even only with synthetic text-image pairs, i.e. without access to real training data. The code and models are available at https://github.com/nmndeep/PerturbAndRecover.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "N. Singh",
        "Francesco Croce",
        "Matthias Hein"
      ],
      "citation_count": 5,
      "url": "https://www.semanticscholar.org/paper/86fe69c151370f3532a28c83726b0bcdc21c31c4",
      "pdf_url": "",
      "publication_date": "2024-12-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d0849e909233dd429213f444d1fd4f6b3fa27bf4",
      "title": "Knowledge-Driven Backdoor Removal in Deep Neural Networks via Reinforcement Learning",
      "abstract": null,
      "year": 2024,
      "venue": "Knowledge Science, Engineering and Management",
      "authors": [
        "Jiayin Song",
        "Yike Li",
        "Yunzhe Tian",
        "Xingyu Wu",
        "Qiong Li",
        "Endong Tong",
        "Wenjia Niu",
        "Zhenguo Zhang",
        "Jiqiang Liu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d0849e909233dd429213f444d1fd4f6b3fa27bf4",
      "pdf_url": "",
      "publication_date": null,
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "050ffe2459a64a30b61b371b833b134e4fe0e95e",
      "title": "Efficient Backdoor Removal Through Natural Gradient Fine-tuning",
      "abstract": "The success of a deep neural network (DNN) heavily relies on the details of the training scheme; e.g., training data, architectures, hyper-parameters, etc. Recent backdoor attacks suggest that an adversary can take advantage of such training details and compromise the integrity of a DNN. Our studies show that a backdoor model is usually optimized to a bad local minima, i.e. sharper minima as compared to a benign model. Intuitively, a backdoor model can be purified by reoptimizing the model to a smoother minima through fine-tuning with a few clean validation data. However, fine-tuning all DNN parameters often requires huge computational costs and often results in sub-par clean test performance. To address this concern, we propose a novel backdoor purification technique, Natural Gradient Fine-tuning (NGF), which focuses on removing the backdoor by fine-tuning only one layer. Specifically, NGF utilizes a loss surface geometry-aware optimizer that can successfully overcome the challenge of reaching a smooth minima under a one-layer optimization scenario. To enhance the generalization performance of our proposed method, we introduce a clean data distribution-aware regularizer based on the knowledge of loss surface curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show that the proposed method achieves state-of-the-art performance on a wide range of backdoor defense benchmarks: four different datasets- CIFAR10, GTSRB, Tiny-ImageNet, and ImageNet; 13 recent backdoor attacks, e.g. Blend, Dynamic, WaNet, ISSBA, etc.",
      "year": 2023,
      "venue": "arXiv.org",
      "authors": [
        "Nazmul Karim",
        "Abdullah Al Arafat",
        "Umar Khalid",
        "Zhishan Guo",
        "Naznin Rahnavard"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/050ffe2459a64a30b61b371b833b134e4fe0e95e",
      "pdf_url": "http://arxiv.org/pdf/2306.17441",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "f6dbd3e817be461a1e73fcc4fabd3db3caa020f3",
      "title": "Robust Backdoor Removal by Reconstructing Trigger-Activated Changes in Latent Representation",
      "abstract": "Backdoor attacks pose a critical threat to machine learning models, causing them to behave normally on clean data but misclassify poisoned data into a poisoned class. Existing defenses often attempt to identify and remove backdoor neurons based on Trigger-Activated Changes (TAC) which is the activation differences between clean and poisoned data. These methods suffer from low precision in identifying true backdoor neurons due to inaccurate estimation of TAC values. In this work, we propose a novel backdoor removal method by accurately reconstructing TAC values in the latent representation. Specifically, we formulate the minimal perturbation that forces clean data to be classified into a specific class as a convex quadratic optimization problem, whose optimal solution serves as a surrogate for TAC. We then identify the poisoned class by detecting statistically small $L^2$ norms of perturbations and leverage the perturbation of the poisoned class in fine-tuning to remove backdoors. Experiments on CIFAR-10, GTSRB, and TinyImageNet demonstrated that our approach consistently achieves superior backdoor suppression with high clean accuracy across different attack types, datasets, and architectures, outperforming existing defense methods.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Kazuki Iwahana",
        "Yusuke Yamasaki",
        "Akira Ito",
        "Takayuki Miura",
        "Toshiki Shibahara"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/f6dbd3e817be461a1e73fcc4fabd3db3caa020f3",
      "pdf_url": "",
      "publication_date": "2025-11-12",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "551203723f8d0f2888a3859bba4347fb74b4d165",
      "title": "Generalizable poisoning-resistant backdoor detection and removal framework: From dataset perspective",
      "abstract": null,
      "year": 2025,
      "venue": "Pattern Recognition",
      "authors": [
        "Baolin Li",
        "Tao Hu",
        "Xinlei Liu",
        "Jichao Xie",
        "Peng Yi"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/551203723f8d0f2888a3859bba4347fb74b4d165",
      "pdf_url": "",
      "publication_date": "2025-11-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8422e0fcd98c7abe2277ee00ad602d7e851c8341",
      "title": "Shortcuts Everywhere and Nowhere: Exploring Multi-Trigger Backdoor Attacks",
      "abstract": "Backdoor attacks have become a significant threat to the pre-training and deployment of deep neural networks (DNNs). Although numerous methods for detecting and mitigating backdoor attacks have been proposed, most rely on identifying and eliminating the ``shortcut\"created by the backdoor, which links a specific source class to a target class. However, these approaches can be easily circumvented by designing multiple backdoor triggers that create shortcuts everywhere and therefore nowhere specific. In this study, we explore the concept of Multi-Trigger Backdoor Attacks (MTBAs), where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks including \\textit{parallel}, \\textit{sequential}, and \\textit{hybrid} attacks, we demonstrate that 1) multiple triggers can coexist, overwrite, or cross-activate one another, and 2) MTBAs easily break the prevalent shortcut assumption underlying most existing backdoor detection/removal methods, rendering them ineffective. Given the security risk posed by MTBAs, we have created a multi-trigger backdoor poisoning dataset to facilitate future research on detecting and mitigating these attacks, and we also discuss potential defense strategies against MTBAs. Our code is available at https://github.com/bboylyg/Multi-Trigger-Backdoor-Attacks.",
      "year": 2024,
      "venue": "IEEE Transactions on Dependable and Secure Computing",
      "authors": [
        "Yige Li",
        "Xingjun Ma",
        "Jiabo He",
        "Hanxun Huang",
        "Yu-Gang Jiang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/8422e0fcd98c7abe2277ee00ad602d7e851c8341",
      "pdf_url": "",
      "publication_date": "2024-01-27",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "b15493107b8e3193aae28d818ef265f0e8790f49",
      "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
      "abstract": "Safety backdoor attacks in large language models (LLMs) enable harmful behaviors to be stealthily triggered while evading detection during normal interactions. The high dimensionality of the trigger search space and the diverse range of potential malicious behaviors in LLMs make this a critical open problem. This paper presents BEEAR, a novel mitigation method based on a key insight: backdoor triggers induce a uniform drift in the model\u2019s embedding space, irrespective of the trigger\u2019s form or targeted behavior. Leveraging this observation, we introduce a bi-level optimization approach. The inner level identifies universal perturbations to the decoder\u2019s embeddings that steer the model towards defender-defined unwanted behaviors; the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. Our experiments demonstrate the effectiveness of this approach, reducing the success rate of safety backdoor attacks from over 95% to <1% for general harmful behaviors and from 47% to 0% for Sleeper Agents, without compromising the model\u2019s helpfulness. Notably, our method relies only on defender-defined sets of safe and unwanted behaviors without any assumptions about the trigger location or attack mechanism. This work represents the first practical framework to counter safety backdoors in LLMs and provides a foundation for future advancements in AI safety and security.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "authors": [
        "Yi Zeng",
        "Weiyu Sun",
        "T. Huynh",
        "Dawn Song",
        "Bo Li",
        "Ruoxi Jia"
      ],
      "citation_count": 42,
      "url": "https://www.semanticscholar.org/paper/b15493107b8e3193aae28d818ef265f0e8790f49",
      "pdf_url": "",
      "publication_date": "2024-06-24",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "3b9ff48d1e0d01c7948b1f1197779920a667c40a",
      "title": "Need for Speed: Taming Backdoor Attacks with Speed and Precision",
      "abstract": "Modern deep neural network models (DNNs) require extensive data for optimal performance, prompting reliance on multiple entities for the acquisition of training datasets. One prominent security threat is backdoor attacks where the adversary party poisons a small subset of training datasets to implant a backdoor into the model, leading to misclassifications during runtime for triggered samples. To mitigate the attack, many defense methods have been proposed, such as detecting and removing poisoned samples or rectifying trojaned model weights in victim DNNs. However, existing approaches suffer from notable inefficiency as they are faced with large-scale training datasets, consequently rendering these defenses impractical in the real world. In this paper, we propose a lightweight backdoor identification and removal scheme, called ReBack. In this scheme, ReBack first extracts a subset of suspicious and benign samples, and then, proceeds with a \"averaging and differencing\" based method to identify target label(s). Next, leveraging the identification results, ReBack invokes a novel reverse engineering method to recover the exact trigger using only basic arithmetic atoms. Our experiments demonstrate that, for ImageNet with 750 labels, ReBack can defend against backdoor attacks in around 2 hours, showcasing a speed improvement of 18.5\u00d7 to 214\u00d7 compared to existing methods. For backdoor removal, the attack success rate can be decreased to 0.05% owing to 99% cosine similarity of the reversed triggers. The code is online available.",
      "year": 2024,
      "venue": "IEEE Symposium on Security and Privacy",
      "authors": [
        "Zhuo Ma",
        "Yilong Yang",
        "Yang Liu",
        "Tong Yang",
        "Xinjing Liu",
        "Teng Li",
        "Zhan Qin"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/3b9ff48d1e0d01c7948b1f1197779920a667c40a",
      "pdf_url": "",
      "publication_date": "2024-05-19",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1bde12d99a445c08aaebd3ef5d7f3bf3b3754893",
      "title": "Expose Before You Defend: Unifying and Enhancing Backdoor Defenses via Exposed Models",
      "abstract": "Backdoor attacks covertly implant triggers into deep neural networks (DNNs) by poisoning a small portion of the training data with pre-designed backdoor triggers. This vulnerability is exacerbated in the era of large models, where extensive (pre-)training on web-crawled datasets is susceptible to compromise. In this paper, we introduce a novel two-step defense framework named Expose Before You Defend (EBYD). EBYD unifies existing backdoor defense methods into a comprehensive defense system with enhanced performance. Specifically, EBYD first exposes the backdoor functionality in the backdoored model through a model preprocessing step called backdoor exposure, and then applies detection and removal methods to the exposed model to identify and eliminate the backdoor features. In the first step of backdoor exposure, we propose a novel technique called Clean Unlearning (CUL), which proactively unlearns clean features from the backdoored model to reveal the hidden backdoor features. We also explore various model editing/modification techniques for backdoor exposure, including fine-tuning, model sparsification, and weight perturbation. Using EBYD, we conduct extensive experiments on 10 image attacks and 6 text attacks across 2 vision datasets (CIFAR-10 and an ImageNet subset) and 4 language datasets (SST-2, IMDB, Twitter, and AG's News). The results demonstrate the importance of backdoor exposure for backdoor defense, showing that the exposed models can significantly benefit a range of downstream defense tasks, including backdoor label detection, backdoor trigger recovery, backdoor model detection, and backdoor removal. We hope our work could inspire more research in developing advanced defense frameworks with exposed models. Our code is available at: https://github.com/bboylyg/Expose-Before-You-Defend.",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Yige Li",
        "Hanxun Huang",
        "Jiaming Zhang",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/1bde12d99a445c08aaebd3ef5d7f3bf3b3754893",
      "pdf_url": "",
      "publication_date": "2024-10-25",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "633c6a67aec2197771941f04e362910be27e9c86",
      "title": "Defending Against Backdoor Attacks with Feature Activation-Based Detection and Model Recovery",
      "abstract": "The characteristics of Federated Learning (FL) make FL highly susceptible to malicious poisoning attacks from adversaries. Existing FL defense methods can detect attacks of fixed poisoning patterns, hence are lack of flexibility. Additionally, they typically remove the malicious node models upon detection, leading to a certain degree of data loss. To address these issues, we propose a novel defense method against backdoor attacks in FL systems, effectively enhancing their robustness to malicious poisoning attacks. Specifically, we introduce a malicious update detection method based on feature activation matrices. This method compares the distribution differences of updates from different clients on the same validation data and detects malicious clients based on the outlier rates of their updates. Furthermore, to mitigate the data loss caused by the removal of malicious clients, the server assesses the distance between the distribution of feature activation matrices from the client\u2019s historical updates and the overall model distribution in the current iteration. Based on this distance, the server performs model recovery to a certain extent. Extensive experiments on two benchmark datasets demonstrate that our method accurately detects malicious clients under various state-of-the-art model poisoning attacks. Additionally, the model recovery method provides a notable improvement to the system, ensuring the robustness and performance of the FL system.",
      "year": 2024,
      "venue": "IEEE International Symposium on Network Computing and Applications",
      "authors": [
        "X. Ma",
        "Hong Shen",
        "Chan-Tong Lam"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/633c6a67aec2197771941f04e362910be27e9c86",
      "pdf_url": "",
      "publication_date": "2024-10-24",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "5e740575a97fb3a3354b5038fed6ebf84c4e421c",
      "title": "On the Weaknesses of Backdoor-based Model Watermarking: An Information-theoretic Perspective",
      "abstract": "Safeguarding the intellectual property of machine learning models has emerged as a pressing concern in AI security. Model watermarking is a powerful technique for protecting ownership of machine learning models, yet its reliability has been recently challenged by recent watermark removal attacks. In this work, we investigate why existing watermark embedding techniques particularly those based on backdooring are vulnerable. Through an information-theoretic analysis, we show that the resilience of watermarking against erasure attacks hinges on the choice of trigger-set samples, where current uses of out-distribution trigger-set are inherently vulnerable to white-box adversaries. Based on this discovery, we propose a novel model watermarking scheme, In-distribution Watermark Embedding (IWE), to overcome the limitations of existing method. To further minimise the gap to clean models, we analyze the role of logits as watermark information carriers and propose a new approach to better conceal watermark information within the logits. Experiments on real-world datasets including CIFAR-100 and Caltech-101 demonstrate that our method robustly defends against various adversaries with negligible accuracy loss (<0.1%).",
      "year": 2024,
      "venue": "arXiv.org",
      "authors": [
        "Aoting Hu",
        "Yanzhi Chen",
        "Renjie Xie",
        "Adrian Weller"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/5e740575a97fb3a3354b5038fed6ebf84c4e421c",
      "pdf_url": "",
      "publication_date": "2024-09-10",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "69dd1b9e8391430a667214a9ca6c0bc94560deb2",
      "title": "Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review",
      "abstract": "This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Yansong Gao",
        "Bao Gia Doan",
        "Zhi Zhang",
        "Siqi Ma",
        "Jiliang Zhang",
        "Anmin Fu",
        "S. Nepal",
        "Hyoungshick Kim"
      ],
      "citation_count": 267,
      "url": "https://www.semanticscholar.org/paper/69dd1b9e8391430a667214a9ca6c0bc94560deb2",
      "pdf_url": "",
      "publication_date": "2020-07-21",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "8adcad33fcede5f745f9fdbcadc56b02804f4049",
      "title": "Imperceptible and Robust Backdoor Attack in 3D Point Cloud",
      "abstract": "With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, the distortion caused by a fixed WLT is both controllable and smooth, resulting in the generated poisoned samples that are imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves $80\\%+$ attack success rate (ASR) in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks. Our code is available at https://github.com/KuofengGao/IRBA.",
      "year": 2022,
      "venue": "IEEE Transactions on Information Forensics and Security",
      "authors": [
        "Kuofeng Gao",
        "Jiawang Bai",
        "Baoyuan Wu",
        "Mengxi Ya",
        "Shutao Xia"
      ],
      "citation_count": 45,
      "url": "https://www.semanticscholar.org/paper/8adcad33fcede5f745f9fdbcadc56b02804f4049",
      "pdf_url": "http://arxiv.org/pdf/2208.08052",
      "publication_date": "2022-08-17",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "bc8853a2eb7cfaf4d19e1dec6a0e94dc0b68d26f",
      "title": "Backdoor Defense via Suppressing Model Shortcuts",
      "abstract": "Recent studies have demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks during the training process. Specifically, the adversaries intend to embed hidden backdoors in DNNs so that malicious model predictions can be activated through pre-defined trigger patterns. In this paper, we explore the backdoor mechanism from the angle of the model structure. We select the skip connection for discussions, inspired by the understanding that it helps the learning of model \u2018shortcuts\u2019 where backdoor triggers are usually easier to be learned. Specifically, we demonstrate that the attack success rate (ASR) decreases significantly when reducing the outputs of some key skip connections. Based on this observation, we design a simple yet effective backdoor removal method by suppressing the skip connections in critical layers selected by our method. We also implement fine-tuning on these layers to recover high benign accuracy and to further reduce ASR. Extensive experiments on benchmark datasets verify the effectiveness of our method.",
      "year": 2022,
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "authors": [
        "Shengyuan Yang",
        "Yiming Li",
        "Yong Jiang",
        "Shutao Xia"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/bc8853a2eb7cfaf4d19e1dec6a0e94dc0b68d26f",
      "pdf_url": "https://arxiv.org/pdf/2211.05631",
      "publication_date": "2022-11-02",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "487b4acb7cd43081875bb50147980dd88882b3f4",
      "title": "Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models",
      "abstract": "Diffusion models (DMs) are advanced generative models, yet recent research reveals their vulnerability to backdoor attacks, which establish hidden associations between input patterns and targeted model behavior, potentially causing malicious outputs during inference. These attacks pose significant risks, including model owner reputation damage and harmful content generation. However, existing defense methods often fail against backdoor attacks on diffusion models. To address this gap, we propose Diff-Cleanse, a two-stage defense framework. The first stage introduces a novel trigger inversion method for backdoor detection, and the second stage applies a structural pruning-based method for backdoor removal. Experiments on 373 models poisoned by three state-of-the-art attacks show that Diff-Cleanse achieves > 97% detection accuracy, completely remove backdoors and maintains the models\u2019 benign performance. Code is available at https://github.com/shymuel/diff-cleanse.",
      "year": 2025,
      "venue": "IEEE International Conference on Multimedia and Expo",
      "authors": [
        "Hao Jiang",
        "Jin Xiao",
        "Xiaoguang Hu",
        "Tianyou Chen",
        "Jiajia Zhao"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/487b4acb7cd43081875bb50147980dd88882b3f4",
      "pdf_url": "",
      "publication_date": "2025-06-30",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "89e9872cf16e058499877efbd578646ebb04db87",
      "title": "Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks",
      "abstract": "We report a new neural backdoor attack, named Hibernated Backdoor, which is stealthy, aggressive and devastating. The backdoor is planted in a hibernated mode to avoid being detected. Once deployed and fine-tuned on end-devices, the hibernated backdoor turns into the active state that can be exploited by the attacker. To the best of our knowledge, this is the first hibernated neural backdoor attack. It is achieved by maximizing the mutual information (MI) between the gradients of regular and malicious data on the model. We introduce a practical algorithm to achieve MI maximization to effectively plant the hibernated backdoor. To evade adaptive defenses, we further develop a targeted hibernated backdoor, which can only be activated by specific data samples and thus achieves a higher degree of stealthiness. We show the hibernated backdoor is robust and cannot be removed by existing backdoor removal schemes. It has been fully tested on four datasets with two neural network architectures, compared to five existing backdoor attacks, and evaluated using seven backdoor detection schemes. The experiments demonstrate the effectiveness of the hibernated backdoor attack under various settings.",
      "year": 2022,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "R. Ning",
        "Jiang Li",
        "Chunsheng Xin",
        "Hongyi Wu",
        "Chong Wang"
      ],
      "citation_count": 11,
      "url": "https://www.semanticscholar.org/paper/89e9872cf16e058499877efbd578646ebb04db87",
      "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/21272/21021",
      "publication_date": "2022-06-28",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "50a9e9c07adc45aa783c45a50ab778cdc84bfad5",
      "title": "Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective",
      "abstract": "Graph Neural Networks (GNNs) have achieved notable success in tasks such as social and transportation networks. However, recent studies have highlighted the vulnerability of GNNs to backdoor attacks, raising significant concerns about their reliability in real-world applications. Despite initial efforts to defend against specific graph backdoor attacks, existing defense methods face two main challenges: either the inability to establish a clear distinction between triggers and clean nodes, resulting in the removal of many clean nodes, or the failure to eliminate the impact of triggers, making it challenging to restore the target nodes to their pre-attack state. Through empirical analysis of various existing graph backdoor attacks, we observe that the triggers generated by these methods exhibit over-similarity in both features and structure. Based on this observation, we propose a novel graph backdoor defense method SimGuard. We first utilizes a similarity-based metric to detect triggers and then employs contrastive learning to train a backdoor detector that generates embeddings capable of separating triggers from clean nodes, thereby improving detection efficiency. Extensive experiments conducted on real-world datasets demonstrate that our proposed method effectively defends against various graph backdoor attacks while preserving performance on clean nodes. The code will be released upon acceptance.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Chang Liu",
        "Hai Huang",
        "Yujie Xing",
        "Xingquan Zuo"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/50a9e9c07adc45aa783c45a50ab778cdc84bfad5",
      "pdf_url": "",
      "publication_date": "2025-02-03",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "629b5ed422752e7d2a16e7a2d6346db1f6ab8b78",
      "title": "Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation",
      "abstract": "Text-to-image diffusion models have revolutionized generative AI, but their vulnerability to backdoor attacks poses significant security risks. Adversaries can inject imperceptible textual triggers into training data, causing models to generate manipulated outputs. Although text-based backdoor defenses in classification models are well-explored, generative models lack effective mitigation techniques against. We address this by selectively erasing the model's learned associations between adversarial text triggers and poisoned outputs, while preserving overall generation quality. Our approach, Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG), uses knowledge distillation to guide the model in correcting responses to poisoned prompts while maintaining image quality by exploiting the fact that the backdoored model still produces clean outputs in the absence of triggers. Using the cross-attention mechanism, SKD-CAG neutralizes backdoor influences at the attention level, ensuring the targeted removal of adversarial effects. Extensive experiments show that our method outperforms existing approaches, achieving removal accuracy 100\\% for pixel backdoors and 93\\% for style-based attacks, without sacrificing robustness or image fidelity. Our findings highlight targeted unlearning as a promising defense to secure generative models. Code and model weights can be found at https://github.com/Mystic-Slice/Sealing-The-Backdoor .",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ashwath Vaithinathan Aravindan",
        "Abha Jha",
        "Matthew Salaway",
        "A. S. Bhide",
        "D. Yaldiz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/629b5ed422752e7d2a16e7a2d6346db1f6ab8b78",
      "pdf_url": "",
      "publication_date": "2025-08-20",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "12060dd0b704572c6a5dc509ae8598e4380f6064",
      "title": "BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS",
      "abstract": "In this paper, we endeavor to address the challenges of backdoor attacks countermeasures in black-box scenarios, thereby fortifying the security of inference under MLaaS. We first categorize backdoor triggers from a new perspective, i.e., their impact on the patched area, and divide them into: high-visibility triggers (HVT), semi-visibility triggers (SVT), and low-visibility triggers (LVT). Based on this classification, we propose a progressive defense framework, BDFirewall, that removes these triggers from the most conspicuous to the most subtle, without requiring model access. First, for HVTs, which create the most significant local semantic distortions, we identify and eliminate them by detecting these salient differences. We then restore the patched area to mitigate the adverse impact of such removal process. The localized purification designed for HVTs is, however, ineffective against SVTs, which globally perturb benign features. We therefore model an SVT-poisoned input as a mixture of a trigger and benign features, where we unconventionally treat the benign features as\"noise\". This formulation allows us to reconstruct SVTs by applying a denoising process that removes these benign\"noise\"features. The SVT-free input is then obtained by subtracting the reconstructed trigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we introduce lightweight noise to disrupt the trigger pattern and then apply DDPM to restore any collateral impact on clean features. Comprehensive experiments demonstrate that our method outperforms state-of-the-art defenses. Compared with baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average of 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up to a 111x speedup in inference time. Code will be made publicly available upon acceptance.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Ye Li",
        "Chengcheng Zhu",
        "Yanchao Zhao",
        "Jiale Zhang"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/12060dd0b704572c6a5dc509ae8598e4380f6064",
      "pdf_url": "",
      "publication_date": "2025-08-05",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "d6a9bd8c6f5e511d4a2466b82c972dedb793864d",
      "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning",
      "abstract": "Text-to-image diffusion models are increasingly vulnerable to backdoor attacks, where malicious modifications to the training data cause the model to generate unintended outputs when specific triggers are present. While classification models have seen extensive development of defense mechanisms, generative models remain largely unprotected due to their high-dimensional output space, which complicates the detection and mitigation of subtle perturbations. Defense strategies for diffusion models, in particular, remain under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a novel technique for mitigating backdoor attacks in diffusion models. SAU leverages latent space manipulation and spatial attention mechanisms to isolate and remove the latent representation of backdoor triggers, ensuring precise and efficient removal of malicious effects. We evaluate SAU across various types of backdoor attacks, including pixel-based and style-based triggers, and demonstrate its effectiveness in achieving 100% trigger removal accuracy. Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing methods while preserving the model's ability to generate high-quality, semantically aligned images. Our results show that SAU is a robust, scalable, and practical solution for securing text-to-image diffusion models against backdoor attacks.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Abha Jha",
        "Ashwath Vaithinathan Aravindan",
        "Matthew Salaway",
        "A. S. Bhide",
        "D. Yaldiz"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/d6a9bd8c6f5e511d4a2466b82c972dedb793864d",
      "pdf_url": "",
      "publication_date": "2025-04-21",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "1dae09cf54ecc9ff7d9dbfbaf852dcdd549e71a5",
      "title": "Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning",
      "abstract": "Backdoor attacks pose a persistent security risk to deep neural networks (DNNs) due to their stealth and durability. While recent research has explored leveraging model unlearning mechanisms to enhance backdoor concealment, existing attack strategies still leave persistent traces that may be detected through static analysis. In this work, we introduce the first paradigm of revocable backdoor attacks, where the backdoor can be proactively and thoroughly removed after the attack objective is achieved. We formulate the trigger optimization in revocable backdoor attacks as a bilevel optimization problem: by simulating both backdoor injection and unlearning processes, the trigger generator is optimized to achieve a high attack success rate (ASR) while ensuring that the backdoor can be easily erased through unlearning. To mitigate the optimization conflict between injection and removal objectives, we employ a deterministic partition of poisoning and unlearning samples to reduce sampling-induced variance, and further apply the Projected Conflicting Gradient (PCGrad) technique to resolve the remaining gradient conflicts. Experiments on CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to state-of-the-art backdoor attacks, while enabling effective removal of backdoor behavior after unlearning. This work opens a new direction for backdoor attack research and presents new challenges for the security of machine learning systems.",
      "year": 2025,
      "venue": "arXiv.org",
      "authors": [
        "Baogang Song",
        "Dongdong Zhao",
        "Jianwen Xiang",
        "Qiben Xu",
        "Zizhuo Yu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/1dae09cf54ecc9ff7d9dbfbaf852dcdd549e71a5",
      "pdf_url": "",
      "publication_date": "2025-10-15",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "ea92eff13521b0b21370551902ef19d7d0d7d0c7",
      "title": "SifterNet: Model-Agnostic Defense against Backdoor Attack in Vision Large Model",
      "abstract": "Vision models have been applied into urban computing, vision-language navigation, intelligent transportation, etc. Nevertheless, various convolution neural networks (CNN)-based vision models, or even the recently-developed vision Transformer-based large models all encounter security and privacy issues. In this paper, aiming at resisting backdoor attacks in these vision models, we proposes a generalized and model-agnostic trigger-purification approach resorting to the classic Ising model in physics. To date, existing trigger detection/removal studies usually require to know the detailed knowledge of target model in advance, access to a large number of clean samples or even model-retraining authorization, which brings the huge inconvenience for practical applications, especially in case of inaccessibility to the target model. Thereby, an ideal countermeasure ought to eliminate the implanted trigger without regarding whatever the target models are. To this end, a lightweight and black-box defense approach SifterNet is proposed through leveraging the memorization-association functionality of Hopfield network, by which the triggers of input samples can be effectively purified in a proper manner. The main novelty of our proposed approach lies in the introduction of ideology of Ising model. A set of experiments also validate the effectiveness of our approach in terms of proper trigger purification and high accuracy achievement, and compared to the state-of-the-art baselines, our proposed SiferNet has a significant superior performance under five popular backdoor attacks.",
      "year": 2025,
      "venue": "Proceedings of the 12th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
      "authors": [
        "Shaoye Luo",
        "Xinxin Fan",
        "Quanliang Jing",
        "Men Niu",
        "Chi Lin",
        "Yunfeng Lu"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/ea92eff13521b0b21370551902ef19d7d0d7d0c7",
      "pdf_url": "",
      "publication_date": "2025-11-11",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    },
    {
      "paper_id": "82ae7fb89ff1b9871ded8ecc65b0d0c8ece7faf3",
      "title": "A Defense Method against Backdoor Attacks in Neural Networks Using an Image Repair Technique",
      "abstract": "With the rapid development of deep learning research and applications, the problem of artificial intelligence security has become increasingly prominent, such as adversarial examples, universal adversarial patch, and data poisoning, especially for the backdoor attack, which is a new type of covert attack, leading to the vulnerability and non-robustness of deep learning models. In a backdoor attack, the attacker will conduct a malicious attack by inserting some poisoned samples into training dataset. Poisoned samples add triggers and modify the labels to the target labels to participate in the training. Infected model has the same accuracy as the clean model in the normal test set, but when confronted with poisoned samples, the triggers will be activated to make the infected model predict the target label. To solve this problem, model parameters adjustment and poisoned data removal methods are widely used. However, they lack real-time performance and accuracy is insufficient. In this paper, we propose a new backdoor attack defense method, in which trigger reverse engineering is used to obtain the right triggers and image repair techniques to make sure that the input model data can be real-time processed without any negative impacts on clean samples.",
      "year": 2022,
      "venue": "2022 12th International Conference on Information Technology in Medicine and Education (ITME)v",
      "authors": [
        "Jiangtao Chen",
        "Huijuan Lu",
        "W. Huo",
        "Shicong Zhang",
        "Yuefeng Chen",
        "Yudong Yao"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/82ae7fb89ff1b9871ded8ecc65b0d0c8ece7faf3",
      "pdf_url": "",
      "publication_date": "2022-11-01",
      "keywords_matched": [
        "backdoor removal"
      ],
      "first_seen": "2026-01-06"
    }
  ]
}