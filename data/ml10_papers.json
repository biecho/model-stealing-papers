{
  "owasp_id": "ML10",
  "owasp_name": "Model Manipulation & Corruption",
  "total": 5,
  "updated": "2026-01-09",
  "papers": [
    {
      "paper_id": "https://openalex.org/W3213849916",
      "title": "AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks",
      "abstract": "Deep neural network (DNN) has been widely utilized in many areas due to its increasingly high accuracy. However, DNN models could also produce wrong outputs due to internal errors, which may lead to severe security issues. Unlike fixing bugs in traditional computer software, tracing the errors in DNN models and fixing them are much more difficult due to the uninterpretability of DNN. In this paper, we present a novel and systematic approach to trace and fix the errors in deep learning models. In particular, we locate the error-inducing neurons that play a leading role in the erroneous output. With the knowledge of error-inducing neurons, we propose two methods to fix the errors: the neuron-flip and the neuron-fine-tuning. We evaluate our approach using five different training datasets and seven different model architectures. The experimental results demonstrate its efficacy in different application scenarios, including backdoor removal and general defects fixing.",
      "year": 2021,
      "venue": null,
      "authors": [
        "Yue Zhao",
        "Hong Zhu",
        "Kai Chen",
        "Shengzhi Zhang"
      ],
      "url": "https://openalex.org/W3213849916",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4322717137",
      "title": "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks",
      "abstract": "Bit-flip attacks (BFAs) have attracted substantial attention recently, in which an adversary could tamper with a small number of model parameter bits to break the integrity of DNNs. To mitigate such threats, a batch of defense methods are proposed, focusing on the untargeted scenarios. Unfortunately, they either require extra trustworthy applications or make models more vulnerable to targeted BFAs. Countermeasures against targeted BFAs, stealthier and more purposeful by nature, are far from well established. In this work, we propose Aegis, a novel defense method to mitigate targeted BFAs. The core observation is that existing targeted attacks focus on flipping critical bits in certain important layers. Thus, we design a dynamic-exit mechanism to attach extra internal classifiers (ICs) to hidden layers. This mechanism enables input samples to early-exit from different layers, which effectively upsets the adversary's attack plans. Moreover, the dynamic-exit mechanism randomly selects ICs for predictions during each inference to significantly increase the attack cost for the adaptive attacks where all defense mechanisms are transparent to the adversary. We further propose a robustness training strategy to adapt ICs to the attack scenarios by simulating BFAs during the IC training phase, to increase model robustness. Extensive evaluations over four well-known datasets and two popular DNN structures reveal that Aegis could effectively mitigate different state-of-the-art targeted attacks, reducing attack success rate by 5-10$\\times$, significantly outperforming existing defense methods.",
      "year": 2023,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Jialai Wang",
        "Ziyuan Zhang",
        "Meiqi Wang",
        "Qiu Han",
        "Tianwei Zhang",
        "Qi Li",
        "Zongpeng Li",
        "Tao Wei",
        "C. Zhang"
      ],
      "url": "https://openalex.org/W4322717137",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "2408.07728",
      "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
      "abstract": "We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.",
      "year": 2024,
      "venue": "ACM CCS",
      "authors": [
        "Peiran Wang",
        "Qiyu Li",
        "Longxuan Yu",
        "Ziyao Wang",
        "Ang Li",
        "Haojian Jin"
      ],
      "url": "https://arxiv.org/abs/2408.07728",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W4417351576",
      "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks",
      "abstract": "We present a novel attack specifically designed against Tree-Ring, a watermarking technique for diffusion models known for its high imperceptibility and robustness against removal attacks. Unlike previous removal attacks, which rely on strong assumptions about attacker capabilities, our attack only requires access to the variational autoencoder that was used to train the target diffusion model, a component that is often publicly available. By leveraging this variational autoencoder, the attacker can approximate the model's intermediate latent space, enabling more effective surrogate-based attacks. Our evaluation shows that this approach leads to a dramatic reduction in the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to 0.153 and from 0.994 to 0.385, respectively, while maintaining high image quality. Notably, our attacks outperform existing methods that assume full access to the diffusion model. These findings highlight the risk of reusing public autoencoders to train diffusion models -- a threat not considered by current industry practices. Furthermore, the results suggest that the Tree-Ring detector's precision, a metric that has been overlooked by previous evaluations, falls short of the requirements for real-world deployment.",
      "year": 2025,
      "venue": "arXiv (Cornell University)",
      "authors": [
        "Lin, Junhua",
        "Juarez, Marc"
      ],
      "url": "https://openalex.org/W4417351576",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    },
    {
      "paper_id": "https://openalex.org/W3034665124",
      "title": "Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack",
      "abstract": "Recently, a new paradigm of the adversarial attack on the quantized neural network weights has attracted great attention, namely, the Bit-Flip based adversarial weight attack, aka. Bit-Flip Attack (BFA). BFA has shown extraordinary attacking ability, where the adversary can malfunction a quantized Deep Neural Network (DNN) as a random guess, through malicious bit-flips on a small set of vulnerable weight bits (e.g., 13 out of 93 millions bits of 8-bit quantized ResNet-18). However, there are no effective defensive methods to enhance the fault-tolerance capability of DNN against such BFA. In this work, we conduct comprehensive investigations on BFA and propose to leverage binarization-aware training and its relaxation - piece-wise clustering as simple and effective countermeasures to BFA. The experiments show that, for BFA to achieve the identical prediction accuracy degradation (e.g., below 11% on CIFAR-10), it requires 19.3\u00d7 and 480.1\u00d7 more effective malicious bit-flips on ResNet-20 and VGG-11 respectively, compared to defend-free counterparts.",
      "year": 2020,
      "venue": null,
      "authors": [
        "Zhezhi He",
        "Adnan Siraj Rakin",
        "Jingtao Li",
        "Chaitali Chakrabarti",
        "Deliang Fan"
      ],
      "url": "https://openalex.org/W3034665124",
      "pdf_url": null,
      "cited_by_count": null,
      "classification_confidence": "HIGH"
    }
  ]
}