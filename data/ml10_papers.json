{
  "updated": "2026-01-06",
  "total": 12,
  "owasp_id": "ML10",
  "owasp_name": "Model Poisoning",
  "description": "Attacks that embed backdoors or trojans in machine learning models. This\n        includes neural trojans, backdoor attacks, trigger-based attacks, and\n        techniques to insert hidden malicious behavior that activates on specific\n        inputs while maintaining normal performance on clean data.",
  "note": "Classified by embeddings (threshold=0.3)",
  "papers": [
    {
      "paper_id": "0d3384cb78be25ed28a3544f175cab59236093dd",
      "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
      "abstract": "Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.",
      "year": 2025,
      "venue": "AAAI Conference on Artificial Intelligence",
      "authors": [
        "Yixu Wang",
        "Tianle Gu",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma"
      ],
      "citation_count": 3,
      "url": "https://www.semanticscholar.org/paper/0d3384cb78be25ed28a3544f175cab59236093dd",
      "pdf_url": "",
      "publication_date": "2025-01-02",
      "keywords_matched": [
        "model extraction",
        "model extraction attack"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
      "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical machine learning while introducing new attack surfaces rooted in the physical and algorithmic layers of quantum computing. Despite a growing body of research on individual attack vectors - ranging from adversarial poisoning and evasion to circuit-level backdoors, side-channel leakage, and model extraction - these threats are often analyzed in isolation, with unrealistic assumptions about attacker capabilities and system environments. This fragmentation hampers the development of effective, holistic defense strategies. In this work, we argue that QML security requires more structured modeling of the attack surface, capturing not only individual techniques but also their relationships, prerequisites, and potential impact across the QML pipeline. We propose adapting kill chain models, widely used in classical IT and cybersecurity, to the quantum machine learning context. Such models allow for structured reasoning about attacker objectives, capabilities, and possible multi-stage attack paths - spanning reconnaissance, initial access, manipulation, persistence, and exfiltration. Based on extensive literature analysis, we present a detailed taxonomy of QML attack vectors mapped to corresponding stages in a quantum-aware kill chain framework that is inspired by the MITRE ATLAS for classical machine learning. We highlight interdependencies between physical-level threats (like side-channel leakage and crosstalk faults), data and algorithm manipulation (such as poisoning or circuit backdoors), and privacy attacks (including model extraction and training data inference). This work provides a foundation for more realistic threat modeling and proactive security-in-depth design in the emerging field of quantum machine learning.",
      "year": 2025,
      "venue": "International Conference on Quantum Computing and Engineering",
      "authors": [
        "Pascal Debus",
        "Maximilian Wendlinger",
        "Kilian Tscharke",
        "Daniel Herr",
        "Cedric Br\u00fcgmann",
        "Daniel de Mello",
        "J. Ulmanis",
        "Alexander Erhard",
        "Arthur Schmidt",
        "Fabian Petsch"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/71ff23a33bc4db47a76808717acc78d9b04a7e1e",
      "pdf_url": "",
      "publication_date": "2025-07-11",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "7d8e5056ee708bf57890027f3b3b025d19d9af73",
      "title": "Combining Causal Models for More Accurate Abstractions of Neural Networks",
      "abstract": "Mechanistic interpretability aims to reverse engineer neural networks by uncovering which high-level algorithms they implement. Causal abstraction provides a precise notion of when a network implements an algorithm, i.e., a causal model of the network contains low-level features that realize the high-level variables in a causal model of the algorithm. A typical problem in practical settings is that the algorithm is not an entirely faithful abstraction of the network, meaning it only partially captures the true reasoning process of a model. We propose a solution where we combine different simple high-level models to produce a more faithful representation of the network. Through learning this combination, we can model neural networks as being in different computational states depending on the input provided, which we show is more accurate to GPT 2-small fine-tuned on two toy tasks. We observe a trade-off between the strength of an interpretability hypothesis, which we define in terms of the number of inputs explained by the high-level models, and its faithfulness, which we define as the interchange intervention accuracy. Our method allows us to modulate between the two, providing the most accurate combination of models that describe the behavior of a neural network given a faithfulness level.",
      "year": 2025,
      "venue": "CLEaR",
      "authors": [
        "Theodora-Mara Pislar",
        "Sara Magliacane",
        "Atticus Geiger"
      ],
      "citation_count": 1,
      "url": "https://www.semanticscholar.org/paper/7d8e5056ee708bf57890027f3b3b025d19d9af73",
      "pdf_url": "",
      "publication_date": "2025-03-14",
      "keywords_matched": [
        "reverse engineer neural network"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
      "abstract": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
      "year": 2025,
      "venue": "",
      "authors": [
        "Sohely Jahan",
        "Ruimin Sun"
      ],
      "citation_count": 0,
      "url": "https://www.semanticscholar.org/paper/0ed57747e5dbfe69ffa822ac1bb9c38067e97c76",
      "pdf_url": "",
      "publication_date": "2025-12-10",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-12-12"
    },
    {
      "paper_id": "fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "title": "B3: Backdoor Attacks against Black-box Machine Learning Models",
      "abstract": "Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the trigger. The reason for backdoor attacks is that resource-limited users usually download sophisticated models from model zoos or query the models from MLaaS rather than training a model from scratch, thus a malicious third party has a chance to provide a backdoored model. In general, the more precious the model provided (i.e., models trained on rare datasets), the more popular it is with users. In this article, from a malicious model provider perspective, we propose a black-box backdoor attack, named B3, where neither the rare victim model (including the model architecture, parameters, and hyperparameters) nor the training data is available to the adversary. To facilitate backdoor attacks in the black-box scenario, we design a cost-effective model extraction method that leverages a carefully constructed query dataset to steal the functionality of the victim model with a limited budget. As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted misclassification label through the neuron with the highest impact on the targeted label. Extensive experiments have been conducted on various simulated deep learning models and the commercial API of Alibaba Cloud Compute Service. We demonstrate that B3 has a high attack success rate and maintains high prediction accuracy for benign inputs. It is also shown that B3 is robust against state-of-the-art defense strategies against backdoor attacks, such as model pruning and NC.",
      "year": 2023,
      "venue": "ACM Transactions on Privacy and Security",
      "authors": [
        "Xueluan Gong",
        "Yanjiao Chen",
        "Wenbin Yang",
        "Huayang Huang",
        "Qian Wang"
      ],
      "citation_count": 13,
      "url": "https://www.semanticscholar.org/paper/fb1f43004e7878c2da6ab86fb7427058a8ddedf7",
      "pdf_url": "",
      "publication_date": "2023-06-22",
      "keywords_matched": [
        "model extraction",
        "(via citation)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "163514e0787a540b8c7983424c5ca8ae8e3f6f9d",
      "title": "MEDIC: Remove Model Backdoors via Importance Driven Cloning",
      "abstract": "We develop a novel method to remove injected backdoors in deep learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result. We theoretically show our method can better recover benign functions of the backdoor model. Meanwhile, we prove our method can be more effective in removing back-doors compared with fine-tuning. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outper-forming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.1",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "authors": [
        "Qiuling Xu",
        "Guanhong Tao",
        "J. Honorio",
        "Yingqi Liu",
        "Guangyu Shen",
        "Siyuan Cheng",
        "Xiangyu Zhang"
      ],
      "citation_count": 9,
      "url": "https://www.semanticscholar.org/paper/163514e0787a540b8c7983424c5ca8ae8e3f6f9d",
      "pdf_url": "",
      "publication_date": "2023-06-01",
      "keywords_matched": [
        "clone model"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "4bda132d5df256a41326ca03d3b569bfe926734f",
      "title": "Metric Learning-Based Neural Network Model for Electromagnetic Compatibility Fault Diagnosis: An Application Study",
      "abstract": "With the growing prevalence of electronic equipment and the increasing severity of the electromagnetic environment, the likelihood of electromagnetic compatibility failures is on the rise. As a result, the difficulty of diagnosing EMC faults is also increasing. However, by employing neural networks in deep learning for EMC fault diagnosis, we can simplify and streamline the process of feature extraction and similarity analysis. Compared to traditional artificial feature extraction methods, neural networks can learn to measure the similarity between features more efficiently, resulting in more accurate diagnoses. To train the model, we obtain response data from each port of the electronic equipment system in a high radio frequency environment and pair it with the corresponding equipment fault status. However, due to the limited availability of labeled data, conventional neural networks are susceptible to overfitting. Therefore, we use a neural network model that is well-suited for few-shot learning, which is based on a metric learning approach. This approach enables the model to learn from a small amount of labeled data, making it more effective in diagnosing EMC faults.",
      "year": 2023,
      "venue": "2023 5th International Conference on Electronic Engineering and Informatics (EEI)",
      "authors": [
        "Xiangguo Shen",
        "Zhongyuan Zhou"
      ],
      "citation_count": 2,
      "url": "https://www.semanticscholar.org/paper/4bda132d5df256a41326ca03d3b569bfe926734f",
      "pdf_url": "",
      "publication_date": "2023-06-30",
      "keywords_matched": [
        "electromagnetic analysis (title)"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "5639133170aa4f4a598994518d34a9b375494876",
      "title": "A Systematic View of Model Leakage Risks in Deep Neural Network Systems",
      "abstract": "As deep neural networks (DNNs) continue to find applications in ever more domains, the exact nature of the neural network architecture becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. While prior work has explored aspects of the risk associated with model leakage, exactly which parts of the model are most sensitive and how one infers the full architecture of the DNN when nothing is known about the structure a priori are problems that have been left unexplored. In this paper we address this gap, first by presenting a schema for reasoning about model leakage holistically, and then by proposing and quantitatively evaluating DeepSniffer, a novel learning-based model extraction framework that uses no prior knowledge of the victim model. DeepSniffer is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. Taking GPU platforms as a showcase, DeepSniffer performs model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models and that the extracted models significantly improve attempts at crafting adversarial inputs. The DeepSniffer project has been released in https://github.com/xinghu7788/DeepSniffer.",
      "year": 2022,
      "venue": "IEEE transactions on computers",
      "authors": [
        "Xing Hu",
        "Ling Liang",
        "Xiaobing Chen",
        "Lei Deng",
        "Yu Ji",
        "Yufei Ding",
        "Zidong Du",
        "Qi Guo",
        "T. Sherwood",
        "Yuan Xie"
      ],
      "citation_count": 10,
      "url": "https://www.semanticscholar.org/paper/5639133170aa4f4a598994518d34a9b375494876",
      "pdf_url": "https://doi.org/10.1109/tc.2022.3148235",
      "publication_date": "2022-12-01",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "title": "On the Security Risks of AutoML",
      "abstract": "Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains. This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.",
      "year": 2021,
      "venue": "USENIX Security Symposium",
      "authors": [
        "Ren Pang",
        "Zhaohan Xi",
        "S. Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "citation_count": 15,
      "url": "https://www.semanticscholar.org/paper/0189e3d93aa73ce3223899bfb47a1a71e7cde394",
      "pdf_url": "",
      "publication_date": "2021-10-12",
      "keywords_matched": [
        "functionality stealing"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "title": "MEME: Generating RNN Model Explanations via Model Extraction",
      "abstract": "Recurrent Neural Networks (RNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering RNN-based approaches is improving their explainability and interpretability. In this work we present MEME: a model extraction approach capable of approximating RNNs with interpretable models represented by human-understandable concepts and their interactions. We demonstrate how MEME can be applied to two multivariate, continuous data case studies: Room Occupation Prediction, and In-Hospital Mortality Prediction. Using these case-studies, we show how our extracted models can be used to interpret RNNs both locally and globally, by approximating RNN decision-making via interpretable concept interactions.",
      "year": 2020,
      "venue": "arXiv.org",
      "authors": [
        "Dmitry Kazhdan",
        "B. Dimanov",
        "M. Jamnik",
        "Pietro Lio'"
      ],
      "citation_count": 14,
      "url": "https://www.semanticscholar.org/paper/31bfc3d4c51534a0c27ea9928423c53deb1ff9f0",
      "pdf_url": "",
      "publication_date": "2020-12-13",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    },
    {
      "paper_id": "a7dd719ce38048df879463f2b2722521754e53a7",
      "title": "Malware Detection in Embedded Systems Using Neural Network Model for Electromagnetic Side-Channel Signals",
      "abstract": null,
      "year": 2019,
      "venue": "Journal of Hardware and Systems Security",
      "authors": [
        "Haider Adnan Khan",
        "Nader Sehatbakhsh",
        "Luong N. Nguyen",
        "Milos Prvulovi\u0107",
        "A. Zaji\u0107"
      ],
      "citation_count": 51,
      "url": "https://www.semanticscholar.org/paper/a7dd719ce38048df879463f2b2722521754e53a7",
      "pdf_url": "",
      "publication_date": "2019-08-22",
      "keywords_matched": [
        "side-channel attack (title)"
      ],
      "first_seen": "2025-12-05"
    },
    {
      "paper_id": "a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "title": "Interpretability via Model Extraction",
      "abstract": "The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.",
      "year": 2017,
      "venue": "arXiv.org",
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "citation_count": 133,
      "url": "https://www.semanticscholar.org/paper/a9007cf4c4ec7e4fc956bead7008a3605451de49",
      "pdf_url": "",
      "publication_date": "2017-06-29",
      "keywords_matched": [
        "model extraction"
      ],
      "first_seen": "2025-11-28"
    }
  ]
}